{"meta":{"title":"乾","subtitle":"","description":"乾","author":"WangQian","url":"https://wangqian0306.github.io","root":"/"},"pages":[{"title":"个人介绍","date":"2025-02-10T08:05:31.134Z","updated":"2025-02-10T08:05:31.134Z","comments":false,"path":"about/index.html","permalink":"https://wangqian0306.github.io/about/index.html","excerpt":"","text":"姓名：王乾 现居：上海 QQ: 512228237 微信: DarrenMiles"},{"title":"categories","date":"2025-02-10T07:43:09.503Z","updated":"2025-01-08T02:56:21.494Z","comments":false,"path":"categories/index.html","permalink":"https://wangqian0306.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-02-10T07:43:09.503Z","updated":"2025-01-08T02:56:21.494Z","comments":false,"path":"tags/index.html","permalink":"https://wangqian0306.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"摄像头视频流","slug":"tools/camera","date":"2025-01-22T13:32:58.000Z","updated":"2025-02-10T07:45:30.739Z","comments":true,"path":"2025/camera/","permalink":"https://wangqian0306.github.io/2025/camera/","excerpt":"","text":"摄像头视频流 简介 在工作中遇到了云台控制和视频接入的相关需求，此处针对相关内容进行了初步的整理。 读取视频流 可以通过如下方式读取摄像头的 RTSP 协议视频信号，需要注意的是登录摄像头的配置网站明确端口和认证方式。 OpenCV 使用如下命令安装依赖： 1pip install opencv-python 使用如下代码即可读取视频流： 123456789101112131415161718192021222324252627282930313233import cv2# RTSP URL ，请替换为你的实际 URL ，默认可以使用 1 作为 channelrtsp_url = &quot;rtsp://&lt;username&gt;:&lt;password&gt;@&lt;ip_address&gt;:&lt;port&gt;/Streaming/Channels/&lt;channel&gt;&quot;# 创建一个VideoCapture对象cap = cv2.VideoCapture(rtsp_url)# 检查是否成功打开视频流if not cap.isOpened(): print(&quot;无法打开RTSP流&quot;) exit()# 循环读取视频帧while True: # 获取一帧 ret, frame = cap.read() # 如果读取失败，跳出循环 if not ret: print(&quot;无法接收帧 (流结束?). 退出.&quot;) break # 显示结果帧 cv2.imshow(&#x27;Frame&#x27;, frame) # 按&#x27;q&#x27;键退出循环 if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;): break# 完成后释放捕捉器和关闭所有窗口cap.release()cv2.destroyAllWindows() 控制云台 可以通过如下方式控制云台，需要注意的是登录摄像头的配置网站明确端口和认证方式。 基本的控制逻辑是采用 ONVIF 协议，在海康威视摄像头上需要摄像机启用集成协议，具体文档如下： 摄像机开启ONVIF协议 简单说就是： 开启开放型网络视频接口 认证方式为 digest/wsse 或 Digest&amp;ws-username token 创建独立用户 选择权限为管理员 onviz-zeep 使用如下命令安装相关依赖库： 12pip install zeeppip install onvif_zeep 然后使用如下代码即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import timefrom onvif import ONVIFCamera# 摄像头的IP地址、端口、用户名和密码ip_address = &#x27;xxx.xxx.xxx.xxx&#x27;port = 80username = &#x27;xxx&#x27;password = &#x27;xxx&#x27;# 创建ONVIFCamera实例mycam = ONVIFCamera(ip_address, port, username, password)# 获取PTZ服务对象ptz = mycam.create_ptz_service()# 获取 Media service 对象并得到配置文件media = mycam.create_media_service()# 获取配置信息media_profile = media.GetProfiles()[0]# 构建请求对象request = ptz.create_type(&#x27;GetConfigurationOptions&#x27;)request.ConfigurationToken = media_profile.PTZConfiguration.tokenptz_configuration_options = ptz.GetConfigurationOptions(request)request = ptz.create_type(&#x27;ContinuousMove&#x27;)request.ProfileToken = media_profile.token# 发送转动停止信号ptz.Stop(&#123;&#x27;ProfileToken&#x27;: media_profile.token&#125;)# 转动请求配置，x 和 y 都是 0-1.0 的数字request.Velocity = &#123; &#x27;PanTilt&#x27;: &#123; &#x27;x&#x27;: 0.5, &#x27;y&#x27;: 0 &#125;&#125;# 发送转动请求ptz.ContinuousMove(request)# 延迟等待云台转动time.sleep(1)# 发送转动停止信号ptz.Stop(&#123;&#x27;ProfileToken&#x27;: media_profile.token&#125;)","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"OpenCV","slug":"OpenCV","permalink":"https://wangqian0306.github.io/tags/OpenCV/"}]},{"title":"qrcode","slug":"tmp/qrcode","date":"2025-01-15T14:26:13.000Z","updated":"2025-01-15T08:28:13.175Z","comments":true,"path":"2025/qrcode/","permalink":"https://wangqian0306.github.io/2025/qrcode/","excerpt":"","text":"qrcode 简介 使用 python-qrcode 库可以生成二维码。 使用方式 使用如下命令安装依赖： 1pip install &quot;qrcode[pil]&quot; 使用如下代码即可生成基本的二维码： 1234567891011def text_to_qr(text, filename): qr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=4, ) qr.add_data(text) qr.make(fit=True) img = qr.make_image(fill_color=&quot;black&quot;, back_color=&quot;white&quot;) img.save(filename) 将图片放在二维码中间： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import qrcodefrom PIL import Imagedef generate_qr_with_logo(data, logo_path, output_path): &quot;&quot;&quot; Generates a QR code with a custom logo in the center. Args: data: The data to encode in the QR code (string). logo_path: Path to the logo image (string). output_path: Path to save the generated QR code (string). &quot;&quot;&quot; try: # Create QR code instance qr = qrcode.QRCode( version=1, # Adjust version for more data error_correction=qrcode.constants.ERROR_CORRECT_H, # High error correction box_size=10, border=4, ) qr.add_data(data) qr.make(fit=True) # Create an image from the QR code qr_image = qr.make_image(fill_color=&quot;black&quot;, back_color=&quot;white&quot;).convert(&quot;RGB&quot;) # Open the logo image logo = Image.open(logo_path).convert(&quot;RGB&quot;) # Calculate logo size (adjust as needed) logo_size = int(qr_image.size[0] * 0.2) # 20% of QR code size # Resize the logo logo = logo.resize((logo_size, logo_size), Image.LANCZOS) # Calculate logo position position = ((qr_image.size[0] - logo.size[0]) // 2, (qr_image.size[1] - logo.size[1]) // 2) # Paste the logo onto the QR code qr_image.paste(logo, position) # Save the QR code qr_image.save(output_path) print(f&quot;QR code with logo saved to &#123;output_path&#125;&quot;) except FileNotFoundError: print(f&quot;Error: Logo file not found at &#123;logo_path&#125;&quot;) except Exception as e: print(f&quot;An error occurred: &#123;e&#125;&quot;)if __name__ == &quot;__main__&quot;: data_to_encode = &quot;&lt;data&gt;&quot; logo_file = &quot;logo.png&quot; output_file = &quot;qrcode_with_logo.png&quot; generate_qr_with_logo(data_to_encode, logo_file, output_file) 注：其它用例参照官方项目。 参考资料 官方项目 PyPI qrcode","categories":[],"tags":[{"name":"QR Code","slug":"QR-Code","permalink":"https://wangqian0306.github.io/tags/QR-Code/"}]},{"title":"生成激活码","slug":"tmp/activation","date":"2025-01-02T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2025/activation/","permalink":"https://wangqian0306.github.io/2025/activation/","excerpt":"","text":"生成激活码 实现方式 使用 Java 自带的 SecureRandom 类可以生成随机码，用于实现卡券等功能。 1234567891011121314151617181920212223242526272829303132import java.security.SecureRandom;import java.util.HashSet;import java.util.Set;public class ActivationCodeUtil &#123; private static final String COMPLEX_CODE = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;; private static final String SIMPLE_CODE = &quot;0123456789&quot;; private static final int SWITCH_SIZE = 999; private static final int CODE_LENGTH = 6; private static final int MAX_TICKET_NUM = 9999; public static Set&lt;String&gt; generate(Integer num) &#123; SecureRandom random = new SecureRandom(); Set&lt;String&gt; result = new HashSet&lt;&gt;(); while (result.size() &lt; num) &#123; String codeSet = num &lt; SWITCH_SIZE ? SIMPLE_CODE : COMPLEX_CODE; StringBuilder activationCode = new StringBuilder(); for (int i = 0; i &lt; CODE_LENGTH; i++) &#123; int index = random.nextInt(codeSet.length()); activationCode.append(codeSet.charAt(index)); &#125; result.add(activationCode.toString()); &#125; return result; &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"}]},{"title":"MyBatis","slug":"spring/mybatis","date":"2024-12-25T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/mybatis/","permalink":"https://wangqian0306.github.io/2024/mybatis/","excerpt":"","text":"MyBatis 简介 MyBatis 是一款为了让服务更好使用关系型数据库的框架。 使用方式 至少引入如下依赖： 12345678910dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.mybatis.spring.boot:mybatis-spring-boot-starter:3.0.4&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;org.mybatis.spring.boot:mybatis-spring-boot-starter-test:3.0.4&#x27; runtimeOnly &#x27;com.mysql:mysql-connector-j&#x27; testRuntimeOnly &#x27;org.junit.platform:junit-platform-launcher&#x27;&#125; 然后编写如下配置 application.yaml ： 123456789spring: datasource: url: jdbc:mysql://xxx.xxx.xxx.xxx:3306/sakila username: xxxx password: xxxx driver-class-name: com.mysql.cj.jdbc.Drivermybatis: configuration: map-underscore-to-camel-case: true 注：此处为了便捷选了 MySQL 提供的示例数据库 sakila 。 之后编写如下代码 Actor.java ： 1234567891011121314151617import lombok.Data;import java.io.Serializable;import java.time.LocalDateTime;@Datapublic class Actor implements Serializable &#123; private Integer actorId; private String firstName; private String lastName; private LocalDateTime lastUpdate;&#125; 然后编写 ActorMapper.java : 123456789101112import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Select;import java.util.List;@Mapperpublic interface ActorMapper &#123; @Select(&quot;SELECT * FROM actor&quot;) List&lt;Actor&gt; findAll();&#125; 之后修改主类即可： 1234567891011121314151617181920212223import org.springframework.boot.CommandLineRunner;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application implements CommandLineRunner &#123; private final ActorMapper actorMapper; public Application(ActorMapper actorMapper) &#123; this.actorMapper = actorMapper; &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Override public void run(String... args) throws Exception &#123; System.out.println(this.actorMapper.findAll()); &#125;&#125; 其他插件 MyBatis Generator 从 Maven 上直接下载 mybatis-generator-core 的 jar 包和数据库驱动包，然后将其放置在同一目录下。 之后结合实际场景修改配置文件即可，样例如下： 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;classPathEntry location=&quot;mysql-connector-j-9.1.0.jar&quot; /&gt; &lt;context id=&quot;DB2Tables&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.cj.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://xxx.xxx.xxx.xxx:3306/sakila?useSSL=false&quot; userId=&quot;xxxx&quot; password=&quot;xxxx&quot;&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver &gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot; /&gt; &lt;/javaTypeResolver&gt; &lt;javaModelGenerator targetPackage=&quot;test.model&quot; targetProject=&quot;.&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot; /&gt; &lt;/javaModelGenerator&gt; &lt;sqlMapGenerator targetPackage=&quot;test.xml&quot; targetProject=&quot;.&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;/sqlMapGenerator&gt; &lt;javaClientGenerator type=&quot;XMLMAPPER&quot; targetPackage=&quot;test.dao&quot; targetProject=&quot;.&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot; /&gt; &lt;/javaClientGenerator&gt; &lt;table schema=&quot;sakila&quot; tableName=&quot;actor&quot; domainObjectName=&quot;Actor&quot;/&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 参考资料 Java &amp; Databases: An Overview of Libraries &amp; APIs 官方网站 官方项目 示例代码 MyBatis Generator","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"CMS","slug":"tools/cms","date":"2024-12-24T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/cms/","permalink":"https://wangqian0306.github.io/2024/cms/","excerpt":"","text":"CMS 简介 CMS (Content Management System，内容管理系统)是一种软件平台，用于创建、管理和修改数字内容，通常用于网站内容的管理和发布。它使得没有技术背景的用户也能轻松地创建、编辑和管理网站内容，而不需要编写代码。 在这类产品中 Strapi 的自托管方式是完全免费的。 使用方式 首先需要搭建一个数据库用来存储数据文件，通过本地或是线上服务商都可以。之后需要创建数据库 strapi 123CREATE DATABASE &lt;dbname&gt; CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 然后就可以新建项目了： 1npx create-strapi@latest 注：安装依赖的时间会比较长，此处 不要按回车键 。 按照提示创建项目即可。 创建完成后可以使用如下命令启动服务，进行测试： 12npm run buildnpm run start 统一部署 可以使用如下方式进行部署，编写项目 Dockerfile : 123456789101112FROM node:18-alpineWORKDIR /appCOPY package*.json ./RUN npm installCOPY . .ENV NODE_ENV=productionCMD [&quot;npm&quot;, &quot;run&quot;, &quot;start&quot;] 记得新建 .dockerignore 文件： 12node_modulesREADME.md 之后即可修改数据库容器的 docker-compose.yaml 文件即可： 1234567891011121314151617services: db: image: mysql:8 restart: always environment: MYSQL_ROOT_PASSWORD: example volumes: - ./mysql:/var/lib/mysql ports: - &quot;3306:3306&quot; strapi: build: ../demo image: demo/strapi:latest environment: DATABASE_HOST: db ports: - &quot;1337:1337&quot; 之后使用 docker-compose up -d 更新服务，就可以访问到管理页面了。 参考资料 官方文档 官方项目","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"CMS","slug":"CMS","permalink":"https://wangqian0306.github.io/tags/CMS/"}]},{"title":"MySQL 优化","slug":"database/mysql-optimization","date":"2024-12-09T14:12:59.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2024/mysql-optimization/","permalink":"https://wangqian0306.github.io/2024/mysql-optimization/","excerpt":"","text":"MySQL 优化 简介 数据库性能取决于数据库中的几个因素 级别，例如 tables、queries 和 configuration settings。这些软件结构导致硬件级别产生不同的 CPU 指令和 I/O 操作，为了优化性能需要将操作数进可能缩减。在提高数据库性能时，首先需要试用设计上的基础规则和软件的指导手册，并采用时间所为评判工具来进行优化。想成为专家，可以更多地了解软件内部发生的事情， 并开始测量 CPU 周期和 I/O 操作等内容。 典型用户的目标是从其现有的软件和硬件配置中获得最佳数据库性能。高级用户寻找机会改进 MySQL 软件本身，或开发自己的存储引擎和硬件设备来扩展 MySQL 生态系统。在官方文档中针对优化方式进行了如下分类： 在数据库级别进行优化 在硬件级别进行优化 平衡可移植性和性能(略) 在数据库级别进行优化 从基础设计上进行优化是最主要的提升方向，具体项目如下： 表结构(tables structure) 索引(indexes) 存储引擎(storage engine) 行格式(row format) 锁策略(locking strategy) 缓存大小(memory areas used for caching) 在上述优化执行前可以先考虑通过优化 SQL 的角度来提升检索速度。 优化 SQL 语句 在优化 SQL 时主要遵循的思路是： 利用索引 隔离并优化查询的任何部分 减少全表扫描 让优化器读取到最新的表结构 了解存储引擎的优化技术、索引技术和配置参数 调整 MySQL 缓存中关于内存区域的大小和配置 减少缓存的大小 处理锁的问题 注：由于此处内容过多且杂乱，此时暂不整理。如有需求请参照 Optimizing SQL Statements 优化索引 大多数 MySQL 索引(PRIMARY KEY ,UNIQUE ,INDEX, FULLTEXT)存储在 B 树(B-Trees,统称，一般应该是 B+ 树)。例外： SPECIAL 类型的索引使用 R 树; MEMORY table 还支持哈希索引; InnoDB 对 FULLTEXT 索引使用倒排索引。 在索引使用上遵循如下规则： 如果有多列索引，检索时要从左往右进行索引拼接。 如果需要跨表，最好让链接列具有相同的数据类型和大小且如果是字符串需要相同的字符集。 如果是 SPATIAL 索引需要指定 SRID(Spatial Reference System Identifier, 例如：EPSG:4326)。 在遇到非二进制数据时需要确保字段编码后长度小于 767 字节。 在检索时可以使用 Explain 语句查看检索方式，优化查询逻辑。 MySQL 存储引擎还会通过读取表统计信息的方式来对查询进行优化，索引对应的数据条目数如果太多则 MySQL 可能不会使用该索引。索引对应的数据条目数可以通过 SHOW INDEX 语句查看，它会显示基于 N/S 的基数值，其中 N 是表中的行数，S 是索引对应的数据条目大小。 在使用索引时 InnoDB 存储引擎默认打开了索引扩展功能，在优化器中会执行索引扩展操作，将二级索引拼接在主键前，在使用时可以检查是否采用此功能。 为了减少数据扫描，可以在创建索引时指定排序方式，例如：CREATE INDEX idx_name ON table_name (col1 ASC ,col2 DESC)。 注：在进行索引优化时如果表比较大删除和重新添加索引的成本会很高，此时可以先将其设置为隐形索引，然后再去检查 optimizer 是否使用隐形索引做检索条件，将其关闭即可进行测试。 优化数据结构 数据结构的优化要安排在设计阶段，目的是将表使用的数据空间尽可能缩小。此时花费的内存也会减少，并且索引也会相对较小检索速度也就会提升。 MySQL 支持许多不同的存储引擎(表类型)和 行格式。对于每个表，可以单独指定存储引擎和要使用的索引方法。 主要的方法如下： 表列(Table Columns) 选择合适的数据类型，比方说 MEDIUMINT 比 INT 使用的空间少 25%。 如有可能，请将列设置为 NOT NULL。它支持更好的使用索引并消除测试每个值为 NULL 的开销并缩减 1 位的空间。 行格式(Row Format) 在不同的存储引擎中支持了不同的行格式，如果使用紧凑的行格式可能会增加 CPU 负载但是会减少磁盘负载。需结合具体需求进行确定。 对于 MyISAM 表，如果没有可变长度的列(VARCHAR, TEXT, BLOB)，可以使用固定大小的行格式(FIXED)。这样会更快，但是浪费了空间。 索引(Indexes) 表的主索引应尽可能短。 仅创建提高查询性能所需的索引。索引的 Key 应当以检索频率进行排序，最左侧的应该是常见且经常重复的列。 如果说长字符串的前 N 位已经足够表达唯一性了，后续内容可以忽略则可以在创建索引的时候指定位数。 链接(Joins) 将频繁被查询的数据和不用于查询只展示的数据进行分离。 在具有相同数据类型的不同表中声明具有相同信息的列，以加快基于相应列的连接。 保持列名简单，以便在不同的表中使用相同的名称并简化联接查询。 范式化(Normalization) 通常，尽量保持所有数据不冗余。 如果速度比磁盘空间和保留多个数据副本的维护成本更重要，则可以考虑冗余。 在选择列数据类型时可以遵循如下逻辑： 数值型 优先选择用数值型进行存储而不要使用字符串。 如果是数值数据，在实时链接下比读取文件快。 字符型 在不需要语言特行排序和比较特性的情况下，可以使用二进制（binary）排序规则（collation）来提高比较和排序操作的性能。 在比较不同列的值时，尽可能使用相同的字符集和排序规则声明这些列，以避免在运行查询时进行字符串转换。 对于内容小于 8KB 的列值，请使用二进制 VARCHAR 而不是 BLOB。 如果表包含字符串列，例如 name 和 address，但许多查询不会检索这些列， 考虑将字符串列拆分为单独的 table 并使用带有外键的联接查询。 如果使用随机的值作为主键(类似 UUID)时最好在前面附加时间数据，便于 InnoDB 索引插入和检索。 BLOB 型 存储包含文本数据的大型 blob 时， 首先考虑压缩它。 对于具有多个列的表，为了减少不使用 BLOB 列的查询的内存需求，可以考虑将 BLOB 列拆分为一个单独的表，并在需要时用联接查询引用它。 可以将 BLOB 数据存储在独立的存储设备或单独的数据库实例上。 可以将列值的哈希值存储在单独的列中，为该列编制索引，并在查询中测试哈希值，而不是针对很长的文本字符串测试内容是否相同。 在库和表以及配置上可以遵循如下逻辑： 确定集群的缓存策略，调整 open_files_limit 系统配置，以便更好地利用缓存策略，还可以微调 table_open_cache 和 max_connections 配置项。 如果库中有许多 MyISAM 表，则需要增加缓存条目数避免打开、关闭和创建操作影响性能。 如果在使用中有很多临时表的创建也会遇到性能问题，可以尝试优化 SQL 或者修改配置将临时表存储在内存中。 优化存储引擎 此处暂时只针对 InnoDB 进行整理，后期如有时间会进行补充完善。 InnoDB 优化表的存储布局 一旦数据趋于稳定或有大量数据插入后可以使用 OPTIMIZE TABLE 语句整理表空间和索引。 尝试避免长字符串主键。 使用 VARCHAR 而不是 CHAR 存储可变长度字符串。 对于较大的表格或包含大量重复文本的表格或数值数据，请考虑使用 COMPRESSED 行格式。 优化事务管理 将事务功能的性能开销和服务器的工作负载之间找到理想的平衡。相关业务逻辑合并到单个事务中，可以减少事务的开销。 对于仅包含单个 SELECT 语句的事务，启用 AUTOCOMMIT 会有所帮助 InnoDB 识别只读事务并对其进行优化。 避免在单个事务中插入更新或删除大量行然后进行回滚，此时数据库若性能下降不要终结数据库进程。回滚会在服务启动时重新执行。为了避免此问题可以做如下处理： 增加缓冲池大小。 设置 innodb_change_buffering=all ，以便 update 和 delete 操作存储在缓存中。 考虑将大量数据分批次处理。 修改或删除行时，行和关联的 redo log 不会立即以物理删除，甚至不会在事务提交后立即删除。旧数据将保留，直到较早或同时开始的事务完成，以便这些事务可以访问已修改或删除行的先前状态。因此，长时间运行的事务可能会阻止 InnoDB 清除由其他事务更改的数据。 在长时间运行的事务中，如果使用 READ COMMITED 和 REPEATABLE READ 隔离级别的其他事务读取相同的行，则必须做更多的工作来重建旧数据。 当一个长时间运行的事务修改了表中的数据时，其他并发事务对同一表的查询可能会失去使用覆盖索引(covering index)的能力。 优化只读事务 InnoDB 可以通过识别只读事务并优化其处理方式来减少与设置 TRX_ID 相关的开销。对于明确为只读的事务，不需要分配事务 ID，因为这些事务不会执行写操作或锁定读取。这种优化减少了内部数据结构的大小和复杂度，进而提升了查询性能，尤其是在高并发环境下。 优化 Redo Log 增加 Redo Log 文件的大小。什么时候 InnoDB 已将重做日志文件写入完整，它必须在检查点中将缓冲池的修改内容写入磁盘。若配置较小会导致许多不必要的磁盘写入。 考虑增加 log 缓冲区。 配置 innodb_log_write_ahead_size 与操作系统或文件系统缓存块大小相匹配的值，使用如下命令即可查看 stat -f --format=&quot;%S&quot; &lt;path&gt;。 若并发低则关闭 innodb_log_writer_threads 。 优化用户线程在等待刷新重做日志（flushed redo）时的自旋延迟（spin delay）设置，可以帮助你根据系统负载情况调整性能和资源使用。通过合理配置 innodb_log_wait_for_flush_spin_hwm、innodb_log_spin_cpu_abs_lwm 和 innodb_log_spin_cpu_pct_hwm 系统变量，可以在高并发和低并发期间实现更高效的资源利用。 innodb_log_wait_for_flush_spin_hwm 当平均日志刷新时间超过该值时，用户线程将不再进行自旋等待，而是进入睡眠状态。高并发时减少，低时增加。 innodb_log_spin_cpu_abs_lwm CPU 使用率低于该阈值时，用户线程将不再进行自旋等待。这个值是基于绝对 CPU 核心使用量的总和。如果系统 CPU 使用率较低，可以通过提高这个阈值。 innodb_log_spin_cpu_pct_hwm 当总的 CPU 使用率超过该百分比时，用户线程将不再进行自旋等待。这个值是基于绝对 CPU 核心使用量的总和。在高并发情况下，降低这个值。 批量插入优化。 在插入大量数据时先关闭 SET autocommit=0 然后结束后再 COMMIT; 。 对于包含唯一约束（UNIQUE 约束）的二级索引，在导入大量数据时临时关闭唯一性检查可以显著提高导入速度。这是因为 InnoDB 可以利用其更改缓冲区(change buffer)来批量写入二级索引记录，从而减少大量的磁盘 I/O 操作。 关闭外键检查可以节省磁盘 I/O ，增加插入性能。 使用多行 INSERT 语法来减少客户端和服务器之间的通信开销。 设置 innodb_autoinc_lock_mode 为 2（交错模式，interleaved）可以在执行批量插入到带有自增列（AUTO_INCREMENT）的表时显著提升性能，特别是在高并发环境下。 在执行批量插入时，按照主键(PRIMARY KEY)顺序插入行可以显著提高性能，尤其是在 InnoDB 表中。这是因为 InnoDB 使用聚簇索引，数据物理上是按照主键顺序存储的。因此，按主键顺序插入数据可以减少页面分裂和随机 I/O 操作，从而提升插入速度。对于那些不能完全放入缓冲池(buffer pool)的大表来说，这一点尤为重要。 为了在将数据加载到InnoDB FULLTEXT索引时获得最佳性能，请执行以下步骤： 在表创建时定义一个类型为 BIGINT UNSIGNED NOT NULL 的列 FTS_DOC_ID，该列具有一个名为 FTS_DOC_ID_index 的唯一索引。 将数据加载到表中。 创建 FULLTEXT 索引。 在向新的 MySQL 实例加载大量数据时，考虑临时禁用重做日志（redo log）可以显著提高数据加载速度。 使用 MySQL Shell 的 util.importTable() 和 util.loadDump() 指令可以显著提高大规模数据文件的导入速度，特别是当处理大容量数据时。这些工具提供了并行加载功能，能够充分利用多核处理器的优势，从而加快数据导入过程。 优化 InnoDB 查询。 选择最重要的列作为主键。 不要在主键中指定太多或太长的列，因为这些列值在每个辅助索引中都是重复的。当索引包含不必要的数据时，读取这些数据的 I/O 和缓存这些数据的内存会降低服务器的性能和可扩展性。 不要为每列创建单独的索引，而是根据查询情况设置联合索引。 如果索引列不能包含任何 NULL 值，请在创建表时将其声明为 NOT NULL。优化器当知道每列是否包含 NULL 值可以更好地确定哪个索引最有效地用于查询。 优化只读事务 优化 InnoDB DDL。 对表和索引的 DDL 操作可以不影响之前的环境。 加载数据之后再建立索引会快一些。 在没有外键时可以使用 TRUNCATE TABLE 清空表，如果有外键的话使用 DROP TABLE 和 CREATE TABLE 的命令可能是最快的方法。 优化 InnoDB 磁盘 I/O。 增加缓冲池大小到系统内存的 50%-75%。 如果数据写入磁盘的速度很慢，可以将 innodb_flush_method 设置为 O_DSYNC。 在多实例多租户情况下考虑调整 innodb_fsync_threshold 的值，批量将数据写入磁盘。 打开 innodb_use_fdatasync 配置。 在 Linux 系统中可以使用 noop 和 deadline 调度器，使用如下命令检查 cat /sys/block/sd&lt;device&gt;/queue/scheduler 输出结果是支持的调度器，用方括号选中的是现在正在使用的。 将数据文件和日志文件分离放置。 使用 SSD 存储数据，并检查如下配置： innodb_checksum_algorithm 应当设置为 crc32。 innodb_flush_neighbors 设置为 0。 调整 innodb_idle_flush_pct 参数，调大会增加硬盘读写次数损耗寿命调小会让内存压力增大。 调整 innodb_io_capacity 参数，例如 1000。 调整 innodb_io_capacity_max 参数，例如 2500。 如果 redo log 在 SSD 上，需要禁用 innodb_log_compressed_pages。 如果 redo log 在 SSD 上，增加 innodb_redo_log_capacity。 查看硬件的扇区大小与 innodb_page_size 进行匹配。 如果日志在 SSD 上，并且所有表都有主键可以将 binlog_row_image 设置为 minimal。 增加 innodb_doublewrite_pages 参数。 调整 innodb_read_io_threads 和 innodb_write_io_threads 线程数，默认配置跑不满磁盘。 调整 innodb_io_capacity 参数。 禁用压缩页面的日志记录 innodb_log_compressed_pages。 优化 InnoDB 配置变量。 配置 InnoDB 缓存的数据操作类型 innodb_change_buffering。 在写入多时可以尝试关闭 innodb_adaptive_hash_index。 配置并发线程数 innodb_thread_concurrency。 合理配置预读缓存 innodb_read_ahead_threshold。 合理配置 InnoDB 的缓冲池刷新算法 (Buffer Pool Flushing Algorithm)。 利用多核处理器及其缓存内存配置优化自旋锁，以最小化上下文切换延迟。 防止 table scans 等一次性操作，干扰 InnoDB 缓冲区缓存。 将日志文件调整为对可靠性和崩溃恢复有意义的大小。 配置多个缓冲池实例。 增加并发事务的最大数量。 将垃圾回收策略配置为后台线程。 调整 innodb_thread_concurrency 和 innodb_concurrency_tickets 参数，限制同时处理的线程数量，并允许每个线程在被换出之前完成更多的工作，从而保持等待线程的数量较低，并使操作能够在不过度进行上下文切换的情况下完成。 如果使用了非持久化优化器统计信息，需要在打开数据库后进行一次读取预热数据。 在硬件级别进行优化 任何数据库应用程序最终都会达到硬件限制，因为数据库变得越来越繁忙。DBA 必须进行评估，然后调整应用程序或重新配置服务器。要避免性能瓶颈，可以使用纵向扩展的思路，增加或优化硬件资源。系统瓶颈通常来自以下来源： 磁盘查找(Disk seeks) 磁盘读取和写入(Disk reading and writing) CPU 周期(CPU cycles) 内存带宽(Memory bandwidth) 参考资料 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"}]},{"title":"Spring AOP","slug":"spring/aop","date":"2024-12-04T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/spring-aop/","permalink":"https://wangqian0306.github.io/2024/spring-aop/","excerpt":"","text":"Spring AOP 简介 面向方面编程(Aspect-oriented Programming,AOP)通过提供另一种思考程序结构的方式来补充面向对象编程 (Object-oriented Programming, OOP)。在 OOP 中，模块化的关键单元是类，而在 AOP 中，模块化的单元是切面。切面可以支持跨多个类型和对象的关注点进行通用的处理。 在 Spring 中， 概念和使用场景 在切面编程的时候有很多的专业名词，但是从使用上讲可以简单归类为以下几处： 切面(Aspect) 连接点(Join Point) 通知(Advice) 切入点(Pointcut) 织入(Weaving) 注：此处内容简略，详细内容请参照官方文档。 在编码时可以简单的只考虑： 切面 切入点 Advice 类型和使用场景 切面编写时，首先需要明确要执行的业务逻辑，也就是通知(Advice)： Advice 类型 描述 使用场景 Before Advice 在目标方法执行之前执行 权限校验、日志记录、参数验证等 After Advice 在目标方法执行后（无论是否成功）执行 清理资源、释放锁、后置操作等 After Returning Advice 在目标方法正常返回后执行 结果日志记录、统计等 After Throwing Advice 在目标方法抛出异常后执行 异常日志记录、异常处理等 Around Advice 在目标方法执行前后都能执行，完全控制目标方法 性能监控、事务管理、缓存处理、全局异常处理等 在选择 Advice 类型时可以使用如下的思路： 如果你只关心方法执行前后发生的事情，可以使用 Before 和 After。 如果你只关心方法的返回值或异常，使用 After Returning 或 After Throwing。 如果你需要对方法的执行过程进行全面控制，使用 Around Advice，它能在方法执行前后插入代码，并能改变方法的返回值或决定是否执行目标方法。 Pointcut Designator 业务逻辑编写完成后即可选择切点，也就是确认切面要执行的位置。可以将其指定为方法，类，注解等： Pointcut Designator 描述 使用场景 execution() 匹配方法执行时的连接点 通常用于方法调用的切点，最常用 within() 匹配指定类或包中的方法 用于限定类或包范围的切点 this() 匹配目标方法所在代理对象的类型 适用于代理对象的类型匹配 target() 匹配目标方法所在目标对象的类型 适用于目标对象类型的匹配 args() 匹配方法参数类型 用于方法参数类型匹配 @args() 匹配方法参数上标注的注解类型 用于基于方法参数的注解类型匹配 @annotation() 匹配标注了特定注解的方法 用于基于注解的方法切点 @within() 匹配类或方法上标注特定注解的类 用于类上注解的匹配 @target() 匹配目标对象上标注特定注解的方法 用于目标对象本身带注解的方法匹配 示例 编写 Convert.java ： 12345public interface Convert&lt;PARAM&gt; &#123; OperateLog convert(PARAM param);&#125; 编写 SaveOrder.java ： 12345public record SaveOrder ( Long id) &#123;&#125; 编写 UpdateOrder.java ： 1234public record UpdateOrder( Long orderId) &#123;&#125; 编写 OperateLog.java ： 123456789101112import lombok.Data;@Datapublic class OperateLog &#123; private Long orderId; private String desc; private String result;&#125; 编写 SaveLogConvert.java ： 12345678public class SaveLogConvert implements Convert&lt;SaveOrder&gt; &#123; @Override public OperateLog convert(SaveOrder saveOrder) &#123; OperateLog operateLog = new OperateLog(); operateLog.setOrderId(saveOrder.id()); return operateLog; &#125;&#125; 编写 UpdateLogConvert.java ： 12345678public class UpdateLogConvert implements Convert&lt;UpdateOrder&gt; &#123; @Override public OperateLog convert(UpdateOrder updateOrder) &#123; OperateLog operateLog = new OperateLog(); operateLog.setOrderId(updateOrder.orderId()); return operateLog; &#125;&#125; 编写 RecordOperate.java ： 12345678910111213import java.lang.annotation.*;@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface RecordOperate &#123; String desc() default &quot;&quot;; Class&lt;? extends Convert&gt; convert();&#125; 编写 OrderService.java ： 1234567891011121314151617181920import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;@Slf4j@Servicepublic class OrderService &#123; @RecordOperate(desc = &quot;保存订单&quot;, convert = SaveLogConvert.class) public Boolean saveOrder(SaveOrder saveOrder) &#123; log.info(&quot;saveOrder: &#123;&#125;&quot;, saveOrder); return true; &#125; @RecordOperate(desc = &quot;更新订单&quot;, convert = UpdateLogConvert.class) public Boolean updateOrder(UpdateOrder updateOrder) &#123; log.info(&quot;updateOrder: &#123;&#125;&quot;, updateOrder); return true; &#125;&#125; 编写 StartUpRunner.java ： 1234567891011121314151617import jakarta.annotation.Resource;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component;@Componentpublic class StartUpRunner implements CommandLineRunner &#123; @Resource private OrderService orderService; @Override public void run(String... args) throws Exception &#123; orderService.saveOrder(new SaveOrder(123L)); orderService.updateOrder(new UpdateOrder(123L)); &#125;&#125; 在入口添加如下注解： 12345678910111213import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.EnableAspectJAutoProxy;@EnableAspectJAutoProxy@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 运行程序即可获得 AOP 产生的日志。 参考资料 官方文档 【Java高级】你真的会切面编程么？技术专家实战演示！全是细节！ 【IT老齐140】非常实用！Spring AOP与自定义注解实现共性需求 如何优雅地记录操作日志？","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"GeoTIFF","slug":"ocean/geotiff","date":"2024-11-25T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/geotiff/","permalink":"https://wangqian0306.github.io/2024/geotiff/","excerpt":"","text":"GeoTIFF 简介 GeoTIFF 是一种公共领域元数据标准，允许将地理参考信息嵌入 TIFF 文件中。潜在的附加信息包括地图投影、坐标系、椭球体、基准以及为文件建立精确空间参考所需的所有其他信息。 使用方式 安装依赖包： 1pip install rasterio 读取文件 1234567891011121314151617181920212223242526272829import rasterio# 打开 merge.tiff 文件file_path = &quot;merge.tiff&quot;with rasterio.open(file_path) as dataset: # 打印文件的元数据信息 print(&quot;文件元数据：&quot;) print(dataset.meta) # 打印更多详细的元数据 print(&quot;\\n详细元数据信息：&quot;) print(f&quot;宽度 (Width): &#123;dataset.width&#125;&quot;) print(f&quot;高度 (Height): &#123;dataset.height&#125;&quot;) print(f&quot;波段数量 (Number of bands): &#123;dataset.count&#125;&quot;) print(f&quot;数据类型 (Data type): &#123;dataset.dtypes&#125;&quot;) print(f&quot;坐标参考系 (CRS): &#123;dataset.crs&#125;&quot;) print(f&quot;仿射变换 (Transform): &#123;dataset.transform&#125;&quot;) print(f&quot;边界范围 (Bounds): &#123;dataset.bounds&#125;&quot;) # 检查并打印波段的描述 for band_id in range(1, dataset.count + 1): print(f&quot;\\n波段 &#123;band_id&#125; 的描述:&quot;) print(dataset.descriptions[band_id - 1]) data = dataset.read(1) demo_list = data.tolist() print(demo_list[0]) print(len(demo_list[0])) 写入文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445def buildArray(number:int): result = [] for i in range(180): cache = [] for j in range(360): cache.append(number) result.append(cache) return resultimport numpy as npimport rasteriofrom rasterio.transform import Affinefrom rasterio.crs import CRSdriver = &#x27;GTiff&#x27;dtype = &#x27;int16&#x27;nodata = Nonewidth = 360height = 180count = 2crs = CRS.from_wkt(&#x27;GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]&#x27;)transform = Affine(1.0, 0.0, -0.5, 0.0, -1.0, 90.0)metadata = &#123; &#x27;driver&#x27;: driver, &#x27;dtype&#x27;: dtype, &#x27;nodata&#x27;: nodata, &#x27;width&#x27;: width, &#x27;height&#x27;: height, &#x27;count&#x27;: count, &#x27;crs&#x27;: crs, &#x27;transform&#x27;: transform&#125;band1 = np.array(buildArray(180), dtype=np.int16)band2 = np.array(buildArray(180), dtype=np.int16)# Write the GeoTIFFoutput_path = &quot;sample.tiff&quot;with rasterio.open(output_path, &#x27;w&#x27;, **metadata) as dst: dst.write(band1, 1) # Write Band 1 dst.write(band2, 2) # Write Band 2print(f&quot;GeoTIFF file created: &#123;output_path&#125;&quot;) 除了把原始数据直接写入 tiff 中之外还可以针对数据进行分级规划。比方说可以按照如下方式进行处理： 12345678910111213141516171819202122232425262728293031323334353637383940414243import jsondef process_2d_array(input_array, func): &quot;&quot;&quot; 对二维数组中的每个元素应用函数处理，返回一个新的二维数组。 :param input_array: 输入的二维数组 :param func: 用于处理每个元素的函数 :return: 处理后的二维数组 &quot;&quot;&quot; return [[func(value) for value in row] for row in input_array]def data_to_int(data, max_value=32): &quot;&quot;&quot; 将数据转换为 -300 到 300 的整型数据。 :param data: 原始数据值，可以为负值 :param max_value: 最大速度的绝对值 (默认 32) :return: 映射到 -300 到 300 的整型值 &quot;&quot;&quot; if abs(data) &gt; max_value: data = max_value if data &gt; 0 else -max_value # 限制在 [-max_value, max_value] # 映射公式：将 [-max_value, max_value] 映射到 [-300, 300] mapped_value = (data / max_value) * 300 return round(mapped_value)def process(filename, result_name): with open(filename, &quot;r&quot;) as f: text = f.read() input_array = json.loads(text) output_array = process_2d_array(input_array, data_to_int) output_json = json.dumps(output_array, indent=4) with open(result_name, &quot;w&quot;) as file: file.write(output_json)if __name__ == &quot;__main__&quot;: process(&quot;u_wind_filtered.json&quot;, &quot;u_wind_modified.json&quot;) process(&quot;v_wind_filtered.json&quot;, &quot;v_wind_modified.json&quot;) 注：此处数据采用 180 个长度为 360 的数组(二维数组，数据分辨率为 1 度)。 数据偏移 如果说由于数据源对应的分辨率和当前地图不一致则可以使用如下代码针对数组进行偏移处理： 123456789101112131415161718192021222324def shift_to_right(matrix, n): if not matrix or n &lt;= 0: return matrix result = [] for row in matrix: if len(row) &lt;= n: result.append(row) else: result.append(row[-n:] + row[:len(row) - n]) return resultdef shift_to_left(matrix, n): if not matrix or n &lt;= 0: return matrix result = [] for row in matrix: if len(row) &lt;= n: result.append(row) else: result.append(row[n:] + row[:n]) return result 参考资料 参考代码 官方项目 官方文档","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"Watchtower","slug":"docker/watchtower","date":"2024-11-22T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/watchtower/","permalink":"https://wangqian0306.github.io/2024/watchtower/","excerpt":"","text":"Watchtower 简介 Watchtower 是一个 Docker，它主要被用来自动更新容器镜像。 注：此项目只能在开发环境使用。 使用方式 可以使用 docker-compose 部署： 12345678910services: watchtower: image: containrrr/watchtower volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - REPO_USER=&lt;user&gt; - REPO_PASS=&lt;password&gt; command: --interval 14400 restart: always 注：interval 表示检查镜像的时间单位为秒，样例是 4 小时拉一次。 参考资料 官方项目 官方文档","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"Xihe-GlobalOceanForecasting","slug":"ocean/xihe","date":"2024-11-19T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/xihe/","permalink":"https://wangqian0306.github.io/2024/xihe/","excerpt":"","text":"Xihe-GlobalOceanForecasting 简介 2024年2月5日，国防科技大学气象海洋学院联合复旦大学大气与海洋科学系、中南大学计算机学院等单位，研制了首个数据驱动的全球1/12°高分辨率海洋环境预报大模型“羲和”。 使用流程和逻辑 羲和海洋大模型的数据来自三个部分： ERA5 大气再分析数据 GHRSST 海表温度数据 GLORYS12 海洋再分析数据 使用这些数据做训练可以形成模型，然后再输入如下变量即可完成预测： 表层变量（surface） 深层变量（deep） 静态变量（包含经纬度网格、掩膜数组和气候统计量） 注：详细描述参见官方项目 README 文件。 再制作输入数据时，可以获取一整月的来源数据，然后计算平均值，再使用插值法处理投影成 (2041, 4320) 大小即可作为输入数据集。 在准备好环境之后即可使用代码中的 inference.py 文件进行预测。 注：此时需要下载模型文件 file_path 和模型权重 project_path 作为基础资源，在 main 函数中需要设定输入和输出问文件夹以及输入文件对应的日期。 安装如下依赖之后即可运行官方项目： 12345678910111213141516171819202122232425262728293031323334353637383940python -m pip install --upgrade pippip install cinrad==1.9.1pip install meteva==1.9.1.2pip install pypots==0.8.1pip install pyproj==3.7.0pip install OWSLib==0.29.3pip install dask==2024.11.1pip install distributed==2024.11.1pip install MetPy==1.6.3pip install Pillow==11pip install xarray==2024.10pip install numpy==1.26.4pip install rioxarray==0.17pip install seaborn==0.13.2pip install pandas==2.2.3pip install imageio==2.36pip install rasterio==1.4.2pip install shapely==2.0.6pip install geopandas==1.0.1pip install cf-xarray==0.10.0pip install networkx==3.3pip install zarr==2.18.3pip install jax==0.4.34pip install transformers==4.46.2pip install lightgbm==4.5.0pip install xgboost==2.1.2pip install cmaps==2.0.1pip install scikit-learn==1.6.0rc1pip install cartopy==0.24.1pip install pytorch-lightning==2.4.0pip install wandb==0.18.7pip install gluonts==0.15.1pip install onnx==1.16.2pip install matplotlib==3.9.2pip install statsmodels==0.14.2pip install shap==0.46pip install pytorch-tabnet==4.1.0pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simplepip install scipy==1.13.1pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.0/MindSpore/unified/x86_64/mindspore-2.4.0-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple 示例代码如下： 1python inference.py --lead_day 7 --save_path output_data 参考资料 论文原文 官方项目 动手用羲和海洋AI大模型预报海温、流速 | GeoAI workshop","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"digital-signage","slug":"tmp/digital-signage","date":"2024-11-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/digital-signage/","permalink":"https://wangqian0306.github.io/2024/digital-signage/","excerpt":"","text":"Digital Signage 简介 数字标牌项目可以用来做大屏展示。 Anthias 部署样例 参考文件 注：在 x86 系统中部署时遇到了 viewer 卡住没能正常启动的问题，暂时没有使用树莓派测试。 参考资料 Anthias Xibo 部署样例 参考项目 客户端 注：在链接时不要使用代码方式，输入在配置中看到的 CMS Secret Key 点击链接后在 Display 页面选择 Authorize 即可，进入界面后按 i 即可得到相关日志。之后的使用可以参考使用文档。 参考资料 官方网站","categories":[],"tags":[{"name":"digital-signage","slug":"digital-signage","permalink":"https://wangqian0306.github.io/tags/digital-signage/"}]},{"title":"Apache Benchmark","slug":"tmp/apache-benchmark","date":"2024-10-31T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/ab/","permalink":"https://wangqian0306.github.io/2024/ab/","excerpt":"","text":"Apache Benchmark 简介 Apache Benchmark (ab) 是一个开源的压力测试工具，它是 Apache HTTP 服务器的一部分。ab 工具主要用于测试Web服务器软件（如Apache、Nginx等）的并发性能和吞吐量。 使用方式 使用如下命令即可完成安装： 1sudo yum insatll httpd-tools 使用如下命令即可完成测试： 1ab -n 200 -c 8 https://xxx.xxx.xxx/xxx 注：-n 表示请求的总数 -c 表示并发量。 参考资料 Apache Benchmark 使用手册","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Apache Benchmark","slug":"Apache-Benchmark","permalink":"https://wangqian0306.github.io/tags/Apache-Benchmark/"}]},{"title":"React","slug":"frount/react","date":"2024-10-22T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/react/","permalink":"https://wangqian0306.github.io/2024/react/","excerpt":"","text":"React 简介 做 Next.js 的时候发现基础还是要看看。 初步试用 井字棋练习题完整版： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149import &#123; useState &#125; from &#x27;react&#x27;;function Square(&#123; value, onSquareClick, isWinningSquare &#125;) &#123; const backgroundColor = isWinningSquare ? &#x27;yellow&#x27; : &#x27;white&#x27;; return ( &lt;button className=&quot;square&quot; onClick=&#123;onSquareClick&#125; style=&#123;&#123; backgroundColor &#125;&#125; &gt; &#123;value&#125; &lt;/button&gt; );&#125;function Board(&#123; xIsNext, squares, onPlay, winningSquares &#125;) &#123; function handleClick(i) &#123; if (calculateWinner(squares) || squares[i]) &#123; return; &#125; const nextSquares = squares.slice(); if (xIsNext) &#123; nextSquares[i] = &#x27;X&#x27;; &#125; else &#123; nextSquares[i] = &#x27;O&#x27;; &#125; onPlay(nextSquares, i); &#125; const winner = calculateWinner(squares)?.winner; const isDraw = !winner &amp;&amp; squares.every(Boolean); let status; if (winner) &#123; status = &#x27;Winner: &#x27; + winner; &#125; else if (isDraw) &#123; status = &quot;It&#x27;s a draw!&quot;; &#125; else &#123; status = &#x27;Next player: &#x27; + (xIsNext ? &#x27;X&#x27; : &#x27;O&#x27;); &#125; return ( &lt;&gt; &lt;div className=&quot;status&quot;&gt;&#123;status&#125;&lt;/div&gt; &#123;Array(3).fill(null).map((_, rowIndex) =&gt; ( &lt;div className=&quot;board-row&quot; key=&#123;rowIndex&#125;&gt; &#123;Array(3).fill(null).map((_, colIndex) =&gt; &#123; const index = rowIndex * 3 + colIndex; return ( &lt;Square key=&#123;index&#125; value=&#123;squares[index]&#125; onSquareClick=&#123;() =&gt; handleClick(index)&#125; isWinningSquare=&#123;winningSquares?.includes(index)&#125; /&gt; ); &#125;)&#125; &lt;/div&gt; ))&#125; &lt;/&gt; );&#125;export default function Game() &#123; const [history, setHistory] = useState([&#123; squares: Array(9).fill(null), position: null &#125;]); // 记录落子位置 const [currentMove, setCurrentMove] = useState(0); const [isAscending, setIsAscending] = useState(true); const xIsNext = currentMove % 2 === 0; const currentSquares = history[currentMove].squares; function handlePlay(nextSquares, position) &#123; const nextHistory = [...history.slice(0, currentMove + 1), &#123; squares: nextSquares, position &#125;]; setHistory(nextHistory); setCurrentMove(nextHistory.length - 1); &#125; function jumpTo(nextMove) &#123; setCurrentMove(nextMove); &#125; function toggleSortOrder() &#123; setIsAscending(!isAscending); &#125; const winnerInfo = calculateWinner(currentSquares); const winningSquares = winnerInfo ? winnerInfo.line : null; const sortedMoves = isAscending ? history : [...history].reverse(); const moves = sortedMoves.map((entry, move) =&gt; &#123; const totalMoves = history.length - 1; const adjustedMove = isAscending ? move : totalMoves - move; let description; if (adjustedMove === totalMoves &amp;&amp; totalMoves !== 0) &#123; description = `You are at move $&#123;totalMoves&#125;`; &#125; else if (adjustedMove &gt; 0) &#123; const row = Math.ceil((entry.position + 1) / 3); const col = (entry.position + 1) - (row - 1) * 3; description = `Go to move #$&#123;adjustedMove&#125; (row: $&#123;row&#125;, col: $&#123;col&#125;)`; &#125; else &#123; description = &#x27;Go to game start&#x27;; &#125; return ( &lt;li key=&#123;adjustedMove&#125;&gt; &lt;button onClick=&#123;() =&gt; jumpTo(adjustedMove)&#125;&gt;&#123;description&#125;&lt;/button&gt; &lt;/li&gt; ); &#125;); return ( &lt;div className=&quot;game&quot;&gt; &lt;div className=&quot;game-board&quot;&gt; &lt;Board xIsNext=&#123;xIsNext&#125; squares=&#123;currentSquares&#125; onPlay=&#123;handlePlay&#125; winningSquares=&#123;winningSquares&#125; /&gt; &lt;/div&gt; &lt;div className=&quot;game-info&quot;&gt; &lt;button onClick=&#123;toggleSortOrder&#125;&gt; &#123;isAscending ? &#x27;Sort Descending&#x27; : &#x27;Sort Ascending&#x27;&#125; &lt;/button&gt; &lt;ol&gt;&#123;moves&#125;&lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; );&#125;function calculateWinner(squares) &#123; const lines = [ [0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6], ]; for (let i = 0; i &lt; lines.length; i++) &#123; const [a, b, c] = lines[i]; if (squares[a] &amp;&amp; squares[a] === squares[b] &amp;&amp; squares[a] === squares[c]) &#123; return &#123; winner: squares[a], line: lines[i] &#125;; &#125; &#125; return null;&#125; 参考资料 官方网站 井字棋教程 Awesome React HTML to JSX","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"React","slug":"React","permalink":"https://wangqian0306.github.io/tags/React/"}]},{"title":"Apache Tika","slug":"tmp/tika","date":"2024-10-14T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/tika/","permalink":"https://wangqian0306.github.io/2024/tika/","excerpt":"","text":"Apache Tika 简介 Apache Tika 可以检测并提取来自一千多种不同文件类型（如PPT、XLS和PDF）的元数据和文本。 使用方式 启动服务 可以选用 Docker 的方式使用： 1234FROM apache/tika:latestUSER rootRUN apt update &amp;&amp; apt install fonts-wqy-zenhei fonts-wqy-microhei xfonts-wqy -yUSER 35002:35002 123456services: tika: build: . image: custom/tika:latest ports: - &quot;9998:9998&quot; 也可以通过下载包，并通过 java 来使用，下载地址 读取数据 首先可以准备好要解析的数据文件，然后安装依赖： 1pip install tika 然后编写如下程序即可： 1234567891011from tika import parserdef read_docx(file_path): parsed = parser.from_file(file_path) text = parsed.get(&#x27;content&#x27;, &#x27;&#x27;) return text.strip()if __name__ == &quot;__main__&quot;: file_path = &#x27;xxx.xxx&#x27; content = read_docx(file_path) print(content) 参考资料 官方网站","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Apache Tika","slug":"Apache-Tika","permalink":"https://wangqian0306.github.io/tags/Apache-Tika/"}]},{"title":"Kivy","slug":"ocean/kivy","date":"2024-10-08T13:32:58.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/kivy/","permalink":"https://wangqian0306.github.io/2024/kivy/","excerpt":"","text":"Kivy 简介 Kivy 是一个开源的 Python GUI 开发框架。 安装依赖 使用如下命令安装： 1pip install kivy kivy-md materialyoucolor 示例： 12345678910from kivymd.app import MDAppfrom kivymd.uix.label import MDLabelclass MainApp(MDApp): def build(self): return MDLabel(text=&quot;Hello, World&quot;, halign=&quot;center&quot;)MainApp().run() 除了使用这样的基本方式外还可以使用配置文件的方式创建页面，具体样例参见下文。 使用方式 对接 matplotlib 安装依赖： 1pip install kivy-matplotlib-widget 编写如下程序即可： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from kivy.utils import platform# avoid conflict between mouse provider and touch (very important with touch device)# no need for android platformif platform != &#x27;android&#x27;: from kivy.config import Config Config.set(&#x27;input&#x27;, &#x27;mouse&#x27;, &#x27;mouse,disable_on_activity&#x27;)from kivy.lang import Builderfrom kivy.app import Appimport matplotlib.pyplot as pltimport kivy_matplotlib_widgetKV = &#x27;&#x27;&#x27;Screen figure_wgt:figure_wgt BoxLayout: orientation:&#x27;vertical&#x27; BoxLayout: size_hint_y:0.2 Button: text:&quot;home&quot; on_release:app.home() ToggleButton: group:&#x27;touch_mode&#x27; state:&#x27;down&#x27; text:&quot;pan&quot; on_release: app.set_touch_mode(&#x27;pan&#x27;) self.state=&#x27;down&#x27; ToggleButton: group:&#x27;touch_mode&#x27; text:&quot;zoom box&quot; on_release: app.set_touch_mode(&#x27;zoombox&#x27;) self.state=&#x27;down&#x27; MatplotFigure: id:figure_wgt&#x27;&#x27;&#x27;class Test(App): lines = [] def build(self): self.screen = Builder.load_string(KV) return self.screen def on_start(self, *args): fig, ax1 = plt.subplots(1, 1) ax1.plot([0, 1, 2, 3, 4], [1, 2, 8, 9, 4], label=&#x27;line1&#x27;) ax1.plot([2, 8, 10, 15], [15, 0, 2, 4], label=&#x27;line2&#x27;) self.screen.figure_wgt.figure = fig def set_touch_mode(self, mode): self.screen.figure_wgt.touch_mode = mode def home(self): self.screen.figure_wgt.home()Test().run() 注：kivy_matplotlib_widget 包如果没有引入则会无法使用 MatplotFigure 组件，IDEA 可能因为代理的问题没能正确读取依赖。 对接 OpenCV 安装依赖： 1pip install opencv-python 编写如下程序即可： 123456789101112131415161718192021222324252627282930313233343536373839import cv2from kivy.app import Appfrom kivy.clock import Clockfrom kivy.graphics.texture import Texturefrom kivy.uix.image import Imageclass KivyCamera(Image): def __init__(self, capture, fps, **kwargs): super(KivyCamera, self).__init__(**kwargs) self.capture = capture Clock.schedule_interval(self.update, 1.0 / fps) def update(self, dt): ret, frame = self.capture.read() if ret: # convert it to texture buf1 = cv2.flip(frame, 0) buf = buf1.tostring() image_texture = Texture.create(size=(frame.shape[1], frame.shape[0]), colorfmt=&#x27;bgr&#x27;) image_texture.blit_buffer(buf, colorfmt=&#x27;bgr&#x27;, bufferfmt=&#x27;ubyte&#x27;) # display image from the texture self.texture = image_textureclass CamApp(App): def build(self): video_path = &#x27;video.mp4&#x27; self.capture = cv2.VideoCapture(video_path) self.my_camera = KivyCamera(capture=self.capture, fps=30) return self.my_camera def on_stop(self): # without this, app will not exit even if the window is closed self.capture.release()if __name__ == &#x27;__main__&#x27;: CamApp().run() 参考资料 Kivy 官网 KivyMD 官网 kivy-matplotlib-widget 官方项目","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Kivy","slug":"Kivy","permalink":"https://wangqian0306.github.io/tags/Kivy/"}]},{"title":"dive","slug":"docker/dive","date":"2024-09-27T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/dive/","permalink":"https://wangqian0306.github.io/2024/dive/","excerpt":"","text":"dive 简介 dive 是一个 Docker 工具，可以用来检索每个 Layer 中的内容。 安装和使用方式 RHEL/CentOS 123DIVE_VERSION=$(curl -sL &quot;https://api.github.com/repos/wagoodman/dive/releases/latest&quot; | grep &#x27;&quot;tag_name&quot;:&#x27; | sed -E &#x27;s/.*&quot;v([^&quot;]+)&quot;.*/\\1/&#x27;)curl -OL https://github.com/wagoodman/dive/releases/download/v$&#123;DIVE_VERSION&#125;/dive_$&#123;DIVE_VERSION&#125;_linux_amd64.rpmrpm -i dive_$&#123;DIVE_VERSION&#125;_linux_amd64.rpm 1dive &lt;image&gt; 注：之前一直对 GraalVM Native Image 的结构没有基础的认知，可以通过此工具查看具体容器的构成和目录情况。 参考资料 官方项目","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"Grafana Loki","slug":"tools/loki","date":"2024-09-06T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/loki/","permalink":"https://wangqian0306.github.io/2024/loki/","excerpt":"","text":"Grafana Loki 简介 Grafana 日志汇集的组件。在看 SpringOne 大会视频的时候了解到 Spring 对日志的处理方式。 安装 Docker 使用如下命令即可： 1234mkdir lokicd lokiwget https://raw.githubusercontent.com/grafana/loki/v3.0.0/production/docker-compose.yaml -O docker-compose.yamldocker-compose up -d Kubernetes 在 Kubernets 上安装分成了如下种安装方式： monolithic 单体 scalable 弹性(支持每日数 TB) microservice 微服务 具体流程参照 官方文档 Istio 在 Istio 环境下需要进行 额外配置 参考资料 官方文档 视频教程 样例代码和配置 Is Your JVM App Flying Blind? Unmask Issues With Observability Superpowers! (SpringOne 2024) Micrometer Mastery: Unleash Advanced Observability In Your JVM Apps (SpringOne 2024) Dapper, a Large-Scale Distributed Systems Tracing Infrastructure Dapper，大规模分布式系统的跟踪系统","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://wangqian0306.github.io/tags/Grafana/"},{"name":"Loki","slug":"Loki","permalink":"https://wangqian0306.github.io/tags/Loki/"},{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/tags/Spring/"}]},{"title":"Grafana Tempo","slug":"tools/tempo","date":"2024-09-06T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/tempo/","permalink":"https://wangqian0306.github.io/2024/tempo/","excerpt":"","text":"Grafana Tempo 简介 Grafana Tempo 是一个开源、易于使用且大规模的分布式跟踪后端。Tempo 允许您搜索跟踪、从 Span 生成指标，并将跟踪数据与日志和指标相关联。 安装 Kubernetes 具体流程参照 官方文档 参考资料 官方网站","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://wangqian0306.github.io/tags/Grafana/"},{"name":"Tempo","slug":"Tempo","permalink":"https://wangqian0306.github.io/tags/Tempo/"}]},{"title":"Spring Modulith","slug":"spring/modulith","date":"2024-09-04T13:32:58.000Z","updated":"2025-01-13T02:27:42.915Z","comments":true,"path":"2024/modulith/","permalink":"https://wangqian0306.github.io/2024/modulith/","excerpt":"","text":"Spring Modulith 简介 Spring Modulith 是一个工具包，用于构建领域驱动的模块化应用程序。Modulith 通过使用 ApplicationEvent 的方式来分离不同的程序模块。并且支持了如下事件的持久化记录： JPA JDBC MongoDB Neo4j 除了将事件写入数据库之外，还可以将数据写出到如下平台： Kafka AMQP JMS AWS SQS AWS SNS 使用 首先需要在 https://start.spring.io 上引入如下依赖： Spring Web Spring Data JPA MySQL Spring Modulith 注：此处建议使用 Maven 管理依赖，Gradle 在单元测试部分有 Bug 新建项目，然后创建 order 包，并在其中建立 package-info.java 文件： 12@org.springframework.lang.NonNullApipackage com.xxx.xxx.order; 创建 OrderCompleted.java 文件： 12345import java.util.UUID;import org.jmolecules.event.types.DomainEvent;public record OrderCompleted(UUID orderId) implements DomainEvent &#123;&#125; 创建 OrderManagement.java 文件： 12345678910111213141516171819import jakarta.annotation.Resource;import org.springframework.context.ApplicationEventPublisher;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import java.util.UUID;@Servicepublic class OrderManagement &#123; @Resource private ApplicationEventPublisher events; @Transactional public void complete() &#123; events.publishEvent(new OrderCompleted(UUID.randomUUID())); &#125;&#125; 创建 OrderController.java 文件： 123456789101112131415161718import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/order&quot;)public class OrderController &#123; @Resource private OrderManagement orderManagement; @PostMapping public void completeOrder() &#123; orderManagement.complete(); &#125;&#125; 创建 inventory 包，并在其中建立 package-info.java 文件： 12@org.springframework.lang.NonNullApipackage com.xxx.xxx.inventory; 创建 InventoryUpdated.java 文件： 1234import java.util.UUID;public record InventoryUpdated(UUID orderId) &#123;&#125; 创建 InventoryManagement.java 文件： 1234567891011121314151617181920212223242526import com.rainbowfish.motest.order.OrderCompleted;import jakarta.annotation.Resource;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.ApplicationEventPublisher;import org.springframework.modulith.events.ApplicationModuleListener;import org.springframework.stereotype.Service;@Serviceclass InventoryManagement &#123; private static final Logger LOG = LoggerFactory.getLogger(InventoryManagement.class); @Resource private ApplicationEventPublisher events; @ApplicationModuleListener void on(OrderCompleted event) throws InterruptedException &#123; var orderId = event.orderId(); LOG.info(&quot;Received order completion for &#123;&#125;.&quot;, orderId); Thread.sleep(1000); events.publishEvent(new InventoryUpdated(orderId)); LOG.info(&quot;Finished order completion for &#123;&#125;.&quot;, orderId); &#125;&#125; 之后就可以在 test 目录下新建单元测试 ModularityTests.java ： 123456789101112131415161718import org.junit.jupiter.api.Test;import org.springframework.modulith.core.ApplicationModules;import org.springframework.modulith.docs.Documenter;class ModularityTests &#123; ApplicationModules modules = ApplicationModules.of(Application.class); @Test void verifiesModularStructure() &#123; modules.verify(); &#125; @Test void createModuleDocumentation() &#123; new Documenter(modules).writeDocumentation(); &#125;&#125; 然后创建 ApplicationIntegrationTests.java 文件： 1234567891011121314151617181920212223242526272829import java.util.Collection;import com.rainbowfish.motest.order.OrderManagement;import com.rainbowfish.motest.inventory.InventoryUpdated;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.modulith.events.core.EventPublicationRegistry;import org.springframework.modulith.test.EnableScenarios;import org.springframework.modulith.test.Scenario;@SpringBootTest@EnableScenariosclass ApplicationIntegrationTests &#123; @Autowired OrderManagement orders; @Autowired EventPublicationRegistry registry; @Test void bootstrapsApplication(Scenario scenario) throws Exception &#123; scenario.stimulate(() -&gt; orders.complete()) .andWaitForStateChange(() -&gt; registry.findIncompletePublications(), Collection::isEmpty) .andExpect(InventoryUpdated.class) .toArrive(); &#125; &#125; 通过单元测试后可以编写 test.http 文件进行测试： 12###POST http://localhost:8080/order 参考资料 官方文档 示例代码 Bootiful Spring Boot (SpringOne 2024) Bootiful Spring Boot 3.4: Spring Modulith 官方样例 mplementing Domain Driven Design with Spring by Maciej Walkowiak @ Spring I/O 2024","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"oha","slug":"tools/oha","date":"2024-09-03T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/oha/","permalink":"https://wangqian0306.github.io/2024/oha/","excerpt":"","text":"oha 简介 在看 Spring IO 会议中看到演示使用的网页测试工具，看着还可以。 安装 使用如下命令即可安装： 1234wget https://github.com/hatoo/oha/releases/download/v1.4.6/oha-linux-amd64mv oha-linux-amd64 ohachmod a+x ohasudo mv oha /usr/local/bin/ 简单使用 可以通过 nginx 容器来快速使用 oha 123docker pull nginxdocker run --rm --name nginx-demo -p 8080:80 -d nginxoha http://localhost:8080 -z 10s 参考资料 官方项目","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"oha","slug":"oha","permalink":"https://wangqian0306.github.io/tags/oha/"}]},{"title":"操作系统","slug":"tmp/os","date":"2024-08-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/os/","permalink":"https://wangqian0306.github.io/2024/os/","excerpt":"","text":"操作系统 简介 在 B 站刷视频的时候发现自己大学的知识像没学一样，回头捡捡操作系统。 xv6 教学操作系统的安装 首先可以通过如下命令安装相关依赖并拉取项目进行编译 123456sudo apt updatesudo apt upgrade -y sudo apt install build-essential qemu-system -ygit clone https://github.com/mit-pdos/xv6-public.gitcd xv6-publicmake 在编译完成后即可使用如下命令进入系统： 1make qemu-nox 注：退出系统时则需要同时按住 Ctrl + A 然后松开，接着按下 X 即可退出虚拟机。 如果需要清除之前编译的内容则可以使用如下命令： 1make clean 进程 一个 xv6 进程由两部分组成，一部分是用户内存空间（指令，数据，栈），另一部分是仅对内核可见的进程状态。 而在 Linux 中也类似，进程内存空间分成以下两个不同区域： 内核空间 (Kernel Space)：这是操作系统（内核）的核心所在的内存受保护区域。内核负责管理硬件、调度任务以及为应用程序提供基本服务。内核空间是共享的，并且对于所有进程都是相同的。 用户空间 (User Space)：这是您的应用程序及其数据所在的位置。出于安全原因，应用程序无法直接访问内核空间。每个进程都有其自己的隔离用户空间，确保一个进程不会干扰另一个进程的内存。 对与每个进程来说，它的用户空间其实是一个内存沙箱(sandbox)。虚拟地址会通过页表映射到物理内存。具体结构如下： 注：样例图片是基于 32 位系统进行表述的，64 位系统暂时没找到说明。 虚拟内存和页表(Page tables) 在 xv6 中为了保证系统安全所以需要对进程的空间进行隔离。对于每个应用程序来说获取到的内容都是地址空间。通过地址管理单元（Memory Management Unit）或者其他的技术，可以将每个进程的虚拟地址空间映射到物理内存地址。 页表是一种在一个物理内存上，创建不同的地址空间的常见方法。其是在硬件中通过处理器和内存管理单元（Memory Management Unit）实现的。 对于任何一条带有地址的指令，其中的地址应该认为是虚拟内存地址而不是物理地址。假设寄存器 a0 中是地址 0x1000，那么这是一个虚拟内存地址。虚拟内存地址会被转到内存管理单元（MMU，Memory Management Unit）内存管理单元会将虚拟地址翻译成物理地址。之后这个物理地址会被用来索引物理内存，并从物理内存加载，或者向物理内存存储数据。 为了能够完成虚拟内存地址到物理内存地址的翻译，MMU会有一个表单，表单中，一边是虚拟内存地址，另一边是物理内存地址。通常来说，内存地址对应关系的表单也保存在内存中。 页表会将内存分解为页(Page)和偏移量(offset)，通过页参数定位区块，通过偏移量参数定位字节。页参数+偏移量参数的总长度代表了实际支持的内存大小，一一般来说每个页的大小是 4 KB ，故偏移量参数就可以确定是 12 位。在处理器架构上会针对虚拟地址进行不同的长度设计。当页的数量过多的时候内存也会有不少的空间被浪费，所以可以通过将页参数进行分块处理例如可以将 27 位长度的分成 L2 , L1 , L0 三级。L2,L1 可以当作索引进行处理，L2 存储了 L1 的地址空间 L1 存储了 L0 的存储空间。 因为存在多级页表所以在检索的时候会调用多次页表，为了加速检索逻辑，处理器会使用页表缓存(Translation Lookaside Buffer) 将检索过的数载入缓存中。为了实现缓存的更新逻辑会流出缓存清除的指令。 参考资料 MIT 公开课 6.S081 操作系统工程 xv6-public 源码 MIT6.S081 视频中文翻译文本 MIT6.S081 教案 MIT6.S081 教案中文翻译 Memory Layout of Kernel and UserSpace in Linux. Linux 内核相关书籍 MIT 公开课 6.824 分布式系统","categories":[],"tags":[{"name":"OS","slug":"OS","permalink":"https://wangqian0306.github.io/tags/OS/"}]},{"title":"Gemma 大模型","slug":"ai/gemma","date":"2024-08-10T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/gemma/","permalink":"https://wangqian0306.github.io/2024/gemma/","excerpt":"","text":"Gemma 大模型 简介 在逛 B 站的时候看到了 Gemma 的相关视频，在此处进行一下记录。 参考资料 Gemma 模型：最新进展揭秘 Gemma Cookbook 官方文档 注：LoRA 微调文档一直无法访问，去找了其他文档。 Fine-tuning Gemma 2 with Keras - and an update from Hugging Face","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"}]},{"title":"Chrome 启动主页被拦截","slug":"windows/chrome-homepage","date":"2024-08-10T12:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2024/chrome-homepage/","permalink":"https://wangqian0306.github.io/2024/chrome-homepage/","excerpt":"","text":"Chrome 启动主页被拦截 简介 今天突然发现打开 Chrome 后自动访问了广告页面，先对此问题进行记录。 解决方式 按下 win + R 打开运行工具，然后输入下面的内容进入注册表编辑器 1regedit 然后检索如下位置： 1HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run 之后就可以在注册表中看到如下项目： 1&lt;user&gt; cmd.exe /c start http://xxx.xxx.xxx 删除完成后还需要重新打开运行工具，然后输入下面的内容进入任务计划程序： 1taskschd.msc 在计划程序中也会出现自己用户名对应的内容，将其删除即可。 参考资料 开机cmd窗口自动打开dinoklafbzor的解决方法 (Solved) How to Remove dinoklafbzor.org Virus Pop-up?","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/tags/Windows/"}]},{"title":"RAGFlow","slug":"ai/ragflow","date":"2024-08-09T08:17:00.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/ragflow/","permalink":"https://wangqian0306.github.io/2024/ragflow/","excerpt":"","text":"RAGFlow 简介 RAGFlow 是一款基于深度文档理解构建的开源 RAG（Retrieval-Augmented Generation）引擎。RAGFlow 可以为各种规模的企业及个人提供一套精简的 RAG 工作流程，结合大语言模型（LLM）针对用户各类不同的复杂格式数据提供可靠的问答以及有理有据的引用。 使用方式 使用如下命令暂时设置 vm.max_map_count 配置： 1sudo sysctl -w vm.max_map_count=262144 修改默认系统配置： 1sudo vim /etc/sysctl.conf 新增或修改以下配置项： 1vm.max_map_count=262144 设置完成后可以使用如下命令进行检查： 1sysctl vm.max_map_count 之后即可拉取项目，并修改环境配置项： 1234git clone https://github.com/infiniflow/ragflow.gitcd ragflow/dockerchmod +x ./entrypoint.shvim .env 然后编辑如下配置项到最新的版本： 1RAGFLOW_VERSION=dev 使用如下命令启动服务并查看启动的容器日志： 12docker-compose up -ddocker-compose logs -f 注：docker 有 10G 多拉起来比较花时间。 启动完成后即可访问 http://localhost 即可。 在进入页面后即可配置 Ollama 地址和模型。 参考资料 项目源码 官方文档","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"SequenceServer","slug":"ocean/gene","date":"2024-08-08T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/sequenceserver/","permalink":"https://wangqian0306.github.io/2024/sequenceserver/","excerpt":"","text":"SequenceServer 简介 SequenceServer 是基于 BLAST 的一款基因检索服务。 使用方式 首先需要拉取镜像： 1docker pull wurmlab/sequenceserver:latest 然后下载样例基因文件： 1234mkdir dbwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gzgunzip GCF_000005845.2_ASM584v2_genomic.fna.gzmv GCF_000005845.2_ASM584v2_genomic.fna ./db/demo.fna 然后根据基因库文件生成索引： 1docker run --rm -v /xxx/db:/db -it wurmlab/sequenceserver:latest /sequenceserver/bin/sequenceserver -m 默认的配置只支持同时运行一个查询任务，需要修改配置文件 sequenceserver.conf。 1234567891011121314151617181920212223242526272829303132333435363738---:host: 0.0.0.0:port: 4567:databases_widget: classic:options: :blastn: :default: :description: :attributes: - &quot;-task blastn&quot; - &quot;-evalue 1e-5&quot; :blastp: :default: :description: :attributes: - &quot;-evalue 1e-5&quot; :blastx: :default: :description: :attributes: - &quot;-evalue 1e-5&quot; :tblastx: :default: :description: :attributes: - &quot;-evalue 1e-5&quot; :tblastn: :default: :description: :attributes: - &quot;-evalue 1e-5&quot;:num_threads: 5:num_jobs: 5:job_lifetime: 43200:cloud_share_url: https://share.sequenceserver.com/api/v1/shared-job:large_result_warning_threshold: 262144000:optimistic: false:database_dir: &quot;/db&quot; 注：job_lifetime 代表查询的保存时间，单位是分钟，默认是 30 天。 之后可以编写 docker-compose.yaml 文件管理服务： 12345678services: sequenceserver: image: wurmlab/sequenceserver:latest ports: - &quot;4567:4567&quot; volumes: - &quot;./db:/db&quot; - &quot;./sequenceserver.conf:/root/.sequenceserver.conf&quot; 使用如下命令运行文件即可： 1docker-compose up -d 之后访问 http://localhost:4567 在文本输入框内进行检索即可，样例如下： 12&gt;demoCTCCTAAAGGGCCCAGCAAGACCAGCTGGTTGATAGGTCGGATGTGGACGCGCTGCAAGGCGTTGAGCTAACCGATACTA 如果有需要也可以使用接口调用数据请参照 接口文档，例如使用 curl ： 获取数据库 ID 用于检索： 1curl $BASEURL/searchdata.json 发起搜索请求： 1jobUrl=$(curl -v -X POST -Fsequence=ATGTTACCACCAACTATTAGAATTTCAG -Fmethod=blastn -Fdatabases[]=3c0a5bc06f2596698f62c7ce87aeb62a --write-out &#x27;%&#123;redirect_url&#125;&#x27; $BASEURL) 注：CSRF 如果异常可以注释掉 lib/sequenceserver/routes.rb 中的 use Rack::Csrf, raise: true, skip: ['POST:/cloud_share'] 参考资料 官方网站 项目源码 接口文档","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"GENE","slug":"GENE","permalink":"https://wangqian0306.github.io/tags/GENE/"}]},{"title":"数据合并","slug":"spring/merge","date":"2024-08-06T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/merge/","permalink":"https://wangqian0306.github.io/2024/merge/","excerpt":"","text":"数据合并 简介 在使用多种来源的数据时，可能会涉及到性能问题，此处针对 WebClient 和 RestClient 方式进行试用。 实现方式 WebClient 编写 WebClientController.java 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.reactive.function.client.WebClient;import reactor.core.publisher.Mono;import java.math.BigDecimal;import java.util.List;import java.util.concurrent.CompletableFuture;import java.util.stream.Collectors;@RestController@RequestMapping(&quot;/api/web-client&quot;)public class WebClientController &#123; private static final Logger log = LoggerFactory.getLogger(WebClientController.class); private final WebClient webClient; public WebClientController(WebClient.Builder builder) &#123; this.webClient = builder.baseUrl(&quot;https://api.open-meteo.com/v1/forecast&quot;).build(); &#125; private CompletableFuture&lt;OpenMeteoResponse&gt; getCMA(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start cma &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); CompletableFuture&lt;OpenMeteoResponse&gt; cache = this.webClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;cma_grapes_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().bodyToMono(OpenMeteoResponse.class).toFuture(); long endTime = System.currentTimeMillis(); log.error(&quot;end cma &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private CompletableFuture&lt;OpenMeteoResponse&gt; getGFS(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start gfs &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); CompletableFuture&lt;OpenMeteoResponse&gt; cache = this.webClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;gfs_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().bodyToMono(OpenMeteoResponse.class).toFuture(); long endTime = System.currentTimeMillis(); log.error(&quot;end gfs &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private CompletableFuture&lt;OpenMeteoResponse&gt; getIcon(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start gfs &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); CompletableFuture&lt;OpenMeteoResponse&gt; cache = this.webClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;icon_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().bodyToMono(OpenMeteoResponse.class).toFuture(); long endTime = System.currentTimeMillis(); log.error(&quot;end gfs &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private CompletableFuture&lt;OpenMeteoResponse&gt; getGraphCast(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start gfs &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); CompletableFuture&lt;OpenMeteoResponse&gt; cache = this.webClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;gfs_graphcast025&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().bodyToMono(OpenMeteoResponse.class).toFuture(); long endTime = System.currentTimeMillis(); log.error(&quot;end gfs &#123;&#125;&quot;, endTime - startTime); return cache; &#125; public Mono&lt;List&lt;OpenMeteoResponse&gt;&gt; getMergedData(BigDecimal lat, BigDecimal lon) &#123; List&lt;CompletableFuture&lt;OpenMeteoResponse&gt;&gt; futures = List.of( getCMA(lat, lon), getGFS(lat, lon), getIcon(lat, lon), getGraphCast(lat, lon) ); CompletableFuture&lt;Void&gt; allOf = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])); return Mono.fromFuture(allOf.thenApply(v -&gt; futures.stream() .map(CompletableFuture::join) .collect(Collectors.toList()))); &#125; @GetMapping private Mono&lt;List&lt;OpenMeteoResponse&gt;&gt; getOpenMeteoResponse(@RequestParam(defaultValue = &quot;52.5625&quot;) BigDecimal lat, @RequestParam(defaultValue = &quot;13.375&quot;) BigDecimal lon) &#123; return getMergedData(lat, lon); &#125;&#125; 之后即可使用 test.http 文件进行测试： 123###GET http://localhost:8080/api/web-client RestClient 编写 RestClientController.java 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestClient;import java.math.BigDecimal;import java.util.ArrayList;import java.util.List;@RestController@RequestMapping(&quot;/api/rest-client&quot;)public class RestClientController &#123; private static final Logger log = LoggerFactory.getLogger(RestClientController.class); private final RestClient restClient; public RestClientController(RestClient.Builder builder) &#123; this.restClient = builder.baseUrl(&quot;https://api.open-meteo.com/v1/forecast&quot;).build(); &#125; private OpenMeteoResponse getCMA(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start cma &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); OpenMeteoResponse cache = this.restClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;cma_grapes_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().body(OpenMeteoResponse.class); long endTime = System.currentTimeMillis(); log.error(&quot;end cma &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private OpenMeteoResponse getGFS(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start gfs &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); OpenMeteoResponse cache = this.restClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;gfs_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().body(OpenMeteoResponse.class); long endTime = System.currentTimeMillis(); log.error(&quot;end gfs &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private OpenMeteoResponse getICON(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start icon &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); OpenMeteoResponse cache = this.restClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;icon_global&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().body(OpenMeteoResponse.class); long endTime = System.currentTimeMillis(); log.error(&quot;end icon &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private OpenMeteoResponse getGraphCast(BigDecimal lat, BigDecimal lon) &#123; log.error(&quot;start graphcast &#123;&#125;&quot;, Thread.currentThread().getName()); long startTime = System.currentTimeMillis(); OpenMeteoResponse cache = this.restClient.get().uri( uriBuilder -&gt; uriBuilder .queryParam(&quot;latitude&quot;, lat) .queryParam(&quot;longitude&quot;, lon) .queryParam(&quot;hourly&quot;, &quot;temperature_2m&quot;) .queryParam(&quot;timezone&quot;, &quot;Asia/Shanghai&quot;) .queryParam(&quot;models&quot;, &quot;gfs_graphcast025&quot;) .queryParam(&quot;forecast_days&quot;, &quot;1&quot;) .build() ).retrieve().body(OpenMeteoResponse.class); long endTime = System.currentTimeMillis(); log.error(&quot;end graphcast &#123;&#125;&quot;, endTime - startTime); return cache; &#125; private List&lt;OpenMeteoResponse&gt; getMergedData(BigDecimal lat, BigDecimal lon) &#123; List&lt;OpenMeteoResponse&gt; data = new ArrayList&lt;&gt;(); data.add(getCMA(lat, lon)); data.add(getGFS(lat, lon)); data.add(getICON(lat, lon)); data.add(getGraphCast(lat, lon)); return data; &#125; @GetMapping private List&lt;OpenMeteoResponse&gt; getOpenMeteoResponse(@RequestParam(defaultValue = &quot;52.5625&quot;) BigDecimal lat, @RequestParam(defaultValue = &quot;13.375&quot;) BigDecimal lon) &#123; return getMergedData(lat, lon); &#125;&#125; 之后即可使用 test.http 文件进行测试： 123### GET http://localhost:8080/api/rest-client 总结 经过多次测试 WebClient 有明显的性能优势。 注：在测试时可以多选几个点避免缓存问题，或者多次测试取平均值。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spring gzip","slug":"spring/gzip","date":"2024-07-29T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/gzip/","permalink":"https://wangqian0306.github.io/2024/gzip/","excerpt":"","text":"Spring gzip 简介 遇到了需要返回大型 JSON 的需求，所以想对 JSON 进行一些压缩，降低带宽压力。 实现方式 编写如下 application.yaml 即可： 1234server: compression: enabled: true min-response-size: 1024 注：在返回是 String 的情况下正常生效，但是在返回 Record 时默认压缩了，具体业务类定位到了 org.apache.coyote.CompressionConfig, 可能是在返回时没有读取到报文长度的问题。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"视频服务器","slug":"tmp/video","date":"2024-07-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/video-server/","permalink":"https://wangqian0306.github.io/2024/video-server/","excerpt":"","text":"视频服务器 简介 为了实现对接多个视频源并对外提供统一协议和接口，现针对市面上的视频服务器和协议进行了初步调研。 目前的解决方案： 现有的网络摄像头等大多采用 RTSP 协议 现有的视频会议服务大多采用 WebRTC 协议 现有的直播平台大多采用 RTMP 协议 服务搭建 视频推流 可以使用 ffmpeg 命令进行推流： 1ffmpeg -re -i input.mp4 -c:v libx264 -c:a aac -f flv rtmp://your_rtmp_server/live/stream_key 1ffmpeg -re -i input.mp4 -c:v libx264 -c:a aac -f rtsp rtsp://your_rtsp_server/live/stream_key 参考资料 Janus WebRTC Server Ant Media Server MediaMTX","categories":[],"tags":[{"name":"Video","slug":"Video","permalink":"https://wangqian0306.github.io/tags/Video/"}]},{"title":"AIS","slug":"ocean/ais","date":"2024-07-15T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/ais/","permalink":"https://wangqian0306.github.io/2024/ais/","excerpt":"","text":"AIS 简介 自动识别系统（Automatic Identification System，AIS），是安装在船舶上的一套自动追踪系统，借由与邻近船舶、AIS岸台、以及卫星等设备交换电子资料，并且供船舶交通管理系统辨识及定位。 使用 总体上来说国内的实现方式是遍历船只清单生成船只图层，然后在细节图层中再去检索船只位置。 国外的实现方式则是根据不同的块去检索船只清单。 参考资料 DMA AisLib rtl-ais 导助航综合服务系统 船讯网 MarineTraffic","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"AIS","slug":"AIS","permalink":"https://wangqian0306.github.io/tags/AIS/"}]},{"title":"antismash","slug":"ocean/antismash","date":"2024-07-11T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/antismash/","permalink":"https://wangqian0306.github.io/2024/antismash/","excerpt":"","text":"简介 antiSMASH 框架允许检测基因组中共存的生物合成基因簇，称为生物合成基因簇 (BGC)。 安装方式 Docker 创建本地卷路径 123mkdir inputmkdir outputwget -P output https://github.com/antismash/antismash/blob/master/antismash/test/integration/data/nisin.gbk 然后编写如下 docker-compose.yaml 文件即可 12345678services: anti: image: antismash/standalone:latest command: [nisin.gbk] user: &lt;uid&gt;:&lt;gid&gt; volumes: - &quot;./input:/input&quot; - &quot;./output:/output&quot; 使用如下命令，等待命令完成后，即可在 /output 目录获取到处理结果。 1docker-compose up 如果需要进入容器中试用其他工具可以使用如下命令： 1docker run --rm -it --entrypoint=&quot;&quot; antismash/standalone:latest /bin/bash 系统安装(失败) 系统基于 Ubuntu 22.04.4 LTS 1234567891011121314151617pyenv install anaconda3-2022.10pyenv local anaconda3-2022.10conda config --add channels defaultsconda config --add channels biocondaconda config --add channels conda-forgeconda config --set channel_priority strictsudo apt updatesudo apt install build-essential zlib1g-devsudo apt-get install hmmer2 hmmer diamond-aligner fasttree prodigal ncbi-blast+ muscleconda install bioconda::glimmerhmmconda install libgcc-ngconda install bioconda::meme=4.11.2wget https://dl.secondarymetabolites.org/releases/6.0.0/antismash-6.0.0.tar.gztar -xvf antismash-6.0.0.tar.gzpip install ./antismash-6.0.0download-antismash-databasesantismash --check-prereqs 注：由于 meme 版本不对所以此处方案没能成功安装，如果后续 antiSMASH 版本更新可以再尝试。 参考资料 官方手册 下载网站","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"antiSMASH ","slug":"antiSMASH","permalink":"https://wangqian0306.github.io/tags/antiSMASH/"}]},{"title":"Tropycal","slug":"ocean/tropycal","date":"2024-07-09T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/tropycal/","permalink":"https://wangqian0306.github.io/2024/tropycal/","excerpt":"","text":"Tropycal 简介 Tropycal 是一个 Python 包，用于检索和分析过去和实时的热带气旋数据。 Tropycal 可以读取 HURDAT2 和 IBTrACS 再分析数据以及美国国家飓风中心(NCAR)的追踪数据，并使它们符合相同的格式，可用于执行气候、季节和个别风暴分析。 使用方式 使用如下命令安装依赖： 123pip install tropycalpip install shapelypip install cartopy 编写如下脚本即可访问当前的气旋数据： 123456from tropycal import realtimerealtime_obj = realtime.Realtime()for storm in realtime_obj.list_active_storms(): print(realtime_obj.get_storm(storm).to_dict()) 可以编写如下代码获取到预测内容： 123456from tropycal import realtimerealtime_obj = realtime.Realtime()for storm in realtime_obj.list_active_storms(): print(realtime_obj.get_storm(storm).get_forecast_realtime()) 由于项目原始的设计问题，导致在转 json 时会出现很多的异常值，可以利用如下思路处理： 123456789101112131415161718192021222324252627import jsonimport mathfrom tropycal import realtimedef set_value(value_list: list): cache = [] for i in value_list: if isinstance(i, float): if math.isnan(i): cache.append(None) else: cache.append(i) return cacherealtime_cache = realtime.Realtime()storm_list = realtime_cache.list_active_storms()result = []for storm in storm_list: ele = realtime_cache.get_storm(storm).to_dict() for key in ele.keys(): if isinstance(ele[key], list): ele[key] = set_value(ele[key]) result.append(ele)print(json.dumps(result)) 参考资料 官方项目 官方文档 HFSA 预测 HFSA 数据","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Tropycal","slug":"Tropycal","permalink":"https://wangqian0306.github.io/tags/Tropycal/"}]},{"title":"Open-Meteo","slug":"ocean/open-meteo","date":"2024-07-04T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/open-meteo/","permalink":"https://wangqian0306.github.io/2024/open-meteo/","excerpt":"","text":"Open-Meteo 简介 Open-Meteo 是一个开源天气 API，可供非商业用途免费使用。无需 API 密钥。其中包含很多预测模型的数据。 使用方式 可以编写如下 docker-compose.yaml 使用： 12345678services: open-meteo: image: ghcr.io/open-meteo/open-meteo volumes: - &quot;./data:/app/data&quot; ports: - &#x27;8080:8080&#x27; command: [&quot;serve&quot;, &quot;--env&quot;, &quot;production&quot;, &quot;--hostname&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8080&quot;] 数据同步 使用 openmeteo-api sync 命令可以从 AWS S3 上下载最新的数据集到本地，此处可以自己构建一个镜像利用 cron 命令实现定时拉取数据。 编写 sync-cron ： 12* * * * * /usr/local/bin/openmeteo-api sync ecmwf_ifs04 temperature_2m &gt;&gt; /var/log/cron.log 2&gt;&amp;1 编写 Dockerfile ： 123456789101112131415161718192021FROM ubuntu:jammyRUN apt-get update &amp;&amp; apt-get -y install cron gpg curlRUN curl -L https://apt.open-meteo.com/public.key | gpg --dearmour -o /etc/apt/trusted.gpg.d/openmeteo.gpgRUN echo &quot;deb [arch=amd64] https://apt.open-meteo.com jammy main&quot; | tee /etc/apt/sources.list.d/openmeteo-api.listRUN apt-get updateRUN apt-get install openmeteo-api -yCOPY sync-cron /etc/cron.d/sync-cronRUN chmod 0644 /etc/cron.d/sync-cronRUN crontab /etc/cron.d/sync-cronRUN touch /var/log/cron.logCMD cron &amp;&amp; tail -f /var/log/cron.log 之后编写 docker-compose.yaml ： 12345678910111213services: open-meteo: image: ghcr.io/open-meteo/open-meteo volumes: - &quot;./data:/app/data&quot; ports: - &#x27;8080:8080&#x27; command: [&quot;serve&quot;, &quot;--env&quot;, &quot;production&quot;, &quot;--hostname&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8080&quot;] sync: build: . image: cron-meteo:latest volumes: - &quot;./data:/var/lib/openmeteo-api/data&quot; 使用如下命令构建容器： 1docker-compose build --no-cache 使用如下命令运行容器： 1docker-compose up -d 参考资料 官网 同步教程","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"MySQL InnoDB Cluster","slug":"database/mysql-innodb-cluster","date":"2024-07-02T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2024/mysql-innodb-cluster/","permalink":"https://wangqian0306.github.io/2024/mysql-innodb-cluster/","excerpt":"","text":"MySQL InnoDB Cluster 简介 MySQL 的一种高可用方案，具体实现方式参见下图： 服务部署 容器版 可以试用 mysql-docker-compose-examples 项目。 注：示例项目的 MySQL Router 只开放了 6446 端口，此端口仅供读写使用，还可以开放 6447 端口，此端口负责只读请求。 本地部署 注：此处需要提前搭建好 MySQL 服务器，可以参照 MySQL 文档。 使用如下命令进入交互式命令行： 1mysqlsh 使用如下命令即可链接到数据库，并将其初始化： 12shell.connect(&#x27;root@localhost:3306&#x27;)dba.configureInstance() 在所有节点被初始化完成后即可使用如下命令创建 InnoDB 集群： 1dba.createCluster(&#x27;devCluster&#x27;) 然后使用如下命令添加实例至集群: 1dba.getCluster().addInstance(&#x27;root@mysql-2:3306&#x27;) 注：此处需要输入密码并选择恢复方式，建议在刚搭建的服务中采用 Clone 。配置完成后需要重启服务，如果使用容器记得配置持久卷。 之后可以采用如下命令检查集群状态： 1dba.getCluster().status() 测试 负载均衡 使用如下命令安装依赖： 1pip install sqlalchemy mysql-connector-python 12345678910111213141516171819from sqlalchemy import create_engine, textfrom sqlalchemy.orm import sessionmakerDATABASE_URI = &#x27;mysql+mysqlconnector://root:mysql@localhost:6447&#x27;engine = create_engine( DATABASE_URI, pool_size=10, max_overflow=20, pool_recycle=3600, pool_pre_ping=True)Session = sessionmaker(bind=engine)session = Session()query = text(&#x27;SELECT @@server_id as server_id&#x27;)result = session.execute(query).fetchone()print(&quot;Server ID&quot; + result[0])session.close() 多次运行此服务即可看到，请求被传输到了不同的设备上执行。 参考资料 MySQL 官方文档 mysql-docker-compose-examples","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"},{"name":"MySQL Router","slug":"MySQL-Router","permalink":"https://wangqian0306.github.io/tags/MySQL-Router/"}]},{"title":"Spring MQTT","slug":"spring/mqtt","date":"2024-06-19T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/spring-mqtt/","permalink":"https://wangqian0306.github.io/2024/spring-mqtt/","excerpt":"","text":"Spring MQTT 简介 Spring 提供了 Message Queueing Telemetry Transport(MQTT) 协议的插件。 搭建环境 编写配置文件 config/mosquitto.conf ： 12listener 1883allow_anonymous true 编写 docker-compose.yaml 123456789services: mqtt-broker: image: eclipse-mosquitto:latest container_name: mqtt-broker ports: - &quot;1883:1883&quot; - &quot;9001:9001&quot; volumes: - ./config:/mosquitto/config 使用 安装依赖(maven)： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt; &lt;artifactId&gt;spring-integration-mqtt&lt;/artifactId&gt; &lt;version&gt;6.3.1&lt;/version&gt;&lt;/dependency&gt; 安装依赖(gradle)： 1compile &quot;org.springframework.integration:spring-integration-mqtt:6.3.1&quot; 编写配置类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import org.eclipse.paho.client.mqttv3.MqttConnectOptions;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.integration.annotation.ServiceActivator;import org.springframework.integration.channel.DirectChannel;import org.springframework.integration.core.MessageProducer;import org.springframework.integration.mqtt.core.DefaultMqttPahoClientFactory;import org.springframework.integration.mqtt.core.MqttPahoClientFactory;import org.springframework.integration.mqtt.inbound.MqttPahoMessageDrivenChannelAdapter;import org.springframework.integration.mqtt.outbound.MqttPahoMessageHandler;import org.springframework.integration.mqtt.support.DefaultPahoMessageConverter;import org.springframework.messaging.MessageChannel;import org.springframework.messaging.MessageHandler;import org.springframework.integration.annotation.MessagingGateway;@Configurationpublic class MqttConfig &#123; @Bean public MqttPahoClientFactory mqttClientFactory() &#123; DefaultMqttPahoClientFactory factory = new DefaultMqttPahoClientFactory(); MqttConnectOptions options = new MqttConnectOptions(); options.setServerURIs(new String[] &#123; &quot;tcp://192.168.2.235:1883&quot;&#125;);// options.setUserName(&quot;username&quot;);// options.setPassword(&quot;password&quot;.toCharArray()); factory.setConnectionOptions(options); return factory; &#125; @Bean @ServiceActivator(inputChannel = &quot;mqttOutboundChannel&quot;) public MessageHandler mqttOutbound() &#123; MqttPahoMessageHandler messageHandler = new MqttPahoMessageHandler(&quot;writeClient&quot;, mqttClientFactory()); messageHandler.setAsync(true); messageHandler.setDefaultTopic(&quot;topic1&quot;); return messageHandler; &#125; @Bean public MessageChannel mqttOutboundChannel() &#123; return new DirectChannel(); &#125; @MessagingGateway(defaultRequestChannel = &quot;mqttOutboundChannel&quot;) public interface MyGateway &#123; void sendToMqtt(String data); &#125; @Bean public MessageChannel mqttInputChannel() &#123; return new DirectChannel(); &#125; @Bean public MessageProducer inbound() &#123; MqttPahoMessageDrivenChannelAdapter adapter = new MqttPahoMessageDrivenChannelAdapter(&quot;readClient&quot;, mqttClientFactory(), &quot;topic1&quot;); adapter.setCompletionTimeout(5000); adapter.setConverter(new DefaultPahoMessageConverter()); adapter.setQos(1); adapter.setOutputChannel(mqttInputChannel()); return adapter; &#125; @Bean @ServiceActivator(inputChannel = &quot;mqttInputChannel&quot;) public MessageHandler handler() &#123; return message -&gt; System.out.println(message.getPayload() + &quot; message received&quot;); &#125;&#125; 编写测试类： 123456789101112131415161718import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource private MqttConfig.MyGateway myGateway; @GetMapping public String test() &#123; myGateway.sendToMqtt(&quot;wqnice&quot;); return &quot;ok&quot;; &#125;&#125; 参考资料 官方文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"MQTT","slug":"MQTT","permalink":"https://wangqian0306.github.io/tags/MQTT/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Leaflet","slug":"frount/leaflet","date":"2024-06-14T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/leaflet/","permalink":"https://wangqian0306.github.io/2024/leaflet/","excerpt":"","text":"Leaflet 简介 Leaflet 是一个交互式的地图库。为了将其对接进入 Next.js 还需要额外安装 React Leaflet 库。 安装和使用 安装依赖库： 12npm install leaflet react-leaflet leaflet-defaulticon-compatibilitynpm install -D @types/leaflet 编写 components/Map.tsx ： tsx123456789101112131415161718192021222324252627282930313233&quot;use client&quot;;// IMPORTANT: the order matters!import &quot;leaflet/dist/leaflet.css&quot;;import &quot;leaflet-defaulticon-compatibility/dist/leaflet-defaulticon-compatibility.webpack.css&quot;;import &quot;leaflet-defaulticon-compatibility&quot;;import &#123; MapContainer, Marker, Popup, TileLayer &#125; from &quot;react-leaflet&quot;;import &#123;LatLngExpression&#125; from &quot;leaflet&quot;;export default function Map() &#123; const position:LatLngExpression = [51.505, -0.09] return ( &lt;MapContainer center=&#123;position&#125; zoom=&#123;11&#125; scrollWheelZoom=&#123;true&#125; style=&#123;&#123; height: &quot;400px&quot;, width: &quot;600px&quot; &#125;&#125; attributionControl=&#123;false&#125; &gt; &lt;TileLayer attribution=&#x27; &#x27; url=&quot;https://&#123;s&#125;.tile.openstreetmap.org/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.png&quot; /&gt; &lt;Marker position=&#123;position&#125;&gt; &lt;Popup&gt; This Marker icon is displayed correctly with &lt;i&gt;leaflet-defaulticon-compatibility&lt;/i&gt;. &lt;/Popup&gt; &lt;/Marker&gt; &lt;/MapContainer&gt; );&#125; 编写 test/page.tsx ： tsx12345678910111213141516&quot;use client&quot;;import dynamic from &quot;next/dynamic&quot;;const LazyMap = dynamic(() =&gt; import(&quot;@/components/Map&quot;), &#123; ssr: false, loading: () =&gt; &lt;p&gt;Loading...&lt;/p&gt;,&#125;);export default function Home() &#123; return ( &lt;main&gt; &lt;LazyMap /&gt; &lt;/main&gt; );&#125; 自定义插件 tsx123456789101112131415161718192021222324252627282930313233343536import L from &quot;leaflet&quot;;import &#123;createLayerComponent, LayerProps &#125; from &quot;@react-leaflet/core&quot;;import &#123; ReactNode &#125; from &quot;react&quot;;interface KittenProps extends LayerProps &#123; userId: string, children?: ReactNode // PropsWithChildren is not exported by @react-leaflet/core&#125;class Kitten extends L.TileLayer &#123; // getTileUrl(coords: L.Coords) &#123; var i = Math.ceil( Math.random() * 4 ); return &quot;https://placekitten.com/256/256?image=&quot; + i; &#125; getAttribution() &#123; return &quot;&lt;a href=&#x27;https://placekitten.com/attribution.html&#x27;&gt;PlaceKitten&lt;/a&gt;&quot; &#125;&#125;const createKittenLayer = (props: KittenProps, context:any) =&gt; &#123; const instance = new Kitten(&quot;placeholder&quot;, &#123;...props&#125;); return &#123;instance, context&#125;;&#125;const updateKittenLayer = (instance: any, props: KittenProps, prevProps: KittenProps) =&gt; &#123; if (prevProps.userId !== props.userId) &#123; if (instance.setUserId) instance.setUserId(props.userId) &#125;&#125;const KittenLayer = createLayerComponent(createKittenLayer, updateKittenLayer);export default KittenLayer; 注：在使用这种方式之后 IDEA 会报错，但是程序是可以正常运行的，是 leaflet 源码的问题。 参考文件 参考资料 Leaflet React Leaflet Nextjs with react-leaflet How to extend TileLayer component in react-leaflet v3?","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"},{"name":"Leaflet","slug":"Leaflet","permalink":"https://wangqian0306.github.io/tags/Leaflet/"},{"name":"React Leaflet","slug":"React-Leaflet","permalink":"https://wangqian0306.github.io/tags/React-Leaflet/"}]},{"title":"NextAuth.js","slug":"frount/next-auth","date":"2024-05-24T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/next-auth/","permalink":"https://wangqian0306.github.io/2024/next-auth/","excerpt":"","text":"NextAuth.js 简介 NextAuth.js 是 Next.js 应用程序的完整开源身份验证解决方案。 使用方式 首先需要安装依赖包： 1npm install next-auth@beta 注：此处由于 5.0 还没发布所以采用 beta 版本。在部署的时候有一些缓存 bug 可以降级 next.js 版本至 14.1.4 。 然后使用如下命令生成 AUTH_SECRET 1npx auth secret 编写 .env.local 文件： 123AUTH_SECRET=&lt;auth_secret&gt;AUTH_&lt;OAuth_Provider&gt;_ID=&lt;oauth_client_id&gt;AUTH_&lt;OAuth_Provider&gt;_SECRET=&lt;oauth_secret&gt; 注：测试服务使用 GitHub 作为服务提供端，服务创建请访问 示例文档 并填写好 .env.local 文件。 编写 auth/index.ts 文件： 123456789101112131415import NextAuth, &#123;NextAuthConfig&#125; from &quot;next-auth&quot;import GitHub from &quot;@auth/core/providers/github&quot;;export const BASE_PATH = &quot;/api/auth&quot;const authOptions:NextAuthConfig = &#123; providers: [ GitHub, ], basePath: BASE_PATH, secret: process.env.AUTH_SECRET, debug: process.env.NODE_ENV !== &quot;production&quot;&#125;export const &#123;handlers, auth, signIn, signOut&#125; = NextAuth(authOptions); 编写 middleware.ts : 1234567891011121314151617181920import &#123; NextResponse &#125; from &quot;next/server&quot;;import &#123; auth, BASE_PATH &#125; from &quot;@/auth&quot;;export const config = &#123; matcher: [&quot;/((?!api|_next/static|_next/image|favicon.ico).*)&quot;],&#125;;export default auth((req) =&gt; &#123; const reqUrl = new URL(req.url); if (!req.auth &amp;&amp; reqUrl?.pathname !== &quot;/&quot;) &#123; return NextResponse.redirect( new URL( `$&#123;BASE_PATH&#125;/signin?callbackUrl=$&#123;encodeURIComponent( reqUrl?.pathname )&#125;`, req.url ) ); &#125;&#125;); 注：此处可以编写若未登录之后的重定向和路由保存逻辑。 编写 api/auth/[...nextauth]/route.ts ： 123import &#123; handlers &#125; from &quot;@/auth&quot;;export const &#123; GET, POST &#125; = handlers; 常用方式 自定义 OAuth 服务器 在 auth.ts 中修改 12345678910111213141516171819202122import &#123;OAuthConfig&#125; from &quot;@auth/core/providers&quot;;const customProvider: OAuthConfig&lt;any&gt; = &#123; id: &#x27;demo&#x27;, name: &#x27;demo&#x27;, type: &#x27;oauth&#x27;, authorization: &#123; url: &quot;http://&lt;host&gt;/oauth2/authorize&quot;, params: &#123; scope: &quot;profile&quot; &#125;, &#125;, issuer: &quot;http://&lt;host&gt;&quot;, token: &#x27;http://&lt;host&gt;/oauth2/token&#x27;, userinfo: &#x27;http://&lt;host&gt;/oauth2/userinfo&#x27;, clientId: process.env.CUSTOM_CLIENT_ID, clientSecret: process.env.CUSTOM_SECRET, profile(profile) &#123; return &#123; id: profile.sub, name: profile.name &#125;; &#125;,&#125; 自定义 OIDC 服务器 12345678910111213141516import &#123;OIDCConfig&#125; from &quot;@auth/core/providers&quot;;const customProvider: OIDCConfig&lt;any&gt; = &#123; id: &#x27;oidc-client&#x27;, name: &#x27;oidc-client&#x27;, type: &#x27;oidc&#x27;, authorization: &#123; url: &quot;http://&lt;host&gt;/oauth2/authorize&quot;, params: &#123; scope: &quot;openid&quot; &#125;, &#125;, issuer: &quot;http://&lt;host&gt;&quot;, token: &#x27;http://&lt;host&gt;/oauth2/token&#x27;, userinfo: &#x27;http://&lt;host&gt;/oauth2/userinfo&#x27;, clientId: process.env.CUSTOM_CLIENT_ID, clientSecret: process.env.CUSTOM_SECRET,&#125; 独立服务简单实现 123456789101112131415161718192021222324252627282930const config = &#123; providers: [ Credentials(&#123; name: &quot;Credentials&quot;, credentials: &#123; username: &#123; label: &quot;Username:&quot;, type: &quot;text&quot;, placeholder: &quot;your-cool-username&quot; &#125;, password: &#123; label: &quot;Password:&quot;, type: &quot;password&quot;, placeholder: &quot;your-awesome-password&quot; &#125; &#125;, async authorize(credentials) &#123; // This is where you need to retrieve user data // to verify with credentials // Docs: https://next-auth.js.org/configuration/providers/credentials const user = &#123; id: &quot;42&quot;, name: &quot;Dave&quot;, password: &quot;nextauth&quot; &#125; if (credentials?.username === user.name &amp;&amp; credentials?.password === user.password) &#123; return user &#125; else &#123; return null &#125; &#125; &#125;) ],&#125; satisfies NextAuthConfig 无页面跳转登录 编写 auth/helpers.ts 文件 12345678910&quot;use server&quot;;import &#123;signIn as naSignIn, signOut as naSignOut&#125; from &quot;.&quot;;export async function signIn(prop:any) &#123; await naSignIn(prop);&#125;export async function signOut() &#123; await naSignOut();&#125; 注：此处可以传入提供方ID，实现快速登录，跳过登录页。之后如果是 oauth2 登录还需要在 oauth2 处登出。 编写 app/AuthButton.client.tsx 文件： jsx1234567891011121314151617181920&quot;use client&quot;import &#123;useSession&#125; from &quot;next-auth/react&quot;;import &#123;Button&#125; from &quot;@mui/material&quot;import &#123;signIn, signOut&#125; from &quot;@/auth/helpers&quot;;export default function AuthButton() &#123; const session = useSession(); return session?.data?.user? ( &lt;Button onClick=&#123;async ()=&gt; await signOut()&#125;&gt; &#123;session.data?.user?.name&#125;: Sign Out &lt;/Button&gt; ) : ( &lt;Button onClick=&#123;async ()=&gt; await signIn(&#x27;oidc-client&#x27;)&#125;&gt; Sign In &lt;/Button&gt; );&#125; 编写 app/AuthButton.server.tsx 文件 jsx1234567891011121314151617181920import &#123;SessionProvider&#125; from &quot;next-auth/react&quot;;import &#123;BASE_PATH, auth&#125; from &quot;@/auth&quot;;import AuthButtonClient from &quot;@/app/AuthButton.client&quot;;export default async function AuthButton() &#123; const session = await auth(); if (session &amp;&amp; session.user) &#123; session.user = &#123; name: session.user.name, email: session.user.email &#125;; &#125; return ( &lt;SessionProvider basePath=&#123;BASE_PATH&#125; session=&#123;session&#125;&gt; &lt;AuthButtonClient/&gt; &lt;/SessionProvider&gt; )&#125; 修改 app/page.tsx 文件 jsx123456789101112131415import &#123;auth&#125; from &quot;@/auth&quot;;import AuthButton from &quot;@/app/AuthButton.server&quot;;export default async function Home() &#123; const session = await auth(); return ( &lt;main className=&quot;flex min-h-screen flex-col items-center justify-between p-24&quot;&gt; &lt;h1 className=&quot;text-3xl font-bold underline&quot;&gt; Hello world! &lt;/h1&gt; &lt;pre&gt;&#123;JSON.stringify(session, null, 2)&#125;&lt;/pre&gt; &lt;AuthButton/&gt; &lt;/main&gt; );&#125; 服务端和客户端获取用户信息 编写 WhoAmIServerAction.tsx ： jsx12345678910111213141516&quot;use client&quot;;import &#123;useEffect, useState&#125; from &quot;react&quot;;export default function WhoAmIServerAction(&#123; onGetUserAction, &#125;: &#123; onGetUserAction: () =&gt; Promise&lt;string | null&gt;;&#125;) &#123; const [user, setUser] = useState&lt;string | null&gt;(); useEffect(() =&gt; &#123; onGetUserAction().then((user) =&gt; setUser(user)); &#125;, []); return &lt;div&gt;Who Am I (server action): &#123;user&#125;&lt;/div&gt;;&#125; 编写 page.tsx ： jsx123456789101112131415161718192021import &#123;auth&#125; from &quot;@/auth&quot;;import WhoAmIServerAction from &quot;./WhoAmIServerAction&quot;;export default async function TestRoute() &#123; const session = await auth(); async function onGetUserAction() &#123; &quot;use server&quot;; const session = await auth(); return session?.user?.name ?? null; &#125; return ( &lt;main&gt; &lt;h1&gt;Test Route&lt;/h1&gt; &lt;div&gt;User: &#123;session?.user?.name&#125;&lt;/div&gt; &lt;WhoAmIServerAction onGetUserAction=&#123;onGetUserAction&#125;/&gt; &lt;/main&gt; )&#125; 参考资料 官方文档 示例项目 视频教程","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"},{"name":"OAuth","slug":"OAuth","permalink":"https://wangqian0306.github.io/tags/OAuth/"},{"name":"OIDC","slug":"OIDC","permalink":"https://wangqian0306.github.io/tags/OIDC/"}]},{"title":"Skaffold","slug":"kubernetes/skaffold","date":"2024-05-23T12:52:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2024/skaffold/","permalink":"https://wangqian0306.github.io/2024/skaffold/","excerpt":"","text":"Skaffold 简介 Skaffold 是一个命令行工具，可促进基于容器的应用程序和 Kubernetes 应用程序的持续开发。 Skaffold 处理构建、推送和部署应用程序的工作流程，并提供用于创建 CI/CD 管道的构建块。 安装 使用如下命令安装脚本： 12curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 &amp;&amp; \\sudo install skaffold /usr/local/bin/ 试用 官方提供了演示项目： 123git clone https://github.com/GoogleContainerTools/skaffoldcd skaffold/examples/getting-startedskaffold dev 此命令会构建服务，并将其部署在 k8s 上，在代码发生变动时会自动重新打包部署。 参考资料 官方文档 演示项目","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"AList","slug":"tmp/alist","date":"2024-05-21T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/alist/","permalink":"https://wangqian0306.github.io/2024/alist/","excerpt":"","text":"AList 简介 AList 是一个支持多种存储的网盘管理工具。 安装 编写如下 docker-compose.yaml : 12345678910111213services: alist: image: &#x27;xhofe/alist:latest&#x27; container_name: alist volumes: - &#x27;/etc/alist:/opt/alist/data&#x27; ports: - &#x27;5244:5244&#x27; environment: - PUID=0 - PGID=0 - UMASK=022 restart: unless-stopped 然后使用如下命令启动服务即可： 1docker-compose up -d 第三方版本 小雅 使用第三方版本可以自带一些内容源。 注：如需使用 API 可以使用命令 alist &lt;user&gt; set &lt;password&gt; 的方式自行设置密码，并在 nginx 配置中打开相应路由。 参考资料 官方项目 官方文档","categories":[],"tags":[{"name":"AList","slug":"AList","permalink":"https://wangqian0306.github.io/tags/AList/"}]},{"title":"瓦片地图","slug":"ocean/tile","date":"2024-04-19T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/tile/","permalink":"https://wangqian0306.github.io/2024/tile/","excerpt":"","text":"瓦片地图 简介 瓦片地图金字塔模型是一种多分辨率层次模型，从瓦片金字塔的底层到顶层，分辨率越来越低，但表示的地理范围不变。 使用场景 坐标转化 安装依赖库： 1pip install cartopy pillow matplotlib 首先可以用如下函数进行坐标和瓦片的互相转化： 123456789101112131415161718192021222324252627282930313233343536373839import mathTILE_SIZE = 256def lon_to_pixel_x(lon, zoom): pixel_x = (lon + 180) / 360 * (TILE_SIZE &lt;&lt; zoom) return pixel_xdef lat_to_pixel_y(lat, zoom): sin_lat = math.sin(lat * math.pi / 180) return (0.5 - math.log((1 + sin_lat) / (1 - sin_lat)) / (4 * math.pi)) * (TILE_SIZE &lt;&lt; zoom)def lat_lon_to_tile(lon, lat, zoom): px = lon_to_pixel_x(lon, zoom) py = lat_to_pixel_y(lat, zoom) tx = int(px / TILE_SIZE) ty = int(py / TILE_SIZE) return tx, tydef tile_to_latlon(x, y, z): n = 2.0 ** z lon_left = x / n * 360.0 - 180.0 lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * y / n))) lat_top = math.degrees(lat_rad) lon_right = (x + 1) / n * 360.0 - 180.0 lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * (y + 1) / n))) lat_bottom = math.degrees(lat_rad) return lat_top, lon_left, lat_bottom, lon_rightif __name__ == &quot;__main__&quot;: print(tile_to_latlon(14, 6, 4)) print(lat_lon_to_tile(135, 40.97, 4)) 绘制大陆架，海岸线 123456789101112import cartopy.crs as ccrsimport cartopy.feature as cfeatureimport matplotlib.pyplot as plt# 创建一个绘图对象fig = plt.figure(figsize=(256, 256))ax = plt.axes(projection=ccrs.Mercator())# 添加地理特征ax.add_feature(cfeature.COASTLINE, edgecolor=&#x27;red&#x27;)# 显示图形plt.savefig(&#x27;map_with_cartopy.png&#x27;, bbox_inches=&#x27;tight&#x27;) 将相关数据绘制成瓦片 此处记录下实现方式： 获取到数据和其 GPS 坐标点位 将坐标点转化为 EPSG:3857 (Web Mercator)投影系坐标 明确投影坐标到实际图上的坐标(x,y) 使用线性插值法补充数据(此处结合实际的层级进行数据插入，例如z=3时总的像素数是 2048*2048) 将点位数组绘制成图","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"GeoServer","slug":"GeoServer","permalink":"https://wangqian0306.github.io/tags/GeoServer/"}]},{"title":"NAS 媒体库管理工具","slug":"tmp/nas-tool","date":"2024-04-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/nas-tool/","permalink":"https://wangqian0306.github.io/2024/nas-tool/","excerpt":"","text":"NAS 媒体库管理工具 简介 nas-tool 是一款 NAS 媒体库管理工具，需要结合 qbittorrent 和 Jellyfin 进行使用。 使用方式 编写如下 docker-compose.yaml 文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768services: nas-tools: image: hsuyelin/nas-tools:latest ports: - 3000:3000 volumes: - ./nas-config:/config - ./media:/media environment: - PUID=3333 - PGID=3333 - UMASK=022 - NASTOOL_AUTO_UPDATE=false - NASTOOL_CN_UPDATE=false restart: always hostname: nas-tools container_name: nas-tools qbittorrent-nox: container_name: qbittorrent-nox environment: - PGID=3333 - PUID=3333 - QBT_EULA=accept - QBT_VERSION=latest - QBT_WEBUI_PORT=8090 - TZ=CST - UMASK=022 image: qbittorrentofficial/qbittorrent-nox:latest ports: - 6881:6881/tcp - 6881:6881/udp - 8090:8090/tcp read_only: true stop_grace_period: 30m tmpfs: - /tmp tty: true hostname: qbittorrent volumes: - ./qbit-config:/config - ./media:/downloads jellyfin: image: nyanmisaka/jellyfin:latest container_name: jellyfin user: 3333:3333 volumes: - ./jellyfin/config:/config - ./jellyfin/cache:/cache - ./media/movies:/media/movies - ./media/tv:/media/tv restart: &#x27;unless-stopped&#x27; ports: - 8096:8096 hostname: jellyfin jackett: image: lscr.io/linuxserver/jackett:latest container_name: jackett environment: - PUID=3333 - PGID=3333 - TZ=Asia/Shanghai - AUTO_UPDATE=true volumes: - ./jackett/data:/config - ./content/downloads:/downloads ports: - 9117:9117 restart: unless-stopped 注：可以把 uid 和 gid 改为部署用户的对应值。 之后就可以访问下列地址按照提示进行配置： http://localhost:8090 配置 qBittorrent-nox 下载器 http://localhost:8096 配置 Jellyfin http://localhost:9117 配置 Jackett http://localhost:3000 配置 nas-tools 链接 Jellyfin, qBittorrent-nox(WEB-UI) 和 Jackett ，并且填入 TMDB API Key 参考资料 项目地址","categories":[],"tags":[{"name":"HTPC","slug":"HTPC","permalink":"https://wangqian0306.github.io/tags/HTPC/"}]},{"title":"Open Interpreter","slug":"ai/open-interpreter","date":"2024-04-10T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/open-interpreter/","permalink":"https://wangqian0306.github.io/2024/open-interpreter/","excerpt":"","text":"Open Interpreter 简介 Open Interpreter 可以让大语言模型运行代码。 安装及使用 1pip install open-interpreter 本地运行可以直接使用： 1interpreter --local 远程运行可以使用： 1interpreter --model ollama/codellama --api_base http://xxx.xxx.xxx.xxx:11434 参考资料 官方网站","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"}]},{"title":"Netty","slug":"spring/netty","date":"2024-04-02T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/netty/","permalink":"https://wangqian0306.github.io/2024/netty/","excerpt":"","text":"Netty 简介 Netty 是一个 NIO 客户端服务器框架，可以快速轻松地开发协议服务器和客户端等网络应用程序。它极大地简化和简化了网络编程，例如 TCP 和 UDP Socket 服务器开发。 使用方式 Socket Server 首先需要引入依赖： 123dependencies &#123; implementation &#x27;io.netty:netty-all:4.1.108.Final&#x27;&#125; 然后需要编写数据接收类： 123456789101112131415161718192021222324252627282930import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import io.netty.util.ReferenceCountUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Component;@Slf4j@Componentpublic class DiscardServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf in = (ByteBuf) msg; try &#123; while (in.isReadable()) &#123; log.error(String.valueOf((char) in.readByte())); &#125; &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 然后编写服务： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioServerSocketChannel;import org.springframework.boot.ApplicationArguments;import org.springframework.boot.ApplicationRunner;import org.springframework.stereotype.Component;@Componentpublic class NettyServerRunner implements ApplicationRunner &#123; private final DiscardServerHandler discardServerHandler; public NettyServerRunner(DiscardServerHandler discardServerHandler) &#123; this.discardServerHandler = discardServerHandler; &#125; @Override public void run(ApplicationArguments args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(discardServerHandler); &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture f = b.bind(8888).sync(); f.channel().closeFuture().sync(); &#125; finally &#123; workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); &#125; &#125;&#125; 之后即可运行如下命令测试服务状态： 1telnet localhost 8888 注：进入命令后可以输入任意内容查看日志。 参考资料 官方网站","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Netty","slug":"Netty","permalink":"https://wangqian0306.github.io/tags/Netty/"}]},{"title":"Iridium Short Burst Data","slug":"ocean/isbd","date":"2024-04-01T13:32:58.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/isbd/","permalink":"https://wangqian0306.github.io/2024/isbd/","excerpt":"","text":"Iridium Short Burst Data 简介 Iridium Short Burst Data 是铱星的一种短报文，报文可以通过 DirectIP 方式或邮件的方式推送到客户侧。由于提供方并没有说明 使用方式 如果需要使用 java 解析报文可以使用如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120import lombok.Data;import java.nio.charset.StandardCharsets;import java.util.Arrays;@Datapublic class Isbdmsg &#123; private String imei; private String payload; private Double lat; private Double lon; private Long cepRadius; private Long timeOfSession; private Long cdrRef; private Integer momsn; private Integer mtmsn; private Boolean finish; private Integer readBytes = 0; public Isbdmsg(byte[] data) &#123; decodeFull(data); while (readBytes &lt; data.length) &#123; int iei = data[readBytes]; if (iei == 1) &#123; decodeHeader(Arrays.copyOfRange(data, readBytes + 1, readBytes + 31)); &#125; else if (iei == 2) &#123; decodePayload(data); &#125; else if (iei == 3) &#123; decodeLocation(Arrays.copyOfRange(data, readBytes + 1, readBytes + 14)); &#125; else &#123; return; &#125; &#125; &#125; public void decodeFull(byte[] data) &#123; int protocolVersion = data[0]; if (protocolVersion != 1) &#123; throw new IllegalArgumentException(&quot;not a valid protocol version&quot;); &#125; int overallMessageLen = ((data[1] &amp; 0xFF) &lt;&lt; 8) | (data[2] &amp; 0xFF); if (data.length != overallMessageLen + 3) &#123; throw new IllegalArgumentException(&quot;message incomplete&quot;); &#125; this.readBytes = this.readBytes + 3; &#125; public void decodeHeader(byte[] data) &#123; int moHeaderLen = ((data[0] &amp; 0xFF) &lt;&lt; 8) | (data[1] &amp; 0xFF); if (moHeaderLen != 28) &#123; throw new IllegalArgumentException(&quot;not a valid moHeaderLen&quot;); &#125; this.cdrRef = ((long) (data[2] &amp; 0xFF) &lt;&lt; 24) | ((long) (data[3] &amp; 0xFF) &lt;&lt; 16) | ((long) (data[4] &amp; 0xFF) &lt;&lt; 8) | ((long) (data[5] &amp; 0xFF)); this.imei = new String(Arrays.copyOfRange(data, 6, 21), StandardCharsets.UTF_8); this.momsn = ((data[22] &amp; 0xFF) &lt;&lt; 8) | (data[23] &amp; 0xFF); this.mtmsn = ((data[24] &amp; 0xFF) &lt;&lt; 8) | (data[25] &amp; 0xFF); this.timeOfSession = ((long) (data[26] &amp; 0xFF) &lt;&lt; 24) | ((long) (data[27] &amp; 0xFF) &lt;&lt; 16) | ((long) (data[28] &amp; 0xFF) &lt;&lt; 8) | ((long) (data[29] &amp; 0xFF)); this.readBytes = this.readBytes + 31; &#125; public void decodeLocation(byte[] data) &#123; int moLocationLen = ((data[0] &amp; 0xFF) &lt;&lt; 8) | (data[1] &amp; 0xFF); if (moLocationLen != 11) &#123; throw new IllegalArgumentException(&quot;not a valid moHeaderLen&quot;); &#125; int nsi = getValueAtIndex(data[2], 1); int ewi = getValueAtIndex(data[2], 0); int latitudeDegrees = data[3]; double latitudeMinute = convertToDecimal(data[4], data[5]); int longitudeDegrees = data[6]; double longitudeMinute = convertToDecimal(data[7], data[8]); this.lat = withSigns(ewi, latitudeDegrees + convertToDegrees(latitudeMinute)); this.lon = withSigns(nsi, longitudeDegrees + convertToDegrees(longitudeMinute)); this.cepRadius = ((long) (data[9] &amp; 0xFF) &lt;&lt; 24) | ((long) (data[10] &amp; 0xFF) &lt;&lt; 16) | ((long) (data[11] &amp; 0xFF) &lt;&lt; 8) | ((long) (data[12] &amp; 0xFF)); this.readBytes = this.readBytes + 14; &#125; private double convertToDecimal(byte msByte, byte lsByte) &#123; int thousandths = (msByte &lt;&lt; 8) | lsByte; return thousandths / 1000.0; &#125; private double convertToDegrees(double decimal) &#123; return decimal / 60.0; &#125; private int getValueAtIndex(byte b, int index) &#123; if (index &lt; 0 || index &gt;= 8) &#123; throw new IllegalArgumentException(&quot;Index out of range&quot;); &#125; return (b &gt;&gt; index) &amp; 1; &#125; private double withSigns(int i, double number) &#123; if (i == 0) &#123; return number; &#125; else &#123; return 0 - number; &#125; &#125; public void decodePayload(byte[] data) &#123; int moPayloadLen = ((data[readBytes + 1] &amp; 0xFF) &lt;&lt; 8) | (data[readBytes + 2] &amp; 0xFF); this.payload = new String(Arrays.copyOfRange(data, readBytes + 3, readBytes + 3 + moPayloadLen)); this.finish = true; this.readBytes = readBytes + 3 + moPayloadLen; &#125;&#125; 并如下改动 netty 接收代码： 123456789101112131415161718192021222324252627282930313233import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Component;@Slf4j@ChannelHandler.Sharable@Componentpublic class DiscardServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf buf = (ByteBuf) msg; try &#123; byte[] receivedData = new byte[buf.readableBytes()]; buf.readBytes(receivedData); Isbdmsg isbdmsg = new Isbdmsg(receivedData); log.info(isbdmsg.getImei() + &quot;: &quot; + isbdmsg.getPayload()); &#125; finally &#123; if (buf != null) &#123; buf.release(); &#125; &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; log.error(&quot;server error: &quot; + cause.getMessage()); ctx.close(); &#125;&#125; 参考资料 DIRECTIP SBD INFORMATION","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Spring AI","slug":"spring/spring-ai","date":"2024-03-29T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/spring-ai/","permalink":"https://wangqian0306.github.io/2024/spring-ai/","excerpt":"","text":"Spring AI 简介 类似于 LangChain，Spring 也提供了和大模型的相关库。目前主要支持文本对话和从文本生成图像。但是对于向量数据库的支持比较好。 使用方式 Ollama Chat 在 Spring Initializer 里可以引入如下内容： Ollama Spring Web Spring Reactive Web 之后为了使用更新版本的 Ollama 需要进行如下修改： 123456789101112131415161718192021222324252627repositories &#123; mavenCentral() maven &#123; url &#x27;https://repo.spring.io/milestone&#x27; &#125; maven &#123; url &#x27;https://repo.spring.io/snapshot&#x27; &#125;&#125;ext &#123; set(&#x27;springAiVersion&#x27;, &quot;1.0.0-SNAPSHOT&quot;)&#125;dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; implementation &#x27;org.springframework.ai:spring-ai-transformers-spring-boot-starter&#x27; implementation &#x27;org.springframework.ai:spring-ai-ollama-spring-boot-starter&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27;&#125;dependencyManagement &#123; imports &#123; mavenBom &quot;org.springframework.ai:spring-ai-bom:$&#123;springAiVersion&#125;&quot; &#125;&#125; 之后编写如下接口即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.springframework.ai.chat.client.ChatClient;import org.springframework.ai.embedding.EmbeddingModel;import org.springframework.ai.embedding.EmbeddingResponse;import org.springframework.ai.ollama.api.OllamaOptions;import org.springframework.core.ParameterizedTypeReference;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import reactor.core.publisher.Flux;import java.util.List;@RestController@RequestMapping(&quot;/ollama&quot;)public class ChatController &#123; private final ChatClient chatClient; private final EmbeddingModel embeddingModel; public ChatController(ChatClient.Builder builder, EmbeddingModel embeddingModel) &#123; this.chatClient = builder.defaultOptions(OllamaOptions.create().withModel(&quot;llama3&quot;)).build(); this.embeddingModel = embeddingModel; &#125; @GetMapping(&quot;/chat&quot;) public String simple(@RequestParam(required = false, defaultValue = &quot;hello&quot;) String message) &#123; return chatClient.prompt().user(message).call().content(); &#125; @GetMapping(&quot;/embedding&quot;) public EmbeddingResponse embedding(@RequestParam(required = false, defaultValue = &quot;hello&quot;) String message) &#123; return this.embeddingModel.embedForResponse(List.of(message)); &#125; @GetMapping(&quot;/chat/stream&quot;) public Flux&lt;String&gt; simpleFlux(@RequestParam(required = false, defaultValue = &quot;hello&quot;) String message) &#123; return chatClient.prompt().user(message).stream().content(); &#125; @GetMapping(&quot;/chat/parser&quot;) public List&lt;Song&gt; simpleParser(@RequestParam(required = false, defaultValue = &quot;Taylor Swift&quot;) String artist) &#123; String question = &quot;&quot;&quot; Please give me a list of top 10 songs and it&#x27;s release year for the artist &#123;artist&#125;. If you don&#x27;t know the answer , just say &quot;I don&#x27;t know&quot;. &quot;&quot;&quot;; return chatClient.prompt().user(u -&gt; u.text(question).param(&quot;artist&quot;, artist)).call().entity(new ParameterizedTypeReference&lt;&gt;() &#123; &#125;); &#125;&#125; 然后需要进行如下配置： 12345678910111213spring: application: name: xxx ai: ollama: base-url: http://xxx.xxx.xxx.xxx:11434 chat: options: model: llama3.1 num-ctx: 2048 embedding: options: model: nomic-embed-text num-ctx 参数是程序上下文的大小配置，如果有需求可以从 2k 提升到 8k, 16k, 32k 最大值为 128k。需要注意的是越大的上下文会影响硬件的内存部分。 注：此处返回的结果与格式和模型有较大的关系，建议使用 ollama run llama3 先进行测试，其他参数详见 配置参考 设置不同模型 如果需要为不同的接口使用不同的模型则可以使用如下代码： 1234567ChatResponse response = chatClient.prompt( new Prompt( &quot;Generate the names of 5 famous pirates.&quot;, OllamaOptions.create() .withModel(&quot;llama2&quot;) .withTemperature(0.4) )).call(); RAG 如果想要使用 RAG 则可以采用如下方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import lombok.extern.slf4j.Slf4j;import org.springframework.ai.document.Document;import org.springframework.ai.embedding.EmbeddingModel;import org.springframework.ai.reader.TextReader;import org.springframework.ai.transformer.splitter.TextSplitter;import org.springframework.ai.transformer.splitter.TokenTextSplitter;import org.springframework.ai.vectorstore.SimpleVectorStore;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.Resource;import java.io.File;import java.util.List;@Slf4j@Configurationpublic class RagConfig &#123; @Value(&quot;./vectorstore.json&quot;) private String vectorStorePath; @Value(&quot;classpath:/docs/olympic-faq.txt&quot;) private Resource faq; @Bean SimpleVectorStore simpleVectorStore(EmbeddingModel embeddingModel) &#123; var simpleVectorStore = new SimpleVectorStore(embeddingModel); var vectorStoreFile = new File(vectorStorePath); if (vectorStoreFile.exists()) &#123; log.info(&quot;Vector Store File Exists,&quot;); simpleVectorStore.load(vectorStoreFile); &#125; else &#123; log.info(&quot;Vector Store File Does Not Exist, load documents&quot;); TextReader textReader = new TextReader(faq); textReader.getCustomMetadata().put(&quot;filename&quot;, &quot;olympic-faq.txt&quot;); List&lt;Document&gt; documents = textReader.get(); TextSplitter textSplitter = new TokenTextSplitter(); List&lt;Document&gt; splitDocuments = textSplitter.apply(documents); simpleVectorStore.add(splitDocuments); simpleVectorStore.save(vectorStoreFile); &#125; return simpleVectorStore; &#125;&#125; 然后编写 RagController : 12345678910111213141516171819202122232425262728import org.springframework.ai.chat.client.ChatClient;import org.springframework.ai.chat.client.advisor.QuestionAnswerAdvisor;import org.springframework.ai.vectorstore.SearchRequest;import org.springframework.ai.vectorstore.VectorStore;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/ollama&quot;)public class RagController &#123; private final ChatClient chatClient; public RagController(ChatClient.Builder builder, VectorStore vectorStore) &#123; this.chatClient = builder.defaultAdvisors(new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults())) .build(); &#125; @GetMapping(&quot;/chat/rag&quot;) public String rag(@RequestParam(value = &quot;message&quot;, defaultValue = &quot;How many athletes compete in the Olympic Games Paris 2024&quot;) String message) &#123; return chatClient.prompt() .user(message) .call() .content(); &#125;&#125; 注：此处的 Advisors 是 Spring 在调用大模型时拦截并处理请求的组件。默认提供的 Advisor 有以下三项：历史记录管理(xxChatMemoryAdvistor)，RAG 增强(QuestionAnswerAdvisor)，敏感词过滤(SafeGuardAdvisor)。具体内容参见 官方博客 最后需要补充 resources/prompts/rag-prompt-template.st 提示词模板： 123456789You are a helpful assistant, conversing with a user about the subjects contained in a set of documents.Use the information from the DOCUMENTS section to provide accurate answers. If unsure or if the answerisn&#x27;t found in the DOCUMENTS section, simply state that you don&#x27;t know the answer.QUESTION:&#123;input&#125;DOCUMENTS:&#123;documents&#125; 和问答资料库 resources/docs/olympic-faq.txt： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121Q: How to buy tickets for the Olympic Games Paris 2024?A: Tickets for the Olympic Games Paris 2024 are available for spectators around the world only on the official ticketing website. To buy tickets, click here.The Paris 2024 Hospitality program offers packages that include tickets for sporting events combined with exceptional services in the competition venues (boxes, lounges) or in the heart of the city (accommodation, transport options, gastronomy, tourist activities, etc.).The Paris 2024 Hospitality program is delivered by the official Paris 2024 Hospitality provider, On Location.For more information about the Paris 2024 Hospitality &amp; Travel offers, click here.Q: What is the official mascot of the Olympic Games Paris 2024?A: The Olympic Games Paris 2024 mascot is Olympic Phryge. The mascot is based on the traditional small Phrygian hats for which they are shaped after.The name and design were chosen as symbols of freedom and to represent allegorical figures of the French republic.The Olympic Phryge is decked out in blue, white and red - the colours of France’s famed tricolour flag - with the golden Paris 2024 logo emblazoned across its chest.Q: When and where are the next Olympic Games?A: The Olympic Games Paris 2024 will take place in France from 26 July to 11 August.Q: What sports are in the Olympic Games Paris 2024?A: 3X3 BasketballArcheryArtistic GymnasticsArtistic SwimmingAthleticsBadmintonBasketballBeach VolleyballBoxingBreakingCanoe SlalomCanoe SprintCycling BMX FreestyleCycling BMX RacingCycling Mountain BikeCycling RoadCycling TrackDivingEquestrianFencingFootballGolfHandballHockeyJudoMarathon SwimmingModern PentathlonRhythmic GymnasticsRowingRugby SevensSailingShootingSkateboardingSport ClimbingSurfingSwimmingTable TennisTaekwondoTennisTrampolineTriathlonVolleyballWater PoloWeightliftingWrestlingQ:Where to watch the Olympic Games Paris 2024?A: In France, the 2024 Olympic Games will be broadcast by Warner Bros. Discovery (formerly Discovery Inc.) via Eurosport, with free-to-air coverage sub-licensed to the country&#x27;s public broadcaster France Télévisions. For a detailed list of the Paris 2024 Media Rights Holders here.Q: How many athletes compete in the Olympic Games Paris 2024?A: Around 10,500 athletes from 206 NOCs will compete.Q: How often are the modern Olympic Games held?A: The summer edition of the Olympic Games is normally held every four years.Q: Where will the 2028 and 2032 Olympic Games be held?A: Los Angeles, USA, will host the next Olympic Games from 14 to 30 July 2028. Brisbane, Australia, will host the Games in 2032.Q: What is the difference between the Olympic Summer Games and the Olympic Winter Games?A: The summer edition of the Olympic Games is a multi-sport event normally held once every four years usually in July or August.The Olympic Winter Games are also held every four years in the winter months of the host location and the multi-sports competitions are practised on snow and ice.Both Games are organised by the International Olympic Committee.Q: Which cities have hosted the Olympic Summer Games?A: 1896 Athens1900 Paris1904 St. Louis1908 London1912 Stockholm1920 Antwerp1924 Paris1928 Amsterdam1932 Los Angeles1936 Berlin1948 London1952 Helsinki1956 Melbourne1960 Rome1964 Tokyo1968 Mexico City1972 Munich1976 Montreal1980 Moscow1984 Los Angeles1988 Seoul1992 Barcelona1996 Atlanta2000 Sydney2004 Athens2008 Beijing2012 London2016 Rio de Janeiro2020 Tokyo2024 ParisQ: What year did the Olympic Games start?A: The inaugural Games took place in 1896 in Athens, Greece. 注：如果不配置 Ollama embedding options model 的话在初次启动时需要拉取 hugginface 和 github 当中的内容，启动时间较长且对网络环境要求很高。 注： 读取 RAG 部分的官方文档在 ETL Pipeline文档中 对话记录 编写如下代码即可： 1234567891011121314151617181920212223242526272829303132import org.springframework.ai.chat.client.ChatClient;import org.springframework.ai.chat.client.advisor.MessageChatMemoryAdvisor;import org.springframework.ai.chat.memory.InMemoryChatMemory;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import static org.springframework.ai.chat.client.advisor.AbstractChatMemoryAdvisor.CHAT_MEMORY_CONVERSATION_ID_KEY;@RestController@RequestMapping(&quot;/ollama&quot;)public class MemoryController &#123; private final ChatClient chatClient; public MemoryController(ChatClient.Builder builder) &#123; this.chatClient = builder.defaultAdvisors(new MessageChatMemoryAdvisor(new InMemoryChatMemory())) .build(); &#125; @GetMapping(&quot;/chat/memory&quot;) public String rag( @RequestParam(defaultValue = &quot;Here is chat room 1&quot;) String message, @RequestParam(defaultValue = &quot;1&quot;) String conversionId) &#123; return chatClient.prompt() .user(message) .advisors(a -&gt; a.param(CHAT_MEMORY_CONVERSATION_ID_KEY, conversionId)) .call() .content(); &#125;&#125; 对话日志 注：此处需要 Spring AI 的版本要大于 1.0.0-SNAPSHOT 。 编写如下代码： 1234567891011121314151617181920212223242526import org.springframework.ai.chat.client.advisor.SimpleLoggerAdvisor;import org.springframework.ai.chat.client.ChatClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/ollama&quot;)public class LogController &#123; private final ChatClient chatClient; public LogController(ChatClient.Builder builder) &#123; this.chatClient = builder.defaultAdvisors((new SimpleLoggerAdvisor()).build(); &#125; @GetMapping(&quot;/chat/log&quot;) public String rag( @RequestParam(defaultValue = &quot;Hi&quot;) String message) &#123; return chatClient.prompt() .user(message) .call() .content(); &#125;&#125; 然后修改日志配置即可： 12345678logging: level: org: springframework: ai: chat: client: advisor: DEBUG 函数调用 注：此处需要 Spring AI 的版本要大于 1.0.0-SNAPSHOT 。 编写需要被调用的函数 MockWeatherService.java ： 12345678910111213import java.util.function.Function;public class MockWeatherService implements Function&lt;MockWeatherService.Request, MockWeatherService.Response&gt; &#123; public enum Unit &#123; C, F &#125; public record Request(String location, Unit unit) &#123;&#125; public record Response(double temp, Unit unit) &#123;&#125; public Response apply(Request request) &#123; System.out.println(&quot;call mock service&quot;); return new Response(30.0, Unit.C); &#125;&#125; 编写配置类 Config.java： 12345678910111213141516import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Description;import java.util.function.Function;@Configurationpublic class Config &#123; @Bean @Description(&quot;Get the weather in location&quot;) public Function&lt;MockWeatherService.Request, MockWeatherService.Response&gt; currentWeatherFunction() &#123; return new MockWeatherService(); &#125;&#125; 编写请求类： 1234567891011121314151617181920212223242526import org.springframework.ai.chat.client.ChatClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class TestController &#123; private final ChatClient client; public TestController(ChatClient.Builder builder) &#123; this.client = builder .defaultSystem(&quot;You are a helpful AI Assistant answering questions about cities around the world.&quot;) .defaultFunctions(&quot;currentWeatherFunction&quot;) .build(); &#125; @GetMapping(&quot;/weather&quot;) public String weather(@RequestParam String message) &#123; return client.prompt() .user(message) .call() .content(); &#125;&#125; 之后启动服务然后编写如下请求即可： 1GET http://localhost:8080/weather?message=What&#x27;s the weather like in Beijing 指标监控 注：此处需要 Spring AI 的版本要大于 1.0.0-SNAPSHOT 。 可以搭配 Spring Boot Actuator 访问官方提供的 监控端点 如果想检查请求的执行逻辑还可以引入 Zipkin 检查用户输入查询后，Spring 请求 LLM 接口花费的时间等细节内容。 参考资料 官方文档 spring-into-ai Spring AI 1.0.0 M1 released Spring AI with Ollama Tool Support Spring Tips: Spring AI Observability AI Model Context Decoded","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"pdf","slug":"tmp/pdf","date":"2024-03-25T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/pdf/","permalink":"https://wangqian0306.github.io/2024/pdf/","excerpt":"","text":"pdf pdf2htmlEX pdf2htmlEX 项目可以将 PDF 转换为 HTML，而不会丢失文本或格式。 使用方式 1docker run -ti --rm -v ./test.pdf:/pdf/ -w /pdf pdf2htmlex/pdf2htmlex --zoom 1.3 test.pdf 参考资料 官方项目 Wiki Stirling-PDF Stirling-PDF 是一个强大的、本地托管的基于 Web 的 PDF 操作工具，功能包括拆分、合并、转换、重新组织、添加图像、旋转、压缩等。 使用方式 1234567891011121314services: stirling-pdf: image: frooodle/s-pdf:latest ports: - &#x27;8080:8080&#x27; volumes: - /location/of/trainingData:/usr/share/tessdata #Required for extra OCR languages - /location/of/extraConfigs:/configs - /location/of/customFiles:/customFiles/ - /location/of/logs:/logs/ environment: - DOCKER_ENABLE_SECURITY=false - INSTALL_BOOK_AND_ADVANCED_HTML_OPS=false - LANGS=en_GB 注：OCR 需要模型文件放在 trainingData 目录下，中文需要 chi_sim.traineddata 文件。 模型文件地址 参考资料 官方项目","categories":[],"tags":[{"name":"PDF","slug":"PDF","permalink":"https://wangqian0306.github.io/tags/PDF/"}]},{"title":"Docker-Android","slug":"docker/android","date":"2024-03-25T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2024/android/","permalink":"https://wangqian0306.github.io/2024/android/","excerpt":"","text":"Docker-Android 简介 Docker-Android 是一个 docker 映像，用于与 Android 相关的所有内容。可以使用 noVNC 和 adb 链接到虚拟机中，它可用于应用程序开发和测试(本机、Web 和混合应用程序)。 注：运行平台只能是 Ubuntu。 使用方式 使用如下命令监测 KVM 是否开启： 12sudo apt install cpu-checkerkvm-ok 若输出字样如下则证明功能正常： 12INFO: /dev/kvm existsKVM acceleration can be used 编写如下 docker-compose.yaml : 1234567891011services: android-container: image: budtmo/docker-android:emulator_14.0 container_name: android-container ports: - &quot;6080:6080&quot; environment: - EMULATOR_DEVICE=Samsung Galaxy S10 - WEB_VNC=true devices: - /dev/kvm 参考资料 官方项目 容器清单 在Docker中安装安卓11、12+Appium【web端android】","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://wangqian0306.github.io/tags/Android/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"SRS","slug":"tmp/srs","date":"2024-03-22T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/srs/","permalink":"https://wangqian0306.github.io/2024/srs/","excerpt":"","text":"SRS 简介 SRS 是一个开源的(MIT协议)简单高效的实时视频服务器，支持RTMP、WebRTC、HLS、HTTP-FLV、SRT、MPEG-DASH和GB28181等协议。SRS 媒体服务器和FFmpeg、OBS、VLC、 WebRTC 等客户端配合使用，提供流的接收和分发的能力，是一个典型的发布(推流)和订阅(播放)服务器模型。SRS 支持互联网广泛应用的音视频协议转换，比如可以将 RTMP或SRT， 转成 HLS 或 HTTP-FLV 或 WebRTC 等协议。 部署和使用 使用容器部署 12345678910111213services: srs-stack: image: registry.cn-hangzhou.aliyuncs.com/ossrs/srs-stack:5 container_name: srs-stack restart: always ports: - &quot;2022:2022&quot; - &quot;2443:2443&quot; - &quot;1935:1935&quot; - &quot;8000:8000/udp&quot; - &quot;10080:10080/udp&quot; volumes: - ./data:/data 使用介绍 在启动服务后会有使用样例的说明，具体请参照 B站视频 注：有视频需求的尽量考虑公共平台。 参考资料 SRS 官方网站","categories":[],"tags":[{"name":"SRS","slug":"SRS","permalink":"https://wangqian0306.github.io/tags/SRS/"}]},{"title":"OpenRewrite","slug":"java/openrewrite","date":"2024-03-19T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2024/open-rewrite/","permalink":"https://wangqian0306.github.io/2024/open-rewrite/","excerpt":"","text":"OpenRewrite 简介 OpenRewrite 是一个用于源代码的自动重构生态系统，可以便捷的完成新版本的代码适配与迁移。 使用方式 首先需要在 Gradle 的 build.gradle 文件中添加如下代码： 12345plugins &#123; id &#x27;org.openrewrite.rewrite&#x27; version &#x27;6.10.0&#x27;&#125;rewrite &#123;&#125; 之后就可以通过 ./gradlew rewriteDiscover 命令来查看 OpenRewrite 中的规则。 例如可以编辑如下所示的 规则 ，升级 Spring Boot 到 3.2 版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243plugins &#123; id &#x27;java&#x27; id &#x27;org.openrewrite.rewrite&#x27; version &#x27;6.10.0&#x27; id &#x27;org.springframework.boot&#x27; version &#x27;3.1.10-SNAPSHOT&#x27; id &#x27;io.spring.dependency-management&#x27; version &#x27;1.1.4&#x27;&#125;java &#123; sourceCompatibility = &#x27;17&#x27;&#125;rewrite &#123; activeRecipe(&quot;org.openrewrite.java.spring.boot3.UpgradeSpringBoot_3_2&quot;)&#125;configurations &#123; compileOnly &#123; extendsFrom annotationProcessor &#125;&#125;repositories &#123; mavenCentral() maven &#123; url &#x27;https://repo.spring.io/milestone&#x27; &#125; maven &#123; url &#x27;https://repo.spring.io/snapshot&#x27; &#125;&#125;dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; rewrite(&quot;org.openrewrite.recipe:rewrite-spring:5.6.0&quot;)&#125;tasks.named(&#x27;bootBuildImage&#x27;) &#123; builder = &#x27;paketobuildpacks/builder-jammy-base:latest&#x27;&#125;tasks.named(&#x27;test&#x27;) &#123; useJUnitPlatform()&#125; 之后即可使用 ./gradlew rewriteRun 命令来运行 OpenRewrite。 除此之外还有很多的 OpenRewrite 的规则，可以参考 规则官方页面。 参考资料 官方网站 Upgrading your Java &amp; Spring Boot applications with OpenRewrite in IntelliJ Upgrading your Java &amp; Spring Applications with OpenRewrite","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Project CRaC","slug":"spring/crac","date":"2024-03-18T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2024/crac/","permalink":"https://wangqian0306.github.io/2024/crac/","excerpt":"","text":"Procject CRac 简介 Coordinated Restore at Checkpoint(CRaC) 是一个 JDK 项目，它允许您以更短的首次事务时间启动 Java 程序，同时减少时间和资源以实现完整的代码速度。CRaC 在完全预热 Java 进程(检查点)时有效地获取 Java 进程(检查点)的快照，然后使用该快照从此捕获状态启动任意数量的 JVM。并非所有现有的 Java 程序都可以在不修改的情况下运行，因为在创建检查点之前，需要显式关闭所有资源(使用 CRaC API)，并且必须在还原后重新初始化这些资源。Spring、Micronaut 和 Quarkus 等流行框架支持开箱即用的 CRaC 检查点。 使用方式 注：目前的运行版本还比较初级，需要以 privileged 权限运行容器，安全性较差。待之后发版更新后再来完善此内容。 参考资料 Introduction to Project CRaC: Enhancing Runtime Efficiency in Java &amp; Spring Development CRaC Project Wiki JVM Checkpoint Restore What is CRaC? Spring Boot CRaC demo","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Kafka Rebalance","slug":"bigdata/kafka-rebalance","date":"2024-03-11T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/kafka-rebalance/","permalink":"https://wangqian0306.github.io/2024/kafka-rebalance/","excerpt":"","text":"Kafka Rebalance 简介 Kafka Rebalance 是 Apache Kafka 中的一个重要概念，用于在消费者组中重新分配分区的过程。 当消费者组中的消费者实例数量发生变化时，例如有新的消费者加入或者已有的消费者退出，Kafka 会触发重新平衡（Rebalance）操作。重新平衡的目的是重新分配主题的分区给各个消费者实例，以确保每个消费者实例负责处理的分区数尽可能均衡。 之前版本的 Rebalance 有如下缺点： 将太多的功能都交给了 Client 导致其过于臃肿。 依赖于组范围的同步屏障。这意味着一个行为不端的消费者可以摧毁或扰乱整个群体，因为每当消费者加入、离开或失败时，都需要重新平衡整个群体。 复杂，经历过 KIP-429 和 KIP-345 增强。 引入了传播机制，在 Client 端和 Broker 端都不是很清晰。 所以在 Kafka 3.7 中引入了 KIP-848 最终的实现目标是： 实现真正的增量和协作重平衡，不再依赖全局同步屏障 复杂性应该从消费者转移到组协调者 该协议仍应允许高级用户（如 Kafka Streams）在客户端上运行分配逻辑。 该协议应提供与当前协议相同的保证，即在最坏的情况下至少完成一次，而在正常情况下恰好完成一次 该协议应支持在不停机的情况下升级 Consumer 实现方式 总的实现思路是采用两种分配方式： Coordinator 使用服务端分配器进行计算(默认) 指定一个消费组成员，使用客户端分配器进行计算(自定义) 在默认方式中，新的分配方式会增强原有的心跳信息，并以此方式让 Coordinator 向组成员分配/撤销分区，同时允许组成员将其当前状态传播到 Coordinator。 在自定义方式中 Coordinator 会通过心跳通知组内的特定成员，该成员收到通知后会向 Coordinator 获取组的当前状态与它被分配到的分区。 处理流程 整个 Rebalance 分为三个阶段：Group Epoch, Assignment Epoch, Member Epoch。 Group Epoch - 触发 Rebalance 在组中的元数据变更时会触发 Rebalance，具体来说有以下情况： 成员加入或离开组 成员更新订阅信息 成员更新 assignors 成员更新 assignors 的原因或元数据 Coordinator 发现某个成员需要被移除或被他隔离 分区元数据变更，例如新增了分区或新匹配的主题被创建 在匹配以上情况的时候会生成新的版本号，而版本号会被存储到组的元数据中同时也意味着会触发一次 assignment。 Assignment Epoch - 计算组的分配 当 Group 的版本号大于 Assignment 的版本号时 Coordinator 会按照当前的 Group 元数据计算重新分配的方法并将其持久化。 Coordinator 返回的分配计划只有两种情况，一种是根据默认的服务端分配器计算，另一种是向组内的一个成员请求新的分配计划。 Member Epoch - 组内协调 在新 Assignment 写入完成时，组员会独立协调当前与新的目标的分配。最终明确自己的目标分区和任务。协调过程有以下三个阶段： Coordinator 撤销不再属于成员目标分配的分区(取交集) 当 Coordinator 收到撤销确认时，它会将成员当前分配更新为其目标分配并持久保留它 Coordinator 将新分区分配给成员，它通过向成员提供目标分区来实现此目的，同时确保尚未被其他成员撤消但已从此集中删除的分区 重新平衡超时由成员在加入组时提供。它基本上是在客户端配置的最大轮询间隔。当组协调器处理检测信号请求时，计时器开始工作。 分配流程 组的新目标基本上就是一个入参为当前组的元数据和当前目标的函数。需要注意的是分配是声明性的而不是像当前版本一样是递增的。换言之，assignors 定义组的所需状态，并让 Coordinator 收敛到该状态。 Assignor 选择 Coordinator 必须确认究竟使用哪种分配策略。在分配时组内的成员可能有不同的 Assignor ，而 Coordinator 会按照如下方式选择 Assignor： 如果可能，使用客户端 Assignor。这意味着服务端 Assignor 必须得到所有成员的支持。如果有多个，则在成员公布其支持的客户端 Assignor 时，它将尊重成员定义的优先级。 否则使用服务器端 Assignor。如果在组中指定了多个服务器端 Assignor，则 Coordinator 将使用最常见的分配器。如果成员未提供Assignor，则组协调器将默认为 group.consumer.assignors 中的第一个 Assignor 。 服务端模式 服务端模式是可以进行配置的，客户端可以在心跳信号中声明要使用的 Assignor。若找不到则会爆出 UNSUPPORTED_ASSIGNOR 错误。支持的 Assignor 会在 Broker 配置中列出。官方提供了两种 Assignor ： range uniform 注：请注意，在这两种情况下，Assignor 都是粘性的。目标是尽量减少分区移动。 客户端模式 客户端分配由消费者执行。整个过程分为以下几个阶段： Coordinator 选择一个成员来运行分配逻辑 Coordinator 通过在成员的下一个检测信号响应中设置 ShouldComputeAssignment 字段来通知成员计算新分配 当成员收到此错误时，应调用 ConsumerGroupPrepareAssignment API 来获取当前组元数据和当前目标分配 成员使用相关 Assignor 计算分配 成员调用 ConsumerGroupInstallAssignment API 来安装新分配。Coordinator 对其进行验证并保留它 注：就算是有新的阶段，也会保留分配，以确保组始终可以继续运行，避免单点故障。Coordinator 一次只运行一次分配任务。 所选成员应在重新平衡超时内完成分配过程。当成员收到通知时 Coordinator 端开始计时。如果该过程未在重新平衡超时内完成，则组协调员将选取另一个成员来运行分配。请注意，此处未对先前选择的成员进行隔离，因为隔离仅基于会话。 简单理解 假设某个主题有 6 个分区，然后有两个消费者 A B 以 Epoch 5 为当前阶段： 123Consumer Group Coordinator Current Group Coordinator TargetA(5) [1,2,3] A(5) [1,2,3] A(5) [1,2,3]B(5) [4,5,6] B(5) [4,5,6] B(5) [4,5,6] 在心跳信息返回正常的情况下会持续进行消费，如果此时来了个 C 客户端，向 Group Coordinator 发送注册请求的时候，Group Coordinator 会回复给他，当前在 Epoch 5，然后向 A B 发出撤销 [3], [6] 命令撤销完成后更新 Epoch，Group Coordinator 再分配给 C： 1234Consumer Group Coordinator Current Group Coordinator TargetA(5) [1,2,3] A(5) [1,2,3] A(6) [1,2]B(5) [4,5,6] B(5) [4,5,6] B(6) [4,5]C(0) [] C(6) [3,6] 假设此时 A 挂了，但是 B 正常运行： 123Consumer Group Coordinator Current Group Coordinator TargetB(6) [4,5] B(6) [4,5] B(7) [4,5,1]C(6) [6] C(6) [6] C(7) [6,2,3] 之后和上一步的流程一样，最后就会变成 B(7) [4,5,1] C(7) [6,2,3]。 参考资料 KIP-848 The Next Generation of the Consumer Rebalance Protocol Apache Kafka’s Next-Gen Rebalance Protocol: Towards More Stable and Scalable Consumer Groups","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"重构 grub 并重装内核","slug":"linux/grubfix","date":"2024-03-08T12:04:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2024/grubfix/","permalink":"https://wangqian0306.github.io/2024/grubfix/","excerpt":"","text":"重构 grub 并重装内核 简介 在导出阿里云虚拟机并将其转储在虚拟机平台上时遇到了 dracut-initqueue timeout could not boot 问题，经过与售后工程师的交流整理出了如下解决方案。 解决方案 进入虚拟机 grub 模式或阿里云 ECS，使用如下命令检查系统版本，之后关闭虚拟机电源。 1cat /etc/centos-release 注：建议小版本也要记录。 访问 CentOS Vault Mirror 下载历史版本的 DVD 版本 iso 镜像。 注：目录样例如下 https://vault.centos.org/7.3.1611/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso 进入虚拟机设置页面，挂载光驱并指定刚刚下载的镜像文件。 进入虚拟机 BIOS 或 UEFI 指定将光驱的启动顺序移动至磁盘前。 开启虚拟机，选择 Troubleshooting -&gt; Rescue a CentOS Linux system -&gt; 1) Continue 。 注：进入交互式命令行后按 1 即可。 挂载系统镜像和光盘并暂时移除数据盘。 输入如下命令进入 bash 并暂时移除数据盘： 12chroot /mnt/sysimagevim /etc/fstab 暂时把文件中 /dev 目录中的内容全部注释。 使用 lsblk 命令检查并挂载光盘： 123lsblkmount /dev/sr0 /mntlsblk 注：NAME: sr0 TYPE: rom RM:1 的是光盘，注意在挂载时会提示为只读模式。 删除并重新安装内核与 boot 分区。 12345678rm -rf /boot/*ls -l /boot/rpm -aq kernel*yum -y remove kernel*cd /mnt/Packagesyum -y install ./kernel*cd /rpm -aq kernel* 可以检查下两次内核是否有差异，然后重新构建内核映像即可： 12dracut -fls -l /boot 若 boot 分区能展示内容则证明操作正常。 更新 grub 123grub2-install /dev/sdamkdir /boot/grub2grub2-mkconfig -o /boot/grub2/grub.cfg 之后可以检查启动配置中的内核 1cat /boot/grub2/grub.cfg | grep menuentry 若 menuentry 和 $menuentry_id_option 中的子版本对应即证明操作正常。 使用 exit 命令退出至图形化界面，选择 Troubleshooting -&gt; Boot from local drive 进入系统即可正常启动。 进入虚拟机设置中，卸载光驱。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Ollama","slug":"ai/ollama","date":"2024-03-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/Ollama/","permalink":"https://wangqian0306.github.io/2024/Ollama/","excerpt":"","text":"Ollama 简介 Ollama 是一款在本地运行大模型的软件。 使用 使用如下命令即可安装： 123456curl -fsSL https://ollama.com/install.sh | shsudo mkdir -p /etc/systemd/system/ollama.service.dsudo echo &#x27;[Service]&#x27; &gt;&gt; /etc/systemd/system/ollama.service.d/environment.confsudo echo &#x27;Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;&#x27; &gt;&gt; /etc/systemd/system/ollama.service.d/environment.confsudo systemctl daemon-reloadsudo systemctl restart ollama 想要更新 ollama 可以使用如下命令： 1curl -fsSL https://ollama.com/install.sh | sh Chat 使用如下命令可以拉取大模型： 1ollama pull llama2:latest 然后可以使用如下命令测试模型运行情况： 1ollama run llama2 然后可以用如下命令进入调试模式： 1ollama run llama2 --verbose 若可以正常使用则可以尝试调用 API： 1234curl http://localhost:11434/api/generate -d &#x27;&#123; &quot;model&quot;: &quot;llama2&quot;, &quot;prompt&quot;:&quot;Why is the sky blue?&quot;&#125;&#x27; 微调后的模型 除了官方模型清单内的模型之外，还可以使用 huggingface 上的其他模型。 1ollama run hf.co/arcee-ai/SuperNova-Medius-GGUF 自然也可以通过修改模型文件的方式，添加自定义的提示词等内容。在对话中也可以使用命令修改参数，例如： 1\\set parameter &#x27;num_ctx&#x27; 32768 之后也可以将其存储为新的模型： 1\\save &lt;name&gt; Embedding 使用如下命令可以拉取嵌入模型： 1ollama pull nomic-embed-text:latest 可以尝试调用 API： 1234curl http://localhost:11434/api/embeddings -d &#x27;&#123; &quot;model&quot;: &quot;nomic-embed-text&quot;, &quot;prompt&quot;: &quot;Llamas are members of the camelid family&quot;&#125;&#x27; 模型 对于 Ollama 来说，除了基本的权重文件之外还需要一个 Modelfile 。这个 Modelfile 包含如下元素： 系统提示词(system prompt) 回答问题的模板(template) 参数(parameters，可选) 适配器(adapter，可选) 样例如下： 编写 modelfile 12FROM llama3.2PARAMETER temperature 1 通过如下命令即可创建一个模型： 1ollama create &lt;name&gt; -f ./modelfile 检查一个 model 的模型文件也可以通过如下命令完成： 1ollama show --modelfile &lt;name&gt; 结构化输出 Ollama 提供了官方的结构化输出 API，可以将数据统一转化为对象进行响应。相较于之前的 JSON 模式来说对于枚举的处理更好，但是在转化失败时可能会返回空对象，需按情况使用。 123456789101112131415161718192021222324252627curl -X POST http://localhost:11434/api/chat -H &quot;Content-Type: application/json&quot; -d &#x27;&#123; &quot;model&quot;: &quot;llama3.1&quot;, &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me about Canada.&quot;&#125;], &quot;stream&quot;: false, &quot;format&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;capital&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;languages&quot;: &#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125;, &quot;required&quot;: [ &quot;name&quot;, &quot;capital&quot;, &quot;languages&quot; ] &#125;&#125;&#x27; 图形化界面 Open WebUI 编写如下 docker-compose.yaml 文件： 12345678910111213services: open-webui: image: ghcr.io/open-webui/open-webui:main ports: - &quot;3000:8080&quot; environment: - &#x27;OLLAMA_BASE_URL=&lt;ollama_address&gt;&#x27; - &#x27;WEBUI_SECRET_KEY=&#x27; volumes: - ./data:/app/backend/data extra_hosts: - host.docker.internal:host-gateway restart: always 然后使用如下命令启动即可： 1docker-compose up -d 注：启动时间有些长，记得使用 docker-compose logs -f 看日志。 LobeChat 编写如下 docker-compose.yaml 文件： 1234567services: lobe-chat: image: lobehub/lobe-chat ports: - &quot;3210:3210&quot; environment: - OLLAMA_PROXY_URL=&lt;ollama_address&gt;/v1 然后使用如下命令启动即可： 1docker-compose up -d 使用前端库调用 使用如下代码安装依赖： 1npm install ollama 然后即可编写如下程序访问 API： jsx123456789101112131415161718192021222324252627282930313233343536373839&#x27;use client&#x27;;import &#123;Ollama&#125; from &#x27;ollama/browser&#x27;import React,&#123; useState &#125; from &#x27;react&#x27;;export default function Dashboard() &#123; const ollama = new Ollama(&#123;host: &#x27;http://localhost:11434&#x27;&#125;) const [data, setData] = useState&lt;String&gt;(); const [isLoading, setIsLoading] = useState(false); const fetchData = async () =&gt; &#123; try &#123; setIsLoading(true); const response = await ollama.chat(&#123; model: &#x27;llama3&#x27;, messages: [&#123;role: &#x27;user&#x27;, content: &#x27;Why is the sky blue?&#x27;&#125;], &#125;) setData(response.message.content); &#125; catch (error) &#123; console.error(&#x27;Error fetching data:&#x27;, error); &#125; finally &#123; setIsLoading(false); &#125; &#125;; return ( &lt;div&gt; &lt;h1&gt;Example Page&lt;/h1&gt; &lt;button onClick=&#123;fetchData&#125; disabled=&#123;isLoading&#125;&gt; &#123;isLoading ? &#x27;Loading...&#x27; : &#x27;Fetch Data&#x27;&#125; &lt;/button&gt; &#123;data &amp;&amp; ( &lt;div&gt; &lt;p&gt;&#123;data&#125;&lt;/p&gt; &lt;/div&gt; )&#125; &lt;/div&gt; );&#125; 软件清理与卸载 如果需要删除服务可以使用如下命令： 12sudo rm /etc/systemd/system/ollama.servicesudo rm /etc/systemd/system/ollama.service.d 删除软件则可以使用： 1sudo rm $(which ollama) 模型文件位于： 1sudo rm -r /usr/share/ollama 注：需要检查下 OLLAMA_MODELS 环境变量，它可以进行额外配置。 删除用户和组： 12sudo userdel ollamasudo groupdel ollama Gollama 模型管理工具 Gollama 是一款支持 macOS / Linux 的模型管理工具。可以使用如下代码安装软件： 12go install github.com/sammcj/gollama@HEADexport PATH=$PATH:$(go env GOPATH)/bin 然后即可使用命令查看模型了： 1gollama 内存优化 在上下文过多的情况下也可以通过量化的方式去减少内存占用量，修改如下配置： 12OLLAMA_FLASH_ATTENTION=trueOLLAMA_KV_CACHE_TYPE=f16 注：需要重启服务才能生效，且不同模型效果可能不同，建议先用 Qwen2.5 看下显存占用，修改配置后启动服务再看下。 参考资料 官方项目 官方文档 大模型清单 Embedding models The Ollama Course The Ollama Course: Advanced Gollama 项目 Structured outputs","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"}]},{"title":"阿里云 ECS 数据导出","slug":"tmp/alibaba-cloud","date":"2024-03-04T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/alibaba-cloud/","permalink":"https://wangqian0306.github.io/2024/alibaba-cloud/","excerpt":"","text":"阿里云 ECS 数据导出 简介 如果有从阿里云 ECS 导出数据到本地的需求，可以参考以下方式。 使用方式 导出数据需要先安装如下工具，然后再按照导入流程进行操作。 ossutil ossutil 是阿里云 OSS 的命令行管理工具。 可以访问如下地址获取 安装手册和下载地址 安装之后需要使用如下命令进行配置： 1ossutil config 然后依次配置如下参数： 语言，按需选择即可。 endpoint , 此参数可以定位到 oss bucket 清单 中的详情进行查看。 stsToken ，此参数可以访问 官方手册 进行创建，也可以不使用此种方式，转而使用 accessKey。 accessKeyID ，访问 RAM 访问控制页 选择用户，在 AccessKey 部分即可创建。 accessKeySecret ，同上。 配置完成后可以使用如下命令进行检测： 列出目录 1ossutil ls 下载或上传文件 1ossutil cp &lt;path/ObjectName&gt; &lt;path/ObjectName&gt; QEMU QEMU 是一个通用的开源机器模拟器和虚拟器。 可以访问 下载地址获取软件 注：在 windows 中可以使用 chocolatey 便捷安装 choco install qemu。 导入流程 打开阿里云 ECS 控制台，进入实例列表页面，选择实例，选择快照一致性组，然后创建一致性组。 进入镜像功能，创建自定义镜像，选择创建方式为快照，选择一致性组，创建镜像(建议勾选增加数据盘)。 等待任务完成。 进入 OSS 控制台，选择要下载的文件，使用 ossutil 工具下载文件并进行解压。 注：此处可以选择晚上0点到8点进行下载，收费少一些。 1tar -zxvf &lt;filename&gt; 使用 qemu-img 工具转换镜像文件为 vmdk 格式。 1qemu-img convert -p -f raw &lt;filename&gt;.raw -O vmdk &lt;diskname&gt;.vmdk 创建一个空的虚拟机。 将 vmdk 做为磁盘配置在虚拟机中。 注：在本地启动服务时遇到了 dracut-initqueue timeout could not boot 问题，解决方案参见重构 grub 并重装内核文档。 参考资料 使用快照创建自定义镜像 QEMU 软件下载 阿里云命令行工具 安装 ossutil","categories":[],"tags":[{"name":"Alibaba Cloud","slug":"Alibaba-Cloud","permalink":"https://wangqian0306.github.io/tags/Alibaba-Cloud/"}]},{"title":"Umami","slug":"tmp/umami","date":"2024-02-27T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/umami/","permalink":"https://wangqian0306.github.io/2024/umami/","excerpt":"","text":"Umami 简介 Umami 是一种简单、快速、注重隐私的开源分析解决方案。Umami 是 Google Analytics 的本地部署替代品。 使用方式 云端使用 如果是个人使用，可以注册 Umami Cloud 账户，免费添加三个网站。 收费说明 本地部署 123git clone https://github.com/umami-software/umami.gitcd umaimidocker-compose up -d 之后即可访问 http://localhost:3000 使用用户名 admin 和密码：umami 登录控制页，新增网站并插入脚本即可。 参考资料 官方文档 官方项目","categories":[],"tags":[{"name":"Umami","slug":"Umami","permalink":"https://wangqian0306.github.io/tags/Umami/"}]},{"title":"ComfyUI","slug":"ai/comfyui","date":"2024-02-20T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/comfyui/","permalink":"https://wangqian0306.github.io/2024/comfyui/","excerpt":"","text":"ComfyUI 简介 Comfy 是一款 stable diffusion 的 GUI 和后端。 安装 注：建议使用带 GPU 的设备，本文以 Ubuntu 20.04 系统 CUDA 12.2 Python 3.10 做为样例。 使用如下命令进行安装： 1234sudo apt-get install nvidia-cudnn -ypip3 install torch torchvision torchaudiogit clone https://github.com/comfyanonymous/ComfyUI.gitpip3 install -r requirements.txt 在运行前需要先获取到 sdxl 模型 下载后将其放置在 models/checkpoints 目录。 然后需要获取 sdxl-vae 模型 下载后将其放置在 models/vae 目录。 之后即可使用如下命令开启服务： 1python3 main.py --listen 0.0.0.0 点击 Queue Prompt 即可触发一次绘制。 点击 View Queue 即可查看任务清单。 绘制样例 在 ComfyUI 的 web 控制台中可以通过拖拽放入图片或者载入图片或配置文件的方式，来快速拷贝绘图配置。 功能示例 注：在使用的时候注意切换模型。 参考资料 官方项目 客制化模型下载 社区文档","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"IDS 和 IPS","slug":"linux/ids_ips","date":"2024-02-04T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2024/ids_ips/","permalink":"https://wangqian0306.github.io/2024/ids_ips/","excerpt":"","text":"IDS 和 IPS 简介 IDS 是英文“Intrusion Detection Systems”的缩写，中文意思是“入侵检测系统”。专业上讲就是依照一定的安全策略，对网络、系统的运行状况进行监视，尽可能发现各种攻击企图、攻击行为或者攻击结果，以保证网络系统资源的机密性、完整性和可用性。 IPS 是英文“Intrusion Prevention System”的缩写,中文意思为“入侵防御系统”，IPS 可以说是 IDS 的新一代产品。IPS 位于防火墙和网络的设备之间。这样，如果检测到攻击，IPS 会在这种攻击扩散到网络的其它地方之前阻止这个恶意的通信。IDS 是存在于网络之外起到报警的作用，而不是在你的网络前面起到防御的作用。目前有很多种 IPS 系统，它们使用的技术都不相同。但是，一般来说，IPS 系统都依靠对数据包的检测。IPS 将检查入网的数据包，确定这种数据包的真正用途，然后决定是否允许这种数据包进入你的网络。 Suricata 安装和使用 注：此处以 Rocky Linux 样例。 123456sudo dnf install epel-release -ysudo dnf install suricata -ysudo suricata-update update-sourcessudo suricata-update enable-source et/opensudo suricata-updatesystemctl enable suricata.service --now 参考资料 常见网络安全设备：IDS（入侵检测系统） 使用 Azure 网络观察程序和开源工具执行网络入侵检测 基于网络的入侵检测系统的几种？常用开源NIDS，让我们来了解一下 Suricata Snort Security Onion","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"IDS","slug":"IDS","permalink":"https://wangqian0306.github.io/tags/IDS/"}]},{"title":"NCBI","slug":"ocean/ncbi","date":"2024-02-02T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2024/ncbi/","permalink":"https://wangqian0306.github.io/2024/ncbi/","excerpt":"","text":"NCBI 简介 美国国家生物技术信息中心(National Center for Biotechnology Information) 通过提供对生物医学和基因组信息的访问来促进科学和健康。 数据下载 NCBI 中的数据需要使用 Aspera 软件或官网指定的下载工具进行下载。 数据使用 在本地使用需要 Blast 工具进行检索和使用。 参考资料 NCBI 数据下载页 Aspera 软件下载","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"NCBI","slug":"NCBI","permalink":"https://wangqian0306.github.io/tags/NCBI/"}]},{"title":"Cockpit","slug":"linux/cockpit","date":"2024-01-15T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2024/cockpit/","permalink":"https://wangqian0306.github.io/2024/cockpit/","excerpt":"","text":"Cockpit 简介 Cockpit 是一个基于 Web 的服务器管理图形界面。 安装 注：本文以 Rocky Linux 做为样例。 1234sudo yum install cockpitsudo systemctl enable --now cockpit.socketsudo firewall-cmd --permanent --zone=public --add-service=cockpitsudo firewall-cmd --reload 插件 Cockpit 还提供了很多 插件 ： 文件管理 1234git clone https://github.com/45Drives/cockpit-navigator.gitcd cockpit-navigator/sudo make installsudo systemctl restart cockpit.socket cockpit-navigator 用户及 NFS 与 Samba 管理 且在安装此服务的时候需要提前配置好 NFS 和 Samba 服务与网络，且将 Selinux 设置为 Permissive。 12345678sudo yum install coreutils glibc-common hostname passwd psmisc samba-common-tools shadow-utils util-linux util-linux-user perl openssh -ycurl -LO https://github.com/45Drives/cockpit-identities/releases/download/v0.1.12/cockpit-identities_0.1.12_generic.zipcd cockpit-identities_0.1.12_generic/sudo make installsudo yum install attr coreutils glibc-common nfs-utils samba-common-tools samba-winbind-clients gawk -ycurl -LO https://github.com/45Drives/cockpit-file-sharing/releases/download/v3.2.9/cockpit-file-sharing_3.2.9_generic.zipcd cockpit-file-sharing_3.2.9_genericsudo make install 注：在安装时出现了文件移动错误的问题 cp: cannot stat 'system_files/*': No such file or directory, 可以忽略。 cockpit-identities cockpit-file-sharing 参考资料 官方网站 Cockpit安装配置速记","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Tabby","slug":"tools/tabby","date":"2024-01-10T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2024/tabby/","permalink":"https://wangqian0306.github.io/2024/tabby/","excerpt":"","text":"Tabby 简介 Tabby 是一个自托管的人工智能编码助手 (GitHub Copilot 的开源和本地版)。 安装 在 Linux 环境中可以使用 Docker Compose 进行便捷的部署，在运行之前设备已经安装和配置了 GPU 驱动和容器直通。如果有此方面的配置问题可以参照博客中的 nvidia 文档。 首先需要编写 docker-compose.yaml 文件： 1234567891011121314151617181920services: tabby: restart: always image: tabbyml/tabby command: serve --model TabbyML/StarCoder-1B --device cuda volumes: - &quot;./tabby/data:/data&quot; ports: - 8080:8080 environment: TZ: Asia/Shanghai TABBY_DOWNLOAD_HOST: modelscope.cn TABBY_DISABLE_USAGE_COLLECTION: 1 deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] 注：此处为了测试，所以采用的模型比较小，在实际使用时需按照硬件和需求调整 模型。 然后即可使用如下命令运行容器： 1docker-compose up -d 由于在初次启动时需要拉取模型所以耗时有些高，可以通过查看日志的方式确认服务的运行情况： 1docker-compose logs -f 若出现如下字样则代表程序运行正常。 1INFO tabby::routes: crates/tabby/src/routes/mod.rs:35: Listening at 0.0.0.0:8080 在 IDE 中下载 tabby 插件，并参照 插件配置文档 进行配置即可。 参考资料 官方文档 官方项目 NVIDIA CONTAINER TOOLKIT","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"}]},{"title":"LangChain","slug":"ai/langchain","date":"2024-01-08T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2024/langchain/","permalink":"https://wangqian0306.github.io/2024/langchain/","excerpt":"","text":"LangChain 简介 LangChain 是一个用于开发由语言模型驱动的应用程序的框架，由以下几个部分组成： LangChain Libraries：Python 和 JavaScript 库，通过 LangChain 库可以更便捷的与大模型进行交互 LangChain Templates：一组易于部署的参考架构，适用于各种任务 LangServe：REST API 服务器 LangSmith：调试监控平台(开发中，目前需要在云上) 使用方式 Llama-2 由于 Llama-2 官方下载工具并没有提供运行相关的客户端和交互式命令行等内容，所以此处可以通过 Ollama 项目运行，具体流程如下： 注：最好有 GPU 且可以运行的模型参数与显存相关，设备需要先安装驱动。 1234567curl https://ollama.ai/install.sh | shollama pull llama2sudo mkdir -p /etc/systemd/system/ollama.service.dsudo echo &#x27;[Service]&#x27; &gt;&gt; /etc/systemd/system/ollama.service.d/environment.confsudo echo &#x27;Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;&#x27; &gt;&gt; /etc/systemd/system/ollama.service.d/environment.confsudo systemctl daemon-reloadsudo systemctl restart ollama 然后可以使用如下命令测试 ollama 模型运行情况： 1ollama run llama2 若可以正常使用则可以尝试调用 API： 1234curl http://localhost:11434/api/generate -d &#x27;&#123; &quot;model&quot;: &quot;llama2&quot;, &quot;prompt&quot;:&quot;Why is the sky blue?&quot;&#125;&#x27; 然后即可安装 Langchain ： 123456pip install langchainpip install &quot;langserve[all]&quot;pip install langchain-clipip install pydantic==1.10.13pip install pdfminerpip install pdfminer.six 然后可以编写服务端程序 demo-server.py： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import base64from typing import Any, Dict, List, Tuplefrom fastapi import FastAPIfrom fastapi.middleware.cors import CORSMiddlewarefrom langchain_community.chat_models import ChatOllamafrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers.pdf import PDFMinerParserfrom langchain.pydantic_v1 import BaseModel, Fieldfrom langchain.schema.messages import ( AIMessage, BaseMessage, FunctionMessage, HumanMessage,)from langchain.schema.runnable import RunnableLambdafrom langchain_core.runnables import RunnableParallelfrom langserve import CustomUserTypefrom langserve.server import add_routesapp = FastAPI( title=&quot;LangChain Server&quot;, version=&quot;1.0&quot;, description=&quot;Spin up a simple api server using Langchain&#x27;s Runnable interfaces&quot;,)app.add_middleware( CORSMiddleware, allow_origins=[&quot;*&quot;], allow_credentials=True, allow_methods=[&quot;*&quot;], allow_headers=[&quot;*&quot;], expose_headers=[&quot;*&quot;],)class ChatHistory(CustomUserType): chat_history: List[Tuple[str, str]] = Field( ..., examples=[[(&quot;a&quot;, &quot;aa&quot;)]], extra=&#123;&quot;widget&quot;: &#123;&quot;type&quot;: &quot;chat&quot;, &quot;input&quot;: &quot;question&quot;, &quot;output&quot;: &quot;answer&quot;&#125;&#125;, ) question: strdef _format_to_messages(input: ChatHistory) -&gt; List[BaseMessage]: &quot;&quot;&quot;Format the input to a list of messages.&quot;&quot;&quot; history = input.chat_history user_input = input.question messages = [] for human, ai in history: messages.append(HumanMessage(content=human)) messages.append(AIMessage(content=ai)) messages.append(HumanMessage(content=user_input)) return messagesmodel = ChatOllama(model=&quot;llama2&quot;)chat_model = RunnableParallel(&#123;&quot;answer&quot;: (RunnableLambda(_format_to_messages) | model)&#125;)add_routes( app, chat_model.with_types(input_type=ChatHistory), config_keys=[&quot;configurable&quot;], path=&quot;/chat&quot;,)class FileProcessingRequest(BaseModel): file: bytes = Field(..., extra=&#123;&quot;widget&quot;: &#123;&quot;type&quot;: &quot;base64file&quot;&#125;&#125;) num_chars: int = 100def process_file(input: Dict[str, Any]) -&gt; str: &quot;&quot;&quot;Extract the text from the first page of the PDF.&quot;&quot;&quot; content = base64.decodebytes(input[&quot;file&quot;]) blob = Blob(data=content) documents = list(PDFMinerParser().lazy_parse(blob)) content = documents[0].page_content return content[: input[&quot;num_chars&quot;]]add_routes( app, RunnableLambda(process_file).with_types(input_type=FileProcessingRequest), config_keys=[&quot;configurable&quot;], path=&quot;/pdf&quot;,)add_routes( app, model, path=&quot;/ollama&quot;,)if __name__ == &quot;__main__&quot;: import uvicorn uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000) 使用如下命令即可运行服务端： 1python demo-server.py 然后访问如下页面即可： doc ollama playground chat playground pdf playground 参考资料 官方手册 官方项目 Ollama LangChain Tools","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"LLAMA","slug":"LLAMA","permalink":"https://wangqian0306.github.io/tags/LLAMA/"}]},{"title":"MDC","slug":"spring/mdc","date":"2023-12-21T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/mdc/","permalink":"https://wangqian0306.github.io/2023/mdc/","excerpt":"","text":"MDC 简介 MDC(Mapped Diagnostic Context) 是 Log4j 日志库的一个概念或功能，可用于将相关日志消息分组在一起。 使用方式 首先需要编写一个过滤器用于拦截请求： 123456789101112131415161718192021222324252627282930import jakarta.servlet.*;import jakarta.servlet.http.HttpServletRequest;import lombok.extern.slf4j.Slf4j;import org.slf4j.MDC;import java.io.IOException;import java.time.LocalDateTime;import java.time.ZoneOffset;import java.util.UUID;@Slf4jpublic class MdcFilter implements Filter &#123; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; try &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; String traceId = httpServletRequest.getHeader(&quot;traceId&quot;); if (traceId == null) &#123; traceId = UUID.randomUUID().toString(); &#125; MDC.put(&quot;traceId&quot;, traceId); MDC.put(&quot;ts&quot;, String.valueOf(LocalDateTime.now().toInstant(ZoneOffset.UTC).toEpochMilli())); log.info(httpServletRequest.getRequestURL() + &quot; call received&quot;); chain.doFilter(request, response); &#125; finally &#123; MDC.clear(); &#125; &#125;&#125; 然后需要将此过滤器加载到 Spring ： 123456789101112131415161718192021import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class Application &#123; @Bean public FilterRegistrationBean&lt;MdcFilter&gt; loggingFilter() &#123; FilterRegistrationBean&lt;MdcFilter&gt; registrationBean = new FilterRegistrationBean&lt;&gt;(); registrationBean.setFilter(new MdcFilter()); registrationBean.addUrlPatterns(&quot;/*&quot;); return registrationBean; &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 编写如下配置文件设置显示样式： 12345678server: port: 8080spring: application: name: demoLogging: pattern: level: &#x27;%5p [$&#123;spring.application.name:&#125;,%mdc&#123;traceId:-&#125;,%mdc&#123;ts:-&#125;]&#x27; 然后需要编写一个样例的 Controller ： 123456789101112131415161718import lombok.extern.slf4j.Slf4j;import org.slf4j.MDC;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@Slf4j@RestController@RequestMappingpublic class DemoController &#123; @GetMapping(&quot;/demo&quot;) public String hello() &#123; log.info(&quot;TraceId: &#123;&#125; - Hello World!&quot;, MDC.get(&quot;traceId&quot;)); return &quot;Hello World!&quot;; &#125;&#125; 访问 http://localhost:8080/demo 即可看到结果日志。 参考资料 How to distinguish logging per Client or Request in Java? Use MDC or Mapped Diagnostic Context in Log4j Example 基于MDC实现长链路跟踪","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Emengercy Mode","slug":"linux/emergency","date":"2023-12-13T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/emengercy/","permalink":"https://wangqian0306.github.io/2023/emengercy/","excerpt":"","text":"Emengercy Mode 简介 当操作系统崩溃，无法正常启动时就会进入紧急模式。 常见问题及解决办法 进入紧急模式之后，需要输入 root 密码进入交互式命令行。然后可以使用如下命令查看启动日志： 1journalctl -xb 磁盘故障 注：日志比较杂需要多翻翻，如果出现 mount xxx 字样则代表磁盘故障。 可以使用如下命令看到出现故障的磁盘： 1df -TH 出现故障的磁盘会有标识类似如下样例： 1/dev/sdx1 解决此问题可以通过解除挂载的方式实现： 1vim /etc/fstab 注：然后使用 # 注释该行即可。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Jenkins","slug":"tmp/jenkins","date":"2023-12-12T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/jenkins/","permalink":"https://wangqian0306.github.io/2023/jenkins/","excerpt":"","text":"Jenkins 简介 Jenkins 是一个开源、可扩展的持续集成、交付、部署(软件/代码的编译、打包、部署)的平台。 密码重置 找到配置目录 注：服务安装版 1cd /var/lib/jenkins 注：Docker 版 1cd /var/jenkins_home 修改配置文件 1vim config.xml 改写如下配置： 1&lt;useSecurity&gt;false&lt;/useSecurity&gt; 重新启动服务 访问服务，改写如下配置： 1234567- Manage Jenkins - Configure Global Security - Enable Security - Security Realm - Jenkins&#x27;s own user database - Authentication - Role-based strategy 注：此处权限部分请根据实际情况修改。 重新设置密码，改写如下配置： 1234- People - &lt;user&gt; - Configure - Password 重新启动服务即可，无需修改配置文件 参考资料 官网","categories":[],"tags":[]},{"title":"PyTorch","slug":"ai/pytorch","date":"2023-12-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/torch/","permalink":"https://wangqian0306.github.io/2023/torch/","excerpt":"","text":"PyTorch 简介 PyTorch 是一个开源的机器学习库，由 Facebook 的人工智能研究实验室 (FAIR)开发。它提供了一个动态计算图，并建立在 Python 之上。PyTorch 主要用于构建深度神经网络，但也可用于其他类型的机器学习模型。 安装 注：建议使用带 GPU 的设备，本文以 Linux 系统 CUDA 12.0 Python 3.9.18 做为样例。 1pip3 install torch torchvision torchaudio opencv-python 常见用例 目标识别(Object Detection) 注：PyTorch 官方提供了很多的预训练模型，由于想最快的显示标注结果所以选择 FasterRCNN_MobileNet 作为样例，其余 预训练模型 可以在这里找到。 识别图像(使用 CPU) 123456789101112131415161718192021222324252627282930313233343536373839404142from torchvision.io.image import read_imagefrom torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn,FasterRCNN_MobileNet_V3_Large_FPN_Weightsfrom torchvision.utils import draw_bounding_boxesfrom torchvision.transforms.functional import to_pil_imagedef main(): # Step 0: Set input and output file paths input_path = &quot;input.jpg&quot; output_path = &quot;output.jpg&quot; # Step 1: Read the input image img = read_image(input_path) # Step 2: Initialize model with the best available weights weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights, box_score_thresh=0.9) model.eval() # Step 3: Initialize the inference transforms preprocess = weights.transforms() # Step 4: Apply inference preprocessing transforms batch = [preprocess(img)] # Step 5: Use the model and visualize the prediction prediction = model(batch)[0] labels = [weights.meta[&quot;categories&quot;][i] for i in prediction[&quot;labels&quot;]] boxed_img = draw_bounding_boxes(img, boxes=prediction[&quot;boxes&quot;], labels=labels, colors=&quot;red&quot;, width=4, font=&quot;LiberationSans-Regular&quot;) # Step 6: Save the output image im = to_pil_image(boxed_img.detach()) im.save(output_path)if __name__ == &quot;__main__&quot;: main() 识别视频(使用 GPU)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from torchvision.models.detection import FasterRCNN_MobileNet_V3_Large_FPN_Weights, fasterrcnn_mobilenet_v3_large_fpnimport torchimport torchvision.transforms as Timport cv2import datetimedef main(): # Step 0: Set input and output file paths input_path = &quot;input.mp4&quot; output_path = &quot;output.mp4&quot; weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights, box_score_thresh=0.9) model = model.cuda() model.eval() cap = cv2.VideoCapture(input_path) fps = cap.get(cv2.CAP_PROP_FPS) width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Define the codec and create VideoWriter object for output video fourcc = cv2.VideoWriter_fourcc(*&#x27;mp4v&#x27;) out = cv2.VideoWriter(output_path, fourcc, fps, (width, height)) start = datetime.datetime.now() while cap.isOpened(): ret, frame = cap.read() if not ret: break # Perform inference on the frame here transform = T.Compose([T.ToTensor()]) img_tensor = transform(frame).unsqueeze(0) img_tensor = img_tensor.cuda() with torch.no_grad(): prediction = model(img_tensor) boxes = prediction[0][&#x27;boxes&#x27;].cpu().numpy() labels = [weights.meta[&quot;categories&quot;][i] for i in prediction[0][&quot;labels&quot;]] for box, label in zip(boxes, labels): box = list(map(int, box)) frame = cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2) frame = cv2.putText(frame, f&#x27;Label: &#123;label&#125;&#x27;, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) # Display or save the marked frame out.write(frame) cv2.imshow(&#x27;Object Detection&#x27;, frame) if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;): break end = datetime.datetime.now() print(end-start) cap.release() cv2.destroyAllWindows()if __name__ == &quot;__main__&quot;: main() 注：在 model 和 img_tensor 处使用 cuda() 方法即可。 参考资料 项目官网 预训练模型","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"GluonCV","slug":"ai/gluoncv","date":"2023-11-29T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/gluoncv/","permalink":"https://wangqian0306.github.io/2023/gluoncv/","excerpt":"","text":"GluonCV 简介 GloonCV 提供了最先进的(SOTA) 深度学习算法在计算机视觉中的实现。 注：此项目很久没更新了，而且依赖的 Apache MXNet 项目已经处于 archive 状态。 安装 注：在测试中采用的系统版本是 Rocky Linux 9.3 Python 版本是 3.9.18 ，在训练和使用模型时使用显卡可以显著的减少训练时常。 访问 安装手册页 和PyTorch 官网 可以获取更多的安装样例，本文仅使用 CPU 作为样例。 1234pip3 install --upgrade mxnetpip3 install torch torchvision torchaudiopip3 install --upgrade gluoncvpip3 install mxnet-mkl==1.5.0 使用 目标识别(Object Detection) 编写如下样例程序即可： 12345678910from gluoncv import model_zoo, data, utilsfrom matplotlib import pyplot as pltnet = model_zoo.get_model(&#x27;ssd_512_resnet50_v1_voc&#x27;, pretrained=True)im_fname = utils.download(&#x27;https://github.com/dmlc/web-data/blob/master/&#x27; + &#x27;gluoncv/detection/street_small.jpg?raw=true&#x27;,path=&#x27;street_small.jpg&#x27;)x, img = data.transforms.presets.ssd.load_test(im_fname, short=512)print(&#x27;Shape of pre-processed image:&#x27;, x.shape)class_IDs, scores, bounding_boxes = net(x)ax = utils.viz.plot_bbox(img, bounding_boxes[0], scores[0], class_IDs[0], class_names=net.classes)plt.savefig(&#x27;output.png&#x27;) 可以使用如下程序监控视频流： 123456789101112131415161718192021222324252627282930import gluoncv as gcvfrom gluoncv.utils import try_import_cv2cv2 = try_import_cv2()import datetimeimport mxnet as mxnet = gcv.model_zoo.get_model(&#x27;ssd_512_mobilenet1.0_voc&#x27;, pretrained=True)net.hybridize()cap = cv2.VideoCapture(&#x27;face-demographics-walking.mp4&#x27;)axes = Noneprint(datetime.datetime.now())while True: ret, frame = cap.read() if ret: frame = mx.nd.array(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).astype(&#x27;uint8&#x27;) rgb_nd, frame = gcv.data.transforms.presets.ssd.transform_test(frame, short=512, max_size=700) class_IDs, scores, bounding_boxes = net(rgb_nd) img = gcv.utils.viz.cv_plot_bbox(frame, bounding_boxes[0], scores[0], class_IDs[0], class_names=net.classes) gcv.utils.viz.cv_plot_image(img) else: break # Press &#x27;q&#x27; to quit if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;): breakprint(datetime.datetime.now())cap.release()cv2.destroyAllWindows() 参考资料 官方网站 官方手册 测试视频项目","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"Simple Python Version Management(pyenv)","slug":"ai/pyenv","date":"2023-11-27T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/pyenv/","permalink":"https://wangqian0306.github.io/2023/pyenv/","excerpt":"","text":"Simple Python Version Management(pyenv) 简介 pyenv 是一款多个版本 Python 的管理工具。 安装方式 使用如下命令安装： CentOS, Fedora, Rocky Linux 12yum -y install git gcc zlib-devel bzip2-devel readline-devel sqlite-devel openssl-devel libffi libffi-devel tk-devel xz xz-devel -ycurl https://pyenv.run | bash Ubuntu 12sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev openssl git -ycurl https://pyenv.run | bash 然后在 ~/.bashrc 中添加如下内容： 12345# pyenvexport PYENV_ROOT=&quot;$HOME/.pyenv&quot;[[ -d $PYENV_ROOT/bin ]] &amp;&amp; export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;eval &quot;$(pyenv init -)&quot;eval &quot;$(pyenv virtualenv-init -)&quot; 基本使用 列出已装的 Python ： 1pyenv versions 查看可以安装的 Python 版本： 1pyenv install -l 安装特定版本的 Python： 1pyenv install &lt;VERSION&gt; 注：如果没有精细的版本可以省略，比方说 3.10 的最新版可以略写为 3.10 切换版本： 1pyenv local &lt;VERSION&gt; 创建 venv 环境： 1pyenv virtualenv &lt;version&gt; &lt;venv_name&gt; 列出 venv 环境： 1pyenv virtualenvs 使用 venv 环境： 1pyenv activate &lt;venv_name&gt; 退出 venv 环境： 1pyenv deactivate 卸载版本： 1pyenv uninstall &lt;VERSION&gt; 更新软件： 1pyenv update 参考资料 官方项目 安装脚本项目","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"YOLO","slug":"ai/yolo","date":"2023-11-23T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/yolo/","permalink":"https://wangqian0306.github.io/2023/yolo/","excerpt":"","text":"YOLO 简介 YOLO(You Only Look Once) 是一款目标检测和图像分割模型。 注：AGPL-3.0 协议，可以付费商用。 使用场景 车辆跟踪识别 此样例严格限制 Python 版本 &gt;=3.7,❤️.11 注：在测试中采用的系统版本是 Rocky Linux 9.3 Python 版本是 3.9.18 ，在训练和使用模型时使用显卡可以显著的减少训练时常。 在使用前可以访问 PyTorch 官网根据实际情况获取安装命令然后进行安装。 使用如下命令安装依赖： 1pip3 install ultralytics 使用如下 Python 程序进行测试： 12import ultralyticsultralytics.checks() 使用如下命令安装 C 环境： 1yum install gcc gcc-c++ cmake -y 使用如下命令安装 ByteTrack : 12git clone https://github.com/ifzhang/ByteTrack.gitcd ByteTrack 然后编辑 requirements.txt： 12345678910111213141516171819202122232425# overwrite protobuf versionprotobuf==3.19.6# TODO: Update with exact module versionnumpy==1.23.4torch&gt;=1.7opencv_pythonloguruscikit-imagetqdmtorchvision&gt;=0.10.0Pillowthopninjatabulatetensorboardlapxmotmetricsfilterpyh5py# verified versionsonnx==1.9.0onnxruntimeonnx-simplifier==0.3.5 123456pip3 install -r requirements.txtpython3 setup.py developpip3 install cython; pip3 install &#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27;pip3 install cython_bboxpip3 install -q onemetricpip3 install -q loguru lap thop 使用如下 Python 程序进行测试： 123456import syssys.path.append(f&quot;&#123;HOME&#125;/ByteTrack&quot;)import yoloxprint(&quot;yolox.__version__:&quot;, yolox.__version__) 使用如下命令安装 Roboflow Supervision ： 1pip3 install supervision==0.1.0 使用如下 Python 程序进行测试： 12import supervisionprint(&quot;supervision.__version__:&quot;, supervision.__version__) 下载演示视频流： 1wget --load-cookies /tmp/cookies.txt &quot;https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate &#x27;https://docs.google.com/uc?export=download&amp;id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-&#x27; -O- | sed -rn &#x27;s/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p&#x27;)&amp;id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-&quot; -O vehicle-counting.mp4 &amp;&amp; rm -rf /tmp/cookies.txt 编写示例程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162SOURCE_VIDEO_PATH = &quot;&lt;VIDEO_PATH&gt;&quot;TARGET_VIDEO_PATH = &quot;&lt;PATH&gt;/vehicle-counting-result.mp4&quot;import syssys.path.append(&quot;&lt;PATH&gt;/ByteTrack&quot;)import yoloxfrom yolox.tracker.byte_tracker import BYTETracker, STrackfrom onemetric.cv.utils.iou import box_iou_batchfrom dataclasses import dataclass@dataclass(frozen=True)class BYTETrackerArgs: track_thresh: float = 0.25 track_buffer: int = 30 match_thresh: float = 0.8 aspect_ratio_thresh: float = 3.0 min_box_area: float = 1.0 mot20: bool = Falseimport supervisionfrom supervision.draw.color import ColorPalettefrom supervision.geometry.dataclasses import Pointfrom supervision.video.dataclasses import VideoInfofrom supervision.video.source import get_video_frames_generatorfrom supervision.video.sink import VideoSinkfrom supervision.notebook.utils import show_frame_in_notebookfrom supervision.tools.detections import Detections, BoxAnnotatorfrom supervision.tools.line_counter import LineCounter, LineCounterAnnotatorfrom typing import Listimport numpy as np# converts Detections into format that can be consumed by match_detections_with_tracks functiondef detections2boxes(detections: Detections) -&gt; np.ndarray: return np.hstack(( detections.xyxy, detections.confidence[:, np.newaxis] ))# converts List[STrack] into format that can be consumed by match_detections_with_tracks functiondef tracks2boxes(tracks: List[STrack]) -&gt; np.ndarray: return np.array([ track.tlbr for track in tracks ], dtype=float)# matches our bounding boxes with predictionsdef match_detections_with_tracks( detections: Detections, tracks: List[STrack]) -&gt; Detections: if not np.any(detections.xyxy) or len(tracks) == 0: return np.empty((0,)) tracks_boxes = tracks2boxes(tracks=tracks) iou = box_iou_batch(tracks_boxes, detections.xyxy) track2detection = np.argmax(iou, axis=1) tracker_ids = [None] * len(detections) for tracker_index, detection_index in enumerate(track2detection): if iou[tracker_index, detection_index] != 0: tracker_ids[detection_index] = tracks[tracker_index].track_id return tracker_idsMODEL = &quot;yolov8x.pt&quot;from ultralytics import YOLOmodel = YOLO(MODEL)model.fuse()CLASS_NAMES_DICT = model.model.names# class_ids of interest - car, motorcycle, bus and truckCLASS_ID = [2, 3, 5, 7]# create frame generatorgenerator = get_video_frames_generator(SOURCE_VIDEO_PATH)# create instance of BoxAnnotatorbox_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)# acquire first video frameiterator = iter(generator)frame = next(iterator)# model prediction on single frame and conversion to supervision Detectionsresults = model(frame)detections = Detections( xyxy=results[0].boxes.xyxy.cpu().numpy(), confidence=results[0].boxes.conf.cpu().numpy(), class_id=results[0].boxes.cls.cpu().numpy().astype(int))# format custom labelslabels = [ f&quot;&#123;CLASS_NAMES_DICT[class_id]&#125; &#123;confidence:0.2f&#125;&quot; for _, confidence, class_id, tracker_id in detections]# annotate and display frame#frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)# show_frame_in_notebook(frame, (16, 16))LINE_START = Point(50, 1500)LINE_END = Point(3840-50, 1500)VideoInfo.from_video_path(SOURCE_VIDEO_PATH)# from tqdm.notebook import tqdm# create BYTETracker instancebyte_tracker = BYTETracker(BYTETrackerArgs())# create VideoInfo instancevideo_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)# create frame generatorgenerator = get_video_frames_generator(SOURCE_VIDEO_PATH)# create LineCounter instanceline_counter = LineCounter(start=LINE_START, end=LINE_END)# create instance of BoxAnnotator and LineCounterAnnotatorbox_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)# open target video filewith VideoSink(TARGET_VIDEO_PATH, video_info) as sink: # loop over video frames for frame in generator: # model prediction on single frame and conversion to supervision Detections results = model(frame) detections = Detections( xyxy=results[0].boxes.xyxy.cpu().numpy(), confidence=results[0].boxes.conf.cpu().numpy(), class_id=results[0].boxes.cls.cpu().numpy().astype(int) ) # filtering out detections with unwanted classes mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool) detections.filter(mask=mask, inplace=True) # tracking detections tracks = byte_tracker.update( output_results=detections2boxes(detections=detections), img_info=frame.shape, img_size=frame.shape ) tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks) detections.tracker_id = np.array(tracker_id) # filtering out detections without trackers mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool) detections.filter(mask=mask, inplace=True) # format custom labels labels = [ f&quot;#&#123;tracker_id&#125; &#123;CLASS_NAMES_DICT[class_id]&#125; &#123;confidence:0.2f&#125;&quot; for _, confidence, class_id, tracker_id in detections ] # updating line counter line_counter.update(detections=detections) # annotate and display frame frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels) line_annotator.annotate(frame=frame, line_counter=line_counter) sink.write_frame(frame) 运行程序即可在设定目录找到渲染视频。 参考资料 官方项目 官方手册 车辆跟踪识别","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"Jupyter notebook","slug":"ocean/jupiter","date":"2023-11-22T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/jupyter/","permalink":"https://wangqian0306.github.io/2023/jupyter/","excerpt":"","text":"Jupyter notebook 简介 Jupyter Notebook(前身是IPython Notebook)是一个基于 Web 的交互式计算环境，用于创建 Jupyter Notebook 文档。Jupyter Notebook 文档是一个 JSON 文档，包含一个有序的输入/输出单元格列表，这些单元格可以包含代码、文本、数学、图表和媒体内容，通常使用“.ipynb”作为扩展名。 使用方式 注：安装需要 Python 3 环境。 1234pip install notebookfirewall-cmd --permanent --add-port=8888/tcpfirewall-cmd --reloadjupyter notebook --ip 0.0.0.0 之后根据命令行提示即可通过局域网访问此网页 参考资料 官方网站 官方文档","categories":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"GraphCast","slug":"ocean/graphcast","date":"2023-11-21T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/graphcast/","permalink":"https://wangqian0306.github.io/2023/graphcast/","excerpt":"","text":"GraphCast 简介 GraphCast 是一种基于机器学习的天气预报方式，单纯使用数据进行训练和预测而不是采用当前数据进行计算。 使用方式 原生样例 安装依赖： 1234567pip install --upgrade https://github.com/deepmind/graphcast/archive/master.zippip uninstall -y shapelyyum install gcc gcc-c++ python3.11-devel epel-release -yyum install geos geos-devel -ypip install shapely --no-binary shapelygit clone https://github.com/deepmind/graphcastcd graphcast 具体使用请参见 Colab 在项目中可以找到 graphcast.py 文件，此文件即时程序运行的入口。可以参照如下代码运行程序获取预测数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import dataclassesimport functoolsfrom graphcast import autoregressivefrom graphcast import castingfrom graphcast import data_utilsfrom graphcast import graphcastfrom graphcast import normalizationfrom graphcast import rolloutfrom graphcast import xarray_jaxfrom graphcast import xarray_treeimport haiku as hkimport jaximport numpy as npimport xarraydef parse_file_parts(file_name): return dict(part.split(&quot;-&quot;, 1) for part in file_name.split(&quot;_&quot;))def construct_wrapped_graphcast( model_config: graphcast.ModelConfig, task_config: graphcast.TaskConfig): &quot;&quot;&quot;Constructs and wraps the GraphCast Predictor.&quot;&quot;&quot; # Deeper one-step predictor. predictor = graphcast.GraphCast(model_config, task_config) # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to # from/to float32 to/from BFloat16. predictor = casting.Bfloat16Cast(predictor) # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from # BFloat16 happens after applying normalization to the inputs/targets. predictor = normalization.InputsAndResiduals( predictor, diffs_stddev_by_level=diffs_stddev_by_level, mean_by_level=mean_by_level, stddev_by_level=stddev_by_level) # Wraps everything so the one-step model can produce trajectories. predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True) return predictor@hk.transform_with_statedef run_forward(model_config, task_config, inputs, targets_template, forcings): predictor = construct_wrapped_graphcast(model_config, task_config) return predictor(inputs, targets_template=targets_template, forcings=forcings)@hk.transform_with_statedef loss_fn(model_config, task_config, inputs, targets, forcings): predictor = construct_wrapped_graphcast(model_config, task_config) loss, diagnostics = predictor.loss(inputs, targets, forcings) return xarray_tree.map_structure( lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True), (loss, diagnostics))def grads_fn(params, state, model_config, task_config, inputs, targets, forcings): def _aux(params, state, i, t, f): (loss, diagnostics), next_state = loss_fn.apply( params, state, jax.random.PRNGKey(0), model_config, task_config, i, t, f) return loss, (diagnostics, next_state) (loss, (diagnostics, next_state)), grads = jax.value_and_grad( _aux, has_aux=True)(params, state, inputs, targets, forcings) return loss, diagnostics, next_state, grads# Jax doesn&#x27;t seem to like passing configs as args through the jit. Passing it# in via partial (instead of capture by closure) forces jax to invalidate the# jit cache if you change configs.def with_configs(fn): return functools.partial( fn, model_config=model_config, task_config=task_config)# Always pass params and state, so the usage below are simplerdef with_params(fn): return functools.partial(fn, params=params, state=state)# Our models aren&#x27;t stateful, so the state is always empty, so just return the# predictions. This is requiredy by our rollout code, and generally simpler.def drop_state(fn): return lambda **kw: fn(**kw)[0]if __name__ == &quot;__main__&quot;: file = &quot;xxxx.nc&quot; params = None state = &#123;&#125; model_config = graphcast.ModelConfig( resolution=0, mesh_size=random_mesh_size.value, latent_size=random_latent_size.value, gnn_msg_steps=random_gnn_msg_steps.value, hidden_layers=1, radius_query_fraction_edge_length=0.6) task_config = graphcast.TaskConfig( input_variables=graphcast.TASK.input_variables, target_variables=graphcast.TASK.target_variables, forcing_variables=graphcast.TASK.forcing_variables, pressure_levels=graphcast.PRESSURE_LEVELS[random_levels.value], input_duration=graphcast.TASK.input_duration, ) example_batch = xarray.load_dataset(file).compute() train_steps = 1 eval_steps = 1 train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings( example_batch, target_lead_times=slice(&quot;6h&quot;, f&quot;&#123;train_steps*6&#125;h&quot;), **dataclasses.asdict(task_config)) eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings( example_batch, target_lead_times=slice(&quot;6h&quot;, f&quot;&#123;eval_steps*6&#125;h&quot;), **dataclasses.asdict(task_config)) print(&quot;All Examples: &quot;, example_batch.dims.mapping) print(&quot;Train Inputs: &quot;, train_inputs.dims.mapping) print(&quot;Train Targets: &quot;, train_targets.dims.mapping) print(&quot;Train Forcings:&quot;, train_forcings.dims.mapping) print(&quot;Eval Inputs: &quot;, eval_inputs.dims.mapping) print(&quot;Eval Targets: &quot;, eval_targets.dims.mapping) print(&quot;Eval Forcings: &quot;, eval_forcings.dims.mapping) with open (&quot;stats/diffs_stddev_by_level.nc&quot;, &quot;rb&quot;) as f: diffs_stddev_by_level = xarray.load_dataset(f).compute() with open (&quot;stats/mean_by_level.nc&quot;, &quot;rb&quot;) as f: mean_by_level = xarray.load_dataset(f).compute() with open (&quot;stats/stddev_by_level.nc&quot;, &quot;rb&quot;) as f: stddev_by_level = xarray.load_dataset(f).compute() init_jitted = jax.jit(with_configs(run_forward.init)) if params is None: params, state = init_jitted( rng=jax.random.PRNGKey(0), inputs=train_inputs, targets_template=train_targets, forcings=train_forcings) loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply)))) grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn))) run_forward_jitted = drop_state(with_params(jax.jit(with_configs( run_forward.apply)))) assert model_config.resolution in (0, 360. / eval_inputs.sizes[&quot;lon&quot;]), ( &quot;Model resolution doesn&#x27;t match the data resolution. You likely want to &quot; &quot;re-filter the dataset list, and download the correct data.&quot;) print(&quot;Inputs: &quot;, eval_inputs.dims.mapping) print(&quot;Targets: &quot;, eval_targets.dims.mapping) print(&quot;Forcings:&quot;, eval_forcings.dims.mapping) predictions = rollout.chunked_prediction( run_forward_jitted, rng=jax.random.PRNGKey(0), inputs=eval_inputs, targets_template=eval_targets * np.nan, forcings=eval_forcings) 注：由于不知道怎样构建其它数据的 stats 参数和模型，而且官方文档并未对这些内容进行详细说明，故需要找一下其他的方式。 Nvidia Modulus Modulus 是一个开源的深度学习框架，用于使用最先进的物理ML方法构建、训练和微调深度学习模型。 在 Modulus 示例中有预先调整和下载的 GraphCast 样例。 安装环境： 123git clone https://github.com/NVIDIA/modulus.gitcd modulus/examples/weather/graphcastpip install nvidia-modulus matplotlib wandb netCDF4 scikit-learn 注：目前处于 Beta 阶段资料不全，项目运行时读取的静态文件部分没有说明，代码中的读取方式和样例数据不一致，依赖库没有完全安装，所以没有进行详细测试。 Graph Weather Graph Weather 是一种使用 PyTorch 的开源实现。 安装环境： 注：此处需要首先访问 官网 下载对应版本的 PyTorch，然后再安装其余内容。 12git clone https://github.com/openclimatefix/graph_weather.gitpip install graph-weather 获取数据 注：此样例严格限制 Python 版本为 3.10。在演示中采用的具体版本是 3.10.13 ECMWF 提供了一个样例项目用于便捷的生成预测数据，安装方式如下： 1234pip install ai-modelspip install ai-models-graphcastgit clone https://github.com/ecmwf-lab/ai-models-graphcast.git cd ai-models-graphcast 如果使用 CPU 则需要使用如下命令： 1pip install -r requirements.txt 如果使用 GPU 则需要使用如下命令： 1pip install -r requirements-gpu.txt -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html 然后使用如下命令即可获取预测数据： 1ai-models graphcast --date 20231214 --input cds --output file --download-assets 注：此程序需要从 cds 获取数据，需要一个账号。(可以免费注册) 参考资料 Learning skillful medium-range global weather forecasting 论文 GraphCast: AI model for faster and more accurate global weather forecasting 博客 graphcast 官方项目 Colab (Notepad) GraphCast for weather forecasting(Modulus) Modulus Globus Files Graph Weather ai-models-graphcast","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"TypeScript","slug":"frount/typescript","date":"2023-11-09T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/typescript/","permalink":"https://wangqian0306.github.io/2023/typescript/","excerpt":"","text":"TypeScript 简介 TypeScript 是由微软进行开发和维护的一种开源的编程语言。 TypeScript 是 JavaScript 的严格语法超集，提供了可选的静态类型检查。 基本写法 读取特定 key 123456789101112131415161718192021const myObject = &#123; a: &#x27;apple&#x27;, b: &#x27;banana&#x27;, c: &#x27;cherry&#x27;&#125;;function getValueFromKey(key: string): string | null &#123; if (key in myObject) &#123; const value = myObject[&lt;keyof typeof myObject&gt;key]; console.log(value); return value; &#125; else &#123; console.log(&quot;Key not found&quot;); return null; &#125;&#125;console.log(getValueFromKey(&#x27;a&#x27;)); // Output: &#x27;apple&#x27;console.log(getValueFromKey(&#x27;b&#x27;)); // Output: &#x27;banana&#x27;console.log(getValueFromKey(&#x27;c&#x27;)); // Output: &#x27;cherry&#x27;console.log(getValueFromKey(&#x27;d&#x27;)); // Output: &quot;Key not found&quot; 参考资料 维基百科 官方项目 官方文档","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://wangqian0306.github.io/tags/TypeScript/"}]},{"title":"Drools","slug":"spring/drools","date":"2023-11-08T13:05:12.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/drools/","permalink":"https://wangqian0306.github.io/2023/drools/","excerpt":"","text":"Drools 简介 Drools 是一个业务规则管理系统(Business Rules Management System,BRMS) 解决方案。主要目的是将复杂的业务逻辑抽离出来，将其编辑成为配置文件便于后期进行修改。 使用方式 首先需要引入相关包： 123456789101112dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; implementation &#x27;org.drools:drools-core:9.44.0.Final&#x27; implementation &#x27;org.kie:kie-spring:7.74.1.Final&#x27; implementation &#x27;org.kie:kie-api:9.44.0.Final&#x27; implementation &#x27;org.drools:drools-compiler:9.44.0.Final&#x27; implementation &#x27;org.drools:drools-mvel:9.44.0.Final&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27;&#125; 编写业务类： 123456789101112import lombok.Data;@Datapublic class Fare &#123; private Long nightSurcharge = 0L; private Long rideFare = 0L; public Long getTotalFare() &#123; return nightSurcharge + rideFare; &#125;&#125; 1234567import lombok.Data;@Datapublic class TaxiRide &#123; private Boolean isNightSurcharge; private Long distanceInMile;&#125; 编写规则文件： 1234567891011121314import com.example.xxx.model.TaxiRideimport com.example.xxx.model.Fareimport java.util.*global com.example.xxx.model.Fare rideFare;dialect &quot;mvel&quot;rule &quot;Calculate Taxi Fare - Scenario 1&quot; when taxiRideInstance:TaxiRide(isNightSurcharge == false &amp;&amp; distanceInMile &lt; 10); then rideFare.setNightSurcharge(0L); rideFare.setRideFare(70L);end 编写配置类： 123456789101112131415161718192021222324252627282930import org.kie.api.KieServices;import org.kie.api.builder.KieBuilder;import org.kie.api.builder.KieFileSystem;import org.kie.api.builder.KieModule;import org.kie.api.runtime.KieContainer;import org.kie.internal.io.ResourceFactory;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@ComponentScan(&quot;com.example.xxx.service&quot;)public class TaxiFareConfiguration &#123; public static final String drlFile = &quot;TAXI_FARE_RULE.drl&quot;; @Bean public KieContainer kieContainer() &#123; KieServices kieServices = KieServices.Factory.get(); KieFileSystem kieFileSystem = kieServices.newKieFileSystem(); kieFileSystem.write(ResourceFactory.newClassPathResource(drlFile)); KieBuilder kieBuilder = kieServices.newKieBuilder(kieFileSystem); kieBuilder.buildAll(); KieModule kieModule = kieBuilder.getKieModule(); return kieServices.newKieContainer(kieModule.getReleaseId()); &#125;&#125; 编写服务类： 12345678910111213141516171819202122232425import org.kie.api.runtime.KieContainer;import org.kie.api.runtime.KieSession;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class TaxiFareCalculatorService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(TaxiFareCalculatorService.class); @Autowired private KieContainer kContainer; public Long calculateFare(TaxiRide taxiRide, Fare rideFare) &#123; KieSession kieSession = kContainer.newKieSession(); kieSession.setGlobal(&quot;rideFare&quot;, rideFare); kieSession.insert(taxiRide); kieSession.fireAllRules(); kieSession.dispose(); LOGGER.debug(&quot;!! RIDE FARE !! &quot; + rideFare.getTotalFare()); return rideFare.getTotalFare(); &#125;&#125; 之后即可使用测试类进行测试： 1234567891011121314151617181920212223242526272829import jakarta.annotation.Resource;import org.junit.jupiter.api.Test;import org.springframework.boot.test.context.SpringBootTest;import static org.junit.jupiter.api.Assertions.assertEquals;import static org.junit.jupiter.api.Assertions.assertNotNull;@SpringBootTestclass ApplicationTests &#123; @Resource private TaxiFareCalculatorService taxiFareCalculatorService; @Test void contextLoads() &#123; &#125; @Test public void whenNightSurchargeFalseAndDistLessThan10_thenFixWithoutNightSurcharge() &#123; TaxiRide taxiRide = new TaxiRide(); taxiRide.setIsNightSurcharge(false); taxiRide.setDistanceInMile(9L); Fare rideFare = new Fare(); Long totalCharge = taxiFareCalculatorService.calculateFare(taxiRide, rideFare); assertNotNull(totalCharge); assertEquals(Long.valueOf(70), totalCharge); &#125;&#125; 参考资料 项目官网 官方手册 Drools Spring Integration","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Drools","slug":"Drools","permalink":"https://wangqian0306.github.io/tags/Drools/"}]},{"title":"three.js","slug":"frount/three-js","date":"2023-10-26T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/three-js/","permalink":"https://wangqian0306.github.io/2023/three-js/","excerpt":"","text":"three.js 简介 three.js 是一款 JavaScript 的 3D 库，目的在于可以便捷、轻量化、跨浏览器的展示 3D 内容。 安装和使用 基本使用 对于 React 项目来说使用 react-three-fiber 作为渲染器可以更方便的集成 three.js 具体操作逻辑如下： 安装依赖： 1npm install three @types/three @react-three/fiber @react-three/drei 根据如下样例编写页面即可： 12345678910111213141516171819202122232425262728293031323334353637383940import * as THREE from &#x27;three&#x27;import React, &#123; useRef, useState &#125; from &#x27;react&#x27;import &#123; Canvas, useFrame, ThreeElements &#125; from &#x27;@react-three/fiber&#x27;import &#123;Inter&#125; from &#x27;next/font/google&#x27;const inter = Inter(&#123;subsets: [&#x27;latin&#x27;]&#125;)function Box(props: ThreeElements[&#x27;mesh&#x27;]) &#123; const meshRef = useRef&lt;THREE.Mesh&gt;(null!) const [hovered, setHover] = useState(false) const [active, setActive] = useState(false) useFrame((state, delta) =&gt; (meshRef.current.rotation.x += delta)) return ( &lt;mesh &#123;...props&#125; ref=&#123;meshRef&#125; scale=&#123;active ? 1.5 : 1&#125; onClick=&#123;(event) =&gt; setActive(!active)&#125; onPointerOver=&#123;(event) =&gt; setHover(true)&#125; onPointerOut=&#123;(event) =&gt; setHover(false)&#125;&gt; &lt;boxGeometry args=&#123;[1, 1, 1]&#125; /&gt; &lt;meshStandardMaterial color=&#123;hovered ? &#x27;hotpink&#x27; : &#x27;orange&#x27;&#125; /&gt; &lt;/mesh&gt; )&#125;export default function Home() &#123; return ( &lt;main className=&#123;`flex min-h-screen flex-col items-center justify-between p-24 $&#123;inter.className&#125;`&#125; &gt; &lt;Canvas&gt; &lt;ambientLight/&gt; &lt;pointLight position=&#123;[10, 10, 10]&#125;/&gt; &lt;Box position=&#123;[-1.2, 0, 0]&#125;/&gt; &lt;Box position=&#123;[1.2, 0, 0]&#125;/&gt; &lt;/Canvas&gt; &lt;/main&gt; )&#125; 载入模型 可以从互联网上下载 glb 格式的模型，然后将其放置在项目 public 文件夹内。 注：如果是 Windows 电脑则可以使用自带的 3D Builder 快速建模。 通过将模型上传至 https://gltf.pmnd.rs/ 工具页即可获得模型的展示效果与文档。 代码样例如下： /src/compoment/demo.tsx 1234567891011121314151617181920212223242526272829303132import * as THREE from &quot;three&quot;;import React, &#123; useRef &#125; from &quot;react&quot;;import &#123; useGLTF &#125; from &quot;@react-three/drei&quot;;import &#123; GLTF &#125; from &quot;three-stdlib&quot;;type GLTFResult = GLTF &amp; &#123; nodes: &#123; mesh_0: THREE.Mesh; &#125;; materials: &#123; Turquoise: THREE.MeshStandardMaterial; &#125;;&#125;;export function Model(props: JSX.IntrinsicElements[&quot;group&quot;]) &#123; const &#123; nodes, materials &#125; = useGLTF(&quot;/demo.glb&quot;) as GLTFResult; return ( &lt;group &#123;...props&#125; dispose=&#123;null&#125;&gt; &lt;mesh castShadow receiveShadow geometry=&#123;nodes.mesh_0.geometry&#125; material=&#123;materials.Turquoise&#125; position=&#123;[-0.007, 0, 0]&#125; rotation=&#123;[-1.571, 0, 0]&#125; scale=&#123;1.429&#125; /&gt; &lt;/group&gt; );&#125;useGLTF.preload(&quot;/demo.glb&quot;); /src/pages/index.tsx 1234567891011121314151617181920212223242526import React, &#123;Suspense&#125; from &#x27;react&#x27;import &#123;Canvas&#125; from &#x27;@react-three/fiber&#x27;import &#123;Inter&#125; from &#x27;next/font/google&#x27;import &#123;OrbitControls, Stage&#125; from &quot;@react-three/drei&quot;;import &#123;Model&#125; from &quot;@/compoment/demo&quot;;const inter = Inter(&#123;subsets: [&#x27;latin&#x27;]&#125;)export default function Home() &#123; return ( &lt;main className=&#123;`flex min-h-screen flex-col items-center justify-between p-24 $&#123;inter.className&#125;`&#125; &gt; &lt;Canvas shadows dpr=&#123;[1, 2]&#125; camera=&#123;&#123;fov: 50&#125;&#125;&gt; &lt;Suspense fallback=&#123;null&#125;&gt; &lt;Stage preset=&quot;rembrandt&quot; intensity=&#123;1&#125; environment=&quot;city&quot;&gt; false &lt;Model/&gt; false &lt;/Stage&gt; &lt;/Suspense&gt; &lt;OrbitControls autoRotate/&gt; &lt;/Canvas&gt; &lt;/main&gt; )&#125; 之后正常运行项目即可。 注：在载入 Canvas 时需要访问 https://raw.githubusercontent.com 中读取 potsdamer_platz_1k.hdr 文件，如果网络链接不通则会报错。 参考资料 官方网站 官方项目 react-three-fiber 文档 react-three-fiber 项目 drei 项目","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"}]},{"title":"摄影测量","slug":"windows/photogrammetry","date":"2023-10-25T13:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2023/photogrammetry/","permalink":"https://wangqian0306.github.io/2023/photogrammetry/","excerpt":"","text":"摄影测量 简介 摄影测量法(Photogrammetry)是一种利用被摄物体影像来重建物体空间位置和三维形状的技术。 如果使用相机或无人机等设备则可以使用： Meshroom: 基于 AliceVision 摄影测量计算机视觉框架的免费开源三维建模软件。 如果是手机则可以使用： RealityScan：一款免费下载的移动应用程序，能够仅使用手机或平板电脑创建高保真 3D 模型。 Meshroom 安装方式 从 官网 下载安装包 解压至安装路径 安装 Microsoft Visual C++ Redistributable Package 如已经安装则可以跳过 运行 Meshroom.exe RealityScan 可以在 Google Play 或 App Store 上免费检索到此程序，根据教程即可完成建模。 注：最后的模型会分享至 Sketchfab 平台上。 参考资料 Meshroom 项目主页 Meshroom 用户手册 RealityScan Sketchfab","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/tags/Windows/"}]},{"title":"OpenTelemetry","slug":"tools/opentelemetry","date":"2023-10-19T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/opentelemetry/","permalink":"https://wangqian0306.github.io/2023/opentelemetry/","excerpt":"","text":"OpenTelemetry 简介 OpenTelemetry 是一个可观测性框架和工具包，旨在创建和管理遥测数据，如跟踪、指标和日志。至关重要的是，OpenTelemetry 与供应商和工具无关，这意味着它可以与各种可观测性后端一起使用，包括 Jaeger 和 Prometheus 等开源工具，以及商业产品。 运行方式 OpenTelemetry 在使用过程中可以分成两种角色： 开发者 在开发过程中引入 OpenTelemetry 支持如下语言 C++ .Net Erlang/Elixir Go Java JavaScript/TypeScript PHP Python Ruby Rust Swift Lua Perl Julia 运维 运维人员可以在程序运行前添加例如 Java Agent 的方式引入监控，支持如下语言 .Net Java JavaScript PHP Python 从使用上来说整体的运行方式如下： 注：Otel 为 OpenTelemetry 的缩写且此处的 Open source/Vendor 代表 Jaeger 等可视化工具。 Kubernetes 部署 可以使用 Helm 部署 OpenTelemetry Collector，具体步骤如下： Kubernetes 部署 本地测试 SigNoz 本地测试 参考资料 官方文档 Let’s use OpenTelemetry with Spring","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"OpenTelemetry","slug":"OpenTelemetry","permalink":"https://wangqian0306.github.io/tags/OpenTelemetry/"},{"name":"Jaeger","slug":"Jaeger","permalink":"https://wangqian0306.github.io/tags/Jaeger/"}]},{"title":"NASA 开源数据","slug":"ocean/nasa","date":"2023-09-13T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/nasa/","permalink":"https://wangqian0306.github.io/2023/nasa/","excerpt":"","text":"NASA 开源数据 简介 NASA 有很多数据共享在了网络上，本文会对用到的数据进行一些整理。 GIBS Global Imagery Browse Services(GIBS) 系统类似于 Google Earth 但是有很多关于地球科学的图层。 卫星图 WMS 样例程序","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"IPTV","slug":"tmp/iptv","date":"2023-09-12T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/iptv/","permalink":"https://wangqian0306.github.io/2023/iptv/","excerpt":"","text":"IPTV 简介 网络协议电视(Internet Protocol Television) 是宽带电视的一种。但是由于不同的运营商采用了不同的逻辑去处理 IPTV。所以在使用上需要找到适合自己的播放列表。 常见工具 wtv 项目地址 原来叫 wtv 现在改名叫 一个橙子 pro 工具箱(当前仅支持 Windows 平台)，使用方式： 传入直播源 点击检测 保存检测完成且处于可用状态的直播源 注：延时超过 1000ms 可能都不会好用。 iptvchecker 项目地址 使用方式，编写如下 docker-compose.yaml 文件: 123456services: website: image: zmisgod/iptvchecker:latest ports: - &quot;8081:8080&quot; restart: always 启动容器，然后访问 http://localhost:8081 即可上传文件进行测试。 参考资料 iptv-org 直播源 zbefine 直播源 wcb1969 直播源 wtv 直播源","categories":[],"tags":[]},{"title":"VNC","slug":"linux/vnc","date":"2023-09-07T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/vnc/","permalink":"https://wangqian0306.github.io/2023/vnc/","excerpt":"","text":"VNC 简介 VNC (Virtual Network Console) 为一种使用 RFB 协议的屏幕画面分享及远程操作软件。此软件借由网络，可发送键盘与鼠标的动作及即时的屏幕画面。 服务端 使用如下命令进行安装： 1234dnf install tigervnc-serverfirewall-cmd --add-service=vnc-serverfirewall-cmd --runtime-to-permanentfirewall-cmd --reload 然后使用如下命令配置密码： 1vncpasswd 然后编写如下链接偏好配置文件 ~/.vnc/config： 1234567# create new# session=(display manager you use)# securitytypes=(security options)# geometry=(screen resolution)session=gnomesecuritytypes=vncauth,tlsvncgeometry=1920x1080 编写链接端口和远程用户 /etc/tigervnc/vncserver.users 123456789101112# add to the end# specify [:(display number)=(username] as comments# display number 1 listens port 5901# display number n + 5900 = listening port## This file assigns users to specific VNC display numbers.# The syntax is &lt;display&gt;=&lt;username&gt;. E.g.:## :2=andrew# :3=lisa:1=fedora:2=redhat 使用如下命令启动服务： 1systemctl enable --now vncserver@:1 vncserver@:2 注：此处的编号既是用户，如有多个用户需要自行添加。 客户端 Windows 可以使用 TightVNC 开源软件。 Linux 1dnf -y install tigervnc 参考资料 VNC Server Desktop Environment : Configure VNC Server","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"argo","slug":"ocean/argo","date":"2023-09-01T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/argo/","permalink":"https://wangqian0306.github.io/2023/argo/","excerpt":"","text":"Argo Progam 简介 Argo 是一项国际计划，它使用一系列机器人仪器测量全球海洋的水属性，这些仪器随洋流漂移，并在地表和中水位之间上下移动。 应用场景 展示风场和洋流 哥白尼计划 参考资料 argovis 可视化 中国 Argo 浮标清单","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Argo","slug":"Argo","permalink":"https://wangqian0306.github.io/tags/Argo/"}]},{"title":"GeoServer","slug":"ocean/geoserver","date":"2023-09-01T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/geoserver/","permalink":"https://wangqian0306.github.io/2023/geoserver/","excerpt":"","text":"GeoServer 简介 GeoServer 是一款支持多种 GIS 协议类型的自建服务器。 部署方式 可以使用 docker 部署 12345678910services: geoserver: image: docker.osgeo.org/geoserver:2.24.x ports: - &quot;8080:8080&quot; environment: - INSTALL_EXTENSIONS=true - STABLE_EXTENSIONS=ysld,h2 volumes: - ./data:/opt/geoserver_data 注：若没有数据可以不填写数据目录。 之后即可访问 http://localhost:8080/geoserver/web ，然后使用如下账号密码即可完成登录。 账号：admin 密码：geoserver 数据来源 GeoServer 可以作为代理，接管天地图或 OpenStreetMap 等 WMS,WMTS 服务。 也可以接入 shp 文件或 PostGIS 数据等内容。 注：POI数据 可以在此处下载国内的 shp 数据文件。 常见问题 忘记密码 在容器中的 /opt/geoserver_data/security/usergroup/default 目录中有一个 users.xml 文件，该文件存储的就是用户有和密码信息。可以通过修改该文件的方式来完成密码重置，默认的用户信息如下： 1&lt;user enabled=&quot;true&quot; name=&quot;admin&quot; password=&quot;digest1:D9miJH/hVgfxZJscMafEtbtliG0ROxhLfsznyWfG38X2pda2JOSV4POi55PQI4tw&quot;/&gt; 注：此处情况仅适用于默认加密方式，如果还出了问题就直接覆盖 security 文件夹。 自行实现 如有相关简单的需求也可以通过其他方式接入 SpringBoot , 例如将结构 shp 文件展示成图片。 编辑 build.gradle 文件： 1234567891011121314151617181920repositories &#123; maven &#123; url &#x27;https://repo.osgeo.org/repository/release/&#x27;&#125; mavenCentral()&#125;dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-actuator&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27; implementation &#x27;org.geotools:gt-main:31.2&#x27; implementation &#x27;org.geotools:gt-image:31.2&#x27; implementation &#x27;org.geotools:gt-shapefile:31.2&#x27; implementation &#x27;org.geotools:gt-render:31.2&#x27; testRuntimeOnly &#x27;org.junit.platform:junit-platform-launcher&#x27;&#125; 编辑 TestController.java ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import org.springframework.beans.factory.annotation.Value;import org.springframework.core.io.Resource;import org.geotools.api.data.FileDataStore;import org.geotools.api.data.FileDataStoreFinder;import org.geotools.api.data.SimpleFeatureSource;import org.geotools.api.referencing.crs.CoordinateReferenceSystem;import org.geotools.api.style.Style;import org.geotools.geometry.jts.ReferencedEnvelope;import org.geotools.map.FeatureLayer;import org.geotools.map.Layer;import org.geotools.map.MapContent;import org.geotools.renderer.lite.StreamingRenderer;import org.geotools.styling.SLD;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.http.MediaType;import org.springframework.http.ResponseEntity;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import javax.imageio.ImageIO;import java.awt.*;import java.awt.image.BufferedImage;import java.io.ByteArrayOutputStream;import java.io.File;import java.io.IOException;@RestControllerpublic class TestController &#123; private static final Logger log = LoggerFactory.getLogger(TestController.class); @Value(&quot;classpath:/static/ne_110m_coastline.shp&quot;) private Resource shp; @GetMapping(value = &quot;/demo&quot;,produces = MediaType.IMAGE_PNG_VALUE) public ResponseEntity&lt;byte[]&gt; getTile() throws IOException &#123; File shpFile = shp.getFile(); FileDataStore store = FileDataStoreFinder.getDataStore(shpFile); SimpleFeatureSource featureSource = store.getFeatureSource(); MapContent map = new MapContent(); Style style = SLD.createSimpleStyle(featureSource.getSchema()); Layer layer = new FeatureLayer(featureSource, style); map.layers().add(layer); CoordinateReferenceSystem coordinateReferenceSystem = map.getCoordinateReferenceSystem(); log.warn(coordinateReferenceSystem.getCoordinateSystem().getName().getCode()); Rectangle imageBounds = new Rectangle(800, 600); ReferencedEnvelope mapArea = map.getViewport().getBounds(); BufferedImage image = new BufferedImage(imageBounds.width, imageBounds.height, BufferedImage.TYPE_INT_ARGB); StreamingRenderer renderer = new StreamingRenderer(); renderer.setMapContent(map); Graphics2D g2d = image.createGraphics(); renderer.paint(g2d, imageBounds, mapArea); g2d.dispose(); ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); ImageIO.write(image, &quot;png&quot;, outputStream); return ResponseEntity.ok().contentType(MediaType.IMAGE_PNG).body(outputStream.toByteArray()); &#125;&#125; 参考资料 官方文档","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"GeoServer","slug":"GeoServer","permalink":"https://wangqian0306.github.io/tags/GeoServer/"}]},{"title":"Spring RSocket","slug":"spring/rsocket","date":"2023-08-29T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/rsocket/","permalink":"https://wangqian0306.github.io/2023/rsocket/","excerpt":"","text":"Spring RSocket 简介 RSocket 是一种使用异步二进制流提供 Reactive Streams 语义的应用程序协议，使用它可以不关注底层的实现方式。 使用方式 引入如下依赖： 1234567dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-rsocket&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27;&#125; 编写测试 Record： 12public record Message(String name, String content) &#123;&#125; 编写测试 Controller： 1234567891011121314151617181920import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.stereotype.Controller;import reactor.core.publisher.Mono;import java.time.Instant;@Controllerpublic class TestController &#123; @MessageMapping(&quot;getByName&quot;) Mono&lt;Message&gt; getByName(String name) &#123; return Mono.just(new Message(name, Instant.now().toString())); &#125; @MessageMapping(&quot;create&quot;) Mono&lt;Message&gt; create(Message message) &#123; return Mono.just(message); &#125;&#125; 编写配置文件 application.yaml： 123456server: port: 8080spring: rsocket: server: port: 7000 编写单元测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.messaging.rsocket.RSocketRequester;import org.springframework.messaging.rsocket.RSocketStrategies;import org.springframework.util.MimeTypeUtils;import reactor.core.publisher.Mono;import reactor.test.StepVerifier;import static org.assertj.core.api.Assertions.assertThat;@SpringBootTestpublic class TestControllerTest &#123; private RSocketRequester requester; @Autowired private RSocketStrategies rSocketStrategies; @BeforeEach public void setup() &#123; requester = RSocketRequester.builder() .rsocketStrategies(rSocketStrategies) .dataMimeType(MimeTypeUtils.APPLICATION_JSON) .tcp(&quot;localhost&quot;, 7000); &#125; @Test public void testGetByName() &#123; Mono&lt;Message&gt; result = requester .route(&quot;getByName&quot;) .data(&quot;demo&quot;) .retrieveMono(Message.class); // Verify that the response message contains the expected data StepVerifier .create(result) .consumeNextWith(message -&gt; &#123; assertThat(message.name()).isEqualTo(&quot;demo&quot;); &#125;) .verifyComplete(); &#125; @Test public void testCreate() &#123; Mono&lt;Message&gt; result = requester .route(&quot;create&quot;) .data(new Message(&quot;TEST&quot;, &quot;Request&quot;)) .retrieveMono(Message.class); // Verify that the response message contains the expected data StepVerifier .create(result) .consumeNextWith(message -&gt; &#123; assertThat(message.name()).isEqualTo(&quot;TEST&quot;); assertThat(message.content()).isEqualTo(&quot;Request&quot;); &#125;) .verifyComplete(); &#125;&#125; WebSocket 除了使用 tcp 链接之外还可以采用 WebSocket 协议，仅需完成如下配置: 12345678server: port: 8080spring: rsocket: server: port: 7000 mapping-path: /rsocket transport: websocket 测试类的部分则需要修改链接的建立方式： 1234567@BeforeEachpublic void setup() &#123; requester = RSocketRequester.builder() .rsocketStrategies(rSocketStrategies) .dataMimeType(MimeTypeUtils.APPLICATION_JSON) .websocket(URI.create(&quot;ws://localhost:7000/rsocket&quot;));&#125; 调试工具 RSocket Requests In HTTP Client 在 JetBrains Marketplace 中寻找 RSocket Requests In HTTP Client 插件并安装即可使用如下的 test.http 文件进行测试： 1234567891011121314151617181920212223242526272829### rsocket getByNameRSOCKET getByNameHost: localhost:9090Content-Type: application/json1### rsocket createRSOCKET createHost: localhost:9090Content-Type: application/json&#123; &quot;name&quot;: &quot;wq&quot;, &quot;content&quot;: &quot;wqnice&quot;&#125;### rsocket getByName websocketRSOCKET getByNameHost: ws://localhost:7000/rsocketContent-Type: application/json1### rsocket streamSTREAM stHost: ws://localhost:7000/rsocketContent-Type: application/json RSocket Client CLI (RSC) 访问 https://github.com/making/rsc/releases 即可获取到最新的命令行工具。 使用如下命令即可完成测试： 1java -jar rsc.jar --debug --request --data &quot;wq&quot; --route getByName tcp://localhost:7000 或： 1java -jar rsc.jar --debug --request --data &#x27;&#123;&quot;name&quot;:&quot;wq&quot;,&quot;content&quot;:&quot;nice&quot;&#125;&#x27; --route create tcp://localhost:7000 注：不要使用 CMD 或 Power Shell 直接用 Bash，否则会报错。 在 WebSocket 协议中应该采用下面的命令： 1java -jar rsc.jar --debug --request --data &#x27;1&#x27; --route getByName ws://localhost:7000/rsocket 常见问题 聊天室 Demo 1234567891011121314151617181920212223242526import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.messaging.handler.annotation.Payload;import org.springframework.stereotype.Controller;import reactor.core.publisher.Flux;import reactor.core.publisher.FluxSink;import reactor.core.publisher.Mono;import java.util.concurrent.ConcurrentHashMap;@Controllerpublic class RsocketController &#123; private final ConcurrentHashMap&lt;String, FluxSink&lt;Message&gt;&gt; clientMap = new ConcurrentHashMap&lt;&gt;(); @MessageMapping(&quot;data&quot;) Flux&lt;Message&gt; data(@Payload String id) &#123; return Flux.create(sink -&gt; clientMap.put(id, sink)); &#125; @MessageMapping(&quot;chat&quot;) Mono&lt;Void&gt; chat(@Payload Message message) &#123; clientMap.forEach((userId, sink) -&gt; sink.next(message)); return Mono.empty(); &#125;&#125; 与 SpringSecurity 和 JWT 集成 需要修改引入的类 123456789101112dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-rsocket&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-security&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-oauth2-resource-server&#x27; implementation &#x27;org.springframework.security:spring-security-messaging&#x27; implementation &#x27;org.springframework.security:spring-security-rsocket&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27; testImplementation &#x27;org.springframework.security:spring-security-test&#x27;&#125; 与 Spring Security 集成 123456789101112131415161718192021222324252627282930313233343536373839import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.security.config.Customizer;import org.springframework.security.config.annotation.rsocket.RSocketSecurity;import org.springframework.security.oauth2.jwt.NimbusReactiveJwtDecoder;import org.springframework.security.oauth2.jwt.ReactiveJwtDecoder;import org.springframework.security.rsocket.core.PayloadSocketAcceptorInterceptor;import org.springframework.stereotype.Component;import java.security.interfaces.RSAPrivateKey;import java.security.interfaces.RSAPublicKey;@Componentpublic class RSocketSecurityConfig &#123; @Value(&quot;$&#123;jwt.public.key&#125;&quot;) RSAPublicKey publicKey; @Value(&quot;$&#123;jwt.private.key&#125;&quot;) RSAPrivateKey privateKey; @Bean PayloadSocketAcceptorInterceptor rsocketInterceptor(RSocketSecurity rsocket) &#123; rsocket .authorizePayload(authorize -&gt; authorize .anyRequest().authenticated() .anyExchange().permitAll() ) .jwt(Customizer.withDefaults()); return rsocket.build(); &#125; @Bean ReactiveJwtDecoder jwtDecoder() &#123; return NimbusReactiveJwtDecoder.withPublicKey(publicKey).build(); &#125;&#125; 参考资料 Spring Framework 官方文档 Spring Boot 官方文档 Getting Started With RSocket On Spring Boot RSocket Client CLI (RSC)","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"RSocket","slug":"RSocket","permalink":"https://wangqian0306.github.io/tags/RSocket/"}]},{"title":"AutoBangumi","slug":"tmp/autobangumi","date":"2023-08-23T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/autobangumi/","permalink":"https://wangqian0306.github.io/2023/autobangumi/","excerpt":"","text":"AutoBangumi 简介 AutoBangumi 一个是基于 Mikan Project、qBittorrent 的全自动追番整理下载工具。只需要在 Mikan Project 上订阅番剧，就可以全自动追番。并且整理完成的名称和目录可以直接被 Plex、Jellyfin 等媒体库软件识别，无需二次刮削。 使用 1234567891011121314151617181920212223242526272829services: qbittorrent: image: lscr.io/linuxserver/qbittorrent:latest container_name: qbittorrent ports: - 8989:8989 environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - WEBUIPORT=8989 volumes: - ./qbittorrent/config:/config - ./content/downloads:/downloads restart: unless-stopped autobangumi: image: estrellaxd/auto_bangumi:latest container_name: autobangumi ports: - 7892:7892 depends_on: - qbittorrent environment: - PUID=1000 - PGID=1000 volumes: - ./autobangumi/config:/app/config - ./autobangumi/data:/app/data restart: unless-stopped 配置流程： 进入 qBittorrent 配置账号密码下载路径 进入 AutoBangumi WebUI 然后配置 qBittorrent 的链接信息 在 MiKan Project 中注册账号订阅内容然后获取到 token 将 token 导入 AutoBangumi 中 参考资料 项目官网","categories":[],"tags":[{"name":"Jellyfin","slug":"Jellyfin","permalink":"https://wangqian0306.github.io/tags/Jellyfin/"}]},{"title":"大模型","slug":"ai/ai","date":"2023-08-08T14:26:13.000Z","updated":"2025-02-10T08:03:58.878Z","comments":true,"path":"2023/ai/","permalink":"https://wangqian0306.github.io/2023/ai/","excerpt":"","text":"大模型 简介 大语言模型(large language model，LLM) 是一种语言模型，由具有许多参数(通常数十亿个权重或更多)的人工神经网络组成，使用自监督学习或半监督学习对大量未标记文本进行训练。 基础知识 课程和基础概念 LLM/AI 大模型入门指南 李宏毅2024春《生成式人工智能导论》 微调(Fine Tuning) 微调是指对已经在大量通用文本数据上训练好的大型语言模型进行进一步的调整，以优化其在特定任务或领域中的性能。 注：如果需要引入最新的信息或对新内容进行检索，此时使用 RAG 是更合适的。 微调的方式有很多，可以全量微调也可以通过 LoRA 或 QLoRa 方式来从数学方面减少微调的性能消耗。 下面是一些微调所必备的参数说明： Learning Rate 决定了优化算法在每次迭代中更新模型参数的步长大小。简单来说，学习率控制了模型权重调整的速度和幅度。太大会很难达到目标，太小微调要运行很久。 Batch Size 决定了模型在更新知识之前查看的示例数量。批次越大学习越稳定，但要更多内存。 Number of Epoch 决定了模型查看数据集的次数，太少可能没学到，太多则会出现过度拟合(只在训练数据上表现好，训练中拿到了很多噪声和异常值)。 Optimizer Selection 决定了不同的教学方法，例如 AdamW 比较全能，其他优化器可能更适合特定情况。 通常来说当准备好几百个样例之后微调才能有好的结果，但是样例数量和实际的效果可能不是正相关的。必须要一致性强的数据才能达到目标。数据要清晰一致且专注于目标内容。通常来说最好遵循以下规范： 格式和风格一致 没有错误和矛盾 示例和实际用例相关 且最好要在训练数据中包含边缘情况和失败场景的处理方法。 Is MLX the best Fine Tuning Framework? 量化(Quantization) LLM 越大效果越好，但是同时也会造成需要更多的内存，量化是目前最突出的解决方案。目前的模型通常会用 32 位的浮点数来表示权重(weights) 和激活函数 (activations) ，但是实际在使用中会将其转化至 float32 -&gt; float16 和 float32 -&gt; int8 。 其中 Q2 Q4 Q8 代表位数，K 代表将数值按照分组然后进行按位优化，K 有 S M L (小中大)三种记录方式携带内容越多模型越大。 A Guide to Quantization in LLMs Optimum Document Quantization Optimize Your AI - Quantization Explained RAG RAG(Retrieval-Augmented Generation)，检索增强生成，即从外部获取额外信息辅助模型生成内容。 学习检索增强生成(RAG)技术，看这篇就够了——热门RAG文章摘译 相关项目 LongChain 项目地址：LongChain LangChain 是一个用于开发由语言模型驱动的应用程序的框架。它使应用程序能够： 感知上下文：将语言模型连接到上下文源(提示指令、少量镜头示例、内容以使其响应为基础等) 理解原因：依靠语言模型进行推理(关于如何根据提供的上下文回答、采取什么行动等) 此框架还提供了网页和插件规范。 Text generation web UI 项目地址：Text generation web UI 此项目可以在本地搭建一个聊天服务器，并且可以替换各种模型 AutoGen AutoGen 是一个用于创建多代理 AI 应用程序的框架，这些应用程序可以自主运行或与人类一起工作。 利用不同的 Agent 可以实现循环调用，使用工具，分角色处理问题等内容。 微软最强AI智能体AutoGen史诗级更新！ Tabby 项目地址：Tabby 此项目可以在本地搭建一个代码提示服务器，并且可以使用不同参数的 CodeLama 和 StarCoder 等模型。 注：目前已有 VS Code，IntelliJ Platform 和 VIM 的支持插件。 Continue 项目地址：Continue 使用 Continue 可以让 Ollama 和 IDE 结合起来。 注：具体配置参见 An entirely open-source AI code assistant inside your editor 博客。 Void 项目地址：Void Void 项目是个开源版本的 Cursor 编辑器。 Docling 项目地址：Docling Docling 可以轻松快速地解析文档并将其导出为所需的格式。 注：此项目可以接受不同格式的文件，并使用 OCR 的方案将其进行格式转化，供 AI 读取。 OpenHands 项目地址：OpenHands OpenHands 是一款网页端的 WebIDE 支持很多的大模型，可以独立分解问题并读取项目文件并进行在线调试和修改。 可以使用如下 docker-compose.yaml 文件运行： 123456789101112131415services: openhands: image: ghcr.io/all-hands-ai/openhands:0.11 pull_policy: always environment: - SANDBOX_RUNTIME_CONTAINER_IMAGE=ghcr.io/all-hands-ai/runtime:0.11-nikolaik - SANDBOX_USER_ID=$&#123;UID&#125; - WORKSPACE_MOUNT_PATH=$&#123;WORKSPACE_BASE&#125; volumes: - $&#123;WORKSPACE_BASE&#125;:/opt/workspace_base - /var/run/docker.sock:/var/run/docker.sock ports: - &quot;3000:3000&quot; extra_hosts: - &quot;host.docker.internal:host-gateway&quot; 环境配置项如下 .env： 12WORKSPACE_BASE=./workspaceUID=$(id -u) MaxKB 项目地址：MaxKB MaxKB 是一个基于大型语言模型（LLM）和检索增强生成（RAG）的聊天机器人。 可以使用如下 docker-compose.yaml 文件运行： 12345678910services: maxkb: image: cr2.fit2cloud.com/1panel/maxkb container_name: maxkb restart: always ports: - &quot;8080:8080&quot; volumes: - ~/.maxkb:/var/lib/postgresql/data - ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 之后即可访问 http://localhost:8080 进入管理页面，查看 swagger 地址，新增知识库等功能。 默认用户名和密码如下： 12# username: admin# pass: MaxKB@123.. 环境准备 PyTorch 使用如下命令即可安装 PyTorch 1pip3 install torch torchvision torchaudio 使用如下脚本可以监测默认设备 12345678import torchif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) print(&quot;默认设备为GPU&quot;)else: device = torch.device(&quot;cpu&quot;) print(&quot;默认设备为CPU&quot;)","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"}]},{"title":"Hugging Face","slug":"ai/huggingface","date":"2023-08-04T14:26:13.000Z","updated":"2025-01-10T07:33:22.549Z","comments":true,"path":"2023/hugging-face/","permalink":"https://wangqian0306.github.io/2023/hugging-face/","excerpt":"","text":"Hugging Face 简介 Hugging Face 是一个专注于机器学习，数据集和 AI 应用的社区。 外部项目 Meta Llama 2 Code Llama 常用内容 Chat https://huggingface.co/chat/ 可以访问上面的网站来调用一些开源的大语言模型。 BigCode BigCode 是一个专注于 编码类 LLM 的开源项目，模型清单如下： BigCode Transformers.js Hugging Face 提供的 Transformers.js 可以直接再浏览器中运行 Transformers 。 在 Next.js 中可以按照如下方式使用： 安装依赖 1npm i @xenova/transformers 设置 next.config.mjs 配置 123456789/** @type &#123;import(&#x27;next&#x27;).NextConfig&#125; */const nextConfig = &#123; output: &#x27;standalone&#x27;, experimental: &#123; serverComponentsExternalPackages: [&#x27;sharp&#x27;, &#x27;onnxruntime-node&#x27;], &#125;,&#125;;export default nextConfig; 编写 app/test/pipeline.ts 123456789101112131415161718192021222324252627282930import &#123;PipelineType&#125; from &quot;@xenova/transformers/types/pipelines&quot;;import &#123;pipeline&#125; from &quot;@xenova/transformers&quot;;const P = () =&gt; class PipelineSingleton &#123; static task: PipelineType = &#x27;text-classification&#x27;; static model = &#x27;Xenova/distilbert-base-uncased-finetuned-sst-2-english&#x27;; static instance:any = null; static async getInstance(progress_callback:any = null) &#123; if (this.instance === null) &#123; this.instance = pipeline(this.task, this.model, &#123; progress_callback &#125;); &#125; return this.instance; &#125;&#125;declare const global: &#123; PipelineSingleton?: any;&#125;;let PipelineSingleton:any;if (process.env.NODE_ENV !== &#x27;production&#x27;) &#123; if (!global.PipelineSingleton) &#123; global.PipelineSingleton = P(); &#125; PipelineSingleton = global.PipelineSingleton;&#125; else &#123; PipelineSingleton = P();&#125;export default PipelineSingleton; 编写 app/test/route.ts 12345678910111213141516171819import &#123;NextRequest, NextResponse&#125; from &#x27;next/server&#x27;import PipelineSingleton from &quot;./pipeline&quot;;export async function GET(request:NextRequest) &#123; const text = request.nextUrl.searchParams.get(&#x27;text&#x27;); if (!text) &#123; return NextResponse.json(&#123; error: &#x27;Missing text parameter&#x27;, &#125;, &#123; status: 400 &#125;); &#125; // Get the classification pipeline. When called for the first time, // this will load the pipeline and cache it for future use. const classifier = await PipelineSingleton.getInstance(); // Actually perform the classification const result = await classifier(text); return NextResponse.json(result);&#125; 编写程序入口 jsx12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#x27;use client&#x27;;import &#123;useState&#125; from &#x27;react&#x27;export default function Home() &#123; const [value,setValue] = useState&lt;string&gt;(&quot;&quot;); const [result, setResult] = useState&lt;string&gt;(); const [ready, setReady] = useState&lt;boolean&gt;(); const handleInputChange = (event: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; &#123; // Update the input value state whenever the input changes setValue(event.target.value); &#125;; const classify = async (text: string) =&gt; &#123; if (!text) return; if (ready === null) setReady(false); const result = await fetch(`/test?text=$&#123;encodeURIComponent(text)&#125;`); if (!ready) setReady(true); const json = await result.json(); setResult(json); &#125;; return ( &lt;main className=&quot;flex min-h-screen flex-col items-center justify-center p-12&quot;&gt; &lt;h1 className=&quot;text-5xl font-bold mb-2 text-center&quot;&gt;Transformers.js&lt;/h1&gt; &lt;h2 className=&quot;text-2xl mb-4 text-center&quot;&gt;Next.js template (server-side)&lt;/h2&gt; &lt;input type=&quot;text&quot; value=&#123;value&#125; className=&quot;w-full max-w-xs p-2 border border-gray-300 rounded mb-4&quot; placeholder=&quot;Enter text here&quot; onChange=&#123;handleInputChange&#125; /&gt; &lt;button onClick=&#123;() =&gt; classify(value)&#125;&gt; Transformers &lt;/button&gt; &#123;ready !== null &amp;&amp; ( &lt;pre className=&quot;bg-gray-100 p-2 rounded&quot;&gt; &#123;(!ready || !result) ? &#x27;Loading...&#x27; : JSON.stringify(result, null, 2)&#125; &lt;/pre&gt; )&#125; &lt;/main&gt; )&#125; 参考资料 Hugging Face Hub 和开源生态介绍 Hugging Face 主页 Transformers.js","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Hugging Face","slug":"Hugging-Face","permalink":"https://wangqian0306.github.io/tags/Hugging-Face/"}]},{"title":"Shell","slug":"linux/shell","date":"2023-08-03T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/shell/","permalink":"https://wangqian0306.github.io/2023/shell/","excerpt":"","text":"Shell 简介 在用 Linux 的过程中遇到了很多不同的 Shell 此处对此进行记录。 sh Bourne shell(sh) 是用于计算机操作系统的 shell 命令行解释器。 注：原版 bash Bash 是由 Brian Fox 为 GNU 项目编写的 Unix shell和命令语言，作为Bourne shell 的自由软件替代品。 注：增强开源版 zsh Z shell(zsh) 是一个 Unix shell 其引入了 Bash、ksh 和 tcsh 的一些特性。 注：开源可配主题与插件 参考资料 Bourne shell Bash （Unix shell） Z shell Comparison_of_command_shells Difference between sh and bash Zsh vs Bash","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Node Version Manager","slug":"frount/nvm","date":"2023-08-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2023/nvm/","permalink":"https://wangqian0306.github.io/2023/nvm/","excerpt":"","text":"Node Version Manager 简介 Node Version Manager(NVM) 是一个管理 Node 版本的工具。 安装方式 Windows 访问 https://github.com/coreybutler/nvm-windows/releases 获取最新的 exe 安装包，双击安装即可。 Linux 使用如下命令安装软件： 12curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/&lt;version&gt;/install.sh | bashsource ~/.bashrc 注：此处需要提供具体的安装版本，详情参见官方项目。 基本使用 列出 node 版本 1nvm ls-remote 安装最新版本 1nvm install node 安装对应版本 1nvm install &lt;version&gt; 查看当前版本列表 1nvm ls 切换版本 1nvm use &lt;version&gt; 参考资料 官方项目 Windows 端官方项目","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"}]},{"title":"Redis Expire","slug":"database/redis-expire","date":"2023-07-27T13:41:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2023/redis-expire/","permalink":"https://wangqian0306.github.io/2023/redis-expire/","excerpt":"","text":"Redis Expire 简介 在 Redis 中可以设置 Key 的过期时间，在过期后删除或覆盖 Key。自从 Redis 6 之后的实现方式有变化，所以进行整理记录。 实现方式 Redis 采用了两种方式实现过期策略：一种是被动的方式，另一种是主动的方式。 当某个客户端尝试访问某个 Key，但发现该 Key 已经过期时，该密钥就以被动方式处理。 当然，这还不够，因为有些过期的 Key 将永远不会被访问。这些密钥无论如何都应该过期，所以 Redis 会定期在带有过期集合的 Key 中随机测试一些密钥。所有已过期的 Key 都将从此空间中移除。 具体来说，Redis 每秒执行 10 次下面的操作： 测试随机 20 个 设置了过期时间的 Key。 删除已经过期的 Key。 如果已过期的 Key 数量超过 25%，则重复步骤 1。 这是一个简单的概率算法，基本上假设我们的样本代表所有 Key，并且我们持续运行主动过期策略，直到可能过期的 Key 与所有 Key 的百分比低于 25%。 这意味着在任何给定时刻，正在使用内存中已过期 Key 的最大数量等于每秒最大写入操作量除以 4。 参考资料 EXPIRE 命令手册 过期业务源码","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"Arthas","slug":"java/arthas","date":"2023-07-24T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/arthas/","permalink":"https://wangqian0306.github.io/2023/arthas/","excerpt":"","text":"Arthas 简介 Arthas 是 Alibaba 开源的 Java 诊断工具，可以用来诊断 Java 应用的各种问题，比如内存泄漏、方法耗时、热部署、线程堆栈、监控方法调用、监控 Spring Bean 等等。 安装 访问 官方项目发行版 页面即可找到最新版软件的压缩包，下载并解压即可。 注: 解压后的软件即可直接使用 java -jar arthas-boot.jar 启动 arthas。 然后运行下面的命令即可完成安装: 1./insta[gradle](gradle)ll-local.sh 使用如下命令即可启动 arthas: 1./as.sh 基本使用 首先需要选择监听的 Java 进程，然后就可以根据需求进行监测了 从大类上来说基本指令有以下几种: jvm 相关 dashboard - 当前系统的实时数据面板 getstatic - 查看类的静态属性 heapdump - dump java heap, 类似 jmap 命令的 heap dump 功能 jvm - 查看当前 JVM 的信息 logger - 查看和修改 logger mbean - 查看 Mbean 的信息 memory - 查看 JVM 的内存信息 ognl - 执行 ognl 表达式 perfcounter - 查看当前 JVM 的 Perf Counter 信息 sysenv - 查看 JVM 的环境变量 sysprop - 查看和修改 JVM 的系统属性 thread - 查看当前 JVM 的线程堆栈信息 vmoption - 查看和修改 JVM 里诊断相关的 option vmtool - 从 jvm 里查询对象，执行 forceGc class/classloader 相关 classloader - 查看 classloader 的继承树，urls，类加载信息，使用 classloader 去 getResource dump - dump 已加载类的 byte code 到特定目录 jad - 反编译指定已加载类的源码 mc - 内存编译器，内存编译.java文件为.class文件 redefine - 加载外部的.class文件，redefine 到 JVM 里 retransform - 加载外部的.class文件，retransform 到 JVM 里 sc - 查看 JVM 已加载的类信息 sm - 查看已加载类的方法信息 monitor/watch/trace 相关 monitor - 方法执行监控 stack - 输出当前方法被调用的调用路径 trace - 方法内部调用路径，并输出方法路径上的每个节点上耗时 tt - 方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测 watch - 方法执行数据观测 profiler/火焰图 profiler - 使用async-profiler对应用采样，生成火焰图 jfr - 动态开启关闭 JFR 记录 目前常用的命令如下 dashboard 可以使用 dashboard 命令查看服务线程，内存，JVM 相关信息。 jad 可以使用 jad 命令反编译源码 1jad &lt;package&gt;.&lt;class&gt; watch 可以使用 watch 命令监测程序返回值(returnObj)，抛出异常(target)和入参(params) 1watch &lt;package&gt;.&lt;class&gt; &lt;function&gt; returnObj -x 4 trace 可以使用 trace 命令检索方法调用路径，统计调用链路上的性能开销 1trace &lt;package&gt;.&lt;class&gt; &lt;function&gt; stack 可以使用 stack 命令输出当前方法被调用的调用路径 1stack &lt;package&gt;.&lt;class&gt; &lt;function&gt; tt 可以使用 tt 命令记录下当时方法调用的所有入参和返回值、抛出的异常 开始记录 1tt -t &lt;package&gt;.&lt;class&gt; &lt;function&gt; 查看当前记录清单 1tt -l 根据 ID 获取记录内容 1tt -i &lt;index&gt; 重新调用进行测试 1tt -i &lt;index&gt; -p profiler profiler 命令支持生成应用热点的火焰图 启动 profiler 开启数据收集 1profiler start 获取当前已采集的样本数量 1profiler getSamples 生成结果 1profiler stop --format html 注：profiler 只支持 Linux 和 Mac 系统。 web 控制台 在链接到 java 进程之后就可以访问 http://127.0.0.1:8563/ 进入控制台。 注：默认监听地址为 127.0.0.1 不建议修改可能有安全风险，如果想要使用可以指定 --target-ip 参数指定监听地址。 参考资料 官方项目 官方文档","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Arthas","slug":"Arthas","permalink":"https://wangqian0306.github.io/tags/Arthas/"}]},{"title":"awt","slug":"java/awt","date":"2023-07-24T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/awt/","permalink":"https://wangqian0306.github.io/2023/awt/","excerpt":"","text":"AWT 简介 抽象视窗工具组(Abstract Window Toolkit，简称：AWT) 是Java的平台独立的视窗系统，也是图形和使用界面的工具包。 常见使用样例 获取图像宽高 123456789101112131415161718192021222324import javax.imageio.ImageIO;import java.awt.image.BufferedImage;import java.io.File;import java.io.IOException;public class ImageSizeExample &#123; public static void main(String[] args) &#123; String imagePath = &quot;path/to/your/image.jpg&quot;; try &#123; File imageFile = new File(imagePath); BufferedImage bufferedImage = ImageIO.read(imageFile); int width = bufferedImage.getWidth(); int height = bufferedImage.getHeight(); System.out.println(&quot;Image Width: &quot; + width); System.out.println(&quot;Image Height: &quot; + height); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 合并图片并设定位置 123456789101112131415161718192021222324import javax.imageio.ImageIO;import java.awt.*;import java.awt.image.BufferedImage;import java.io.File;import java.io.IOException;public class ImageCombine &#123; public static void main(String[] args) &#123; try &#123; BufferedImage image1 = ImageIO.read(new File(&quot;path/to/image1.jpg&quot;)); BufferedImage image2 = ImageIO.read(new File(&quot;path/to/image2.jpg&quot;)); BufferedImage combined = new BufferedImage(image1.getWidth(), image1.getHeight(), BufferedImage.TYPE_INT_ARGB); Graphics g = combined.getGraphics(); g.drawImage(image1, 0, 0, null); g.drawImage(image2, 50, 50, null); g.dispose(); File outputImage = new File(&quot;path/to/output.jpg&quot;); ImageIO.write(combined, &quot;jpg&quot;, outputImage); System.out.println(&quot;Images combined successfully!&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Apache PDFBox","slug":"java/pdfbox","date":"2023-07-24T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/pdfBox/","permalink":"https://wangqian0306.github.io/2023/pdfBox/","excerpt":"","text":"Apache PDFBox 简介 Apache PDFBox 库是一个用于处理 PDF 文档的开源 Java 工具。 使用方式 引入依赖： 123dependencies &#123; implementation &#x27;org.apache.pdfbox:pdfbox:2.0.30&#x27;&#125; 编写生成代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import org.apache.pdfbox.pdmodel.PDDocument;import org.apache.pdfbox.pdmodel.PDPage;import org.apache.pdfbox.pdmodel.PDPageContentStream;import org.apache.pdfbox.pdmodel.common.PDRectangle;import org.apache.pdfbox.pdmodel.font.PDType1Font;import org.apache.pdfbox.pdmodel.graphics.image.PDImageXObject;import java.io.IOException;public class PDFGenerator &#123; public static void main(String[] args) &#123; String pdfFilePath = &quot;output.pdf&quot;; String imgPath = &quot;&lt;image_path&gt;&quot;; try &#123; // Create a new document PDDocument document = new PDDocument(); PDPage page1 = new PDPage(PDRectangle.A4); document.addPage(page1); // Create content stream PDPageContentStream contentStream = new PDPageContentStream(document, page1); // Set image position and size float x = 100; // X-coordinate float y = 500; // Y-coordinate float width = 300; // Image width float height = 200; // Image height // Add image to the page contentStream.drawImage(PDImageXObject.createFromFile(imgPath, document), x, y, width, height); contentStream.close(); PDPage page2 = new PDPage(PDRectangle.A4); document.addPage(page2); PDPageContentStream contentStream2 = new PDPageContentStream(document, page2); // Draw table drawTableHeaders(contentStream2); drawTableRow(contentStream2, 1, &quot;Data 1&quot;, &quot;Value 1&quot;); drawTableRow(contentStream2, 2, &quot;Data 2&quot;, &quot;Value 2&quot;); contentStream2.close(); document.save(pdfFilePath); document.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static void drawTableHeaders(PDPageContentStream contentStream) throws IOException &#123; float margin = 50; float yStart = 700; float tableWidth = 500; float yPosition = yStart; float rowHeight = 20; float cellMargin = 5f; float[] columnWidths = &#123;200, 300&#125;; // Adjust column widths as needed // Draw table headers contentStream.setFont(PDType1Font.HELVETICA_BOLD, 12); contentStream.setLineWidth(1f); contentStream.moveTo(margin, yPosition); contentStream.lineTo(margin + tableWidth, yPosition); contentStream.stroke(); yPosition -= rowHeight; contentStream.beginText(); contentStream.newLineAtOffset(margin + cellMargin, yPosition - 15); contentStream.showText(&quot;Column 1&quot;); contentStream.newLineAtOffset(columnWidths[0], 0); contentStream.showText(&quot;Column 2&quot;); contentStream.endText(); &#125; private static void drawTableRow(PDPageContentStream contentStream, int rowNum, String data1, String data2) throws IOException &#123; float margin = 50; float yStart = 700; float tableWidth = 500; float yPosition = yStart - rowNum * 20; float rowHeight = 20; float cellMargin = 5f; float[] columnWidths = &#123;200, 300&#125;; // Adjust column widths as needed // Draw table row contentStream.setLineWidth(1f); contentStream.moveTo(margin, yPosition); contentStream.lineTo(margin + tableWidth, yPosition); contentStream.stroke(); yPosition -= rowHeight; contentStream.beginText(); contentStream.setFont(PDType1Font.HELVETICA, 12); contentStream.newLineAtOffset(margin + cellMargin, yPosition - 15); contentStream.showText(data1); contentStream.newLineAtOffset(columnWidths[0], 0); contentStream.showText(data2); contentStream.endText(); &#125;&#125; 参考资料 官方网站 官方项目","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"PDF","slug":"PDF","permalink":"https://wangqian0306.github.io/tags/PDF/"}]},{"title":"PostgreSQL","slug":"database/postgresql","date":"2023-07-19T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2023/postgresql/","permalink":"https://wangqian0306.github.io/2023/postgresql/","excerpt":"","text":"PostgreSQL 简介 PostgreSQL 是一个对象关系数据库管理系统（ORDBMS） 部署方式 Docker 编写如下 Docker Compose 文件 123456services: db: image: postgres restart: always environment: POSTGRES_PASSWORD: example 使用如下命令启动运行 1docker-compose up -d 参考资料 官方文档","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://wangqian0306.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://wangqian0306.github.io/tags/PostgreSQL/"}]},{"title":"HTTPie","slug":"tools/httpie","date":"2023-07-18T13:32:58.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/httpie/","permalink":"https://wangqian0306.github.io/2023/httpie/","excerpt":"","text":"HTTPie 简介 HTTPie 是一个命令行HTTP客户端。其目标是尽可能人性化的实现 CLI 与 web 服务的交互。 安装方式 Windows 1choco install httpie CentOS 12yum install epel-releaseyum install httpie 常用命令 请求模板： 1http [METHOD] URL [REQUEST_ITEM ...] 参数说明： : 参数代表此参数会被入 header 中 == 参数代表请求参数会被放在 URL 中 = 参数代表请求参数会被放在 body 中，如果数据类型是 JSON 则需要添加 -j 参数，如果数据类型是 FORM 则需要添加 -f 参数 := 参数代表请求参数会被放在 body 中，且此参数为非 String 类型 @ 参数代表请求参数是文件路径，文件会被放在 body 中 =@ 参数和 = 参数一样，只不过接收的是文件路径，并将其放置在参数内 :=@ 参数和 := 参数一样，只不过接收的是文件路径，并将其放置在参数内 GET 请求 1https httpie.io/hello PUT 请求 1http PUT pie.dev/put X-API-Token:123 name=John -v POST 请求 1http POST pie.dev/post &lt; files/data.json 参考资料 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://wangqian0306.github.io/tags/HTTP/"}]},{"title":"screen 命令","slug":"linux/screen","date":"2023-07-17T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/screen/","permalink":"https://wangqian0306.github.io/2023/screen/","excerpt":"","text":"screen 命令 简介 Linux 中的 screen 命令提供了保存会话(session)的功能，可以让一个用户开启多个不同的会话并保存其状态。 使用方式 安装命令 1yum install screen 查看终端列表 1screen -ls 创建或进入终端 1screen -R &lt;name&gt; 保存并退出终端 ctrl + A 然后按 D 注：在命令行中记得使用英文输入法。 删除终端 1screen -R &lt;pid/name&gt; -X quit 或进入终端使用 exit 命令 参考资料 终端命令神器–Screen命令详解 screen command in Linux with Examples","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"VMware 模拟 U 盘","slug":"tmp/vmware-flash-drive","date":"2023-07-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/vmware-flash-drive/","permalink":"https://wangqian0306.github.io/2023/vmware-flash-drive/","excerpt":"","text":"VMware 模拟 U 盘 简介 在 VMware 中可以轻松的创建新的硬盘，但是创建一个虚拟的 U 盘却需要一些神奇的操作。 使用方式 首先需要找到 VMware Workstation 软件的所在位置，然后运行如下命令生成一个虚拟磁盘。 1vmware-vdiskmanager.exe -c -a buslogic -s &lt;size&gt; -t 0 &quot;&lt;path&gt;&quot; 样例如下： 1vmware-vdiskmanager.exe -c -a buslogic -s 32GB -t 0 &quot;D:\\VMware VMs\\vm-usb-key.vmdk&quot; 然后即可将生成的虚拟磁盘移动到目标虚拟机中 注：目标虚拟机最好是个 Windows 因为新的虚拟磁盘需要进行初始化和分区。 编辑目标虚拟机的配置文件 xxx.vmx，确保以下参数在系统中存在。 12ehci.present = &quot;TRUE&quot;ehci.pciSlotNumber = &quot;xx&quot; 在配置文件中添加如下内容： 1234ehci:0.present = &quot;TRUE&quot;ehci:0.deviceType = &quot;disk&quot;ehci:0.fileName = &quot;vm-usb-key.vmdk&quot;ehci:0.readonly = &quot;FALSE&quot; 启动虚拟机并进行初始化即可。 参考资料 Create a Virtual USB Drive in VMware Workstation","categories":[],"tags":[{"name":"VMware","slug":"VMware","permalink":"https://wangqian0306.github.io/tags/VMware/"}]},{"title":"Jellyfin","slug":"linux/jellyfin","date":"2023-07-06T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/jellyfin/","permalink":"https://wangqian0306.github.io/2023/jellyfin/","excerpt":"","text":"Jellyfin 简介 Jellyfin 是一个开源的媒体解决方案。 容器安装 123456789101112131415161718services: jellyfin: image: nyanmisaka/jellyfin:latest container_name: jellyfin user: &lt;uid:gid&gt; network_mode: &#x27;host&#x27; volumes: - /path/to/config:/config - /path/to/cache:/cache - /path/to/media:/media - /path/to/media2:/media2:ro restart: &#x27;unless-stopped&#x27; # Optional - alternative address used for autodiscovery environment: - JELLYFIN_PublishedServerUrl=http://example.com # Optional - may be necessary for docker healthcheck to pass if running in host network mode extra_hosts: - &quot;host.docker.internal:host-gateway&quot; 参考资料 官方网站 官方项目 中文整合版容器","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"Jellyfin","slug":"Jellyfin","permalink":"https://wangqian0306.github.io/tags/Jellyfin/"}]},{"title":"Spring Session","slug":"spring/session","date":"2023-07-05T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/spring-session/","permalink":"https://wangqian0306.github.io/2023/spring-session/","excerpt":"","text":"Spring Session 简介 Spring Session 提供了用于管理用户会话信息相关的 API 和实现。 使用方式 引入如下依赖： 1234567891011dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-redis&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-security&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.session:spring-session-data-redis&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;org.springframework.security:spring-security-test&#x27;&#125; 然后在主类上添加 @EnableRedisHttpSession 注解即可： 12345678910111213import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;@EnableRedisHttpSession@SpringBootApplicationpublic class SessionRedisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SessionRedisApplication.class, args); &#125;&#125; 编写 Security 配置类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.Customizer;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.core.userdetails.User;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.provisioning.InMemoryUserDetailsManager;import org.springframework.security.web.SecurityFilterChain;import static jakarta.servlet.DispatcherType.ERROR;import static jakarta.servlet.DispatcherType.FORWARD;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Bean public InMemoryUserDetailsManager userDetailsService(PasswordEncoder passwordEncoder) &#123; UserDetails user = User.withUsername(&quot;admin&quot;) .password(passwordEncoder.encode(&quot;admin&quot;)) .roles(&quot;ADMIN&quot;) .build(); return new InMemoryUserDetailsManager(user); &#125; @Bean public SecurityFilterChain filterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize .dispatcherTypeMatchers(FORWARD, ERROR).permitAll() .requestMatchers(&quot;/login&quot;).permitAll() .anyRequest().authenticated() ) .formLogin(Customizer.withDefaults()); return http.build(); &#125; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125;&#125; 编写项目配置文件： 12345spring: data: redis: host: xxx.xxx.xxx.xxx port: 6379 启动程序并进行登录即可在 Redis 中看到 Session 信息。 参考资料 官方文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spring Boot Actuator","slug":"spring/actuator","date":"2023-06-30T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/actuator/","permalink":"https://wangqian0306.github.io/2023/actuator/","excerpt":"","text":"Spring Boot Actuator 简介 spring-boot-actuator 模块提供了将 Spring Boot 程序部署向生产环境中的准备功能，比如说生命监控，版本信息，性能信息等。 使用方式 引入如下依赖即可： 123dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-actuator&#x27;&#125; 在引入后访问 http://localhost:8080/actuator 即可获得目前开放的所有 actuator 。 在使用时仅需要编辑 application.yaml 配置文件即可，具体样例如下： 12345management: endpoints: jmx: exposure: include: &quot;health,info&quot; 注：上面的样例表示了开放 health 和 info 信息，其他内容请参照官方文档。 开启版本和服务信息(开发环境) 12345678910111213141516171819202122232425management: info: env: enabled: true build: enabled: true git: enabled: true mode: full java: enabled: true os: enabled: true endpoints: web: exposure: include: &quot;*&quot;info: app: name: demo description: demo server verion: 1.0.0 author: demo docs: http://www.google.com 自定义端点 1234567891011121314import org.springframework.boot.actuate.endpoint.annotation.Endpoint;import org.springframework.boot.actuate.endpoint.annotation.ReadOperation;import org.springframework.stereotype.Service;@Service@Endpoint(id = &quot;demo&quot;)public class DemoEndpoint &#123; @ReadOperation public String demo() &#123; return &quot;&#123;\\&quot;hello\\&quot;:\\&quot;demo\\&quot;&#125;&quot;; &#125;&#125; 生产环境中的使用 需要注意的是，Spring 官方除了当前 Spring 的基本状态之外，还针对很多其它的 组件 提供了健康检查(Health Check) 。 在生产环境中使用的时候就尤其需要区分不同的组来避免由于类似与数据库这样的基础服务出错导致服务循环重启的问题。 Liveness State 建议进行如下配置： 1234567891011121314151617181920management: endpoint: health: group: liveness: include: &quot;ping&quot; readiness: include: &quot;mongo,redis&quot; # Readiness 检查包括外部依赖 endpoints: web: exposure: include: &quot;health&quot; # 暴露 health 端点spring: data: mongodb: uri: &quot;mongodb://&lt;your-mongo-host&gt;:27017/&lt;your-db&gt;&quot; # MongoDB 配置 redis: host: &quot;&lt;your-redis-host&gt;&quot; # Redis 配置 port: 6379 之后即可访问如下地址获取到 liveness 状态： http://localhost:8080/actuator/health/liveness 参考资料 官方文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Actuator","slug":"Actuator","permalink":"https://wangqian0306.github.io/tags/Actuator/"}]},{"title":"Spring Authorization Server","slug":"spring/authorization-server","date":"2023-06-30T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/authorization-server/","permalink":"https://wangqian0306.github.io/2023/authorization-server/","excerpt":"","text":"Spring Authorization Server 简介 Spring Authorization Server 是一个框架，提供 OAuth 2.1 和 OpenID Connect 1.0 规范以及其他相关规范的实现。 通常与 Spring Authorization Server 一起使用的组件还有 OAuth2 Resource Server (负责保护受保护的资源，并验证访问令牌以确保客户端和用户有权访问这些资源。) 和 OAuth2 Client (负责代表用户请求所需资源) 使用方式 Spring Boot CLI 使用如下命令安装 CLI ： 1sdk install springboot 然后使用如下命令即可生成密码： 1spring encodepassword secret 注：此处保存生成的密码即可。 OAuth2 Authorization Server 在创建项目时引入 OAuth2 Authorization Server 依赖即可，样例如下： 12345678910dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-jpa&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-oauth2-authorization-server&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; runtimeOnly &#x27;com.h2database:h2&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27;&#125; OAuth 编辑如下配置类即可： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.core.userdetails.User;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.oauth2.core.AuthorizationGrantType;import org.springframework.security.oauth2.core.oidc.OidcScopes;import org.springframework.security.oauth2.server.authorization.client.InMemoryRegisteredClientRepository;import org.springframework.security.oauth2.server.authorization.client.RegisteredClient;import org.springframework.security.oauth2.server.authorization.client.RegisteredClientRepository;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.server.authorization.settings.ClientSettings;import org.springframework.security.provisioning.InMemoryUserDetailsManager;@Configurationpublic class CustomOAuthConfig &#123; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; @Bean public UserDetailsService userDetailsService() &#123; UserDetails userDetails = User.withUsername(&quot;admin&quot;) .password(passwordEncoder().encode(&quot;admin&quot;)) .roles(&quot;USER&quot;) .build(); return new InMemoryUserDetailsManager(userDetails); &#125; @Bean public RegisteredClientRepository registeredClientRepository() &#123; RegisteredClient registeredClient = RegisteredClient.withId(&quot;local&quot;) .clientId(&quot;local&quot;) .clientSecret(&quot;$xxxxx&quot;) .redirectUri(&quot;http://localhost:8080/test&quot;) .authorizationGrantType(AuthorizationGrantType.CLIENT_CREDENTIALS) .authorizationGrantType(AuthorizationGrantType.AUTHORIZATION_CODE) .authorizationGrantType(AuthorizationGrantType.REFRESH_TOKEN) .postLogoutRedirectUri(&quot;http://localhost:8080/logout&quot;) .scope(OidcScopes.OPENID) .scope(OidcScopes.PROFILE) .clientSettings(ClientSettings.builder().requireAuthorizationConsent(true).build()) .build(); return new InMemoryRegisteredClientRepository(registeredClient); &#125;&#125; 注：此处可以使用之前生成的密码替换 clientSecret，不要带上 &#123;bcrypt&#125;。 启动程序然后使用如下命令即可获得 Token: 1http -f POST :8080/oauth2/token grant_type=client_credentials scope=&#x27;user.read&#x27; -a client:secret 注：如果没有 httpie 工具则可以使用 IDEA 自带的 Http 工具。 12345POST http://localhost:8080/oauth2/tokenAuthorization: Basic local secretContent-Type: application/x-www-form-urlencodedgrant_type=client_credentials&amp;scope=user.read 或者使用如下方式获取 OAuth Token： 访问如下地址并输入账号密码，点击同意授权： http://localhost:8080/oauth2/authorize?scope=openid+profile+email&amp;response_type=code&amp;client_id=local&amp;redirect_uri=http://localhost:8080/test 之后可以从 URL 中获取到 code, 将其填写至下面的请求中即可获取 Token 。 12345678910### GET TOKENPOST http://localhost:8080/oauth2/tokenAuthorization: Basic local secretContent-Type: application/x-www-form-urlencodedgrant_type = authorization_code &amp;client_id = local &amp;client_secret = secret &amp;code = xxxx &amp;redirect_uri = http://localhost:8080/test Open ID Connect 编写如下 application.yaml 配置文件即可： 12345678910111213141516171819202122232425262728293031323334logging: level: org.springframework.security.oauth2.server.authorization: DEBUG org.springframework.security: DEBUGspring: application: name: auth-playground security: user: name: admin password: admin oauth2: authorizationserver: client: oidc-client: registration: client-id: &quot;oidc-client&quot; client-secret: &quot;&#123;noop&#125;secret&quot; client-authentication-methods: - &quot;client_secret_basic&quot; authorization-grant-types: - &quot;authorization_code&quot; - &quot;refresh_token&quot; redirect-uris: - &quot;http://localhost:3000/auth/callback/oidc-client&quot; - &quot;http://localhost:8080/test&quot; post-logout-redirect-uris: - &quot;http://localhost:3000/&quot; - &quot;http://localhost:8080/&quot; scopes: - &quot;openid&quot; - &quot;profile&quot; require-authorization-consent: true 编写测试接口用于接收 Token ： 12345678910111213import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMappingpublic class WQController &#123; @GetMapping(&quot;/test&quot;) public String test() &#123; return &quot;test&quot;; &#125;&#125; 使用如下方式获取 Token： 访问如下地址并输入账号密码，点击同意授权： http://localhost:8080/oauth2/authorize?scope=openid&amp;response_type=code&amp;client_id=oidc-client&amp;redirect_uri=http://localhost:8080/test 之后可以从 URL 中获取到 code, 将其填写至下面的请求中即可获取 Token 。 12345678910### GET TOKENPOST http://localhost:8080/oauth2/tokenAuthorization: Basic oidc-client secretContent-Type: application/x-www-form-urlencodedgrant_type = authorization_code &amp;client_id = oidc-client &amp;client_secret = secret &amp;code = xxxx &amp;redirect_uri = http://localhost:8080/test 又或者使用 next-auth-example 项目 进行试用。 在接收到 Token 后还可以使用 id_token 进行登出，访问如下地址即可完成登出，并重定向回系统登录地址： http://lcoalhost:8080/connect/logout?post_logout_redirect_uri=http://lcoalhost:3000/&amp;id_token_hint= 自定义 userinfo 编辑 OidcUserInfoService ： 123456789101112131415161718192021222324252627282930313233343536373839import org.springframework.security.oauth2.core.oidc.OidcUserInfo;import org.springframework.stereotype.Service;import java.util.Collections;import java.util.Map;@Servicepublic class OidcUserInfoService &#123; public OidcUserInfo loadUser(String username) &#123; return new OidcUserInfo(createUser(username)); &#125; public Map&lt;String, Object&gt; createUser(String username) &#123; return OidcUserInfo.builder() .subject(username) .name(&quot;First Last&quot;) .givenName(&quot;First&quot;) .familyName(&quot;Last&quot;) .middleName(&quot;Middle&quot;) .nickname(&quot;User&quot;) .preferredUsername(username) .profile(&quot;https://example.com/&quot; + username) .picture(&quot;https://example.com/&quot; + username + &quot;.jpg&quot;) .website(&quot;https://example.com&quot;) .email(username + &quot;@example.com&quot;) .emailVerified(true) .gender(&quot;female&quot;) .birthdate(&quot;1970-01-01&quot;) .zoneinfo(&quot;Europe/Paris&quot;) .locale(&quot;en-US&quot;) .phoneNumber(&quot;+1 (604) 555-1234;ext=5678&quot;) .phoneNumberVerified(false) .claim(&quot;address&quot;, Collections.singletonMap(&quot;formatted&quot;, &quot;Champ de Mars\\n5 Av. Anatole France\\n75007 Paris\\nFrance&quot;)) .updatedAt(&quot;1970-01-01T00:00:00Z&quot;) .build() .getClaims(); &#125;&#125; 新增配置类： 1234567891011121314151617181920212223242526import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.oauth2.core.oidc.OidcUserInfo;import org.springframework.security.oauth2.core.oidc.endpoint.OidcParameterNames;import org.springframework.security.oauth2.server.authorization.token.JwtEncodingContext;import org.springframework.security.oauth2.server.authorization.token.OAuth2TokenCustomizer;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Bean public OAuth2TokenCustomizer&lt;JwtEncodingContext&gt; tokenCustomizer( OidcUserInfoService userInfoService) &#123; return (context) -&gt; &#123; if (OidcParameterNames.ID_TOKEN.equals(context.getTokenType().getValue())) &#123; OidcUserInfo userInfo = userInfoService.loadUser( context.getPrincipal().getName()); context.getClaims().claims(claims -&gt; claims.putAll(userInfo.getClaims())); &#125; &#125;; &#125;&#125; 注：在 scope 中新增 email phone 等 key 后就可以在 /userinfo 路由获取对应信息，或者将 id_token 放在 JWT Debugger 中也可以解析这些内容。 自定义登录页 首先需要引入依赖： 12345678910dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-oauth2-authorization-server&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-thymeleaf&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testRuntimeOnly &#x27;org.junit.platform:junit-platform-launcher&#x27;&#125; 然后需要编写 LoginController.java ： 123456789101112import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;@Controllerpublic class LoginController &#123; @GetMapping(&quot;/login&quot;) public String login() &#123; return &quot;login&quot;; &#125;&#125; 编写如下 resources/templates/login.html ： 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh&quot; xmlns:th=&quot;https://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;title&gt;Please Log In&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Here is Custom Login page. Please Log In: &lt;/h1&gt;&lt;div th:if=&quot;$&#123;param.error&#125;&quot;&gt; Invalid username and password.&lt;/div&gt;&lt;div th:if=&quot;$&#123;param.logout&#125;&quot;&gt; You have been logged out.&lt;/div&gt;&lt;form th:action=&quot;@&#123;/login&#125;&quot; method=&quot;post&quot;&gt; &lt;div&gt; &lt;label&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;Username&quot;/&gt; &lt;/label&gt; &lt;/div&gt; &lt;div&gt; &lt;label&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; placeholder=&quot;Password&quot;/&gt; &lt;/label&gt; &lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Log in&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 最后可以修改配置类，让静态资源和页面可以被正常访问： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.annotation.Order;import org.springframework.http.MediaType;import org.springframework.security.config.Customizer;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityCustomizer;import org.springframework.security.oauth2.core.oidc.OidcUserInfo;import org.springframework.security.oauth2.core.oidc.endpoint.OidcParameterNames;import org.springframework.security.oauth2.server.authorization.config.annotation.web.configuration.OAuth2AuthorizationServerConfiguration;import org.springframework.security.oauth2.server.authorization.config.annotation.web.configurers.OAuth2AuthorizationServerConfigurer;import org.springframework.security.oauth2.server.authorization.token.JwtEncodingContext;import org.springframework.security.oauth2.server.authorization.token.OAuth2TokenCustomizer;import org.springframework.security.web.SecurityFilterChain;import org.springframework.security.web.authentication.LoginUrlAuthenticationEntryPoint;import org.springframework.security.web.util.matcher.MediaTypeRequestMatcher;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Bean @Order(1) public SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http) throws Exception &#123; OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http); http.getConfigurer(OAuth2AuthorizationServerConfigurer.class) .oidc(Customizer.withDefaults()); http .exceptionHandling((exceptions) -&gt; exceptions .defaultAuthenticationEntryPointFor( new LoginUrlAuthenticationEntryPoint(&quot;/login&quot;), new MediaTypeRequestMatcher(MediaType.TEXT_HTML) ) ) .oauth2ResourceServer((resourceServer) -&gt; resourceServer .jwt(Customizer.withDefaults())); return http.build(); &#125; @Bean @Order(2) public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize .requestMatchers(&quot;/picture/**&quot;).permitAll() .requestMatchers(&quot;favicon.ico&quot;).permitAll() .anyRequest().authenticated()) .formLogin(form -&gt; form.loginPage(&quot;/login&quot;).permitAll()); return http.build(); &#125; @Bean WebSecurityCustomizer webSecurityCustomizer() &#123; return (web) -&gt; web.debug(false).ignoring().requestMatchers(&quot;/picture/**&quot;); &#125; @Bean public OAuth2TokenCustomizer&lt;JwtEncodingContext&gt; tokenCustomizer( OidcUserInfoService userInfoService) &#123; return (context) -&gt; &#123; if (OidcParameterNames.ID_TOKEN.equals(context.getTokenType().getValue())) &#123; OidcUserInfo userInfo = userInfoService.loadUser( context.getPrincipal().getName()); context.getClaims().claims(claims -&gt; claims.putAll(userInfo.getClaims())); &#125; &#125;; &#125;&#125; OAuth2 Resource Server 在创建项目时引入 OAuth2 Resource Server 依赖即可，样例如下： 12345dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-oauth2-resource-server&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27;&#125; 然后编写如下 Contorller 即可： 1234567891011121314151617181920212223import org.springframework.http.ResponseEntity;import org.springframework.security.core.context.SecurityContextHolder;import org.springframework.security.oauth2.jwt.Jwt;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMappingpublic class TestController &#123; @GetMapping public ResponseEntity&lt;String&gt; user() &#123; Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal(); if (principal instanceof Jwt) &#123; String username = ((Jwt) principal).getSubject(); return ResponseEntity.ok(username); &#125; else &#123; throw new RuntimeException(&quot;Token error&quot;); &#125; &#125;&#125; 然后编写如下配置项： 12345678server: port: 8081spring: security: oauth2: resourceserver: jwt: issuer-uri: http://localhost:8080 启动服务，然后使用之前获取到的 Token 令牌访问即可： 注: 默认 Token 的有效期是 5 分钟，建议重新生成一个再访问。 123### GET USERGET http://localhost:8081/Authorization: Bearer &#123;token&#125; OAuth2 Client 在创建项目时引入 OAuth2 Client 和 Spring Cloud Gateway 依赖即可，样例如下： 1234567dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-oauth2-client&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; implementation &#x27;org.springframework.cloud:spring-cloud-starter-gateway&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27;&#125; 按照如下代码修改主类： 12345678910111213141516171819202122232425import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.gateway.route.RouteLocator;import org.springframework.cloud.gateway.route.builder.GatewayFilterSpec;import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class AuthclientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AuthclientApplication.class, args); &#125; @Bean RouteLocator gateway(RouteLocatorBuilder rlb) &#123; return rlb .routes() .route(rs -&gt; rs .path(&quot;/&quot;) .filters(GatewayFilterSpec::tokenRelay) .uri(&quot;http://localhost:8081&quot;)) .build(); &#125;&#125; 然后编写配置文件如下即可： 123456789101112131415161718server: port: 8082spring: security: oauth2: client: registration: spring: provider: spring client-id: client client-secret: secret authorization-grant-type: authorization_code client-authentication-method: client_secret_basic redirect-uri: &quot;&#123;baseUrl&#125;/login/oauth2/code/&#123;registrationId&#125;&quot; scope: user.read,openid provider: spring: issuer-uri: http://localhost:8080 测试方式： 访问如下地址，按照页面提示输入用户名和密码登录即可： http://127.0.0.1:8082 注：访问后会自动跳转到 OAuth2 Authorization Server 登录，并将使用 Session 存储用户信息。然后 Spring Cloud Gateway 通过读取 Session 生成 Token 并将请求转发到 OAuth2 Resource Server 中。 参考资料 官方文档 官方博客 视频教程 样例源码 OpenID Connect RP-Initiated Logout","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"OAuth","slug":"OAuth","permalink":"https://wangqian0306.github.io/tags/OAuth/"},{"name":"OIDC","slug":"OIDC","permalink":"https://wangqian0306.github.io/tags/OIDC/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Testcontainers","slug":"spring/test-container","date":"2023-06-28T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/test-containers/","permalink":"https://wangqian0306.github.io/2023/test-containers/","excerpt":"","text":"Testcontainers 简介 Testcontainers 是一个开源框架，用于提供数据库、消息总线、WEB 浏览器或任何可以在 Docker 容器中运行的东西的一次性轻量级实例。 目前，SpringBoot 支持了如下这些容器： CassandraContainer CouchbaseContainer ElasticsearchContainer GenericContainer 可以使用 redis 或 openzipkin/zipkin JdbcDatabaseContainer KafkaContainer MongoDBContainer MariaDBContainer MSSQLServerContainer MySQLContainer Neo4jContainer OracleContainer PostgreSQLContainer RabbitMQContainer RedpandaContainer 使用方式 在项目创建的时候引入 Testcontainer，在样例中以 MySQL 作为样例，依赖如下： 123456789dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-jpa&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; runtimeOnly &#x27;com.mysql:mysql-connector-j&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-testcontainers&#x27; testImplementation &#x27;org.testcontainers:junit-jupiter&#x27; testImplementation &#x27;org.testcontainers:mysql&#x27;&#125; 单元测试样例如下： 1234567891011121314151617181920import org.junit.jupiter.api.Test;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.boot.testcontainers.service.connection.ServiceConnection;import org.testcontainers.containers.MySQLContainer;import org.testcontainers.junit.jupiter.Container;import org.testcontainers.junit.jupiter.Testcontainers;@SpringBootTest@Testcontainersclass DemoApplicationTests &#123; @Container @ServiceConnection static MySQLContainer&lt;?&gt; mysql = new MySQLContainer&lt;&gt;(&quot;mysql:latest&quot;); @Test void contextLoads() &#123; &#125;&#125; 注：除了单元测试之外，还可以直接使用容器作为开发环境。从 test 包中启动如下代码即可： 1234567891011121314151617181920import org.springframework.boot.SpringApplication;import org.springframework.boot.test.context.TestConfiguration;import org.springframework.boot.testcontainers.service.connection.ServiceConnection;import org.springframework.context.annotation.Bean;import org.testcontainers.containers.MySQLContainer;@TestConfiguration(proxyBeanMethods = false)public class TestDemoApplication &#123; @Bean @ServiceConnection MySQLContainer&lt;?&gt; mysqlContainer() &#123; return new MySQLContainer&lt;&gt;(&quot;mysql:latest&quot;); &#125; public static void main(String[] args ) &#123; SpringApplication.from(xxxx::main).with(TestDemoApplication.class).run(args); &#125; &#125; 参考资料 官方网站 SpringBoot Testcontainers 文档 Improved Testcontainers Support in Spring Boot 3.1","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"container","slug":"container","permalink":"https://wangqian0306.github.io/tags/container/"}]},{"title":"Spring Data Elasticsearch","slug":"spring/data-elasticsearch","date":"2023-06-19T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/spring-data-elasticsearch/","permalink":"https://wangqian0306.github.io/2023/spring-data-elasticsearch/","excerpt":"","text":"Spring Data Elasticsearch 简介 Spring Data Elasticsearch 是一款使用 Spring 的核心概念的 Elasticsearch 客户端程序。 注：此项目为开源项目，更新不像原生客户端那样及时，使用时需要特别注意。 使用方式 引入依赖包 123dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-elasticsearch&#x27;&#125; 编写模型类 12345678910111213141516171819202122import lombok.Data;import org.springframework.data.annotation.Id;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.Field;import org.springframework.data.elasticsearch.annotations.FieldType;import java.time.LocalDateTime;@Data@Document(indexName = &quot;book&quot;)public class Book &#123; @Id private String id; @Field(type = FieldType.Keyword, name = &quot;author&quot;) private String author; @Field(type = FieldType.Date_Nanos, name = &quot;publishDate&quot;) private LocalDateTime publishDate; &#125; 编写 Repository 12345import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;public interface BookRepository extends ElasticsearchRepository&lt;Book, String&gt; &#123;&#125; 配置链接地址(使用配置类) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import lombok.SneakyThrows;import lombok.extern.slf4j.Slf4j;import org.apache.http.ssl.SSLContextBuilder;import org.apache.http.ssl.SSLContexts;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.Resource;import org.springframework.core.io.ResourceLoader;import org.springframework.data.elasticsearch.client.ClientConfiguration;import org.springframework.data.elasticsearch.client.elc.ElasticsearchConfiguration;import javax.net.ssl.SSLContext;import java.io.InputStream;import java.security.KeyStore;import java.security.cert.Certificate;import java.security.cert.CertificateFactory;@Slf4j@Configurationpublic class CustomESConfig extends ElasticsearchConfiguration &#123; @Value(&quot;$&#123;elasticsearch.certsPath&#125;&quot;) private String certsPath; @jakarta.annotation.Resource private ResourceLoader resourceLoader; @SneakyThrows @Override public ClientConfiguration clientConfiguration() &#123; Resource resource = resourceLoader.getResource(certsPath); CertificateFactory factory = CertificateFactory.getInstance(&quot;X.509&quot;); Certificate trustedCa; try (InputStream is = resource.getInputStream()) &#123; trustedCa = factory.generateCertificate(is); &#125; KeyStore trustStore = KeyStore.getInstance(&quot;pkcs12&quot;); trustStore.load(null, null); trustStore.setCertificateEntry(&quot;ca&quot;, trustedCa); SSLContextBuilder sslContextBuilder = SSLContexts.custom() .loadTrustMaterial(trustStore, null); final SSLContext sslContext = sslContextBuilder.build(); return ClientConfiguration.builder() .connectedTo(&quot;xxx.xxx.xxx.xxx:9200&quot;) .usingSsl(sslContext) .withBasicAuth(&quot;xxx&quot;, &quot;xxxxx&quot;) .build(); &#125;&#125; 然后编写如下配置文件： 12elasticsearch: certsPath: classpath:http_ca.crt 配置链接地址(使用配置文件，需要 SpringBoot 版本大于 3.1) 1234567891011121314spring: ssl: bundle: pem: es: truststore: certificate: &quot;classpath:http_ca.crt&quot; elasticsearch: uris: https://xxx.xxx.xxx.xxx:9200 username: xxx password: xxxx restclient: ssl: bundle: &quot;es&quot; 注：bundle 方式还可以用到其他需要 SSL 配置的数据库中，例如：MongoDB，Redis 等。 复杂查询 IN 查询 123TermsQueryField termsQueryField = new TermsQueryField.Builder() .value(List.of(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;).stream().map(FieldValue::of).toList()) .build(); 子查询 构建查询： 12345NativeQueryBuilder nativeQueryBuilder = NativeQuery.builder();Aggregation avgAgg = AggregationBuilders.avg(a -&gt; a.field(&quot;value&quot;));Aggregation dateAgg = new Aggregation.Builder().dateHistogram(dH -&gt; dH.field(&quot;messageTime&quot;).calendarInterval(CalendarInterval.Hour)).aggregations(&quot;avg_value&quot;,avgAgg).build();nativeQueryBuilder.withAggregation(&quot;agg_by_date&quot;, dateAgg);NativeQuery nativeQuery = nativeQueryBuilder.build() 结果解析： 1234567List&lt;ElasticsearchAggregation&gt; aggregationList = (List&lt;ElasticsearchAggregation&gt;) searchHit.getAggregations().aggregations();for (ElasticsearchAggregation elasticsearchAggregation : aggregationList) &#123; List&lt;DateHistogramBucket&gt; byHour = elasticsearchAggregation.aggregation().getAggregate().dateHistogram().buckets().array(); for (DateHistogramBucket dbk : byHour) &#123; Double value = dbk.aggregations().get(&quot;avg_value&quot;).avg().value() &#125;&#125; 单元测试 在单元测试时可以加上 @DataElasticsearchTest 注解避免实际插入数据。 注：此注解当前失效，如需使用可以参照 Issue 参考资料 官方文档 Securing Spring Boot Applications With SSL","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wangqian0306.github.io/tags/Elasticsearch/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"SSL","slug":"SSL","permalink":"https://wangqian0306.github.io/tags/SSL/"}]},{"title":"Spring for GraphQL","slug":"spring/graphql","date":"2023-05-22T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/graphql/","permalink":"https://wangqian0306.github.io/2023/graphql/","excerpt":"","text":"Spring for GraphQL 简介 GraphQL 是用于 API 的查询语言，也是一个服务器端的运行时，被用来执行指定类型的查询。 注：经过和 JPA 结合，它可以做到仅仅返回用户查询的字段，并不会做额外的查询，而且可以在单次查询中调用多个接口。 基础使用 引入如下依赖： Lombok Spring Web Spring Data JPA 数据库(本文使用 MySQL) 123456789101112dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-jpa&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-graphql&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; runtimeOnly &#x27;com.mysql:mysql-connector-j&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;org.springframework:spring-webflux&#x27; testImplementation &#x27;org.springframework.graphql:spring-graphql-test&#x27;&#125; 编写如下模型类： 12345678910111213141516171819202122232425262728293031323334353637383940import jakarta.persistence.*;import lombok.*;import org.hibernate.Hibernate;import java.util.ArrayList;import java.util.List;import java.util.Objects;@Getter@Setter@ToString@RequiredArgsConstructor@AllArgsConstructor@Entitypublic class Author &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; @OneToMany(mappedBy = &quot;author&quot;, cascade = CascadeType.ALL) @ToString.Exclude private List&lt;Book&gt; books = new ArrayList&lt;&gt;(); @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || Hibernate.getClass(this) != Hibernate.getClass(o)) return false; Author author = (Author) o; return getId() != null &amp;&amp; Objects.equals(getId(), author.getId()); &#125; @Override public int hashCode() &#123; return getClass().hashCode(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940import jakarta.persistence.*;import lombok.*;import org.hibernate.Hibernate;import java.util.Objects;@Getter@Setter@ToString@RequiredArgsConstructor@AllArgsConstructor@Entitypublic class Book &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String title; private String publisher; @ManyToOne(fetch = FetchType.LAZY) @ToString.Exclude private Author author; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || Hibernate.getClass(this) != Hibernate.getClass(o)) return false; Book book = (Book) o; return getId() != null &amp;&amp; Objects.equals(getId(), book.getId()); &#125; @Override public int hashCode() &#123; return getClass().hashCode(); &#125;&#125; 编写如下 Repository ： 123456import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface AuthorRepository extends JpaRepository&lt;Author,Long&gt; &#123;&#125; 123456import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface BookRepository extends JpaRepository&lt;Book, Long&gt; &#123;&#125; 在主类中插入测试数据： 12345678910111213141516171819202122232425262728import org.springframework.boot.ApplicationRunner;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import java.util.ArrayList;import java.util.List;@SpringBootApplicationpublic class GqlApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GqlApplication.class, args); &#125; @Bean ApplicationRunner applicationRunner(AuthorRepository authorRepository, BookRepository bookRepository) &#123; return args -&gt; &#123; Author josh = authorRepository.save(new Author(null, &quot;Josh&quot;, new ArrayList&lt;&gt;())); Author mark = authorRepository.save(new Author(null, &quot;Mark&quot;, new ArrayList&lt;&gt;())); bookRepository.saveAll(List.of( new Book(null, &quot;Java 11&quot;, &quot;Tom&quot;, josh), new Book(null, &quot;Java 12&quot;, &quot;Jerry&quot;, mark), new Book(null, &quot;Java 13&quot;, &quot;Spike&quot;, josh) )); &#125;; &#125;&#125; 编写 Controller : 123456789101112131415161718import jakarta.annotation.Resource;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.stereotype.Controller;import java.util.List;@Controllerpublic class TestController &#123; @Resource private AuthorRepository authorRepository; @QueryMapping List&lt;Author&gt; authors() &#123; return authorRepository.findAll(); &#125;&#125; 编写 src/main/resources/graphql/schema.graphqls 配置文件： 123456789101112131415type Query &#123; authors: [Author]&#125;type Author &#123; id: ID! name: String! books: [Book]&#125;type Book &#123; id: ID! title: String! publisher: String&#125; 编写 application.yaml 配置文件： 12345678910111213141516171819server: port: 8080spring: application: name: gql datasource: driver-class-name: $&#123;JDBC_DRIVER:com.mysql.cj.jdbc.Driver&#125; url: $&#123;MYSQL_URI:jdbc:mysql://xxx.xxx.xxx.xxx:3306/xxx&#125; username: $&#123;MYSQL_USERNAME:xxxx&#125; password: $&#123;MYSQL_PASSWORD:xxxx&#125; jackson: time-zone: Asia/Shanghai jpa: show-sql: true hibernate: ddl-auto: create-drop graphql: graphiql: enabled: true 启动程序，然后访问 http://localhost:8080/graphiql 即可看到调试控制台，输入如下内容即可完成测试。 12345678910query &#123; authors &#123; id name books &#123; id title &#125; &#125;&#125; 数据分页 GraphQL 本身包含自己的 分页请求模型与方式，在项目中可以使用如下方式实现。 修改 Repository： 123456789101112import org.springframework.data.domain.Limit;import org.springframework.data.domain.ScrollPosition;import org.springframework.data.domain.Window;import org.springframework.data.repository.ListCrudRepository;import org.springframework.graphql.data.GraphQlRepository;@GraphQlRepositorypublic interface AuthorRepository extends ListCrudRepository&lt;Author, Long&gt; &#123; Window&lt;Author&gt; findBy(ScrollPosition position, Limit limit);&#125; 注：此处可以额外添加查询参数和排序等内容。 修改请求类： 12345678910111213141516171819202122import jakarta.annotation.Resource;import org.springframework.data.domain.Limit;import org.springframework.data.domain.ScrollPosition;import org.springframework.data.domain.Window;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.graphql.data.query.ScrollSubrange;import org.springframework.stereotype.Controller;@Controllerpublic class TestController &#123; @Resource private AuthorRepository authorRepository; @QueryMapping Window&lt;Author&gt; authors(ScrollSubrange subrange) &#123; ScrollPosition scrollPosition = subrange.position().orElse(ScrollPosition.offset()); Limit limit = Limit.of(subrange.count().orElse(1)); return authorRepository.findBy(scrollPosition, limit); &#125;&#125; 修改 graphql 配置文件： 12345678type Query &#123; authors(first: Int,last: Int,before: String,after: String): AuthorConnection&#125;type Author &#123; id: ID! name: String!&#125; 在 graphiql 页面中即可使用如下查询： 12345678910111213141516query&#123; authors &#123; edges&#123; node &#123; id name &#125; &#125; pageInfo &#123; hasPreviousPage hasNextPage startCursor endCursor &#125; &#125;&#125; 嵌套查询 在 GraphQL 中还可以嵌套查询逻辑，样例如下： 12345678910111213141516171819202122232425262728import jakarta.annotation.Resource;import org.springframework.graphql.data.method.annotation.Argument;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.graphql.data.method.annotation.SchemaMapping;import org.springframework.stereotype.Controller;import java.util.List;@Controllerpublic class TestController &#123; @Resource private AuthorRepository authorRepository; @Resource private BookRepository bookRepository; @QueryMapping List&lt;Author&gt; authors() &#123; return authorRepository.findAll(); &#125; @SchemaMapping List&lt;Book&gt; books(Author author,@Argument String publisher) &#123; return bookRepository.findAllByAuthorIdAndPublisherLike(author.getId(), publisher); &#125;&#125; src/main/resources/graphql/schema.graphqls 配置文件： 123456789101112131415type Query &#123; authors: [Author]&#125;type Author &#123; id: ID! name: String! books(publisher:String): [Book]&#125;type Book &#123; id: ID! title: String! publisher: String&#125; WebSocket 首先需要切换 Web 至 WebFlux，样例如下： 12345678910dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-graphql&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27; testImplementation &#x27;org.springframework.graphql:spring-graphql-test&#x27;&#125; 编写 Controller： 1234567891011121314151617181920212223242526272829import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.graphql.data.method.annotation.SubscriptionMapping;import org.springframework.stereotype.Controller;import reactor.core.publisher.Flux;import java.time.Duration;import java.time.Instant;import java.util.function.Supplier;import java.util.stream.Stream;@Controllerpublic class SampleController &#123; @QueryMapping public String greeting() &#123; return &quot;Hello world!&quot;; &#125; @SubscriptionMapping public Flux&lt;String&gt; greetings() &#123; return Flux.fromStream(Stream.generate(new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; return &quot;Hello &quot; + Instant.now() + &quot;!&quot;; &#125; &#125;)).delayElements(Duration.ofSeconds(1)).take(5); &#125;&#125; 编写 src/main/resources/graphql/schema.graphqls 配置文件： 123456type Query &#123; greeting: String&#125;type Subscription &#123; greetings: String&#125; 编写 application.yaml 配置文件： 12345678910server: port: 8080spring: application: name: gql graphql: graphiql: enabled: true websocket: path: /graphql 启动程序，然后访问 http://localhost:8080/graphiql 即可看到调试控制台，输入如下内容即可完成测试。 123subscription &#123; greetings&#125; 还可以按照如下样例编写单元测试 12345678910111213141516171819202122232425262728293031323334353637import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.graphql.GraphQlTest;import org.springframework.graphql.test.tester.GraphQlTester;import reactor.core.publisher.Flux;import reactor.test.StepVerifier;@GraphQlTest(SampleController.class)public class WebFluxWebSocketSampleTests &#123; @Autowired private GraphQlTester graphQlTester; @Test void greetingMono() &#123; this.graphQlTester.document(&quot;&#123;greeting&#125;&quot;) .execute() .path(&quot;greeting&quot;) .entity(String.class) .isEqualTo(&quot;Hello world!&quot;); &#125; @Test void subscriptionWithResponse() &#123; Flux&lt;GraphQlTester.Response&gt; result = this.graphQlTester.document(&quot;subscription &#123; greetings &#125;&quot;) .executeSubscription() .toFlux(); StepVerifier.create(result) .consumeNextWith(response -&gt; response.path(&quot;greetings&quot;).hasValue()) .consumeNextWith(response -&gt; response.path(&quot;greetings&quot;).hasValue()) .consumeNextWith(response -&gt; response.path(&quot;greetings&quot;).hasValue()) .expectNextCount(2) .verifyComplete(); &#125;&#125; 在页面中可以按照如下样例编写读取程序 src/main/resources/static/index.html： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;GraphQL over WebSocket&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;https://unpkg.com/graphql-ws/umd/graphql-ws.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;Check the console for subscription messages.&lt;/p&gt;&lt;script type=&quot;text/javascript&quot;&gt; const client = graphqlWs.createClient(&#123; url: &#x27;ws://localhost:8080/graphql&#x27;, &#125;); // query (async () =&gt; &#123; const result = await new Promise((resolve, reject) =&gt; &#123; let result; client.subscribe( &#123; query: &#x27;&#123; greeting &#125;&#x27;, &#125;, &#123; next: (data) =&gt; (result = data), error: reject, complete: () =&gt; resolve(result), &#125;, ); &#125;); console.log(&quot;Query result: &quot; + result); &#125;)(); // subscription (async () =&gt; &#123; const onNext = (data) =&gt; &#123; console.log(&quot;Subscription data:&quot;, data); &#125;; await new Promise((resolve, reject) =&gt; &#123; client.subscribe( &#123; query: &#x27;subscription &#123; greetings &#125;&#x27;, &#125;, &#123; next: onNext, error: reject, complete: resolve, &#125;, ); &#125;); &#125;)();&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; RSocket 首先需要切换 Web 至 WebFlux，并引入 RSocket 样例如下： 1234567891011dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-graphql&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-rsocket&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testImplementation &#x27;io.projectreactor:reactor-test&#x27; testImplementation &#x27;org.springframework.graphql:spring-graphql-test&#x27;&#125; 编写 Record 类： 12public record Message(String name, String content) &#123;&#125; 编写 Controller： 12345678910111213141516171819202122232425262728import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.stereotype.Controller;import reactor.core.publisher.Flux;import java.time.Duration;import java.time.Instant;import java.util.function.Supplier;import java.util.stream.Stream;@Controllerpublic class SampleController &#123; @MessageMapping(&quot;graphql&quot;) public String greeting() &#123; return &quot;Hello world!&quot;; &#125; @MessageMapping(&quot;graphql&quot;) public Flux&lt;String&gt; greetings() &#123; return Flux.fromStream(Stream.generate(new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; return &quot;Hello &quot; + Instant.now() + &quot;!&quot;; &#125; &#125;)).delayElements(Duration.ofSeconds(1)).take(5); &#125;&#125; 编写 src/main/resources/graphql/schema.graphqls 配置文件： 123456type Query &#123; greeting: String&#125;type Subscription &#123; greetings: String&#125; 编写 src/main/resources/application.yaml 配置文件 12345678server: port: 8080spring: rsocket: server: port: 7000 mapping-path: /rsocket transport: websocket 参照 RSocket 文档使用如下命令即可完成测试： 12java -jar rsc.jar --request --route=graphql --dataMimeType=&quot;application/graphql+json&quot; --data &#x27;&#123;&quot;query&quot;: &quot;query &#123;\\n greeting\\n&#125;&quot;&#125;&#x27; --debug ws://localhost:7000/rsocketjava -jar rsc.jar --stream --route=graphql --dataMimeType=&quot;application/graphql+json&quot; --data=&#x27;&#123;&quot;subscription&quot;: &quot;subscription &#123; greetings &#123; greeting &#125; &#125;&quot;&#125;&#x27; --debug ws://localhost:7000/rsocket 或者使用 RSocket Requests In HTTP Client： 123456789### queryGRAPHQL rsocketws://localhost:8080/rsocket/graphqlquery &#123;greeting&#125;### subGRAPHQL rsocketws://localhost:8080/rsocket/graphqlsubscription &#123; greetings &#125; IDEA 插件 在 IDEA 插件中可以找到 GraphQL 插件，此插件可以完成一些代码提示和运行测试的功能。 在安装完成后可以编写如下配置文件 graphql.config.yaml： 12345678910schema: schema.graphqlsdocuments: &#x27;**/*.graphql&#x27;exclude: &#x27;src/**/__tests__/**&#x27;include: src/**extensions: endpoints: default: url: http://localhost:8080/graphql headers: Authorization: Bearer $&#123;TOKEN&#125; 单元测试 可以编写如下样例进行单元测试： 12345678910111213141516171819202122232425262728293031323334import jakarta.annotation.Resource;import org.junit.jupiter.api.Test;import org.springframework.boot.test.autoconfigure.graphql.tester.AutoConfigureGraphQlTester;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;import org.springframework.context.annotation.Import;import org.springframework.graphql.test.tester.GraphQlTester;import org.springframework.security.test.context.support.WithMockUser;@Import(&#123;AuthorRepository.class&#125;)@DataJpaTest@AutoConfigureGraphQlTester@AutoConfigureMockMvcclass TestControllerTest &#123; @Resource private GraphQlTester graphQlTester; @Test @WithMockUser(username = &quot;test&quot;, roles = &quot;USER&quot;) void testFindAll() &#123; // language=GraphQL String document = &quot;&quot;&quot; query &#123; authors &#123; id name &#125; &#125; &quot;&quot;&quot;; graphQlTester.document(document).execute().path(&quot;authors&quot;).entityList(Author.class).hasSize(2); &#125;&#125; 与 Spring Security 集成 注：此处样例默认使用 JWT ，如需详细代码请参照 Spring Security 文档。需要值得注意的是，在配置完成后 graphqil 就不能正常使用了，我尝试将相应链接进行开放但还是在发送请求时遇到了 js 相关的问题，且 graphiql 无法访问 IntrospectionQuery 也就没有了代码提示等功能，但是 Postman 是可以正常工作的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import com.nimbusds.jose.jwk.JWK;import com.nimbusds.jose.jwk.JWKSet;import com.nimbusds.jose.jwk.RSAKey;import com.nimbusds.jose.jwk.source.ImmutableJWKSet;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;import org.springframework.security.config.http.SessionCreationPolicy;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.jwt.JwtDecoder;import org.springframework.security.oauth2.jwt.JwtEncoder;import org.springframework.security.oauth2.jwt.NimbusJwtDecoder;import org.springframework.security.oauth2.jwt.NimbusJwtEncoder;import org.springframework.security.oauth2.server.resource.web.BearerTokenAuthenticationEntryPoint;import org.springframework.security.oauth2.server.resource.web.access.BearerTokenAccessDeniedHandler;import org.springframework.security.web.SecurityFilterChain;import java.security.interfaces.RSAPrivateKey;import java.security.interfaces.RSAPublicKey;import static jakarta.servlet.DispatcherType.ERROR;import static jakarta.servlet.DispatcherType.FORWARD;@Configuration@EnableWebSecurity@EnableMethodSecurity(securedEnabled = true)public class SecurityConfig &#123; @Value(&quot;$&#123;jwt.public.key&#125;&quot;) RSAPublicKey publicKey; @Value(&quot;$&#123;jwt.private.key&#125;&quot;) RSAPrivateKey privateKey; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize .dispatcherTypeMatchers(FORWARD, ERROR).permitAll() .anyRequest().authenticated() ) .csrf(AbstractHttpConfigurer::disable) .sessionManagement((session) -&gt; session.sessionCreationPolicy(SessionCreationPolicy.STATELESS)) .oauth2ResourceServer(oauth2 -&gt; oauth2 .jwt(jwt -&gt; jwt .decoder(jwtDecoder()) ) ) .exceptionHandling((exceptions) -&gt; exceptions.authenticationEntryPoint(new BearerTokenAuthenticationEntryPoint()) .accessDeniedHandler(new BearerTokenAccessDeniedHandler()) ); return http.build(); &#125; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; @Bean JwtDecoder jwtDecoder() &#123; return NimbusJwtDecoder.withPublicKey(this.publicKey).build(); &#125; @Bean JwtEncoder jwtEncoder() &#123; JWK jwk = new RSAKey.Builder(this.publicKey).privateKey(this.privateKey).build(); return new NimbusJwtEncoder(new ImmutableJWKSet&lt;&gt;(new JWKSet(jwk))); &#125;&#125; 然后在方法上添加如下注解： 12345678910111213141516171819202122232425262728293031import jakarta.annotation.Resource;import org.springframework.graphql.data.method.annotation.Argument;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.security.access.annotation.Secured;import org.springframework.security.access.prepost.PreAuthorize;import org.springframework.stereotype.Controller;import java.util.List;@Controllerpublic class TestController &#123; @Resource private AuthorRepository authorRepository; @Resource private UserServiceImpl userService; @Secured(&quot;SCOPE_ROLE_USER&quot;) @QueryMapping List&lt;Author&gt; authors() &#123; return authorRepository.findAll(); &#125; @PreAuthorize(&quot;permitAll()&quot;) @QueryMapping String login(@Argument String username, @Argument String password) &#123; return userService.login(username, password); &#125;&#125; 参考资料 Does Your API Need A REST? Check Out GraphQL GraphQL 样例程序 Introduction to Spring GraphQL with Spring Boot Spring Fro GraphQL 官方文档 GraphQL 官方文档 官方 WebSocket 例程 GrpahQL 的网关- Fedration Apollo Federation 文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Spring Data JPA","slug":"Spring-Data-JPA","permalink":"https://wangqian0306.github.io/tags/Spring-Data-JPA/"}]},{"title":"Spring Boot SSE","slug":"spring/sse","date":"2023-05-04T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/sse/","permalink":"https://wangqian0306.github.io/2023/sse/","excerpt":"","text":"Spring Boot SSE 简介 Server-Sent Events (SSE) 是一种服务器推送技术，使客户端能够通过HTTP连接从服务器接收自动更新。 实现方式 编写 Message 类： 12345678910import lombok.Data;@Datapublic class Message &#123; private String userId; private String message;&#125; 编写 EventHandler： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import lombok.extern.slf4j.Slf4j;import org.springframework.http.MediaType;import org.springframework.stereotype.Component;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter.SseEventBuilder;import java.io.IOException;import java.util.concurrent.ConcurrentHashMap;@Slf4j@Componentpublic class EventHandler &#123; private static final long DEFAULT_TIMEOUT = 60 * 60 * 1000L; private final ConcurrentHashMap&lt;String, SseEmitter&gt; emitterConcurrentHashMap = new ConcurrentHashMap&lt;&gt;(); public SseEmitter registerClient(String userId) &#123; if (emitterConcurrentHashMap.containsKey(userId)) &#123; return emitterConcurrentHashMap.get(userId); &#125; else &#123; SseEmitter emitter = new SseEmitter(DEFAULT_TIMEOUT); emitter.onCompletion(() -&gt; emitterConcurrentHashMap.remove(userId)); emitter.onError((err) -&gt; removeAndLogError(userId)); emitter.onTimeout(() -&gt; removeAndLogError(userId)); return emitterConcurrentHashMap.put(userId, emitter); &#125; &#125; private void removeAndLogError(String userId) &#123; log.info(&quot;Error during communication. Unregister client &#123;&#125;&quot;, userId); emitterConcurrentHashMap.remove(userId); &#125; public void broadcastMessage(Message message) &#123; emitterConcurrentHashMap.forEach((k, v) -&gt; sendMessage(k, message)); &#125; private void sendMessage(String userId, Message message) &#123; var sseEmitter = emitterConcurrentHashMap.get(userId); try &#123; log.info(&quot;Notify client &quot; + userId + &quot; &quot; + message.toString()); SseEventBuilder eventBuilder = SseEmitter.event().data(message, MediaType.APPLICATION_JSON).name(&quot;chat&quot;); sseEmitter.send(eventBuilder); &#125; catch (IOException e) &#123; sseEmitter.completeWithError(e); &#125; &#125;&#125; 编写接口 SSEController： 123456789101112131415161718192021import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.*;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;@RestControllerpublic class SSEController &#123; @Resource private EventHandler eventHandler; @PostMapping(&quot;/message&quot;) public void sendMessage(@RequestBody Message message) &#123; eventHandler.broadcastMessage(message); &#125; @GetMapping(&quot;/connect&quot;) public SseEmitter connect(@RequestParam(defaultValue = &quot;admin&quot;) String userId) &#123; return eventHandler.registerClient(userId); &#125;&#125; 使用 IDEA Httpclient 检测 123456789101112### SSEGET http://localhost:8080/connectAccept: text/event-stream### MessagePOST http://localhost:8080/messageContent-Type: application/json&#123; &quot;userId&quot;: &quot;admin&quot;, &quot;message&quot;: &quot;Hello&quot;&#125; 使用前端代码检测 编写 src/main/resources/static/chat.js 前端业务逻辑： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&quot;use strict&quot;;async function postData(url, data) &#123; const response = await fetch(url, &#123; method: &#x27;POST&#x27;, mode: &#x27;cors&#x27;, cache: &#x27;no-cache&#x27;, credentials: &#x27;same-origin&#x27;, headers: &#123; &#x27;Content-Type&#x27;: &#x27;application/json&#x27; &#125;, redirect: &#x27;follow&#x27;, referrerPolicy: &#x27;no-referrer&#x27;, body: JSON.stringify(data) &#125;); return response;&#125;function send() &#123; const input = document.getElementById(&#x27;messageInput&#x27;).value; console.error(window.assignedName); postData(&#x27;/message&#x27;,&#123; message: input, userId: window.assignedName&#125;);&#125;function handleChatEvent(eventData) &#123; const userNameNode = document.createElement(&#x27;span&#x27;); userNameNode.innerHTML = eventData.userId + &#x27;: &#x27;; const li = document.createElement(&quot;li&quot;); li.appendChild(userNameNode); li.appendChild(document.createTextNode(eventData.message)); const ul = document.getElementById(&quot;list&quot;); ul.appendChild(li);&#125;function registerSSE(url) &#123; const source = new EventSource(url); source.addEventListener(&#x27;chat&#x27;, event =&gt; &#123; console.error(&quot;ininini&quot;); handleChatEvent(JSON.parse(event.data)); &#125;) source.onopen = event =&gt; console.log(&quot;Connection opened&quot;); source.onerror = event =&gt; console.error(&quot;Connection error&quot;); source.onmessage = event =&gt; console.error(&quot;ininini&quot;); return source;&#125;function connect() &#123; let url = &quot;/connect?userId=&quot; + document.getElementById(&#x27;username&#x27;).value document.getElementById(&#x27;connect&#x27;).hidden = true; window.assignedName = document.getElementById(&#x27;username&#x27;).value; window.eventSource = registerSSE(url);&#125; 编写页面 src/main/resources/static/index.html: 1234567891011121314151617181920212223242526&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;center&quot;&gt; &lt;input class=&quot;text&quot; id=&quot;username&quot; placeholder=&quot;&quot;&gt; &lt;button id=&quot;connect&quot; type=&quot;submit&quot; value=&quot;Connect&quot; onclick=&quot;connect()&quot;&gt;Connect&lt;/button&gt;&lt;/div&gt;&lt;div&gt; &lt;input class=&quot;text&quot; id=&quot;messageInput&quot; placeholder=&quot;&quot;&gt; &lt;button type=&quot;submit&quot; value=&quot;Send&quot; onclick=&quot;send()&quot;&gt;Send Message&lt;/button&gt; &lt;ul id=&quot;list&quot;&gt;&lt;/ul&gt;&lt;/div&gt;&lt;script src=&quot;chat.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 常见问题 链接超时 由于是长连接所以可能出现链接超时的情况，例如在 k8s 上使用 nginx-ingress-controller 就可能遇到，建议参照 ingress 文档进行配置。 123456789101112131415161718192021222324apiVersion: networking.k8s.io/v1kind: Ingressmetadata: labels: app: &lt;name&gt; name: &lt;name&gt; namespace: &lt;namespace&gt; annotations: nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;3600&quot; nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;3600&quot; nginx.ingress.kubernetes.io/connection-proxy-header: &quot;keep-alive&quot;spec: ingressClassName: nginx rules: - host: &lt;host&gt; http: paths: - backend: service: name: &lt;service_name&gt; port: number: &lt;service_port&gt; path: &lt;path&gt; pathType: Prefix Nginx 代理配置 如果需要 Nginx 代理则可以进行如下配置： 1234567891011121314151617181920212223server &#123; listen 80; server_name rehab.rainbowfish.com.cn; client_max_body_size 5M; location &lt;path&gt; &#123; # 代理到你的后端服务器 proxy_pass http://&lt;domain&gt;&lt;path&gt;; # 保持连接 proxy_http_version 1.1; proxy_set_header Connection &#x27;&#x27;; proxy_buffering off; # 添加适当的超时设置 proxy_connect_timeout 60s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; gzip off; &#125;&#125; 参考资料 Server-Sent Events in Spring demo-chat-app-sse-spring-boot","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spring Statemachine","slug":"spring/statemachine","date":"2023-05-04T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/statemachine/","permalink":"https://wangqian0306.github.io/2023/statemachine/","excerpt":"","text":"Spring Statemachine 简介 Spring Statemachine (SSM) 是一个允许应用程序开发人员在 Spring 应用程序中使用传统状态机概念的框架。 注：状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学计算模型。 使用方式 首先需要在 build.gradle 中引入 starter： 123dependencies &#123; compileOnly &#x27;org.springframework.statemachine:spring-statemachine-starter:4.0.0&#x27;&#125; 然后创建以下枚举类： 123public enum StatesEnum &#123; SI, S1, S2&#125; 123public enum EventsEnum &#123; E1, E2&#125; 创建以下任务配置类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.statemachine.config.EnableStateMachine;import org.springframework.statemachine.config.EnumStateMachineConfigurerAdapter;import org.springframework.statemachine.config.builders.StateMachineConfigurationConfigurer;import org.springframework.statemachine.config.builders.StateMachineStateConfigurer;import org.springframework.statemachine.config.builders.StateMachineTransitionConfigurer;import org.springframework.statemachine.listener.StateMachineListener;import org.springframework.statemachine.listener.StateMachineListenerAdapter;import org.springframework.statemachine.state.State;import java.util.EnumSet;@Configuration@EnableStateMachinepublic class StateMachineConfig extends EnumStateMachineConfigurerAdapter&lt;StatesEnum, EventsEnum&gt; &#123; @Override public void configure(StateMachineConfigurationConfigurer&lt;StatesEnum, EventsEnum&gt; config) throws Exception &#123; config .withConfiguration() .autoStartup(true) .listener(listener()); &#125; @Override public void configure(StateMachineStateConfigurer&lt;StatesEnum, EventsEnum&gt; states) throws Exception &#123; states .withStates() .initial(StatesEnum.SI) .states(EnumSet.allOf(StatesEnum.class)); &#125; @Override public void configure(StateMachineTransitionConfigurer&lt;StatesEnum, EventsEnum&gt; transitions) throws Exception &#123; transitions .withExternal() .source(StatesEnum.SI).target(StatesEnum.S1).event(EventsEnum.E1) .and() .withExternal() .source(StatesEnum.S1).target(StatesEnum.S2).event(EventsEnum.E2); &#125; @Bean public StateMachineListener&lt;StatesEnum, EventsEnum&gt; listener() &#123; return new StateMachineListenerAdapter&lt;&gt;() &#123; @Override public void stateChanged(State&lt;StatesEnum, EventsEnum&gt; from, State&lt;StatesEnum, EventsEnum&gt; to) &#123; System.out.println(&quot;State change to &quot; + to.getId()); &#125; &#125;; &#125;&#125; 创建如下测试类： 123456789101112131415161718192021222324252627282930import jakarta.annotation.Resource;import org.springframework.messaging.Message;import org.springframework.messaging.support.MessageBuilder;import org.springframework.statemachine.StateMachine;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import reactor.core.publisher.Mono;@RestController@RequestMapping(&quot;/demo&quot;)public class TestController &#123; @Resource private StateMachine&lt;StatesEnum, EventsEnum&gt; stateMachine; @GetMapping public String test() &#123; Message&lt;EventsEnum&gt; eventMessage1 = MessageBuilder .withPayload(EventsEnum.E1) .build(); stateMachine.sendEvent(Mono.just(eventMessage1)).subscribe(); Message&lt;EventsEnum&gt; eventMessage2 = MessageBuilder .withPayload(EventsEnum.E2) .build(); stateMachine.sendEvent(Mono.just(eventMessage2)).subscribe(); return &quot;success&quot;; &#125;&#125; 之后启动程序，访问 http://localhost:8080/demo 即可获取到输出。 参考资料 官方文档 【IT老齐476】十分钟上手Spring 状态机","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"ACME","slug":"tools/acme","date":"2023-04-28T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/acme/","permalink":"https://wangqian0306.github.io/2023/acme/","excerpt":"","text":"ACME 简介 Automatic Certificate Management Environment (ACME) 协议是一种通信协议，用于自动化生成和续期 SSL 证书。 安装 使用如下命令即可完成安装： 1curl https://get.acme.sh | sh -s email=&lt;email&gt; 签发证书(独立签发) 如果没有 nginx 服务器则使用如下命令： 12yum install -y socatacme.sh --issue -d &lt;domain&gt; --standalone 签发证书(Nginx) 先编写基础的服务配置文件： 12345678server &#123; listen 80; server_name xxx.xxx.xxx.xxx; location /.well-known/acme-challenge/ &#123; root /usr/share/nginx/xxx; &#125;&#125; 然后测试配置文件并启动 Nginx: 12nginx -tsystemctl enable nginx --now 然后再使用如下语句即可签发证书： 1acme.sh --issue -d &lt;domain&gt; --nginx 安装证书 签发完成后可以修改 Nginx 配置文件如下： 1234567891011121314151617181920212223242526server &#123; listen 80; server_name xxx.xxx.xxx; location /.well-known/acme-challenge/ &#123; root /usr/share/nginx/xxx; &#125; return 301 https://$host$request_uri;&#125;server &#123; listen 443 ssl; server_name xxx.xxx.xxx; client_max_body_size 5M; ssl_certificate /etc/nginx/ssl/xxx-cert.pem; ssl_certificate_key /etc/nginx/ssl/xxx-key.pem; location / &#123; proxy_pass http://xxx.xxx.xxx.xxx; &#125; location /static &#123; alias /usr/share/nginx/xxx-static; &#125;&#125; 然后使用如下命令，将签发完成的证书转存至 Nginx 配置目录中去： 注：此外 acme 还会启动定时任务，自动刷新续期。 1234acme.sh --install-cert -d xxx.xxx.xxx \\--key-file /path/to/keyfile/in/nginx/key.pem \\--fullchain-file /path/to/fullchain/nginx/cert.pem \\--reloadcmd &quot;service nginx force-reload&quot; 其他指令 查看证书相关信息： 1acme.sh --info -d &lt;domain&gt; 查看当前的域名清单： 1acme.sh --list 手动强制刷新域名有效期： 1acme.sh --renew -d &lt;domain&gt; --force 参考资料 维基百科 acme.sh 官方项目","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"ACME","slug":"ACME","permalink":"https://wangqian0306.github.io/tags/ACME/"}]},{"title":"OpenSSL","slug":"linux/openssl","date":"2023-04-25T15:52:33.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/openssl/","permalink":"https://wangqian0306.github.io/2023/openssl/","excerpt":"","text":"OpenSSL 简介 OpenSSL 是一个强大的、商业级的、功能齐全的通用加密和安全通信工具包。 安装及使用 目前 CentOS 应该默认安装了 OpenSSL 如果没有可以使用如下命令安装： 1yum -y install openssl openssl-devel 注：实际使用中 Let’s Encrypt 的情况更多些。 可以使用如下命令生成相关证书： 生成 Private key 和 CSR 1openssl req -nodes -newkey rsa:2048 -keyout custom.key -out custom.csr 生成 Pem 证书 1openssl req -x509 -sha512 -nodes -days 730 -newkey rsa:2048 -keyout custom.key -out custom.pem 验证 CSR 1openssl req -noout -text -in custom.csr 验证私钥 1openssl rsa -in private.key -check 检查 Pem 有效期 1openssl x509 -in custom.pem -noout -issuer -issuer_hash 参考资料 官方文档 官方项目 使用OpenSSL生成SSL证书的教程","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"OpenSSL","slug":"OpenSSL","permalink":"https://wangqian0306.github.io/tags/OpenSSL/"}]},{"title":"Hibernate 常用功能记录","slug":"java/hibernate","date":"2023-04-20T15:09:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/hibernate/","permalink":"https://wangqian0306.github.io/2023/hibernate/","excerpt":"","text":"Hibernate 常用功能记录 简介 Hibernate 是一个开源的对象关系映射框架，它对 JDBC 的操作数据库的过程进行封装，使得开发者只需要关注业务逻辑，而不需要关注底层的数据库操作。Hibernate 有着良好的性能，支持多种数据库，同时也支持多种编程语言。 常用功能 关联(Associations) @ManyToOne 用户类 1234567891011121314import jakarta.persistence.*;import lombok.Getter;import lombok.Setter;@Getter@Setter@Entity(name = &quot;Person&quot;)public class Person &#123; @Id @GeneratedValue private Long id;&#125; 联系方式类 1234567891011121314151617181920import jakarta.persistence.*;import lombok.Getter;import lombok.Setter;@Getter@Setter@Entity(name = &quot;Phone&quot;)public class Phone &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String number; @ManyToOne(optional = false) @JoinColumn(name = &quot;person_id&quot;, nullable = false, foreignKey = @ForeignKey(name = &quot;PERSON_ID_FK&quot;)) private Person person;&#125; @OneToMany 用户类 1234567891011@Entity(name = &quot;Person&quot;)public class Person &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; @OneToMany(mappedBy = &quot;person&quot;,cascade = CascadeType.ALL, orphanRemoval = true) private List&lt;Phone&gt; phones = new ArrayList&lt;&gt;();&#125; 123456789101112131415161718192021222324252627282930@Entity(name = &quot;Phone&quot;)public static class Phone &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; private String number; @ManyToOne private Person person; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Phone phone = (Phone) o; return Objects.equals(number, phone.number); &#125; @Override public int hashCode() &#123; return Objects.hash(number); &#125;&#125; @OneToOne 联系方式表： 123456789101112131415@Entity(name = &quot;Phone&quot;)public class Phone &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; @Column(name = &quot;number&quot;) private String number; @OneToOne @JoinColumn(name = &quot;details_id&quot;) private PhoneDetails details;&#125; 联系方式详情表： 123456789101112@Entity(name = &quot;PhoneDetails&quot;)public class PhoneDetails &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; private String provider; private String technology;&#125; @ManyToMany 用户表： 1234567891011121314151617181920212223242526272829303132333435363738394041@Entity(name = &quot;Person&quot;)public class Person &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; @NaturalId private String registrationNumber; @ManyToMany(cascade = &#123;CascadeType.PERSIST, CascadeType.MERGE&#125;) private List&lt;Address&gt; addresses = new ArrayList&lt;&gt;(); public void addAddress(Address address) &#123; addresses.add(address); address.getOwners().add(this); &#125; public void removeAddress(Address address) &#123; addresses.remove(address); address.getOwners().remove(this); &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Person person = (Person) o; return Objects.equals(registrationNumber, person.registrationNumber); &#125; @Override public int hashCode() &#123; return Objects.hash(registrationNumber); &#125; &#125; 地址表： 123456789101112131415161718192021222324252627282930313233343536@Entity(name = &quot;Address&quot;)public class Address &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; private String street; private String number; private String postalCode; @ManyToMany(mappedBy = &quot;addresses&quot;) private List&lt;Person&gt; owners = new ArrayList&lt;&gt;(); @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Address address = (Address) o; return Objects.equals(street, address.street) &amp;&amp; Objects.equals(number, address.number) &amp;&amp; Objects.equals(postalCode, address.postalCode); &#125; @Override public int hashCode() &#123; return Objects.hash(street, number, postalCode); &#125;&#125; @NotFound 联系方式类： 12345678910111213141516@Entity(name = &quot;Phone&quot;)public class Phone &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; @Column(name = &quot;number&quot;) private String number; @OneToOne @NotFound(action = NotFoundAction.EXCEPTION) @JoinColumn(name = &quot;details_id&quot;) private PhoneDetails details;&#125; 联系方式详情类： 123456789101112@Entity(name = &quot;PhoneDetails&quot;)public class PhoneDetails &#123; @Id @GeneratedValue(strategy= GenerationType.IDENTITY) private Long id; private String provider; private String technology;&#125; @Any 注：略，不清楚有什么用。 @JoinFormula 注：略，不清楚有什么用。 @JoinColumnOrFormula 注：略，不清楚有什么用。 参考资料 官方文档","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"WebClient 和 RestClient","slug":"spring/webclient","date":"2023-04-14T14:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/webclient-restclient/","permalink":"https://wangqian0306.github.io/2023/webclient-restclient/","excerpt":"","text":"WebClient 和 RestClient 简介 1Fourteen years ago, when RestTemplate was introduced in Spring Framework 3.0, we quickly discovered that exposing every capability of HTTP in a template-like class resulted in too many overloaded methods. 十四年前，当在 Spring Framework 3.0 中引入时 RestTemplate，我们很快发现，在类似模板的类中公开HTTP的所有功能会导致太多的重载方法。 所以在 Spring Framework 中实现了如下两种客户端，用来执行 Http 请求： WebClient：异步客户端 RestClient：同步客户端 WebClient 实现 引入 WebFlux 包： 1234dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-webflux&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27;&#125; 编写请求结果类： 12public record JokeResponse(String id, String joke, Integer status) &#123;&#125; 编写请求类： 12345678import org.springframework.web.service.annotation.GetExchange;public interface JokeClient &#123; @GetExchange(&quot;/&quot;) JokeResponse random();&#125; 编写主类和 Bean： 12345678910111213141516171819202122232425import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.web.reactive.function.client.WebClient;import org.springframework.web.reactive.function.client.support.WebClientAdapter;import org.springframework.web.service.invoker.HttpServiceProxyFactory;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean JokeClient jokeClient(WebClient.Builder builder) &#123; WebClient client = builder .baseUrl(&quot;https://icanhazdadjoke.com/&quot;) .defaultHeader(&quot;Accept&quot;, &quot;application/json&quot;) .build(); HttpServiceProxyFactory factory = HttpServiceProxyFactory.builderFor(WebClientAdapter.create(client)).build(); return factory.createClient(JokeClient.class); &#125;&#125; 编写测试接口： 12345678910111213141516171819import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMappingpublic class TestController &#123; @Resource JokeClient jokeClient; @GetMapping(&quot;/&quot;) public String test() &#123; JokeResponse response = jokeClient.random(); return response.joke(); &#125;&#125; 更换支持库(可选)： 1234567891011121314@Beanpublic JettyResourceFactory resourceFactory() &#123; return new JettyResourceFactory();&#125;@Beanpublic WebClient webClient() &#123; HttpClient httpClient = new HttpClient(); ClientHttpConnector connector = new JettyClientHttpConnector(httpClient, resourceFactory()); return WebClient.builder().clientConnector(connector).build(); &#125; 注：每种库的日志需要单独调节，支持库的清单参阅官方文档。 配置代理(可选)： 1234567891011121314151617181920212223import org.springframework.context.annotation.Bean;import org.springframework.http.client.reactive.ReactorClientHttpConnector;import org.springframework.web.reactive.function.client.WebClient;import org.springframework.web.reactive.function.client.support.WebClientAdapter;import org.springframework.web.service.invoker.HttpServiceProxyFactory;import reactor.netty.http.client.HttpClient;import reactor.netty.transport.ProxyProvider;@BeanJokeClient jokeClient(WebClient.Builder builder) &#123; HttpClient httpClient = HttpClient.create() .proxy(proxy -&gt; proxy .type(ProxyProvider.Proxy.HTTP) .host(&quot;localhost&quot;) .port(7890)); WebClient client = builder .clientConnector(new ReactorClientHttpConnector(httpClient)) .baseUrl(&quot;https://icanhazdadjoke.com/&quot;) .defaultHeader(&quot;Accept&quot;, &quot;application/json&quot;) .build(); HttpServiceProxyFactory factory = HttpServiceProxyFactory.builderFor(WebClientAdapter.create(client)).build(); return factory.createClient(JokeClient.class);&#125; RestClient 实现 引入 Web 包： 123dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27;&#125; 编写请求结果类： 12public record JokeResponse(String id, String joke, Integer status) &#123;&#125; 编写请求类： 12345678import org.springframework.web.service.annotation.GetExchange;public interface JokeClient &#123; @GetExchange(&quot;/&quot;) JokeResponse random();&#125; 编写主类和 Bean： 12345678910111213141516171819202122232425import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestClient;import org.springframework.web.client.support.RestClientAdapter;import org.springframework.web.service.invoker.HttpServiceProxyFactory;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean JokeClient jokeClient(RestClient.Builder builder) &#123; RestClient client = builder .baseUrl(&quot;https://icanhazdadjoke.com/&quot;) .defaultHeader(&quot;Accept&quot;, &quot;application/json&quot;) .build(); HttpServiceProxyFactory factory = HttpServiceProxyFactory.builderFor(RestClientAdapter.create(client)).build(); return factory.createClient(JokeClient.class); &#125;&#125; 编写测试： 12345678910111213141516171819import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMappingpublic class TestController &#123; @Resource JokeClient jokeClient; @GetMapping(&quot;/&quot;) public String test() &#123; JokeResponse response = jokeClient.random(); return response.joke(); &#125;&#125; 更换支持库(可选)： 1234567891011121314151617181920212223import org.springframework.boot.web.client.RestClientCustomizer;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.http.client.JettyClientHttpRequestFactory;import org.springframework.web.client.RestClient;@Configurationpublic class CustomRestClientConf &#123; @Bean public RestClient restClient(RestClient.Builder builder) &#123; return builder.build(); &#125; @Bean public RestClientCustomizer restClientCustomizer() &#123; JettyClientHttpRequestFactory requestFactory = new JettyClientHttpRequestFactory(); return (restClientBuilder) -&gt; restClientBuilder .requestFactory(requestFactory) .baseUrl(&quot;http://localhost:8080/&quot;); &#125;&#125; 注：每种库的日志需要单独调节，支持库的清单参阅官方文档。 设置代理(可选)： 12345678910111213141516171819202122import org.springframework.context.annotation.Bean;import org.springframework.http.client.SimpleClientHttpRequestFactory;import org.springframework.web.client.RestClient;import org.springframework.web.client.support.RestClientAdapter;import org.springframework.web.service.invoker.HttpServiceProxyFactory;import java.net.InetSocketAddress;import java.net.Proxy;@BeanJokeClient jokeClient(RestClient.Builder builder) &#123; Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(&quot;localhost&quot;, 7890)); SimpleClientHttpRequestFactory simpleClientHttpRequestFactory = new SimpleClientHttpRequestFactory(); simpleClientHttpRequestFactory.setProxy(proxy); RestClient client = builder .requestFactory(simpleClientHttpRequestFactory) .baseUrl(&quot;https://icanhazdadjoke.com/&quot;) .defaultHeader(&quot;Accept&quot;, &quot;application/json&quot;) .build(); HttpServiceProxyFactory factory = HttpServiceProxyFactory.builderFor(RestClientAdapter.create(client)).build(); return factory.createClient(JokeClient.class);&#125; 常见问题 获取请求状态码 可以使用如下样例获取请求结果对象，然后读取状态码等信息 123456789import org.springframework.http.ResponseEntity;import org.springframework.web.service.annotation.GetExchange;public interface JokeClient &#123; @GetExchange(&quot;/&quot;) ResponseEntity&lt;JokeResponse&gt; random();&#125; Mock 测试 编写如下测试程序即可： 123456789101112131415161718192021222324252627282930313233343536import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.client.RestClientTest;import org.springframework.http.MediaType;import org.springframework.test.web.client.MockRestServiceServer;import static org.assertj.core.api.Assertions.assertThat;import static org.springframework.test.web.client.match.MockRestRequestMatchers.requestTo;import static org.springframework.test.web.client.response.MockRestResponseCreators.withSuccess;@RestClientTest(JokeClient.class)public class JokeTest &#123; @Autowired MockRestServiceServer server; @Autowired JokeClient jokeClient; @Autowired ObjectMapper objectMapper; @Test public void shouldReturnAllPosts() throws JsonProcessingException &#123; JokeResponse jokeResponse = new JokeResponse(&quot;1&quot;, &quot;demo&quot;, 1); this.server .expect(requestTo(&quot;https://icanhazdadjoke.com/&quot;)) .andRespond(withSuccess(objectMapper.writeValueAsString(jokeResponse), MediaType.APPLICATION_JSON)); JokeResponse result = jokeClient.random(); assertThat(result.joke()).isEqualTo(&quot;demo&quot;); &#125;&#125; 拦截器 可以使用拦截器的方式改变请求中的内容样例如下： 1234567891011121314151617181920212223import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.http.HttpRequest;import org.springframework.http.client.ClientHttpRequestExecution;import org.springframework.http.client.ClientHttpRequestInterceptor;import org.springframework.http.client.ClientHttpResponse;import org.springframework.stereotype.Component;import java.io.IOException;@Componentpublic class TokenInterceptor implements ClientHttpRequestInterceptor &#123; private static final Logger log = LoggerFactory.getLogger(TokenInterceptor.class); @Override public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException &#123; log.info(&quot;Intercepting request: &quot; + request.getURI()); request.getHeaders().add(&quot;x-request-id&quot;, &quot;12345&quot;); return execution.execute(request, body); &#125;&#125; 12345678910111213141516171819202122232425import org.springframework.http.client.ClientHttpRequestInterceptor;import org.springframework.stereotype.Component;import org.springframework.web.client.RestClient;@Componentpublic class JokeClient &#123; private final RestClient restClient; public JokeClient(RestClient.Builder builder, ClientHttpRequestInterceptor tokenInterceptor) &#123; this.restClient = builder .baseUrl(&quot;https://icanhazdadjoke.com&quot;) .defaultHeader(&quot;Accept&quot;, &quot;application/json&quot;) .requestInterceptor(tokenInterceptor) .build(); &#125; public JokeResponse random() &#123; return restClient.get() .uri(&quot;/&quot;) .retrieve() .body(JokeResponse.class); &#125;&#125; 处理不同的返回状态码和对象 123456789101112131415161718192021222324import org.springframework.http.HttpStatus;import org.springframework.web.reactive.function.client.ClientResponse;import org.springframework.web.reactive.function.client.WebClient;import reactor.core.publisher.Mono;public class AsyncRequestService &#123; private final WebClient webClient; public AsyncRequestService(WebClient.Builder webClientBuilder) &#123; this.webClient = webClientBuilder.baseUrl(&quot;http://xxx.xxx.xxx&quot;).build(); &#125; public Mono&lt;ClientResponse&gt; asyncFetchDataWithStatus() &#123; return webClient.get() .uri(&quot;/xxx&quot;) .exchangeToMono(response -&gt; &#123; if (response.statusCode().equals(HttpStatus.OK) || response.statusCode().equals(HttpStatus.ACCEPTED)) &#123; return Mono.just(response); &#125; return Mono.error(new RuntimeException(&quot;Failed to fetch data&quot;)); &#125;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import reactor.core.publisher.Mono;@RestControllerpublic class AsyncRequestController &#123; private final AsyncRequestService asyncRequestService; public AsyncRequestController(AsyncRequestService asyncRequestService) &#123; this.asyncRequestService = asyncRequestService; &#125; @GetMapping(&quot;/fetch-data&quot;) public Mono&lt;ResponseEntity&lt;String&gt;&gt; fetchData() &#123; return asyncRequestService.asyncFetchDataWithStatus() .flatMap(response -&gt; &#123; if (response.statusCode().equals(HttpStatus.OK)) &#123; return response.bodyToMono(String.class) .map(body -&gt; ResponseEntity.ok(body)); &#125; else if (response.statusCode().equals(HttpStatus.ACCEPTED)) &#123; return Mono.just(ResponseEntity.status(HttpStatus.ACCEPTED).body(&quot;Processing...&quot;)); &#125; else &#123; return Mono.just(ResponseEntity.status(response.statusCode()).body(&quot;Unhandled status code&quot;)); &#125; &#125;) .onErrorResume(e -&gt; &#123; return Mono.just(ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(&quot;Error occurred&quot;)); &#125;); &#125;&#125; 日志记录 注：使用 SpringBoot Actuator 就可以拿到请求日志，但是问题在于不是很方便读取。之后可以测下 Micrometer 会不会集成。 参考代码 之后在 RestClient 构建的时候配置拦截器就即可。 参考资料 WebClient 文档 WebClient 单元测试样例项目 RestClient 文档 Spring Security 6.4 中 OAuth2 的 RestClient 支持 &quot;Spring Boot REST Client Logging Made Easy","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Github Copilot 技巧梳理","slug":"ai/copilot","date":"2023-04-11T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/copilot/","permalink":"https://wangqian0306.github.io/2023/copilot/","excerpt":"","text":"Github Copilot 技巧梳理 简介 GitHub Copilot 是一款编码辅助工具。 小技巧 问答模式 12# q: How to write bubble sort?# a: https://stackoverflow.com/questions/18262306/quicksort-with-python 根据函数名生成代码 1234import timedef getUnixTimeStamp(): return int(time.time()) 生成测试用例 12345678910111213import osdef get_file_content(filename): if not os.path.exists(filename): raise Exception(&quot;File does not exist: &quot; + filename) with open(filename) as f: return f.read()if __name__ == &#x27;__main__&#x27;: # test get_file_content print(get_file_content(&quot;test.py&quot;)) 使用 JSON 生成类 1234567891011121314151617181920212223242526272829303132333435# create User class from json &#123;&quot;code&quot;: 200, &quot;username&quot;:&quot;test&quot;, &quot;email&quot;:&quot;123@asd.com&quot;, &quot;date&quot;:&quot;2022-04-17&quot;&#125;class User: def __init__(self, code, username, email, date): self.code = code self.username = username self.email = email self.date = date def __repr__(self): return f&quot;User(code=&#123;self.code&#125;, username=&#123;self.username&#125;, email=&#123;self.email&#125;, date=&#123;self.date&#125;)&quot; def __eq__(self, other): return self.code == other.code and self.username == other.username and self.email == other.email and self.date == other.date def __hash__(self): return hash((self.code, self.username, self.email, self.date)) def to_dict(self): return &#123;&quot;code&quot;: self.code, &quot;username&quot;: self.username, &quot;email&quot;: self.email, &quot;date&quot;: self.date&#125; @staticmethod def from_dict(obj): assert isinstance(obj, dict) code = obj.get(&quot;code&quot;) username = obj.get(&quot;username&quot;) email = obj.get(&quot;email&quot;) date = obj.get(&quot;date&quot;) return User(code, username, email, date) def to_json(self): return json.dumps(self.to_dict(), indent=2) @staticmethod def from_json(json_str): return User.from_dict(json.loads(json_str)) 使用建表语句生成代码 1234567891011# create User class from sql# CREATE TABLE User( `id` INT UNSIGNED AUTO_INCREMENT KEY,`username` VARCHAR(20) NOT NULL UNIQUE, `password` CHAR(32) NOT NULL,`email` VARCHAR(50) NOT NULL) CHARSET=UTF8;class User: def __init__(self, id, username, password, email): self.id = id self.username = username self.password = password self.email = email def __repr__(self): return &#x27;&lt;User %r&gt;&#x27; % self.username Copilot Chat Copilot Chat 是 Copilot X 当中的一个子功能，类似于 ChatGPT。目前可以使用 VS Code 访问此功能。 参考资料 官方文档 Github Copilot 使用技巧 Using GitHub Copilot to Automate Tests Github Copilot X","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Github Copilot","slug":"Github-Copilot","permalink":"https://wangqian0306.github.io/tags/Github-Copilot/"}]},{"title":"Spring CORS","slug":"spring/spring-cors","date":"2023-03-27T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2023/spring-cors/","permalink":"https://wangqian0306.github.io/2023/spring-cors/","excerpt":"","text":"Spring CORS 简介 解决跨域可以引入如下代码 配置 1234567891011121314151617181920212223242526272829303132333435import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.core.Ordered;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;@Configurationpublic class WebConfig implements WebMvcConfigurer &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .exposedHeaders(&quot;Access-Control-Allow-Origin&quot;); &#125; @Bean public FilterRegistrationBean&lt;CorsFilter&gt; corsFilterRegistrationBean() &#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration config = new CorsConfiguration(); config.addAllowedOrigin(&quot;*&quot;); config.addAllowedMethod(&quot;*&quot;); config.addAllowedHeader(&quot;*&quot;); source.registerCorsConfiguration(&quot;/**&quot;, config); FilterRegistrationBean&lt;CorsFilter&gt; bean = new FilterRegistrationBean&lt;&gt;(new CorsFilter(source)); bean.setOrder(Ordered.HIGHEST_PRECEDENCE); return bean; &#125; &#125; 如果使用了 SpringSecurity 还需要进行额外的配置，样例如下： 123456789import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;@BeanSecurityWebFilterChain springSecurityFilterChain(ServerHttpSecurity http) &#123; http // ... .cors(AbstractHttpConfigurer::disable); return http.build();&#125; 测试 使用如下命令即可： 1curl -X &#x27;GET&#x27; &#x27;http://localhost:8080/test&#x27; -H &#x27;accept: */*&#x27; -H &#x27;origin:*&#x27; -v 若返回头中包含如下内容则证明配置成功： 1&lt; Access-Control-Allow-Origin: * 注：在请求时必须加入 origin 头，否则不会返回 Access-Control-Allow-Origin","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Next.js","slug":"frount/nextjs","date":"2023-03-17T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2023/next.js/","permalink":"https://wangqian0306.github.io/2023/next.js/","excerpt":"","text":"Next.js 简介 Next.js 是一个用于生产环境的 React 应用框架。Redux 则是一种模式和库，用于管理和更新应用程序状态，使用称为“操作”的事件。它是需要在整个应用程序中使用的状态的集中存储，规则确保状态只能以可预测的方式更新。 创建项目 在安装完 Node 之后就可以使用如下命令快速生成项目 1npx create-next-app@latest 注：剩余配置全部默认即可 MUI 插件 使用如下命令安装即可： 1npm install @mui/material @emotion/react @emotion/styled @mui/material-nextjs @emotion/cache 与 Next.js 带主题集成并进行本地化 主题配置类如下 app/theme.ts： jsx123456789101112131415161718&#x27;use client&#x27;;import &#123; Roboto &#125; from &#x27;next/font/google&#x27;;import &#123; createTheme &#125; from &#x27;@mui/material/styles&#x27;;const roboto = Roboto(&#123; weight: [&#x27;300&#x27;, &#x27;400&#x27;, &#x27;500&#x27;, &#x27;700&#x27;], subsets: [&#x27;latin&#x27;], display: &#x27;swap&#x27;,&#125;);const theme = createTheme(&#123; typography: &#123; fontFamily: roboto.style.fontFamily, &#125;,&#125;);export default theme; 向 layout 文件 app/layout.tsx 中引入主题配置类： jsx12345678910111213141516171819202122232425262728293031import type &#123;Metadata&#125; from &#x27;next&#x27;import &#123;Inter&#125; from &#x27;next/font/google&#x27;import &#x27;./globals.css&#x27;import &#123;ThemeProvider&#125; from &#x27;@mui/material/styles&#x27;;import &#123;AppRouterCacheProvider&#125; from &#x27;@mui/material-nextjs/v14-appRouter&#x27;import theme from &quot;@/app/theme&quot;;const inter = Inter(&#123;subsets: [&#x27;latin&#x27;]&#125;)export const metadata: Metadata = &#123; title: &#x27;Create Next App&#x27;, description: &#x27;Generated by create next app&#x27;,&#125;export default function RootLayout(&#123; children, &#125;: &#123; children: React.ReactNode&#125;) &#123; return ( &lt;html lang=&quot;en&quot;&gt; &lt;body className=&#123;inter.className&#125;&gt; &lt;AppRouterCacheProvider options=&#123;&#123;key: &#x27;css&#x27;, enableCssLayer: true&#125;&#125;&gt; &lt;ThemeProvider theme=&#123;theme&#125;&gt; &#123;children&#125; &lt;/ThemeProvider&gt; &lt;/AppRouterCacheProvider&gt; &lt;/body&gt; &lt;/html&gt; )&#125; 自定义带参数 mui 插件 jsx12345678910111213141516import * as React from &#x27;react&#x27;;import &#123; styled &#125; from &#x27;@mui/material/styles&#x27;;import Slider, &#123; SliderProps &#125; from &#x27;@mui/material/Slider&#x27;;interface StyledSliderProps extends SliderProps &#123; success?: boolean;&#125;const StyledSlider = styled(Slider, &#123; shouldForwardProp: (prop) =&gt; prop !== &#x27;success&#x27;,&#125;)&lt;StyledSliderProps&gt;((&#123; success, theme &#125;) =&gt; (&#123; ...(success &amp;&amp; &#123; // TODO &#125;),&#125;)); 自定义主题参数 jsx12345678910111213declare module &#x27;@mui/material/styles&#x27; &#123; interface Theme &#123; status: &#123; danger: string; &#125;; &#125; // allow configuration using `createTheme` interface ThemeOptions &#123; status?: &#123; danger?: string; &#125;; &#125;&#125; 日期选择器 安装依赖： 1npm install @mui/x-date-pickers dayjs 然后需要在使用日期选择器的使用页面编写如下代码即可 jsx123456789101112131415&#x27;use client&#x27;;import &#123;LocalizationProvider&#125; from &#x27;@mui/x-date-pickers&#x27;;import &#123;AdapterDayjs&#125; from &quot;@mui/x-date-pickers/AdapterDayjs&quot;;import &#123;DatePicker&#125; from &quot;@mui/x-date-pickers&quot;;export default function Home() &#123; return ( &lt;LocalizationProvider dateAdapter=&#123;AdapterDayjs&#125; adapterLocale=&quot;zh-cn&quot;&gt; &lt;main className=&quot;flex min-h-screen flex-col items-center justify-between p-24&quot;&gt; &lt;DatePicker/&gt; &lt;/main&gt; &lt;/LocalizationProvider&gt; );&#125; 复杂表格 需要额外安装插件： 1npm install @mui/x-data-grid 然后即可编辑如下样例表格： jsx123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150&#x27;use client&#x27;import * as React from &quot;react&quot;;import &#123; DataGrid, gridFilteredTopLevelRowCountSelector, gridPageSizeSelector, GridPagination, GridToolbarContainer, GridToolbarExport, GridToolbarColumnsButton, GridToolbarFilterButton, GridToolbarDensitySelector, useGridApiContext, useGridRootProps, useGridSelector&#125; from &quot;@mui/x-data-grid&quot;;import &#123;createFakeServer&#125; from &quot;@mui/x-data-grid-generator&quot;;import MuiPagination from &quot;@mui/material/Pagination&quot;;import &#123;TablePaginationProps&#125; from &quot;@mui/material/TablePagination&quot;;import &#123;Box&#125; from &quot;@mui/material&quot;;import &#123; Input as BaseInput &#125; from &#x27;@mui/base/Input&#x27;;const SERVER_OPTIONS = &#123; useCursorPagination: false,&#125;;const &#123;useQuery, ...data&#125; = createFakeServer(&#123;&#125;, SERVER_OPTIONS);const getPageCount = (rowCount: number, pageSize: number): number =&gt; &#123; if (pageSize &gt; 0 &amp;&amp; rowCount &gt; 0) &#123; return Math.ceil(rowCount / pageSize); &#125; return 0;&#125;;function MyToolbar(props: any) &#123; return ( &lt;Box sx=&#123;&#123; display: &quot;flex&quot;, justifyContent: &quot;space-around&quot;, alignItems: &quot;center&quot;, width: &quot;100%&quot; &#125;&#125;&gt; &lt;BaseInput value=&#123;props.value&#125; onChange=&#123;props.change&#125; placeholder=&#123;&quot;Search by name&quot;&#125; /&gt; &lt;GridToolbarContainer&gt; &lt;GridToolbarColumnsButton/&gt; &lt;GridToolbarFilterButton/&gt; &lt;GridToolbarDensitySelector/&gt; &lt;GridToolbarExport/&gt; &lt;/GridToolbarContainer&gt; &lt;/Box&gt; )&#125;function Pagination(&#123; page, onPageChange, className, &#125;: Pick&lt;TablePaginationProps, &quot;page&quot; | &quot;onPageChange&quot; | &quot;className&quot;&gt;) &#123; const apiRef = useGridApiContext(); const rootProps = useGridRootProps(); const pageSize = useGridSelector(apiRef, gridPageSizeSelector); const visibleTopLevelRowCount = useGridSelector( apiRef, gridFilteredTopLevelRowCountSelector ); const pageCount = getPageCount( rootProps.rowCount ?? visibleTopLevelRowCount, pageSize ); return ( &lt;MuiPagination showFirstButton=&#123;true&#125; showLastButton=&#123;true&#125; color=&quot;primary&quot; className=&#123;className&#125; count=&#123;pageCount&#125; page=&#123;page + 1&#125; onChange=&#123;(event, newPage) =&gt; &#123; onPageChange(event as any, newPage - 1); &#125;&#125; /&gt; );&#125;function CustomPagination(props: any) &#123; return &lt;GridPagination ActionsComponent=&#123;Pagination&#125; &#123;...props&#125; /&gt;;&#125;export default function ServerPaginationGrid() &#123; const [paginationModel, setPaginationModel] = React.useState(&#123; page: 0, pageSize: 5, &#125;); const &#123;isLoading, rows, pageInfo&#125; = useQuery(paginationModel); // Some API clients return undefined while loading // Following lines are here to prevent `rowCountState` from being undefined during the loading const [rowCountState, setRowCountState] = React.useState( pageInfo?.totalRowCount || 0 ); const [searchText, setSearchText] = React.useState(&#x27;&#x27;); const handleSearch = (e: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; &#123; setSearchText(e.target.value); &#125;; React.useEffect(() =&gt; &#123; setRowCountState((prevRowCountState) =&gt; pageInfo?.totalRowCount !== undefined ? pageInfo?.totalRowCount : prevRowCountState ); &#125;, [pageInfo.totalRowCount, setRowCountState]); console.log(rowCountState); return ( &lt;div style=&#123;&#123;height: 400, width: &quot;100%&quot;&#125;&#125;&gt; &lt;DataGrid rows=&#123;rows&#125; &#123;...data&#125; rowCount=&#123;rowCountState&#125; loading=&#123;isLoading&#125; pageSizeOptions=&#123;[5, 10]&#125; paginationModel=&#123;paginationModel&#125; paginationMode=&quot;server&quot; onPaginationModelChange=&#123;setPaginationModel&#125; slots=&#123;&#123; pagination: CustomPagination, toolbar: MyToolbar &#125;&#125; slotProps=&#123;&#123; toolbar: &#123; value: searchText, change: handleSearch, &#125; &#125;&#125; /&gt; &lt;/div&gt; );&#125; 注：表格中开放了很多的插槽，可以修改图标，完善翻页功能等，请参照 官方文档 chakra-ui 插件 使用如下命令安装即可： 1npm i @chakra-ui/react @chakra-ui/next-js @emotion/react @emotion/styled framer-motion 在安装完成后需要编写 src/app/provider.tsx 文件，引入 ui 插件： jsx12345678910111213141516171819202122232425262728293031&#x27;use client&#x27;import &#123; CacheProvider &#125; from &#x27;@chakra-ui/next-js&#x27;import &#123; ChakraProvider &#125; from &#x27;@chakra-ui/react&#x27;import &#123; extendTheme &#125; from &#x27;@chakra-ui/react&#x27;const colors = &#123; brand: &#123; 900: &#x27;#1a365d&#x27;, 800: &#x27;#153e75&#x27;, 700: &#x27;#2a69ac&#x27;, &#125;,&#125;export const theme = extendTheme(&#123; colors &#125;)export function Providers(&#123; children, &#125;: &#123; children: React.ReactNode&#125;) &#123; return ( &lt;CacheProvider&gt; &lt;ChakraProvider theme=&#123;theme&#125;&gt; &#123;children&#125; &lt;/ChakraProvider&gt; &lt;/CacheProvider&gt; )&#125; 然后在 src/app/layout.tsx 文件中引入 Provider： jsx12345678910111213141516171819202122232425import type &#123; Metadata &#125; from &#x27;next&#x27;import &#123; Inter &#125; from &#x27;next/font/google&#x27;import &#123; Providers&#125; from &quot;@/app/provider&quot;;import &#x27;./globals.css&#x27;const inter = Inter(&#123; subsets: [&#x27;latin&#x27;] &#125;)export const metadata: Metadata = &#123; title: &#x27;Create Next App&#x27;, description: &#x27;Generated by create next app&#x27;,&#125;export default function RootLayout(&#123; children,&#125;: &#123; children: React.ReactNode&#125;) &#123; return ( &lt;html lang=&quot;en&quot;&gt; &lt;body className=&#123;inter.className&#125;&gt; &lt;Providers&gt;&#123;children&#125;&lt;/Providers&gt; &lt;/body&gt; &lt;/html&gt; )&#125; chroma-js chroma-js 是一个用于操作颜色的库。其在 typescript 中的安装方式如下： 1npm install @types/chroma-js chroma-js 和后台项目集成 可以参考 Spring boot + nextjs starter kit 视频。 部署 大致的本地部署方式可以划分如下： 作为 Node.js 服务部署 使用 Docker 部署 编译之后使用 Nginx 部署 上面两项差异只在 Docker 上，编译方案则有很多组件是不支持的，所以还是建议使用 Docker 部署。 在 Github 中有 样例部署项目 可以参考。 参考资料 Next.js 官方文档 Next.js 官方示例教程 Spring boot + nextjs starter kit spring-boot-nextjs-starter-kit 源码","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"},{"name":"Node.js","slug":"Node-js","permalink":"https://wangqian0306.github.io/tags/Node-js/"},{"name":"React","slug":"React","permalink":"https://wangqian0306.github.io/tags/React/"}]},{"title":"Redux Toolkit","slug":"frount/redux-toolkit","date":"2023-03-17T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2023/redux-toolkit/","permalink":"https://wangqian0306.github.io/2023/redux-toolkit/","excerpt":"","text":"Redux Toolkit 简介 Redux 是一款针对于 javascript 的可预测状态的容器，而 Redux Toolkit 是为了便于使用而构建的工具包。 使用方式 样例模板 可以使用如下命令创建一个样例模板： 1npx create-next-app --example with-redux my-app 注：默认采用了 Next.js 的 APP Router 但是它和 redux-persist 有兼容性上的问题，所以仅建议作为参考。 手动配置 首先需要安装依赖包： 1npm install @reduxjs/toolkit react-redux redux-persist 然后需要创建 lib/redux/rootReducer.ts，并填入如下样例内容： 123456import &#123; combineReducers &#125; from &#x27;redux&#x27;;const reducer = combineReducers(&#123;&#125;);export default reducer; 创建 lib/redux/store.ts，并填入如下样例内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/* Core */import &#123; configureStore, type ThunkAction, type Action &#125; from &#x27;@reduxjs/toolkit&#x27;import &#123; useSelector as useReduxSelector, useDispatch as useReduxDispatch, type TypedUseSelectorHook,&#125; from &#x27;react-redux&#x27;/* Instruments */import &#123; reducer &#125; from &#x27;./rootReducer&#x27;import &#123;persistStore, persistReducer, FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER&#125; from &#x27;redux-persist&#x27;import createWebStorage from &#x27;redux-persist/lib/storage/createWebStorage&#x27;;const createNoopStorage = () =&gt; &#123; return &#123; getItem(_key: any) &#123; return Promise.resolve(null); &#125;, setItem(_key: any, value: any) &#123; return Promise.resolve(value); &#125;, removeItem(_key: any) &#123; return Promise.resolve(); &#125;, &#125;;&#125;;const storage = typeof window !== &#x27;undefined&#x27; ? createWebStorage(&#x27;local&#x27;) : createNoopStorage();const persistConfig = &#123; key: &#x27;root&#x27;, storage: storage&#125;const persistedReducer = persistReducer(persistConfig, reducer)function makeStore() &#123; return configureStore(&#123; reducer: persistedReducer, devTools: process.env.NODE_ENV !== &quot;production&quot; &#125;);&#125;export const reduxStore = configureStore(&#123; reducer: persistedReducer, middleware: (getDefaultMiddleware) =&gt; getDefaultMiddleware(&#123; serializableCheck: &#123; ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER], &#125;, &#125;),&#125;)export const persist = persistStore(reduxStore);export const useDispatch = () =&gt; useReduxDispatch&lt;ReduxDispatch&gt;()export const useSelector: TypedUseSelectorHook&lt;ReduxState&gt; = useReduxSelector/* Types */export type ReduxStore = typeof reduxStoreexport type ReduxState = ReturnType&lt;typeof reduxStore.getState&gt;export type ReduxDispatch = typeof reduxStore.dispatchexport type ReduxThunkAction&lt;ReturnType = void&gt; = ThunkAction&lt; ReturnType, ReduxState, unknown, Action&gt; 创建空白的 lib/redux/slices/index.ts 文件： 创建 lib/redux/index.ts，并填入如下样例内容： 12export * from &#x27;./store&#x27;export * from &#x27;./slices&#x27; 之后需要编辑 lib/providers.tsx 文件，引入相关配置： jsx123456789101112131415161718&#x27;use client&#x27;/* Core */import &#123; Provider &#125; from &#x27;react-redux&#x27;/* Instruments */import &#123;persist, reduxStore&#125; from &#x27;@/lib/redux&#x27;import &#123;PersistGate&#125; from &quot;redux-persist/integration/react&quot;;export const Providers = (props: React.PropsWithChildren) =&gt; &#123; return ( &lt;Provider store=&#123;reduxStore&#125;&gt; &lt;PersistGate persistor=&#123;persist&#125; loading=&#123;null&#125;&gt; &#123;props.children&#125; &lt;/PersistGate&gt; &lt;/Provider&gt; );&#125; 还需要将 Provider 注入到 _app.tsx 文件中： jsx1234567891011import &#x27;@/styles/globals.css&#x27;import type &#123; AppProps &#125; from &#x27;next/app&#x27;import &#123;Providers&#125; from &quot;@/lib/providers&quot;;export default function App(&#123; Component, pageProps &#125;: AppProps) &#123; return ( &lt;Providers&gt; &lt;Component &#123;...pageProps&#125; /&gt; &lt;/Providers&gt; )&#125; 如果还需要样例可以使用下面的代码： 创建 lib/redux/slices/demoSlice 文件: 12345678910111213141516171819202122232425import &#123;createSlice&#125; from &#x27;@reduxjs/toolkit&#x27;/* Types */export interface DemoSliceState &#123; value: number&#125;// Define the initial state using that typeconst initialState: DemoSliceState = &#123; value: 0,&#125;export const demoSlice = createSlice(&#123; name: &#x27;demo&#x27;, // `createSlice` will infer the state type from the `initialState` argument initialState, reducers: &#123; increment: (state: DemoSliceState) =&gt; &#123; state.value += 1 &#125;, decrement: (state: DemoSliceState) =&gt; &#123; state.value -= 1 &#125;, &#125;,&#125;) 创建 lib/redux/demoSlice/selector.ts 文件： 123import type &#123; ReduxState &#125; from &#x27;@/lib/redux&#x27;export const selectDemo = (state: ReduxState) =&gt; state.demo.value 创建 lib/redux/demoSlice/index.ts 文件 12export * from &#x27;./demoSlice&#x27;export * from &#x27;./selectors&#x27; 修改 lib/redux/slices/index.ts 文件： 1export * from &#x27;./demoSlice&#x27; 修改 lib/redux/rootReducer.ts 文件： 123456import &#123;demoSlice&#125; from &#x27;./slices&#x27;import &#123;combineReducers&#125; from &quot;redux&quot;;export const reducer = combineReducers(&#123; demo: demoSlice.reducer&#125;); 创建 pages/counter/index.tsx 页面： jsx12345678910111213141516171819202122232425262728293031&#x27;use client&#x27;/* Mui */import &#123;Box, Button, Typography&#125; from &quot;@mui/material&quot;;/* redux */import &#123; demoSlice, useSelector, useDispatch, selectDemo,&#125; from &#x27;@/lib/redux&#x27;export default function Test() &#123; const dispatch = useDispatch(); const count = useSelector(selectDemo); return ( &lt;div&gt; &lt;Box&gt; &lt;Typography&gt;Counter: &#123;count&#125;&lt;/Typography&gt; &lt;Button onClick=&#123;() =&gt; dispatch(demoSlice.actions.increment())&#125;&gt; Increment &lt;/Button&gt; &lt;Button onClick=&#123;() =&gt; dispatch(demoSlice.actions.decrement())&#125;&gt; Decrement &lt;/Button&gt; &lt;/Box&gt; &lt;/div&gt; )&#125; 根据 OpenAPI 生成代码 使用如下命令安装相关依赖： 1npm install -D @rtk-query/codegen-openapi esbuild-runner ts-node 然后需要初始化 lib/redux/emptyApi.ts 文件，填入如下内容： 123456import &#123; createApi, fetchBaseQuery &#125; from &#x27;@reduxjs/toolkit/query/react&#x27;export const emptySplitApi = createApi(&#123; baseQuery: fetchBaseQuery(&#123; baseUrl: &#x27;/&#x27; &#125;), endpoints: () =&gt; (&#123;&#125;),&#125;) 在项目根目录编写 openapi-config.ts 文件，填入如下内容： 123456789101112import type &#123; ConfigFile &#125; from &#x27;@rtk-query/codegen-openapi&#x27;const config: ConfigFile = &#123; schemaFile: &#x27;http://localhost:8080/v3/api-docs&#x27;, apiFile: &#x27;./lib/redux/emptyApi.ts&#x27;, apiImport: &#x27;emptySplitApi&#x27;, outputFile: &#x27;./lib/redux/api.ts&#x27;, exportName: &#x27;api&#x27;, hooks: true,&#125;export default config 之后可以使用如下命令生成代码了： 1npx @rtk-query/codegen-openapi openapi-config.ts JWT 验证 如果说项目使用了 JWT 等验证方式则需要进一步进行配置，具体样例如下： 编写 lib/redux/slices/authSlice/authSlice.ts 文件并填入如下内容： 12345678910111213141516171819202122232425262728import &#123;createSlice, PayloadAction&#125; from &#x27;@reduxjs/toolkit&#x27;;import &#123;User&#125; from &quot;@/lib/redux/api&quot;;export interface AuthSliceState &#123; token: string | null; user: User | null;&#125;const initialState: AuthSliceState = &#123; token: null, user: null&#125;;export const authSlice = createSlice(&#123; name: &#x27;auth&#x27;, initialState, reducers: &#123; setToken: (state: AuthSliceState, action: PayloadAction&lt;string | null&gt;) =&gt; &#123; state.token = action.payload; if (!action.payload) &#123; state.user = null &#125; &#125;, setUser: (state: AuthSliceState, action: PayloadAction&lt;User | null&gt;) =&gt; &#123; state.user = action.payload &#125; &#125;&#125;); 编写 lib/redux/slices/authSlice/selector.ts : 1234/* Instruments */import type &#123; ReduxState &#125; from &#x27;@/lib/redux&#x27;export const selectAuth = (state: ReduxState) =&gt; state.auth 注：此处因为还没有引入 reducer 所以会暂时报错，无需关注 编写 lib/redux/slices/authSlice/index.ts 12export * from &#x27;./authSlice&#x27;export * from &#x27;./selectors&#x27; 修改 lib/redux/slices/index.ts : 1export * from &#x27;./authSlice&#x27; 在 lib/redux/rootReducer.ts 中引入 authReducer: 123456789/* Instruments */import &#123;authSlice&#125; from &#x27;./slices&#x27;import &#123;combineReducers&#125; from &quot;redux&quot;;import &#123;api&#125; from &#x27;@/lib/redux/api&#x27;export const reducer = combineReducers(&#123; auth: authSlice.reducer, [api.reducerPath]: api.reducer&#125;); 在 ‘store’ 配置中也需要引入中间件： 123456789export const reduxStore = configureStore(&#123; reducer: persistedReducer, middleware: (getDefaultMiddleware) =&gt; getDefaultMiddleware(&#123; serializableCheck: &#123; ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER], &#125;, &#125;).contact(api.middleware),&#125;) 在 lib/redux/emptyApi.ts 修改请求地址位置并放置 Token: 12345678910111213141516171819202122232425262728293031323334353637383940414243import &#123;BaseQueryFn, createApi, FetchArgs, fetchBaseQuery, FetchBaseQueryError&#125; from &#x27;@reduxjs/toolkit/query/react&#x27;import &#123;ReduxState&#125; from &quot;@/lib/redux/store&quot;;import &#123;authSlice&#125; from &#x27;@/lib/redux/slices/authSlice&#x27;;export interface MessageData &#123; message: string;&#125;const customBaseQuery = fetchBaseQuery(&#123; baseUrl: process.env.NODE_ENV === &quot;development&quot; ? &#x27;http://localhost:8080&#x27; : &#x27;/&#x27;, prepareHeaders: (headers, api) =&gt; addDefaultHeaders(headers, api),&#125;);function addDefaultHeaders(headers: Headers, api: &#123; getState: () =&gt; unknown &#125;) &#123; headers.set(&#x27;Accept&#x27;, &#x27;application/json&#x27;); headers.set(&#x27;Content-Type&#x27;, &#x27;application/json&#x27;); const token: any = (api.getState() as ReduxState).auth.token; if (token !== null) &#123; headers.set(&#x27;Authorization&#x27;, `Bearer $&#123;token&#125;`); &#125; return headers;&#125;const BaseQueryWithAuth: BaseQueryFn&lt; string | FetchArgs, unknown, FetchBaseQueryError&gt; = async (args, api, extraOptions) =&gt; &#123; let result = await customBaseQuery(args, api, extraOptions); if (result.error &amp;&amp; result.error.status === 401) &#123; api.dispatch(authSlice.actions.setToken(null)); &#125; if (result.error !== undefined) &#123; let message: string = (result.error.data as MessageData).message; console.error(message); &#125; return result;&#125;;export const emptySplitApi = createApi(&#123; baseQuery: BaseQueryWithAuth, endpoints: () =&gt; (&#123;&#125;),&#125;) 之后即可编写页面进行测试。 修改内容后自动刷新页面 在 api.ts 中可以定义 tagTypes 属性，标识缓存内容的类型。此外还可以在获取数据的 API 上标识请求返回的数据为 providesTags: ['xxx'],，在更新数据的 API 上标识 invalidatesTags: ['Post'], 即可完成自动更新逻辑。 1234567891011121314151617181920212223242526272829303132333435import &#123; createApi, fetchBaseQuery &#125; from &#x27;@reduxjs/toolkit/query&#x27;import type &#123; Post, User &#125; from &#x27;./types&#x27;const api = createApi(&#123; baseQuery: fetchBaseQuery(&#123; baseUrl: &#x27;/&#x27;, &#125;), tagTypes: [&#x27;Post&#x27;, &#x27;User&#x27;], endpoints: (build) =&gt; (&#123; getPosts: build.query&lt;Post[], void&gt;(&#123; query: () =&gt; &#x27;/posts&#x27;, providesTags: [&#x27;Post&#x27;], &#125;), getUsers: build.query&lt;User[], void&gt;(&#123; query: () =&gt; &#x27;/users&#x27;, providesTags: [&#x27;User&#x27;], &#125;), addPost: build.mutation&lt;Post, Omit&lt;Post, &#x27;id&#x27;&gt;&gt;(&#123; query: (body) =&gt; (&#123; url: &#x27;post&#x27;, method: &#x27;POST&#x27;, body, &#125;), invalidatesTags: [&#x27;Post&#x27;], &#125;), editPost: build.mutation&lt;Post, Partial&lt;Post&gt; &amp; Pick&lt;Post, &#x27;id&#x27;&gt;&gt;(&#123; query: (body) =&gt; (&#123; url: `post/$&#123;body.id&#125;`, method: &#x27;POST&#x27;, body, &#125;), invalidatesTags: [&#x27;Post&#x27;], &#125;), &#125;),&#125;) 参考资料 Redux 官方文档 使用 OpenAPI 接口生成代码 App Router 配置说明","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"},{"name":"Node.js","slug":"Node-js","permalink":"https://wangqian0306.github.io/tags/Node-js/"},{"name":"React","slug":"React","permalink":"https://wangqian0306.github.io/tags/React/"}]},{"title":"NVIDIA 相关配置","slug":"linux/nvidia","date":"2023-03-08T15:52:33.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/nvidia/","permalink":"https://wangqian0306.github.io/2023/nvidia/","excerpt":"","text":"NVIDIA 相关配置 驱动安装 使用如下命令查看驱动版本号: 1ubuntu-drivers devices 然后即可安装 1sudo apt-get install nvidia-driver-&lt;version&gt; 待安装完成后重启即可 验证 注: 此时可能会导致无法正常进行图像显示，建议检查一次，如有问题可以参照参考资料。 使用如下命令可以显示驱动程序及显卡信息： 1nvidia-smi 容器直通 运行如下命令： 123456789distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\ sed &#x27;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#x27; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.listsudo apt-get updatesudo apt-get install -y nvidia-container-toolkitsudo nvidia-ctk runtime configure --runtime=dockersudo systemctl restart docker 性能检查 可以通过如下命令安装性能检测工具： 1sudo apt install nvtop -y 然后使用如下命令即可看到性能图表： 1nvtop 参考资料 如何在 Ubuntu 20.04 安装 Nvidia 驱动程序 NVIDIA Container Toolkit BinaryDriverHowto/Nvidia 黑屏解决方案","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://wangqian0306.github.io/tags/NVIDIA/"}]},{"title":"FauxPilot","slug":"ai/fauxpilot","date":"2023-03-03T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2023/fauxpilot/","permalink":"https://wangqian0306.github.io/2023/fauxpilot/","excerpt":"","text":"FauxPilot 简介 FauxPilot 是一款在本地托管的 GitHub Copilot。 注：如果显卡不好就不要用了，单张 2060 或同等算力基本歇菜，同样数量 CUDA 核心怕是没用，且显存最好 16G 以上(8GB 模型单机使用都遇到卡顿)。 安装方式梳理 在安装项目前需要满足如下依赖： Docker Docker-Compose &gt;= 1.28 NVIDIA GPU(Compute Capability &gt;= 6.0) nvidia-docker curl 和 zstd 命令 克隆项目： 1git clone https://github.com/fauxpilot/fauxpilot.git 使用脚本下载模型： 12cd fauxpilot./setup.sh 启动服务： 1./launch.sh 关闭服务 1./shutdown.sh VSCode 插件 在配置页面修改 settings.json 配置文件并加入下面的内容： 1234567&#123; &quot;github.copilot.advanced&quot;: &#123; &quot;debug.overrideEngine&quot;: &quot;codegen&quot;, &quot;debug.testOverrideProxyUrl&quot;: &quot;http://localhost:5000&quot;, &quot;debug.overrideProxyUrl&quot;: &quot;http://localhost:5000&quot; &#125;&#125; IDEA 配置 注：目前还没有官方插件可以使用，好像也没开发的意思，还是用官方插件 github copilot。 IDEA 官方插件当中并没有参数可以设置，但是在项目 Issue 当中的表述则是看起来代码中应该也去获取了 debug.overrideProxyUrl 等。经过检索发现代码中还是有这方面的内容具体位置如下： 12windows: C:\\Users\\&lt;user&gt;\\AppData\\Roaming\\JetBrains\\&lt;version&gt;\\plugins\\github-copilot-intellij\\copilot-agent\\dist\\agent.jslinux: ~/.local/share/JetBrains/&lt;version&gt;/github-copilot-intellij/copilot-agent/dist/agent.js 在配置中可以寻找如下配置项并进行修改： 1234567&#123; &quot;github.copilot.advanced&quot;: &#123; &quot;debug.overrideEngine&quot;: &quot;codegen&quot;, &quot;debug.testOverrideProxyUrl&quot;: &quot;http://localhost:5000&quot;, &quot;debug.overrideProxyUrl&quot;: &quot;http://localhost:5000&quot; &#125;&#125; 注: 由于效果不好，没继续花时间进行测试，只修改默认值是不行的。 参考资料 官方项目 客户端配置","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"FauxPilot","slug":"FauxPilot","permalink":"https://wangqian0306.github.io/tags/FauxPilot/"}]},{"title":"Time4J","slug":"java/time4j","date":"2023-03-01T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/time4j/","permalink":"https://wangqian0306.github.io/2023/time4j/","excerpt":"","text":"Time4J 简介 在处理农历等日期问题时可以引入 Time4J 。 使用方式 引入依赖： 123dependencies &#123; implementation group: &#x27;net.time4j&#x27;, name: &#x27;time4j-base&#x27;, version: &#x27;5.9.2&#x27;&#125; 编写代码： 123456789101112131415import net.time4j.PlainDate;import net.time4j.calendar.ChineseCalendar;import net.time4j.calendar.EastAsianMonth;import net.time4j.calendar.EastAsianYear;import java.time.LocalDate;public class Test &#123; public static void main(String[] args) &#123; // 农历转公历 ChineseCalendar calendar = ChineseCalendar.of(EastAsianYear.forGregorian(2023), EastAsianMonth.valueOf(1), 1); LocalDate chineseNewYear = calendar.transform(PlainDate.axis()).toTemporalAccessor(); System.out.println(chineseNewYear); &#125;&#125; 123456789101112131415import net.time4j.PlainDate;import net.time4j.calendar.ChineseCalendar;import java.util.Locale;public class Test &#123; public static void main(String[] args) &#123; PlainDate plainDate = PlainDate.of(2023, 1, 1); ChineseCalendar calendar = plainDate.transform(ChineseCalendar.axis()); System.out.println(calendar.get(ChineseCalendar.YEAR_OF_CYCLE).getZodiac(Locale.CHINES)); System.out.println(calendar.get(ChineseCalendar.YEAR_OF_CYCLE).getDisplayName(Locale.CHINES)); System.out.println(calendar.getMonth()); System.out.println(calendar.getDayOfMonth()); &#125;&#125; 参考资料 官方项目","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Git 麻瓜操作","slug":"tmp/git","date":"2023-02-22T13:57:04.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/self-git/","permalink":"https://wangqian0306.github.io/2023/self-git/","excerpt":"","text":"Git 麻瓜操作 简介 不小心把本地文件推送到 Github 了，还好遇到好心人提醒。以此文铭记自己的麻瓜操作。 删除某次错误的提交 首先需要使用如下命令，定位到错误的 commit-id 和上一次正确的 commit-id: 1git log 然后使用如下命令切换到正确的 commit-id ： 1git rebase -i &lt;correct_commit_id&gt; 在新开的文本中应该可以看到之后的 commit log 清单，将错误的 commit 前的 pick 改为 drop 然后保存退出即可。 还可以使用如下命令强制推送到服务器： 1git push origin HEAD --force 参考资料 git删除某个提交commit记录","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wangqian0306.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"日历折腾记录","slug":"tmp/calendar","date":"2023-02-20T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/calendar/","permalink":"https://wangqian0306.github.io/2023/calendar/","excerpt":"","text":"日历折腾记录 简介 目前市面上的大多数日历都支持 CalDAV 协议，可以通过订阅的方式持续拉取日程信息。 注：对于国外日历软件来说非常有用，可以在 Home-Assistant 或 Google Calendar 上标记节假日和调休。 常用技巧 节假日和调休 可以订阅如下地址： 1https://www.shuyz.com/githubfiles/china-holiday-calender/master/holidayCal.ics 生日 由于 Google Calendar 不支持农历格式的生日提醒，所以需要手动进行添加，可以访问如下地址在线生成订阅地址。 1https://lunar-calendar-anniversary-ics.vercel.app/ 注：如果是公历的生日可以直接添加联系人并指定生日即可。 纪念日检索 可以使用如下 SQL 语句检索最近 7 天内的纪念日。 1234567SELECT *FROM usersWHERE DATE_ADD(brith, INTERVAL YEAR(CURDATE()) - YEAR(brith) + IF(DAYOFYEAR(CURDATE()) &gt; DAYOFYEAR(brith), 1, 0) YEAR) BETWEEN CURDATE() AND DATE_ADD(CURDATE(), INTERVAL 7 DAY); 参考资料 中国节假日补班日历 农历纪念日日历订阅生成工具","categories":[],"tags":[{"name":"CalDAV","slug":"CalDAV","permalink":"https://wangqian0306.github.io/tags/CalDAV/"}]},{"title":"AdGuardHome","slug":"tmp/adguard","date":"2023-02-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/ad-guard/","permalink":"https://wangqian0306.github.io/2023/ad-guard/","excerpt":"","text":"AdGuard Home 简介 AdGuard Home是一款用于拦截广告和跟踪的全网络软件，通过作为 DNS 服务器的方式来为所有家庭设备提供服务。 安装及配置 Docker 创建 work, conf 文件夹，编写如下 docker-compose.yaml 文件： 123456789101112services: adguardhome: image: adguard/adguardhome container_name: adguardhome ports: - &quot;53:53&quot; - &quot;80:80&quot; - &quot;443:443&quot; - &quot;3000:3000&quot; volumes: - ./work:/opt/adguardhome/work - ./conf:/opt/adguardhome/conf 注：此处还支持很多其他端口和功能，如有需要可以参照官方文档进行修改。 使用如下命令启动服务： 1docker-compose up -d 访问如下地址即可完成初始化，及设置： http://localhost:3000 参考资料 官方项目 容器主页 视频教程 anti-AD 规则列表 Adguard 过滤规则分享 GOODBYEADS 过滤规则","categories":[],"tags":[{"name":"AdGuardHome","slug":"AdGuardHome","permalink":"https://wangqian0306.github.io/tags/AdGuardHome/"}]},{"title":"Linux 下载器","slug":"linux/downloader","date":"2023-02-07T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/downloader/","permalink":"https://wangqian0306.github.io/2023/downloader/","excerpt":"","text":"Linux 下载器 Aria2 aria2 是一款下载软件 AriaNg 是针对 aria2 研发的前端网页。 安装方式 CentOS 12yum install epel-release -yyum install aria2 -y Debian 1apt-get install aria2 -y 然后编写配置文件 /etc/aria2/aria2.conf： 12345678910111213141516dir=/downloadscontinue=truemax-concurrent-downloads=5max-connection-per-server=5min-split-size=10Minput-file=/etc/aria2/aria2.sessionsave-session=/etc/aria2/aria2.sessionenable-rpc=truerpc-allow-origin-all=truerpc-listen-all=truerpc-listen-port=6800listen-port=51413enable-dht=falseenable-peer-exchange=falseseed-ratio=0bt-seed-unverified=true 最后可以通过如下命令启动服务： 1aria2c --conf-path=/etc/aria2/aria2.conf 前端网页 AriaNg 项目可以使用 Web 服务器进行部署，在 Nginx 服务器上部署的参考命令如下： 12345cd /usr/share/nginx/htmlwget https://github.com/mayswind/AriaNg-DailyBuild/archive/master.zipunzip master.zipmv AriaNg-DailyBuild-master/* .rm -rf AriaNg-DailyBuild-master 统一部署 可以使用编写 docker-compose.yaml 来快速部署服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748services: Aria2-Pro: container_name: aria2-pro image: p3terx/aria2-pro environment: - PUID=65534 - PGID=65534 - UMASK_SET=022 - RPC_SECRET=P3TERX - RPC_PORT=6800 - LISTEN_PORT=6888 - DISK_CACHE=64M - IPV6_MODE=false - UPDATE_TRACKERS=true - CUSTOM_TRACKER_URL= - TZ=Asia/Shanghai volumes: - $&#123;PWD&#125;/aria2-config:/config - $&#123;PWD&#125;/aria2-downloads:/downloads# If you use host network mode, then no port mapping is required.# This is the easiest way to use IPv6 networks. network_mode: host# network_mode: bridge# ports:# - 6800:6800# - 6888:6888# - 6888:6888/udp restart: unless-stopped# Since Aria2 will continue to generate logs, limit the log size to 1M to prevent your hard disk from running out of space. logging: driver: json-file options: max-size: 1m# AriaNg is just a static web page, usually you only need to deploy on a single host. AriaNg: container_name: ariang image: p3terx/ariang command: --port 6880 --ipv6 network_mode: host# network_mode: bridge# ports:# - 6880:6880 restart: unless-stopped logging: driver: json-file options: max-size: 1m Deluge 一体化容器部署 12345678910111213141516services: deluge: image: lscr.io/linuxserver/deluge:latest container_name: deluge environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - DELUGE_LOGLEVEL=error volumes: - ./deluge/config:/config - ./content/downloads:/downloads ports: - 8112:8112 - 58846:58846 restart: unless-stopped qBittorrent 一体化容器部署 1234567891011121314151617services: qbittorrent: image: lscr.io/linuxserver/qbittorrent:latest container_name: qbittorrent environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - WEBUI_PORT=8080 volumes: - ./qbittorrent/config:/config - ./content/downloads:/downloads ports: - 8080:8080 - 6881:6881 - 6881:6881/udp restart: unless-stopped 参考资料 Aria2 项目 AriaNg 项目 aria2-ariang-docker 配置参考 Aria2 Pro - 更好用的 Aria2 Docker 容器镜像","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"aria2","slug":"aria2","permalink":"https://wangqian0306.github.io/tags/aria2/"}]},{"title":"Samba 服务安装及配置流程","slug":"linux/samba","date":"2023-02-06T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2023/samba/","permalink":"https://wangqian0306.github.io/2023/samba/","excerpt":"","text":"Samba 服务配置 简介 Samba 是种用来让 UNIX 系列的操作系统与微软 Windows 操作系统的 SMB/CIFS(Server Message Block/Common Internet File System)网络协议做链接的自由软件。 安装及使用 安装软件及依赖，并备份配置文件： 12yum install -y sambamv /etc/samba/smb.conf /etc/samba/smb.conf.bk 创建分享目录 &lt;path&gt;，编辑配置文件 /etc/samba/smb.conf： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[global] workgroup = SAMBA security = user passdb backend = tdbsam printing = cups printcap name = cups load printers = yes cups options = raw map to guest = Bad User # Install samba-usershares package for support include = /etc/samba/usershares.conf[homes] comment = Home Directories valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes[printers] comment = All Printers path = /var/tmp printable = Yes create mask = 0600 browseable = No[print$] comment = Printer Drivers path = /var/lib/samba/drivers write list = @printadmin root force group = @printadmin create mask = 0664 directory mask = 0775[myshare] path=&lt;path&gt; public=yes browseable=yes writable=yes create mask=0644 directory mask=0755 12firewall-cmd --permanent --add-service=sambafirewall-cmd --reload 启动服务 1systemctl enable smb --now 账户配置 使用如下命令创建账户： 1smbpasswd -a &lt;username&gt; 使用如下命令列出账户： 1pdbedit -L 在 [myshare] 类似的配置单元中可以写入如下限制条件： 12valid users=&lt;user_1&gt;,&lt;user_2&gt;write list=&lt;user&gt; 重新启动服务： 1systemctl restart smb 参考资料 Centos7下Samba服务器配置（实战）","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"Samba","slug":"Samba","permalink":"https://wangqian0306.github.io/tags/Samba/"}]},{"title":"Kubernetes-Dashboard","slug":"kubernetes/kubernetes-dashboard","date":"2023-02-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/kubernetes-dashboard/","permalink":"https://wangqian0306.github.io/2023/kubernetes-dashboard/","excerpt":"","text":"Kubernetes-Dashboard 简介 Kubernetes Dashboard 是一个用于 Kubernete 集群的通用 Web UI。它允许用户在网页上管理集群。由于其本身需要外部 URL 所以本文的前置依赖条件是已经配置好了 Ingress-Nginx 或同等内容，最好配置了 External-DNS。 部署 使用如下命令部署仪表版： 1kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml 编写 dashboard-ingress.yaml 文件开放外网访问： 12345678910111213141516171819202122apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot; name: dashboard-ingress namespace: kubernetes-dashboardspec: ingressClassName: nginx rules: - host: &lt;host&gt; http: paths: - backend: service: name: kubernetes-dashboard port: number: 443 path: / pathType: Prefixstatus: loadBalancer: &#123;&#125; 注：host (样例 k8s-dashboard.xxx.xxx) 需要写入 dns 服务器或者 hosts 文件中。 部署完成后可以使用如下命令检查 Ingress 状态，若能获得 ADDRESS 则可以正常访问 https://&lt;host&gt;： 1kubectl get ingress -n kubernetes-dashboard 之后可以新建 dashboard-sa.yaml 并填入如下内容和命令来创建 service account 并赋予权限生成 token: 12345678910111213141516171819apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 1kubectl apply -f dashboard-sa.yaml Token 方式登录 使用如下命令: 1kubectl -n kubernetes-dashboard create token admin-user 将生成的 Token 填入 dashboard 即可。 Config 方式登录 在服务器上获取样例配置信息： 1kubectl config view --raw 使用如下命令生成 Token，并将其保存下来: 1kubectl create token admin-user --namespace kubernetes-dashboard --duration 4294967296s 注：过期时间约有 136 年，不建议在生产环境中进行如此配置。 参考样例配置，在客户端编写 config 文件： 123456789101112131415161718apiVersion: v1clusters:- cluster: certificate-authority-data: &lt;certificate-authority-data&gt; server: &lt;server&gt; name: &lt;cluster_name&gt;contexts:- name: admin-user@&lt;cluster_name&gt; context: cluster: &lt;cluster_name&gt; user: admin-usercurrent-context: admin-user@&lt;cluster_name&gt;kind: Configpreferences: &#123;&#125;users:- name: admin-user user: token: &lt;token&gt; 然后使用此文件登录 dashboard 即可。 参考资料 官方文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes 外部 DNS","slug":"kubernetes/kubernetes-external-dns","date":"2023-02-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/kubernetes-external-dns/","permalink":"https://wangqian0306.github.io/2023/kubernetes-external-dns/","excerpt":"","text":"Kubernetes 外部 DNS 简介 ExternalDNS 项目可以把 Kubernetes 集群内的 Service 和 Ingress 实例同步到外部的 DNS 服务提供方。 注：不同的服务提供方有不同的对接方式，具体支持的平台请参阅官方项目。 安装方式 注：本示例采用支持 RFC2136 标准的 BIND 服务，前期准备参见 BIND 文档的 动态配置 章节。 注：！！！由于服务在启动的时候会将上次创建的内容全部删除，所以一个集群只可以链接一个 DNS 服务器。千万注意对应关系！！！ 然后在任意可以链接到 Kubernetes 的设备上编写 external-dns.yaml 文件并部署即可： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586apiVersion: v1kind: Namespacemetadata: name: external-dns labels: name: external-dns---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: external-dns namespace: external-dnsrules:- apiGroups: - &quot;&quot; resources: - services - endpoints - pods - nodes verbs: - get - watch - list- apiGroups: - extensions - networking.k8s.io resources: - ingresses verbs: - get - list - watch---apiVersion: v1kind: ServiceAccountmetadata: name: external-dns namespace: external-dns---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: external-dns-viewer namespace: external-dnsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dnssubjects:- kind: ServiceAccount name: external-dns namespace: external-dns---apiVersion: apps/v1kind: Deploymentmetadata: name: external-dns namespace: external-dnsspec: selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.4 args: - --registry=txt - --txt-prefix=external-dns- - --txt-owner-id=k8s - --provider=rfc2136 - --rfc2136-host=&lt;dns_server_host&gt; - --rfc2136-port=53 - --rfc2136-zone=&lt;zone&gt; - --rfc2136-tsig-secret=&lt;key.secret&gt; - --rfc2136-tsig-secret-alg=hmac-sha256 - --rfc2136-tsig-keyname=externaldns-key - --rfc2136-tsig-axfr - --source=ingress - --domain-filter=&lt;zone&gt; 使用如下命令部署即可： 1kubectl apply -f external-dns.yaml 之后正常创建 Ingress ，并使用如下命令可以检查服务日志： 1kubectl logs -f deploy/external-dns -n external-dns 若出现如下内容即可访问对应 URL： 1&quot;Adding RR: xxxx 0 A xxx.xxx.xxx.xxx&quot; 参考资料 官方项目 RFC2136 配置方式","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"},{"name":"DNS","slug":"DNS","permalink":"https://wangqian0306.github.io/tags/DNS/"}]},{"title":"Kubernetes 安装","slug":"kubernetes/kubernetes-install","date":"2023-02-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/kubernetes/","permalink":"https://wangqian0306.github.io/2023/kubernetes/","excerpt":"","text":"简介 Kubernetes是一个可移植的，可扩展的开源平台，用于管理容器化的工作负载和服务，。 安装前的准备 检查系统兼容性 Kubernetes 为基于 Debian 和 Red Hat 的通用 Linux 发行版提供了支持，对其他发行版提供了通用说明。 检查硬件配置 2 CPU 及以上的处理器 2 GB 及以上内存 检查网络配置 确保集群中的设备可以互通(公有’DNS’或私有‘host’皆可) 使用 ping 命令检测 集群中每个设备都需要独立的 Hostname, MAC 地址 和 Product_uuid。 使用 ifconfig -a 检测 Hostname, MAC 地址 是否冲突 使用 sudo cat /sys/class/dmi/id/product_uuid 检测 Product_uuid 是否冲突 检测集群中的端口是否开放 控制节点 Control-plane node(s) 所需端口如下： 协议类型 绑定方式 端口区域 作用 对应服务 TCP Inbound 6443* Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 10251 kube-scheduler Self TCP Inbound 10252 kube-controller-manager Self 工作节点 Worker node(s) 所需端口如下 协议类型 绑定方式 端口区域 作用 对应服务 TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services† All 注： 标记的端口是可以修改的，确保对应端口开放即可。 † 标记的端口是 NodePort 服务的默认端口范围。 关闭 Swap 使用如下命令单次禁用 Swap 1sudo swapoff -a 取消 Swap 挂载 1vim /etc/fstab 使用 # 号注释 Swap 所处行即可 配置 iptables 使用如下命令配置网络 12345678910cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOFsudo sysctl --system 配置 Containerd 注：参照 Containerd 文档。 安装 kubeadm,kubelet,kubectl 命令 使用如下命令进行安装 123456789101112131415161718cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-\\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF# Set SELinux in permissive mode (effectively disabling it)sudo setenforce 0sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/configsudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetessudo systemctl enable --now kubelet 初始化控制节点 编写如下配置文件 kubeadm-config.yaml： 1234567891011121314151617181920212223apiVersion: kubeadm.k8s.io/v1beta3kind: InitConfigurationnodeRegistration: criSocket: &quot;unix:///run/containerd/containerd.sock&quot;localAPIEndpoint: advertiseAddress: &quot;&lt;hostOrIp&gt;&quot; bindPort: 6443---apiVersion: kubeadm.k8s.io/v1beta3kind: ClusterConfigurationnetworking: serviceSubnet: &quot;10.96.0.0/16&quot; podSubnet: &quot;10.244.0.0/24&quot; dnsDomain: &quot;cluster.local&quot;kubernetesVersion: &quot;&lt;version&gt;&quot;controlPlaneEndpoint: &quot;&lt;hostOrIp&gt;:6443&quot;certificatesDir: &quot;/etc/kubernetes/pki&quot;imageRepository: &quot;registry.aliyuncs.com/google_containers&quot;clusterName: &quot;demo-cluster&quot;---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd 使用如下命令拉取镜像并启动服务： 12kubeadm config images pull --config kubeadm-config.yamlkubeadm init --config kubeadm-config.yaml -v 5 注：如果遇到问题，可以根据命令提示进行修复，并使用 kubeadm reset -f --cri-socket unix:///run/containerd/containerd.sock 移除之前的配置。 用户配置 root 用户配置 12345cat &lt;&lt;EOF | sudo tee /etc/profile.d/k8s.shexport KUBECONFIG=/etc/kubernetes/admin.confEOFchmod a+x /etc/profile.d/k8s.shsource /etc/profile.d/k8s.sh 普通用户配置 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config (可选) 在主机上运行除集群管理外的其他服务 1kubectl taint nodes --all node-role.kubernetes.io/control-plane- 加入集群 如果原先 token 过期需要刷新 token，(默认一天) 1kubeadm token create 如果没有 discovery-token-ca-cert-hash 可以使用如下命令生成 12openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \\ openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27; 注： kubeadm init 命令会在命令行中输出加入集群的命令具体结构如下： kubeadm join &lt;host&gt;:&lt;port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 运行 join 命令 部署网络插件 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 检查集群 使用如下命令检查 Kubernetes 节点列表 1kubectl get nodes --all-namespaces 使用如下命令检查 Kubernetes 集群中运行的所有的 Pod 1kubectl get pods --all-namespaces 证书续期 集群的默认证书只有一年，可以通过如下命令配置延长： 1kubeadm init --cert-dir /etc/kubernetes/pki --cert-expiration 8760h 使用如下命令检测过期时长： 1kubeadm certs check-expiration 如果有 RESIDUAL TIME 项异常则可以使用此命令进行续期： 1kubeadm certs renew &lt;CERTIFICATE&gt; 或者使用如下命令批量更新： 1kubeadm certs renew all 在更新完成后需要重启服务，通过重启设备或者使用如下命令： 1234kubectl -n kube-system delete pod -l &#x27;component=kube-apiserver&#x27;kubectl -n kube-system delete pod -l &#x27;component=kube-controller-manager&#x27;kubectl -n kube-system delete pod -l &#x27;component=kube-scheduler&#x27;kubectl -n kube-system delete pod -l &#x27;component=etcd&#x27; 参考资料 官方文档 安装文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes 生产环境","slug":"kubernetes/kubernetes-prod","date":"2023-02-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/kubernetes-prod/","permalink":"https://wangqian0306.github.io/2023/kubernetes-prod/","excerpt":"","text":"Kubernetes 生产环境 简介 看到 SpringOne 大会上提到的生产级别 K8s 集群具有的内容，此处简单进行下记录。 部署方式 从结构上来说应该分成以下几个部分： 使用 Terraform 来部署 K8s ，其中包含 kubernetes 集群，容器仓库，helm 。 使用 Argo CD 完成持续部署，此外还有 cert-manager ， Lets’Encrypt 和 Traefik。 使用 Robusta 完成系统监控，其中还包含 Prometheus ，Grafana ，Loki 等。 注：此处好像部署了多套 Prometheus 。 参考资料 Let’s Generate Art With Kubernetes And Spring! (SpringOne 2024) Kubernetes Bootstrapper cert-manager Traefik Kyverno metrics-server Loki-Stack Helm Trivy","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Prometheus 警报","slug":"tools/prometheus-alert","date":"2023-01-30T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2023/prometheus-alert/","permalink":"https://wangqian0306.github.io/2023/prometheus-alert/","excerpt":"","text":"Prometheus 警报 简介 Prometheus 的警报由两部分内容构成，分别是 Prometheus 中的警报规则和聚合消息向外发送警报的 Alertmanager 服务。 安装与部署 注：本示例采用 MySQL 作为监控源，邮件作为告警方式。 编写 Prometheus 配置文件 prometheus.yml ： 1234567891011121314global: scrape_interval: 15sscrape_configs: - job_name: &#x27;check-web&#x27; scrape_interval: 5s static_configs: - targets: [ &#x27;mysqlexporter:9104&#x27; ]alerting: alertmanagers: - scheme: http static_configs: - targets: [ &#x27;alertmanager:9093&#x27; ]rule_files: - &quot;/etc/prometheus/mysql_rule.yml&quot; 编写 MySQL 监控文件 mysql_rule.yml ： 12345678910groups: - name: mysql_group rules: - alert: MySQL_Down expr: mysql_up == 0 for: 15s labels: severity: page annotations: summary: MySQL Down 编写 Alertmanager 配置文件 alertmanager.yml ： 123456789101112131415global: smtp_smarthost: &#x27;&lt;smtp_server&gt;&#x27; smtp_from: &#x27;&lt;email_sender&gt;&#x27; smtp_auth_username: &#x27;&lt;email_user&gt;&#x27; smtp_auth_password: &#x27;&lt;email_pass&gt;&#x27;route: receiver: &#x27;demo&#x27; group_by: [&#x27;alertname&#x27;, &#x27;cluster&#x27;] group_wait: 30s group_interval: 5m repeat_interval: 3hreceivers: - name: &#x27;demo&#x27; email_configs: - to: &#x27;&lt;receiver_email&gt;&#x27; 编写 docker-compose.yaml 文件： 12345678910111213141516171819202122232425262728293031323334353637services: mysql: image: mysql:latest command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: demo ports: - &quot;3306:3306&quot; mysqlexporter: image: prom/mysqld-exporter:latest ports: - &quot;9104:9104&quot; environment: - DATA_SOURCE_NAME=root:123456@(mysql:3306)/demo depends_on: - mysql prometheus: image: prom/prometheus:latest ports: - &quot;9090:9090&quot; volumes: - type: bind source: ./prometheus.yml target: /etc/prometheus/prometheus.yml - type: bind source: ./mysql_rule.yml target: /etc/prometheus/mysql_rule.yml alertmanager: image: prom/alertmanager:latest ports: - &quot;9093:9093&quot; volumes: - type: bind source: ./alertmanager.yml target: /etc/alertmanager/alertmanager.yml 启动测试服务 1docker-compose up -d 关闭 MySQL 服务 1docker-compose stop mysql 注：关闭之后即可访问 prometheus 和 alertmanager 页面，查看警报相关信息。 参考资料 Alertmanager 配置 Prometheus 配置","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"}]},{"title":"Prometheus Operator","slug":"kubernetes/prometheus-operator","date":"2023-01-16T12:52:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2023/prometheus-operator/","permalink":"https://wangqian0306.github.io/2023/prometheus-operator/","excerpt":"","text":"Prometheus Operator 简介 Prometheus Operator 提供 Kubernetes 原生部署和管理 Prometheus 和相关监控组件。该项目的目的是简化和自动化基于 Prometheus 的 Kubernetes 集群监控堆栈的配置。 安装 拉取项目： 12git clone https://github.com/prometheus-operator/kube-prometheus.gitcd kube-prometheus 初始化基本环境： 12kubectl create -f manifests/setupuntil kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo &quot;&quot;; done 注：若输出为 No resources found 则代表可执行下一步。 拉取镜像并修改容器地址： prometheusAdapter-deployment.yaml kubeStateMetrics-deployment.yaml 12registry.k8s.io/prometheus-adapter/prometheus-adapter:&#123;version&#125;registry.k8s.io/kube-state-metrics/kube-state-metrics:&#123;version&#125; 部署服务： 1kubectl create -f manifests/ 编写 prometheus.yaml 并部署 Ingress ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTP&quot; name: grafana-ingress namespace: monitoringspec: ingressClassName: nginx rules: - host: grafana.&lt;host&gt; http: paths: - backend: service: name: grafana port: number: 3000 path: / pathType: Prefix---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTP&quot; name: alertmanager-ingress namespace: monitoringspec: ingressClassName: nginx rules: - host: alert.&lt;host&gt; http: paths: - backend: service: name: alertmanager-main port: number: 9093 path: / pathType: Prefix---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTP&quot; name: prometheus-ingress namespace: monitoringspec: ingressClassName: nginx rules: - host: prometheus.&lt;host&gt; http: paths: - backend: service: name: prometheus-k8s port: number: 9090 path: / pathType: Prefix 1kubectl apply -f prometheus.yaml 参考资料 官方文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Cloud Native Buildpacks","slug":"docker/cnb","date":"2023-01-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2023/cnb/","permalink":"https://wangqian0306.github.io/2023/cnb/","excerpt":"","text":"Cloud Native Buildpacks 简介 Cloud Native Buildpacks 是一种标准接口，目标是将任何应用程序转化为在任何云平台上都能运行的镜像。 核心概念 使用人员 程序开发人员 构建包作者 运维人员 组件及其负责内容 BuildPack：Buildpack 负责检查应用程序源代码(框架级别)，识别和收集依赖关系，并输出符合 OCI 规范的应用程序和依赖容器层 Builder：Builder 是一系列有序 buildpack 的集合，以及 build image ，运行生命周期，运行 run image 。它会获取代码并构建成品的 app image 。 Build Image：构建环境的镜像 Run Image：为应用程序映像提供基础映像。 Lifecycle(生命周期)：Analyze，Detect，Restore，Build，Export，Create，Launch，Rebase 安装方式 Pack 是 Cloud Native Buildpacks 项目的基础构建工具，提供了默认的 Cli 和 Go 库。 可以使用如下命令安装 Cli : 1(curl -sSL &quot;https://github.com/buildpacks/pack/releases/download/&#123;version&#125;/pack-&#123;verison&#125;-linux.tgz&quot; | sudo tar -C /usr/local/bin/ --no-same-owner -xzv pack) 注：也可以使用容器的方式跳过此处环境安装。 初步试用 1234git clone https://github.com/buildpacks/samplescd samples/apps/java-mavenpack build myapp --builder cnbs/sample-builder:jammydocker run --rm -p 8080:8080 myapp 在生产环境中使用 在实际的使用流程中可以选择 Paketo Buildpacks 项目提供的各种 Builder，具体支持以下语言： Java Node.js .Net Core Go Python PHP Ruby 注：使用过程请参照 Paketo Buildpacks 文档和 样例代码。 参考资料 官方文档 Paketo Buildpacks","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"CNB","slug":"CNB","permalink":"https://wangqian0306.github.io/tags/CNB/"}]},{"title":"Helm","slug":"kubernetes/helm","date":"2022-12-21T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/helm/","permalink":"https://wangqian0306.github.io/2022/helm/","excerpt":"","text":"Helm 简介 Helm 是一款管理 Kubernetes 应用程序的便捷工具。 安装 使用如下命令安装 Helm： 123curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.sh 安装完成后可以使用如下命令检查服务安装情况： 1helm version 配置国内源(可选) 123helm repo add kaiyuanshe http://mirror.kaiyuanshe.cn/kubernetes/chartshelm repo add azure http://mirror.azure.cn/kubernetes/chartshelm repo add bitnami https://charts.bitnami.com/bitnami 注: 默认使用官方源速度我看着还行，此处仅留作备用。 常用命令 检索软件包： 1helm search &lt;package&gt; 安装软件包： 1helm install &lt;package&gt; -n &lt;namespace&gt; 查看已经安装的软件包： 1helm list -n &lt;namespace&gt; 卸载软件包： 1helm uninstall &lt;package&gt; -n &lt;namespace&gt; 参考资料 官方文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"},{"name":"Helm","slug":"Helm","permalink":"https://wangqian0306.github.io/tags/Helm/"}]},{"title":"NFS StorageClass","slug":"kubernetes/nfs-sc","date":"2022-12-21T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/nfs-sc/","permalink":"https://wangqian0306.github.io/2022/nfs-sc/","excerpt":"","text":"NFS StorageClass 简介 StorageClass 是 Kubernetes 为了动态配置存储而产生的概念，本文会整理为 NFS 服务安装 Storage Class 的过程。NFS 服务安装的部分请参照其他文档。 Kubernetes 官方并没有提供内置的驱动而建议采用如下两种外部驱动： NFS Ganesha NFS subdir 注：由于 NFS subdir 提供了 Helm Chart 安装较为方便，所以本文优先采用此种方式。 NFS subdir 外部驱动 注：运行需要拉取 k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 镜像，此处采用了本地源的方式解决了此问题。 然后运行下面的命令即可： 123helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/helm inspect values nfs-subdir-external-provisioner/nfs-subdir-external-provisioner &gt; values.yamlvim values.yaml 修改下面的内容： 12345image: repository: &lt;custom_repo&gt;/sig-storage/nfs-subdir-external-provisionernfs: server: &lt;server&gt; path: &lt;dir&gt; 然后使用下面的命令进行部署即可： 1helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner -f values.yaml 参考资料 官方文档 nfs-subdir-external-provisioner","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes 服务 Yaml 样例","slug":"kubernetes/kubernetes-yaml","date":"2022-12-20T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/kubernetes-yaml/","permalink":"https://wangqian0306.github.io/2022/kubernetes-yaml/","excerpt":"","text":"Kubernetes 服务 Yaml 样例 简介 本文是在 Kubernetes 中部署一个简单服务的 Yaml 记录。 样例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566apiVersion: v1kind: Namespacemetadata: name: demo---apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deployment labels: app: tomcat namespace: demospec: replicas: 1 selector: matchLabels: app: tomcat template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat:9.0 ports: - containerPort: 8080---kind: ServiceapiVersion: v1metadata: labels: app: tomcat name: tomcat-svc namespace: demospec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: tomcat---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: tomcat-ingress namespace: demospec: ingressClassName: nginx rules: - host: &lt;host&gt; http: paths: - backend: service: name: tomcat-svc port: number: 8080 path: / pathType: Prefixstatus: loadBalancer: &#123;&#125; 参考资料 官方文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Nextcloud","slug":"tools/nextcloud","date":"2022-12-19T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/nextcloud/","permalink":"https://wangqian0306.github.io/2022/nextcloud/","excerpt":"","text":"Nextcloud 简介 Nextcloud 是一款开源网盘服务。提供了 Windows Linux 和 mac 平台的客户端，可以方便的存储文件。 部署方式 Docker 本地运行版 编写 docker-compose.yaml 文件： 12345678910111213141516171819202122232425262728293031323334volumes: nextcloud: db:services: db: image: mariadb:10.5 restart: always command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW volumes: - db:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=&lt;root_password&gt; - MYSQL_PASSWORD=&lt;password&gt; - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud app: image: nextcloud restart: always ports: - &quot;8080:80&quot; links: - db volumes: - nextcloud:/var/www/html environment: - MYSQL_PASSWORD=&lt;password&gt; - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_HOST=db - NEXTCLOUD_ADMIN_USER=&lt;username&gt; - NEXTCLOUD_ADMIN_PASSWORD=&lt;password&gt; - NEXTCLOUD_TRUSTED_DOMAINS=&lt;domains_xxx.xxx.xxx xxx.xxx.xxx&gt; 启动服务 1docker-compose up -d 登陆网页 http://localhost:8080 并根据页面提示进行初始化即可 注：建议配合 客户端 一起使用。 公网运行版 此处 NextCloud 提供了 官方示例 可以使用如下样例 db.env： 123MYSQL_PASSWORD=&lt;password&gt;MYSQL_DATABASE=nextcloudMYSQL_USER=nextcloud 样例 docker-compoes.yaml： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889services: db: image: mariadb:10.6 command: --transaction-isolation=READ-COMMITTED --log-bin=binlog --binlog-format=ROW restart: always volumes: - &lt;mariadb_storage_path&gt;:/var/lib/mysql:Z environment: - MYSQL_ROOT_PASSWORD=rainbowfish - MARIADB_AUTO_UPGRADE=1 - MARIADB_DISABLE_UPGRADE_BACKUP=1 env_file: - db.env redis: image: redis:alpine restart: always app: image: nextcloud:apache restart: always volumes: - &lt;nextcloud_storage_path&gt;:/var/www/html:z environment: - VIRTUAL_HOST=&lt;hostname&gt; - LETSENCRYPT_HOST=&lt;hostname&gt; - LETSENCRYPT_EMAIL=&lt;email&gt; - MYSQL_HOST=db - REDIS_HOST=redis - NEXTCLOUD_ADMIN_USER=&lt;username&gt; - NEXTCLOUD_ADMIN_PASSWORD=&lt;password&gt; - NEXTCLOUD_TRUSTED_DOMAINS=&lt;domains_xxx.xxx.xxx xxx.xxx.xxx&gt; env_file: - db.env depends_on: - db - redis networks: - proxy-tier - default cron: image: nextcloud:apache restart: always volumes: - &lt;nextcloud_storage_path&gt;:/var/www/html:z entrypoint: /cron.sh depends_on: - db - redis proxy: build: ./proxy restart: always ports: - 80:80 - 443:443 labels: com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy: &quot;true&quot; volumes: - certs:/etc/nginx/certs:z,ro - vhost.d:/etc/nginx/vhost.d:z - html:/usr/share/nginx/html:z - /var/run/docker.sock:/tmp/docker.sock:z,ro networks: - proxy-tier letsencrypt-companion: image: nginxproxy/acme-companion restart: always volumes: - certs:/etc/nginx/certs:z - acme:/etc/acme.sh:z - vhost.d:/etc/nginx/vhost.d:z - html:/usr/share/nginx/html:z - /var/run/docker.sock:/var/run/docker.sock:z,ro networks: - proxy-tier depends_on: - proxyvolumes: certs: acme: vhost.d: html:networks: proxy-tier: 注：仅仅改动上述文件就可以，还需要引入样例中的 proxy 文件夹才可以运行。 运行方式： 12docker-compose build --pulldocker-compose up -d 参考资料 容器页 官方文档 容器样例","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Nextcloud","slug":"Nextcloud","permalink":"https://wangqian0306.github.io/tags/Nextcloud/"},{"name":"PHP","slug":"PHP","permalink":"https://wangqian0306.github.io/tags/PHP/"}]},{"title":"Containerd","slug":"docker/containerd","date":"2022-12-16T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/containerd/","permalink":"https://wangqian0306.github.io/2022/containerd/","excerpt":"","text":"Containerd 简介 containerd 是一个行业标准的容器运行时，强调简单性、健壮性和可移植性。它可作为 Linux 和 Windows 的守护进程，可以管理其主机系统的完整容器生命周期。 安装 使用二进制包 123456789wget https://github.com/containerd/containerd/releases/download/v1.6.20/containerd-1.6.20-linux-amd64.tar.gztar Cxzvf /usr/local containerd-1.6.20-linux-amd64.tar.gzwget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /usr/local/lib/systemd/system/wget https://github.com/opencontainers/runc/releases/download/v1.1.6/runc.amd64install -m 755 runc.amd64 /usr/local/sbin/runcwget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgzmkdir -p /opt/cni/bintar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.2.0.tgzsystemctl daemon-reload 使用软件源安装 12345678910111213sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-enginesudo yum install -y yum-utilssudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.reposudo yum install -y containerd.io 配置 生成配置文件： 12mkdir -p /etc/containerdcontainerd config default &gt; /etc/containerd/config.toml 编辑配置文件 /etc/containerd/config.toml，主要改动以下项： 123456789[plugins.&quot;io.containerd.grpc.v1.cri&quot;] sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.9&quot;[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options] SystemdCgroup = true[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;&lt;custom_repo&gt;&quot;] endpoint = [&quot;http://&lt;custom_repo&gt;&quot;] 编写如下配置文件 /etc/crictl.yaml： 1234runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: false 启动服务： 1systemctl enable --now containerd 配置文件参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252disabled_plugins = []imports = []oom_score = 0plugin_dir = &quot;&quot;required_plugins = []root = &quot;/var/lib/containerd&quot;state = &quot;/run/containerd&quot;temp = &quot;&quot;version = 2[cgroup] path = &quot;&quot;[debug] address = &quot;&quot; format = &quot;&quot; gid = 0 level = &quot;&quot; uid = 0[grpc] address = &quot;/run/containerd/containerd.sock&quot; gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = &quot;&quot; tcp_tls_ca = &quot;&quot; tcp_tls_cert = &quot;&quot; tcp_tls_key = &quot;&quot; uid = 0[metrics] address = &quot;&quot; grpc_histogram = false[plugins] [plugins.&quot;io.containerd.gc.v1.scheduler&quot;] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = &quot;0s&quot; startup_delay = &quot;100ms&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;] device_ownership_from_security_context = false disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true enable_selinux = false enable_tls_streaming = false enable_unprivileged_icmp = false enable_unprivileged_ports = false ignore_image_defined_volumes = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.9&quot; selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = &quot;4h0m0s&quot; stream_server_address = &quot;127.0.0.1&quot; stream_server_port = &quot;0&quot; systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni] bin_dir = &quot;/opt/cni/bin&quot; conf_dir = &quot;/etc/cni/net.d&quot; conf_template = &quot;&quot; ip_pref = &quot;&quot; max_conf_num = 1 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd] default_runtime_name = &quot;runc&quot; disable_snapshot_annotations = true discard_unpacked_layers = false ignore_rdt_not_enabled_errors = false no_pivot = false snapshotter = &quot;overlayfs&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime.options] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;io.containerd.runc.v2&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options] BinaryName = &quot;&quot; CriuImagePath = &quot;&quot; CriuPath = &quot;&quot; CriuWorkPath = &quot;&quot; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = &quot;&quot; ShimCgroup = &quot;&quot; SystemdCgroup = true [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime.options] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.image_decryption] key_model = &quot;node&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] config_path = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.auths] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.headers] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.2.129:5000&quot;] endpoint = [&quot;http://192.168.2.129:5000&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.x509_key_pair_streaming] tls_cert_file = &quot;&quot; tls_key_file = &quot;&quot; [plugins.&quot;io.containerd.internal.v1.opt&quot;] path = &quot;/opt/containerd&quot; [plugins.&quot;io.containerd.internal.v1.restart&quot;] interval = &quot;10s&quot; [plugins.&quot;io.containerd.internal.v1.tracing&quot;] sampling_ratio = 1.0 service_name = &quot;containerd&quot; [plugins.&quot;io.containerd.metadata.v1.bolt&quot;] content_sharing_policy = &quot;shared&quot; [plugins.&quot;io.containerd.monitor.v1.cgroups&quot;] no_prometheus = false [plugins.&quot;io.containerd.runtime.v1.linux&quot;] no_shim = false runtime = &quot;runc&quot; runtime_root = &quot;&quot; shim = &quot;containerd-shim&quot; shim_debug = false [plugins.&quot;io.containerd.runtime.v2.task&quot;] platforms = [&quot;linux/amd64&quot;] sched_core = false [plugins.&quot;io.containerd.service.v1.diff-service&quot;] default = [&quot;walking&quot;] [plugins.&quot;io.containerd.service.v1.tasks-service&quot;] rdt_config_file = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.aufs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.btrfs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.devmapper&quot;] async_remove = false base_image_size = &quot;&quot; discard_blocks = false fs_options = &quot;&quot; fs_type = &quot;&quot; pool_name = &quot;&quot; root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.native&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.overlayfs&quot;] root_path = &quot;&quot; upperdir_label = false [plugins.&quot;io.containerd.snapshotter.v1.zfs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.tracing.processor.v1.otlp&quot;] endpoint = &quot;&quot; insecure = false protocol = &quot;&quot;[proxy_plugins][stream_processors] [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar&quot;] accepts = [&quot;application/vnd.oci.image.layer.v1.tar+encrypted&quot;] args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;] env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;] path = &quot;ctd-decoder&quot; returns = &quot;application/vnd.oci.image.layer.v1.tar&quot; [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar.gzip&quot;] accepts = [&quot;application/vnd.oci.image.layer.v1.tar+gzip+encrypted&quot;] args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;] env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;] path = &quot;ctd-decoder&quot; returns = &quot;application/vnd.oci.image.layer.v1.tar+gzip&quot;[timeouts] &quot;io.containerd.timeout.bolt.open&quot; = &quot;0s&quot; &quot;io.containerd.timeout.shim.cleanup&quot; = &quot;5s&quot; &quot;io.containerd.timeout.shim.load&quot; = &quot;5s&quot; &quot;io.containerd.timeout.shim.shutdown&quot; = &quot;3s&quot; &quot;io.containerd.timeout.task.state&quot; = &quot;2s&quot;[ttrpc] address = &quot;&quot; gid = 0 uid = 0 参考资料 官方文档 安装手册","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"CentOS 内核更新","slug":"linux/centos-kernel","date":"2022-12-15T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/centos-kernel/","permalink":"https://wangqian0306.github.io/2022/centos-kernel/","excerpt":"","text":"CentOS 内核更新 操作方式 查看当前内核版本： 1uname -mrs 更新软件包： 1yum update -y 引入内核更新相关依赖包： 12rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 注：此处样例引入的是 CentOS 7 查看更新包中的内核： 1yum list available -disablerepo=&#x27;*&#x27; -enablerepo=elrepo-kernel 安装最新稳定版内核： 1yum -enablerepo=elrepo-kernel install kernel-lt 设置默认内核为最新版： 1grub2-set-default 0 生成内核启动文件： 1grub2-mkconfig -o /boot/grub2/grub.cfg 重启之后检查内核版本即可 常见问题 pstore: unknown compression: deflate 开机报错，但是 ssh 可以正常链接，可以使用如下命令 1vim /etc/default/grub 参照如下内容新增 mgag200.modeset=0 配置: 1234567GRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &#x27;s, release .*$,,g&#x27; /etc/system-release)&quot;GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet mgag200.modeset=0&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot; 重新生成 grub 1grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg 设置默认内核为最新版： 1grub2-set-default 0 参考资料 How to Upgrade Linux Kernel on CentOS 7 centos7更换内核后出现 pstore: unknown compression: deflate 问题解决","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"CentOS","slug":"CentOS","permalink":"https://wangqian0306.github.io/tags/CentOS/"}]},{"title":"Spring Data Rest","slug":"spring/rest","date":"2022-12-12T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spring-data-rest/","permalink":"https://wangqian0306.github.io/2022/spring-data-rest/","excerpt":"","text":"Spring Data Rest 简介 Spring Data Rest 可以轻松地在 Spring Data 存储库之上构建 REST Web 服务。 使用方式 引入依赖包： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-rest&lt;/artifactId&gt;&lt;/dependency&gt; 或 123dependencies &#123; compile(&quot;org.springframework.boot:spring-boot-starter-data-rest&quot;)&#125; 配置入口 123456789101112spring: data: rest: base-path: /api datasource: url: $&#123;MYSQL_URI:jdbc:mysql://xxx.xxx.xxx.xxx:3306/xxx&#125; driver-class-name: $&#123;JDBC_DRIVER:com.mysql.cj.jdbc.Driver&#125; username: $&#123;MYSQL_USERNAME:xxxx&#125; password: $&#123;MYSQL_PASSWORD:xxxx&#125; jpa: hibernate: ddl-auto: update 编写实体类 12345678910111213141516171819202122232425262728293031323334import jakarta.persistence.*;import lombok.*;import org.hibernate.Hibernate;import java.util.Objects;@Getter@Setter@ToString@RequiredArgsConstructor@Entity@Table(name = &quot;USER&quot;)public class User &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || Hibernate.getClass(this) != Hibernate.getClass(o)) return false; User user = (User) o; return id != null &amp;&amp; Objects.equals(id, user.id); &#125; @Override public int hashCode() &#123; return getClass().hashCode(); &#125;&#125; 编写 Repository 1234567891011121314import org.springframework.data.domain.Pageable;import org.springframework.data.jpa.repository.JpaSpecificationExecutor;import org.springframework.data.repository.CrudRepository;import org.springframework.data.repository.query.Param;import org.springframework.data.rest.core.annotation.RepositoryRestResource;import java.util.List;@RepositoryRestResource(collectionResourceRel = &quot;user&quot;, path = &quot;user&quot;)public interface UserRepository extends JpaSpecificationExecutor&lt;User&gt;, CrudRepository&lt;User, Long&gt; &#123; List&lt;User&gt; findByNameLike(@Param(&quot;name&quot;) String name, Pageable pageable);&#125; 运行项目 request12345678910111213141516171819202122232425262728293031### infoGET http://localhost:8080/api### listGET http://localhost:8080/api/user### insertPOST http://localhost:8080/api/userContent-Type: application/json&#123; &quot;name&quot;: &quot;demo&quot;&#125;### updatePOST http://localhost:8080/api/userContent-Type: application/json&#123; &quot;id&quot;: 1, &quot;name&quot; :&quot;update&quot;&#125;### deleteDELETE http://localhost:8080/api/user/1### search interface listGET http://localhost:8080/api/user/search### searchByNameGET http://localhost:8080/api/user/search/findByName?name=demo 使用关联表： 注：设顶或更新关联字段为 URL 即可，例如 /&#123;id&#125;。但是如果输入 id 不存在则会跳过该字段，不会报错。如需报错需要设置改字段为必填。 1234567891011121314151617181920import jakarta.persistence.*;import lombok.Getter;import lombok.Setter;@Getter@Setter@Entity(name = &quot;Phone&quot;)public class Phone &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String number; @ManyToOne(optional = false) @JoinColumn(name = &quot;person_id&quot;, nullable = false, foreignKey = @ForeignKey(name = &quot;PERSON_ID_FK&quot;)) private Person person;&#125; 参考资料 官方文档 Accessing JPA Data with REST","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"ksqlDB","slug":"bigdata/ksqldb","date":"2022-12-09T14:43:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/ksqldb/","permalink":"https://wangqian0306.github.io/2022/ksqldb/","excerpt":"","text":"ksqlDB 简介 ksqlDB 是专门为流处理应用程序构建的数据库。使用它的好处在于依赖项较少，仅仅需要 Kafka 且构建应用较为方便。 可以大致这样理解其关键元素： Stream：结构化的 kafka topic Table：物化视图(方便查询或存储中间结果) Source：数据源(Kafka Connect) Sink：数据输出(Kafka Connect) 注：Kafka Connect 可以嵌入 ksqlDB 与其一同部署，也可以独立部署。 部署和安装 Docker 方式 编写 docker-compose.yaml： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# Docker compose from bringing up a local ksqlDB cluster and dependencies.## By default, the cluster has two ksqlDB servers. You can scale the number of ksqlDB nodes in the# cluster by using the docker `--scale` command line arg.## e.g. for a 4 node cluster run:# &gt; docker-compose up --scale additional-ksqldb-server=3## or a 1 node cluster run:# &gt; docker-compose up --scale additional-ksqldb-server=0## The default is one `primary-ksqldb-server` and one `additional-ksqdb-server`. The only# difference is that the primary node has a well-known port exposed so clients can connect, where# as the additional nodes use auto-port assignment so that ports don&#x27;t clash.## If you wish to run with locally built ksqlDB docker images then:## 1. Follow the steps in https://github.com/confluentinc/ksql/blob/master/ksqldb-docker/README.md# to build a ksqlDB docker image with local changes.## 2. Update .env file to use your local images by setting KSQL_IMAGE_BASE=placeholder/ and KSQL_VERSION=local.build.---services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 32181 ZOOKEEPER_TICK_TIME: 2000 kafka: image: confluentinc/cp-enterprise-kafka:latest ports: - &quot;29092:29092&quot; depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100 schema-registry: image: confluentinc/cp-schema-registry:latest depends_on: - zookeeper - kafka environment: SCHEMA_REGISTRY_HOST_NAME: schema-registry SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092 primary-ksqldb-server: image: confluentinc/ksqldb-server:latest hostname: primary-ksqldb-server container_name: primary-ksqldb-server depends_on: - kafka - schema-registry ports: - &quot;8088:8088&quot; environment: KSQL_LISTENERS: http://0.0.0.0:8088 KSQL_BOOTSTRAP_SERVERS: kafka:9092 KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081 KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: &quot;true&quot; KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: &quot;true&quot; additional-ksqldb-server: image: confluentinc/ksqldb-server:latest hostname: additional-ksqldb-server depends_on: - primary-ksqldb-server ports: - &quot;8090&quot; environment: KSQL_LISTENERS: http://0.0.0.0:8090 KSQL_BOOTSTRAP_SERVERS: kafka:9092 KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Access the cli by running: # &gt; docker-compose exec ksqldb-cli ksql http://primary-ksqldb-server:8088 ksqldb-cli: image: confluentinc/ksqldb-cli:latest container_name: ksqldb-cli depends_on: - primary-ksqldb-server entrypoint: /bin/sh tty: true 然后使用如下命令运行服务： 1docker-compose up -d --scale additional-ksqldb-server=&lt;num&gt; 注：num 可以设置为 0。 使用如下命令可以进入交互式命令行： 1docker-compose exec ksqldb-cli ksql http://primary-ksqldb-server:8088 基本命令 Topic 列表 1SHOW TOPICS; Stream 列表： 1SHOW STREAMS; Table 列表： 1SHOW TABLES; 显示详情： 123DESCRIBE &lt;name&gt;;-- Describe &lt;name&gt; in detail:DESCRIBE EXTENDED &lt;name&gt;; 查看 Topic 内容： 1PRINT &#x27;&lt;topic_name&gt;&#x27; FROM BEGINNING; 插入数据： 1INSERT INTO s1 (x, y, z) VALUES (0, 1, 2); 检索数据： 1SELECT SUBSTRING(str, 1, 10) FROM s1 EMIT CHANGES; 查看 Connector： 1SHOW CONNECTORS; 查看 Connector 详情： 1DESCRIBE CONNECTOR conn1; 创建 Source： 12345678910CREATE SOURCE CONNECTOR jdbc_source WITH ( &#x27;connector.class&#x27; = &#x27;io.confluent.connect.jdbc.JdbcSourceConnector&#x27;, &#x27;connection.url&#x27; = &#x27;jdbc:postgresql://localhost:5432/postgres&#x27;, &#x27;connection.user&#x27; = &#x27;user&#x27;, &#x27;topic.prefix&#x27; = &#x27;jdbc_&#x27;, &#x27;table.whitelist&#x27; = &#x27;include_this_table&#x27;, &#x27;mode&#x27; = &#x27;incrementing&#x27;, &#x27;numeric.mapping&#x27; = &#x27;best_fit&#x27;, &#x27;incrementing.column.name&#x27; = &#x27;id&#x27;, &#x27;key&#x27; = &#x27;id&#x27;); 创建 Sink： 12345678CREATE SINK CONNECTOR elasticsearch_sink WITH ( &#x27;connector.class&#x27; = &#x27;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&#x27;, &#x27;key.converter&#x27; = &#x27;org.apache.kafka.connect.storage.StringConverter&#x27;, &#x27;topics&#x27; = &#x27;send_these_topics_to_elasticsearch&#x27;, &#x27;key.ignore&#x27; = &#x27;true&#x27;, &#x27;schema.ignore&#x27; = &#x27;true&#x27;, &#x27;type.name&#x27; = &#x27;&#x27;, &#x27;connection.url&#x27; = &#x27;http://localhost:9200&#x27;); 终止持久查询： 1TERMINATE q1; 删除 Stream： 1DROP STREAM s1; 删除 Table: 1DROP TABLE t1; 参考资料 官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"ksqlDB","slug":"ksqlDB","permalink":"https://wangqian0306.github.io/tags/ksqlDB/"}]},{"title":"Kafka 中的增量协作重平衡","slug":"reference/incremental-cooperative-rebalancing-in-kafka","date":"2022-12-07T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/incremental-cooperative-rebalancing-in-kafka/","permalink":"https://wangqian0306.github.io/2022/incremental-cooperative-rebalancing-in-kafka/","excerpt":"","text":"Kafka 中的增量协作重平衡 简介 注：本文是 Incremental Cooperative Rebalancing in Apache Kafka: Why Stop the World When You Can Change It? 的摘录。 负载平衡和调度是每个分布式系统的核心，Apache Kafka 也不例外。Kafka 客户端——特别是 Kafka 消费者、Kafka Connect 和 Kafka Streams，这是本文的重点——从一开始就使用了一种复杂的、典型的方式来平衡资源。 按照分布式系统中的常见做法，Kafka 客户端使用组管理 API 来构建协作的客户端进程。客户端合并为组的能力来源于 Broker，它会协调客户端进组。至此 Broker 的任务就结束了。在设计中，组内的成员关系是 Broker 作为协调者所得到关于客户端的全部信息。 客户端之间的实际负载分配发生在它们之间，而不会给 Kafka Broker 带来额外的责任。客户端的负载平衡取决于组内领导客户端进程的选择以及只有客户端知道如何解释协议的定义。该协议搭载在组管理的协议中，因此称为嵌入式协议。 到目前为止，消费者、Connect 和 Streams 应用程序使用的嵌入式协议是再平衡协议，其目的是在组内有效地分配资源(用于使用记录的 Kafka 分区、Connector 任务等)。在 Kafka 组管理 API 中定义的嵌入式协议并不只是为了重平衡。这样使用嵌入式协议的方式是任何类型的分布式进程相互协调并实现其自定义逻辑的通用方式，而无需 Kafka 代理的代码知道它们的存在。 将负载平衡算法嵌入到组管理协议本身中提供了一些明显的优势： 自治：客户端可以独立于 Kafka 代理升级或自定义其负载平衡算法。 隔离：Kafka Broker 支持通用组成员 API，负载平衡的详细信息留给客户端。这简化了 Broker 的代码，并使客户端能够随意丰富其负载平衡策略。 更轻松的多租户：对于 Kafka 客户端（如 Kafka Connect），它们在其实例之间平衡异构资源，并且可能属于不同的用户，将此信息抽象并嵌入到重新平衡协议中，使多租户更容易在客户端级别处理。在这种情况下，多租户并不是 Broker 必须担心的另一个功能。 为了简单起见，到目前为止，所有重新平衡协议都是围绕相同的简单原则构建的：每当需要在客户端之间分配负载时，就会开始新一轮的重新平衡，在此期间，所有进程都会释放其资源。在此阶段(重新确认组成员资格并选举组的领导者)结束时，每个客户端都会被分配一组新的资源。简而言之，这也被称为停止世界再平衡(stop-the-world)，这个短语可以追溯到垃圾收集文献。 停止世界再平衡的挑战 在每次重新平衡中停止世界的负载平衡算法具有某些限制，从这些越来越显着的案例中可以看出： 纵向扩展和缩减：不出所料，在重新平衡时停止世界的影响与参与过程中平衡的资源数量有关。例如，在空的 Connect 群集中启动 10 个连接任务不同于在运行 100 个现有连接任务的群集中启动相同数量的任务。 异构负载下的多租户：这里的主要示例是 Kafka Connect。当另一个连接器(可能来自其他用户)添加到群集时，停止连接器任务的副作用不仅是不希望的，而且会造成大规模破坏。 Kubernetes 进程死亡：无论是在云中还是在本地，故障都不罕见。当一个节点发生故障时，另一个节点会迅速替换它，尤其是在使用 Kubernetes 等业务流程协调程序时。理想情况下，一组 Kafka 客户端将能够吸收这种暂时的资源损失，而无需执行完全的重新平衡。节点返回后，先前分配的资源将立即分配给它。 滚动反弹：间歇性中断不仅仅是由于环境因素而偶然发生的。也可以有意将它们安排为计划升级的一部分。但是，应避免完全重新分配资源，因为缩减只是暂时的。 尽管有解决方法来适应这些用例，例如将客户端拆分为更小的组或增加与重新平衡相关的超时(这些超时往往不太灵活)，但很明显，停止世界重新平衡需要用破坏性较小的方法取代。 增量合作再平衡 在 Kafka 社区中获得关注并旨在减轻当前 Eager Rebalancing 协议在大型 Kafka 客户端集群中表现出的再平衡影响的主张是增量合作再平衡。 这种新的再平衡算法的关键思想是： 完整的全局负载均衡不需要在一轮再均衡中完成。相反，如果客户端在连续几次重新平衡后很快收敛到平衡负载状态就足够了。 世界不应该被停止。不需要转手的资源不应停止使用。 当然，这些原则适合于 Kafka 客户端中改进的再平衡协议的命名。新的重新平衡是： 增量，因为分阶段达到重新平衡的最终期望状态。不必在每一轮再平衡结束时达到全球平衡的最终状态。可以使用少量连续的再平衡轮次，以使 Kafka 客户端组收敛到所需的平衡资源状态。此外，您可以配置宽限期，以允许离开的成员返回并重新获得其先前分配的资源。 合作，因为要求组中的每个进程自愿释放需要重新分配的资源。然后，如果被要求释放这些资源的客户端按时释放这些资源，则可以重新安排这些资源。 Kafka Connect 中的实现- Connect tasks 是新的线程 第一个提供增量合作再平衡协议的 Kafka 客户端是 Kafka Connect。在 Kafka Connect 中，在工作线程之间平衡的资源是连接器及其任务。连接器是一种特殊组件，主要执行与外部数据系统的协调和簿记，并充当 Kafka 记录的源或接收器。连接任务(Connect tasks)是执行实际数据传输的单元。 尽管 Connect 任务通常不会在本地存储状态，并且可以在从 Kafka 恢复状态后快速停止和恢复执行，但在每次重新平衡中停止世界可能会导致明显的延迟。在某些情况下（也称为再平衡风暴），它可能会使集群进入连续重新平衡的状态，并且 Connect 集群可能需要几分钟才能稳定下来。在增量合作再平衡之前，由于重新平衡延迟，集群可以托管的 Connect 任务数通常限制在实际容量以下，从而给人一种错误的印象，即 Connect 任务是现成的重量级实体。 使用增量协作再平衡，连接任务可以是它一直以来的用途：一个运行时执行线程，它是轻量级的，可以在 Connect 集群中的任何位置快速全局调度。 调度这些轻量级实体(可能基于特定于 Kafka Connect 的信息，例如连接器类型、所有者或任务大小等)为 Connect 提供了理想的灵活性，而不会过度扩展其职责。配置和部署工作线程是 Connect 集群的主要工具，仍然是正在使用的编排器(即 Kubernetes 或类似的基础架构)的责任。 现在让我们来看看当我们需要在 Kafka Connect 集群中重新平衡连接器和任务时会发生什么。 新节点加入(图1)。在第一次重新平衡期间，领导者(Worker1)会计算一个新的全局分配，导致每个现有工作线程(Worker1 和 Worker2)撤销一项任务。由于第一轮重新平衡包括任务吊销，因此第一次重新平衡之后会立即进行第二次重新平衡，在此期间，吊销的任务将分配给组的新成员(Worker3)。在两次重新平衡期间，未受影响的任务将继续运行而不会中断。 现有节点断开(图2)。在此方案中，节点(Worker2)离开组。它的离开触发了重新平衡。在此再平衡轮次中，领导者(Worker1)检测到与上一个分配相比缺少一个连接器和三个任务。这将启用由配置属性控制的计划的重新平衡延迟(默认情况下，它等于五分钟)。 现有节点离开(图3)。此方案与前一个方案相同，只是此处的节点(Worker2)断开且没有及时重新加入组。在这种情况下，其任务（一个连接器和三个任务）在等于 scheduled.rebalance.max.delay.ms 的时间内保持未分配状态。之后，剩余的两个节点(Worker1 和Worker3)重新加入组，领导者将计划再平衡延迟期间未计算的任务重新分配给现有的活动节点集(Worker1 和Worker3)。 参考资料 Incremental Cooperative Rebalancing in Apache Kafka: Why Stop the World When You Can Change It? THE MAGICAL REBALANCE PROTOCOL OF APACHE KAFKA","categories":[{"name":"参考资料","slug":"参考资料","permalink":"https://wangqian0306.github.io/categories/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"}],"tags":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"}]},{"title":"TSDS","slug":"database/tsds","date":"2022-12-05T15:09:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/tsds/","permalink":"https://wangqian0306.github.io/2022/tsds/","excerpt":"","text":"TSDS 简介 时序数据流(time series data stream,TSDS)是将带有时间戳的指标数据建模为一个或多个时间序列的一种存储方式，相较于普通的存储方式来说更节省存储空间。 注：此功能暂时处于试用阶段。 使用方式 创建生命周期 123456789101112131415161718192021222324252627282930313233343536PUT _ilm/policy/demo-sensor-lifecycle-policy&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123; &quot;max_age&quot;: &quot;1d&quot;, &quot;max_primary_shard_size&quot;: &quot;50gb&quot; &#125; &#125; &#125;, &quot;warm&quot;: &#123; &quot;min_age&quot;: &quot;30d&quot;, &quot;actions&quot;: &#123; &quot;shrink&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;forcemerge&quot;: &#123; &quot;max_num_segments&quot;: 1 &#125; &#125; &#125;, &quot;cold&quot;: &#123; &quot;min_age&quot;: &quot;60d&quot;, &quot;actions&quot;: &#123;&#125; &#125;, &quot;delete&quot;: &#123; &quot;min_age&quot;: &quot;735d&quot;, &quot;actions&quot;: &#123; &quot;delete&quot;: &#123;&#125; &#125; &#125; &#125; &#125;&#125; 注：官方网站中还设置了快照，如有需求可以进行参照。 创建模板 123456789101112131415161718192021222324252627282930313233343536373839PUT _component_template/demo-sensor-mappings&#123; &quot;template&quot;: &#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;date_optional_time||epoch_millis&quot; &#125;, &quot;sensor_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;time_series_dimension&quot;: true &#125;, &quot;temperature&quot;: &#123; &quot;type&quot;: &quot;half_float&quot;, &quot;time_series_metric&quot;: &quot;gauge&quot; &#125; &#125; &#125; &#125;, &quot;_meta&quot;: &#123; &quot;description&quot;: &quot;Mappings for @timestamp and sensor data&quot; &#125;&#125;# Creates a component template for index settingsPUT _component_template/demo-sensor-settings&#123; &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;index.lifecycle.name&quot;: &quot;demo-sensor-lifecycle-policy&quot;, &quot;index.look_ahead_time&quot;: &quot;3h&quot;, &quot;index.codec&quot;: &quot;best_compression&quot; &#125; &#125;, &quot;_meta&quot;: &#123; &quot;description&quot;: &quot;Index settings for weather sensor data&quot; &#125;&#125; 创建索引模板 12345678910111213141516PUT _index_template/demo-sensor-index-template&#123; &quot;index_patterns&quot;: [&quot;demo-sensor*&quot;], &quot;data_stream&quot;: &#123; &#125;, &quot;template&quot;: &#123; &quot;settings&quot;: &#123; &quot;index.mode&quot;: &quot;time_series&quot;, &quot;index.routing_path&quot;: [&quot;sensor_id&quot;] &#125; &#125;, &quot;composed_of&quot;: [ &quot;demo-sensor-mappings&quot;, &quot;demo-sensor-settings&quot;], &quot;priority&quot;: 500, &quot;_meta&quot;: &#123; &quot;description&quot;: &quot;Template for my weather sensor data&quot; &#125;&#125; 插入数据： 12345678POST demo-sensor-dev/_doc&#123; &quot;@timestamp&quot;: &quot;2022-05-06T16:21:15.000Z&quot;, &quot;sensor_id&quot;: &quot;SYKENET-000001&quot;, &quot;location&quot;: &quot;swamp&quot;, &quot;temperature&quot;: 32.4, &quot;humidity&quot;: 88.9&#125; 参考资料 TSDS 官方文档","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wangqian0306.github.io/tags/Elasticsearch/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"}]},{"title":"压缩文件","slug":"linux/compress","date":"2022-12-01T12:04:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/compress/","permalink":"https://wangqian0306.github.io/2022/compress/","excerpt":"","text":"压缩文件 zip 压缩文件 1zip -r &lt;name&gt;.zip /&lt;path&gt; 解压文件 1unzip &lt;name&gt;.zip tar 压缩文件 1tar -zcvf &lt;name&gt;.tar.gz /&lt;path&gt; 解压文件 1tar -zxvf &lt;name&gt;.tar.gz","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"cron","slug":"linux/cron","date":"2022-12-01T12:04:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/cron/","permalink":"https://wangqian0306.github.io/2022/cron/","excerpt":"","text":"cron 简介 在Linux系统中，计划任务一般是由 cron 承担。在cron启动后，它会读取它的所有配置文件(全局性配置文件 /etc/crontab，以及每个用户的计划任务配置文件)，然后 cron 会根据命令和执行时间来按时来调用度工作任务。 安装和使用 安装软件 1yum install cronie -y 启动服务 1systemctl enable crond --now cron 表达式说明 12345678# ┌───────────── minute (0 - 59)# │ ┌───────────── hour (0 - 23)# │ │ ┌───────────── day of the month (1 - 31)# │ │ │ ┌───────────── month (1 - 12)# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;# │ │ │ │ │ 7 is also Sunday on some systems)# │ │ │ │ │# * * * * * &lt;command to execute&gt; 查看用户的定时任务 1corntab -l 编辑用户的定时任务 1crontab -e 删除用户的定时任务 1crontab -r 容器中使用 编写 hello-cron 脚本： 12* * * * * echo &quot;Hello world&quot; &gt;&gt; /var/log/cron.log 2&gt;&amp;1 注：此处需要使用 Linux 换行符且最后一行要是空行。 编写 Dockerfile 即可： 12345678910111213FROM ubuntu:latestRUN apt-get update &amp;&amp; apt-get -y install cronCOPY hello-cron /etc/cron.d/hello-cronRUN chmod 0644 /etc/cron.d/hello-cronRUN crontab /etc/cron.d/hello-cronRUN touch /var/log/cron.logCMD cron &amp;&amp; tail -f /var/log/cron.log 参考资料 维基百科 cron 在线Cron表达式生成器 How to run a cron job inside a docker container","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Spring Validation","slug":"spring/valid","date":"2022-11-28T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/valid/","permalink":"https://wangqian0306.github.io/2022/valid/","excerpt":"","text":"Spring Validation 简介 Spring 框架官方提供的参数检验方式，通过对 hibernate validation 的二次封装而实现的。 使用方式 引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt;&lt;/dependency&gt; 基本功能注解清单： 注解 简介 @Null 验证元素必须为空 @NotNull 验证元素必须不为空 @NotBlank 验证元素必须不为空且至少有一个字符 @NotNull 验证元素必须不为空 @NotEmpty 验证元素必须不为空且至少有一个元素 @Min 验证元素必须是数字且必须小于等于指定值 @Max 验证元素必须是数字且必须大于等于指定值 @Past 被注解的元索必须是一个过去的时间 @PastOrPresent 被注解的元索必须是一个过去或当前的时间 @Future 验证元素必须是一个将来的时间 @FutureOrPresent 验证元素必须是一个将来或当前的时间 @Pattern 验证元素必须符合给定的正则表达式 @AssertTure 验证元素必须为ture @AssertFalse 验证元素必须为false @DecimalMax 验证小数元素最大数值 @DecimalMin 验证小数元素最小数值 @Digits 验证元素整数和小数位数 @Negative 验证元素必须是负数 @NegativeOrZero 验证元素必须是负数或 0 @Positive 验证元素必须是正数 @PositiveOrZero 验证元素必须是正数或 0 @Email 验证元素必须是email地址 额外功能注解清单： 注解 简介 @Length 验证元素必须在指定的范围内 @NotEmpty 验证元素是必须 @Range 验证元素可以是数字或者是数字的字符串必须在指定的范围内 @URL 验证元素必须是一个URL @CreditCardNumber 验证元素必须是一个合规的信用卡号 @DurationMax 验证元素最大的周期 @DurationMin 验证元素最小的周期 @EAN 验证元素为 EAN 条形码 @ISBN 验证元素为 ISBN 书号 @UniqueElements 验证元素是否是唯一的 @UUID 验证元素为 UUID 样例代码： 12345678910111213import lombok.Data;import javax.validation.constraints.NotBlank;import javax.validation.constraints.NotNull;@Datapublic class DemoRole &#123; @NotNull @NotBlank private String role;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import lombok.Data;import org.hibernate.validator.constraints.Length;import javax.validation.Valid;import javax.validation.constraints.*;import java.util.Date;import java.util.List;@Datapublic class DemoUser &#123; @NotNull(groups = &#123;Update.class&#125;) private String code; @NotNull(groups = &#123;Insert.class&#125;) @Length(min = 2, max = 10) private String name; @NotBlank(message = &quot;手机号码不能为空&quot;, groups = &#123;Insert.class&#125;) @NotNull(message = &quot;手机号不能为空&quot;, groups = &#123;Insert.class&#125;) @Length(min = 11, max = 11, message = &quot;手机号只能为11位&quot;) private String phone; @Email(groups = &#123;Insert.class&#125;) private String email; @Past(groups = &#123;Insert.class&#125;) @NotNull(groups = &#123;Insert.class&#125;) private Date birth; @Valid @Size(min = 1, groups = &#123;Insert.class&#125;) @NotNull(groups = &#123;Insert.class&#125;) private List&lt;DemoRole&gt; roleList; public interface Insert &#123; &#125; public interface Update &#123; &#125;&#125; 1234567891011121314151617181920import lombok.extern.slf4j.Slf4j;import org.springframework.validation.annotation.Validated;import org.springframework.web.bind.annotation.*;@Slf4j@RestController@RequestMapping(&quot;/demo&quot;)public class DemoController &#123; @PutMapping public DemoUser insert(@RequestBody @Validated(DemoUser.Insert.class) DemoUser demoUser) &#123; return demoUser; &#125; @PostMapping public DemoUser update(@RequestBody @Validated(DemoUser.Update.class) DemoUser demoUser) &#123; return demoUser; &#125;&#125; 自定义校验注解： 12345678910111213141516171819202122import javax.validation.Constraint;import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import javax.validation.Payload;import static java.lang.annotation.ElementType.*;import static java.lang.annotation.RetentionPolicy.RUNTIME;@Target(&#123;METHOD,FIELD,ANNOTATION_TYPE,CONSTRUCTOR,PARAMETER&#125;)@Retention(RUNTIME)@Documented@Constraint(validatedBy=&#123;CustomIdValidator.class&#125;)public @interface CustomId &#123; String message() default &quot;CustomId not valid&quot;; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 1234567891011121314151617import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class CustomIdValidator implements ConstraintValidator&lt;CustomId, String&gt; &#123; @Override public void initialize(CustomId constraintAnnotation) &#123; ConstraintValidator.super.initialize(constraintAnnotation); &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; // valid function return false; &#125;&#125; 在 Controller 中校验： 1234567891011121314151617181920212223242526272829303132import com.example.demo.request.DemoUser;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.PutMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import jakarta.annotation.Resource;import javax.validation.ConstraintViolation;import javax.validation.ConstraintViolationException;import javax.validation.Validator;import java.util.Set;@Slf4j@RestController@RequestMapping(&quot;/demo&quot;)public class DemoController &#123; @Resource private Validator validator; @PutMapping public DemoUser update(@RequestBody DemoUser demoUser) &#123; Set&lt;ConstraintViolation&lt;DemoUser&gt;&gt; validate = validator.validate(demoUser, DemoUser.Insert.class); if (validate.isEmpty()) &#123; // argument is valid return demoUser; &#125; else &#123; throw new ConstraintViolationException(validate); &#125; &#125;&#125; 注：此种方式不太适合原版异常的抛出形式。 参考资料 官方文档 hibernate validation 验证注解 SpringBoot 实现各种参数校验 Differences in @Valid and @Validated Annotations in Spring","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spring Integration FTP/FTPS Adapters","slug":"spring/ftp","date":"2022-11-24T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spring-integration-ftp/","permalink":"https://wangqian0306.github.io/2022/spring-integration-ftp/","excerpt":"","text":"Spring Integration FTP/FTPS Adapters 简介 Spring 官方为 FTP 和 FTPS 文件传输提供了插件。 使用方式 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt; &lt;artifactId&gt;spring-integration-ftp&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt;&lt;/dependency&gt; 编写链接配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import org.apache.commons.net.ftp.FTP;import org.apache.commons.net.ftp.FTPClient;import org.apache.commons.net.ftp.FTPFile;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.expression.ExpressionParser;import org.springframework.expression.spel.standard.SpelExpressionParser;import org.springframework.integration.file.remote.session.CachingSessionFactory;import org.springframework.integration.file.remote.session.SessionFactory;import org.springframework.integration.ftp.session.DefaultFtpSessionFactory;import org.springframework.integration.ftp.session.FtpRemoteFileTemplate;import org.springframework.integration.ftp.session.FtpSession;@Configurationpublic class FTPConfiguration &#123; @Bean public SessionFactory&lt;FTPFile&gt; ftpSessionFactory() &#123; DefaultFtpSessionFactory sf = new DefaultFtpSessionFactory() &#123; @Override public synchronized FtpSession getSession() &#123; return super.getSession(); &#125; &#125;; sf.setHost(&quot;xxx.xxx.xxx.xxx&quot;); sf.setPort(21); sf.setUsername(&quot;xxx&quot;); sf.setPassword(&quot;xxx&quot;); sf.setControlEncoding(&quot;UTF-8&quot;); sf.setFileType(FTP.BINARY_FILE_TYPE); sf.setClientMode(FTPClient.PASSIVE_LOCAL_DATA_CONNECTION_MODE); CachingSessionFactory&lt;FTPFile&gt; csf = new CachingSessionFactory&lt;&gt;(sf); csf.setPoolSize(1); return csf; &#125; @Bean public FtpRemoteFileTemplate ftpRemoteFileTemplate(@Qualifier(&quot;ftpSessionFactory&quot;) SessionFactory&lt;FTPFile&gt; sessionFactory) &#123; FtpRemoteFileTemplate ftpRemoteFileTemplate = new FtpRemoteFileTemplate(sessionFactory); ExpressionParser parser = new SpelExpressionParser(); ftpRemoteFileTemplate.setRemoteDirectoryExpression(parser.parseExpression(&quot;&#x27;&#x27;&quot;)); ftpRemoteFileTemplate.setExistsMode(FtpRemoteFileTemplate.ExistsMode.STAT); return ftpRemoteFileTemplate; &#125;&#125; 注：在使用匿名模式时将用户设为 “anonymous” 密码设为 “” 即可。 编写业务类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import lombok.extern.slf4j.Slf4j;import org.apache.commons.net.ftp.FTPClient;import org.apache.commons.net.ftp.FTPFile;import org.springframework.http.ResponseEntity;import org.springframework.integration.file.support.FileExistsMode;import org.springframework.integration.ftp.session.FtpRemoteFileTemplate;import org.springframework.messaging.support.GenericMessage;import org.springframework.util.FileCopyUtils;import org.springframework.web.bind.annotation.*;import org.springframework.web.multipart.MultipartFile;import jakarta.annotation.Resource;import javax.servlet.http.HttpServletResponse;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.OutputStream;import java.nio.file.FileSystems;import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;@Slf4j@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource FtpRemoteFileTemplate ftpRemoteFileTemplate; @GetMapping public ResponseEntity&lt;List&lt;FTPFile&gt;&gt; ls(@RequestParam String path) &#123; FTPFile[] result = ftpRemoteFileTemplate.list(path); return ResponseEntity.ok(Arrays.stream(result).collect(Collectors.toList())); &#125; @GetMapping public ResponseEntity&lt;Boolean&gt; check(@RequestParam String path) &#123; return ResponseEntity.ok(ftpRemoteFileTemplate.exists(path)); &#125; @GetMapping(&quot;/read&quot;) public ResponseEntity&lt;String&gt; read(@RequestParam String path) &#123; if (!ftpRemoteFileTemplate.exists(path)) &#123; throw new RuntimeException(&quot;file not exist&quot;); &#125; final ByteArrayOutputStream cache = new ByteArrayOutputStream(); boolean success = ftpRemoteFileTemplate.get(path, stream -&gt; FileCopyUtils.copy(stream, cache)); if (success) &#123; return ResponseEntity.ok(cache.toString()); &#125; else &#123; throw new RuntimeException(&quot;file not exist&quot;); &#125; &#125; @GetMapping(&quot;/download&quot;) public void download(@RequestParam String path, HttpServletResponse httpServletResponse) &#123; if (!ftpRemoteFileTemplate.exists(path)) &#123; throw new RuntimeException(&quot;file not exist&quot;); &#125; FTPFile file = ftpRemoteFileTemplate.list(path)[0]; if (file.isDirectory()) &#123; throw new RuntimeException(&quot;path is directory&quot;); &#125; httpServletResponse.reset(); httpServletResponse.setContentType(&quot;application/octet-stream;charset=utf-8&quot;); httpServletResponse.setHeader( &quot;Content-disposition&quot;, &quot;attachment; filename=&quot; + file.getName()); OutputStream out; try &#123; out = httpServletResponse.getOutputStream(); &#125; catch (IOException e) &#123; throw new RuntimeException(&quot;outputStream error&quot;); &#125; boolean success = ftpRemoteFileTemplate.get(path, stream -&gt; FileCopyUtils.copy(stream, out)); if (!success) &#123; throw new RuntimeException(&quot;copy error&quot;); &#125; &#125; @PostMapping public Boolean write(@RequestParam MultipartFile file, @RequestParam String path) &#123; String dest = FileSystems.getDefault().getPath(path, file.getOriginalFilename()).toString(); Session&lt;FTPFile&gt; session = ftpRemoteFileTemplate.getSession(); session.write(file.getInputStream(), dest); session.close(); return true; &#125; @PutMapping public Boolean mkdir(@RequestParam String path) &#123; try &#123; return ftpRemoteFileTemplate.getSession().mkdir(path); &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; &#125; @DeleteMapping public Boolean delete(@RequestParam String path) throws IOException &#123; if (!ftpRemoteFileTemplate.exists(path)) &#123; throw new RuntimeException(&quot;file or directory not exist&quot;); &#125; FTPFile[] files = ftpRemoteFileTemplate.list(path); if (files.length == 0) &#123; return ftpRemoteFileTemplate.getSession().rmdir(path); &#125; else if (files.length == 1) &#123; FTPClient ftpClient = (FTPClient) ftpRemoteFileTemplate.getSession().getClientInstance(); if (ftpClient.deleteFile(path)) &#123; return true; &#125; else &#123; throw new RuntimeException(&quot;directory not empty&quot;); &#125; &#125; else &#123; throw new RuntimeException(&quot;directory not empty&quot;); &#125; &#125;&#125; 注：此处代码并没有详细进行完善，使用时请注意。 编写测试类： 12345678910111213141516171819202122232425262728### mkdirPUT http://localhost:8080/test?path=demo### upload filePOST http://localhost:8080/testContent-Type: multipart/form-data; boundary=WebAppBoundary--WebAppBoundaryContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;demo.txt&quot;&lt; ./demo.txt--WebAppBoundary--Content-Disposition: form-data; name=&quot;path&quot;demo--WebAppBoundary--### lsGET http://localhost:8080/test?path=/### catGET http://localhost:8080/test/read?path=demo/demo.txt### downloadGET http://localhost:8080/test/download?path=demo/demo.txt### rmDELETE http://localhost:8080/test?path=demo 参考文档 官方手册","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"FTP","slug":"FTP","permalink":"https://wangqian0306.github.io/tags/FTP/"}]},{"title":"DataX","slug":"bigdata/datax","date":"2022-11-15T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/datax/","permalink":"https://wangqian0306.github.io/2022/datax/","excerpt":"","text":"DataX 简介 DataX 是阿里云 DataWorks数据集成 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS 等各种异构数据源之间高效的数据同步功能。 安装 前置条件 JDK 1.8+ Python 2+ 123wget https://datax-opensource.oss-cn-hangzhou.aliyuncs.com/&lt;version&gt;/datax.tar.gztar -zxvf datax.tar.gzcd datax 检测环境脚本： 1python ./bin/datax.py ./job/job.json 读取 MySQL 并存储至 Hive DataX 是通过写入 HDFS 文件的方式完成了写入 Hive 的功能，所以需要注意写入文件的位置和数据表的存储类型。 123456789use default;CREATE TABLE demo( id INT, username String) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27; STORED AS TEXTFILE; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;job&quot;: &#123; &quot;setting&quot;: &#123; &quot;speed&quot;: &#123; &quot;channel&quot;: 1 &#125;, &quot;errorLimit&quot;: &#123; &quot;record&quot;: 0, &quot;percentage&quot;: 0.02 &#125; &#125;, &quot;content&quot;: [ &#123; &quot;reader&quot;: &#123; &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: &#123; &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: &quot;xxxx&quot;, &quot;column&quot;: [ &quot;id&quot;, &quot;username&quot; ], &quot;connection&quot;: [ &#123; &quot;table&quot;: [ &quot;user&quot; ], &quot;jdbcUrl&quot;: [ &quot;jdbc:mysql://xxx.xxx.xxx.xxx:3306/test&quot; ] &#125; ] &#125; &#125;, &quot;writer&quot;: &#123; &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: &#123; &quot;defaultFS&quot;: &quot;hdfs://xxxx.xxxx.xxxx:8020&quot;, &quot;fileType&quot;: &quot;text&quot;, &quot;path&quot;: &quot;/user/hive/warehouse/demo&quot;, &quot;fileName&quot;: &quot;20221116&quot;, &quot;column&quot;: [ &#123; &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;INT&quot; &#125;, &#123; &quot;name&quot;: &quot;username&quot;, &quot;type&quot;: &quot;STRING&quot; &#125; ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\\t&quot;, &quot;haveKerberos&quot;: true, &quot;kerberosKeytabFilePath&quot;: &quot;/&lt;path&gt;/hive.keytab&quot;, &quot;kerberosPrincipal&quot;: &quot;hive/xxx&quot; &#125; &#125; &#125; ] &#125;&#125; 1python ./bin/datax.py &lt;path_to_json_file&gt; 参考资料 官方项目","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"DataX","slug":"DataX","permalink":"https://wangqian0306.github.io/tags/DataX/"}]},{"title":"DolphinScheduler","slug":"bigdata/dolphinscheduler","date":"2022-11-14T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/dolphin-scheduler/","permalink":"https://wangqian0306.github.io/2022/dolphin-scheduler/","excerpt":"","text":"DolphinScheduler 简介 Apache DolphinScheduler 提供了一个分布式且易于扩展的可视化工作流任务调度开源平台。适用于企业级场景。它提供了一种可视化操作任务、工作流和整个数据处理程序的解决方案。 Apache DolphinScheduler 旨在解决复杂的大数据任务依赖关系，并触发各种大数据应用的数据 OPS 编排中的关系。解决了数据研发 ETL 错综复杂的依赖关系，以及无法监控任务健康状态的问题。 DolphinScheduler 以有向无环图(DAG)流模式组装任务，可以及时监控任务的执行状态，并支持重试、指定节点恢复失败、暂停、恢复和终止任务等操作。 注：定时执行部分采用 Quartz 作为调度器。 安装及运行 单机容器试用 123456services: dolphin-scheduler-standalone-server: image: apache/dolphinscheduler-standalone-server:latest ports: - &quot;12345:12345&quot; - &quot;25333:25333&quot; 使用如下信息即可完成登录试用： 登录页面：http://localhost:12345/dolphinscheduler 默认用户名：admin 默认密码：dolphinscheduler123 容器化部署 在使用容器化部署的时候需要下载软件包，并在软件包中进入 apache-dolphinscheduler-&quot;$&#123;DOLPHINSCHEDULER_VERSION&#125;&quot;-src/deploy/docker 目录中并按照如下命令启动服务： 12345# 初始化数据库docker-compose --profile schema up -d# 启动所有服务docker-compose --profile all up -d 注：如有需求可以在 env 文件和 docker-compose.yml 中自行配置 Zookeeper 和 PostgresSQL。 参考资料 官方文档 架构解析","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"DolphinScheduler","slug":"DolphinScheduler","permalink":"https://wangqian0306.github.io/tags/DolphinScheduler/"}]},{"title":"Home Assistant 折腾教程","slug":"tmp/home-assistant","date":"2022-11-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/home-assistant/","permalink":"https://wangqian0306.github.io/2022/home-assistant/","excerpt":"","text":"Home Assistant 折腾教程 简介 Home Assistant 是一款开源的家庭自动化控制平台，主要针对于本地控制和隐私性。 安装方式 Home Assistant 可以通过四种方式进行安装： OS(系统级别安装) Container(独立容器) Core(使用 Python venv 手动安装) Supervised(在系统中安装软件，但是只支持 Debian 系统) 官方推荐的安装方式为： OS Container 不同的安装方式有不同的支持项： 功能 OS Container Core Supervised Automations ⭕ ⭕ ⭕ ⭕ Dashboards ⭕ ⭕ ⭕ ⭕ Integrations ⭕ ⭕ ⭕ ⭕ Blueprints ⭕ ⭕ ⭕ ⭕ Uses container ⭕ ⭕ ❌ ⭕ Supervisor ⭕ ❌ ❌ ⭕ Add-ons ⭕ ❌ ❌ ⭕ Backups ⭕ ⭕ ⭕ ⭕ Managed OS ⭕ ❌ ❌ ❌ 注：如果采用系统安装的方式则会遇到一些网络问题而且相对而言较为封闭，有很多系统级别的限制，不太适合在 HAOS 上运行其他的服务。 Supervised 方式安装记录 安装依赖 123456789101112apt install \\apparmor \\jq \\wget \\curl \\udisks2 \\libglib2.0-bin \\network-manager \\dbus \\lsb-release \\systemd-journal-remote \\systemd-resolved -y 安装 Docker 1curl -fsSL get.docker.com | sh 安装 OS-Agent 12wget https://github.com/home-assistant/os-agent/releases/download/&lt;version&gt;/os-agent_&lt;version&gt;_linux_x86_64.debdpkg -i os-agent_&lt;version&gt;_linux_x86_64.deb 注：具体版本号需要手动替换一下。 安装主程序包 12wget https://github.com/home-assistant/supervised-installer/releases/latest/download/homeassistant-supervised.debapt install ./homeassistant-supervised.deb 安装完成后需要进行一次重启 1reboot now 重启完成后访问 &lt;ip&gt;:8123 即可看到 homeassistant 的页面了。 VMware 安装方式记录 可以使用 VMware Workstation 先在本地进行测试安装和试用： 创建虚拟机然后选择稍后安装操作系统，指定虚拟磁盘为下载解压后的虚拟磁盘并启用 UEFI 启动方式即可。 测试安装文档 插件 HomeAssistant 还支持很多官方插件，可以控制智能家居或者安装一些常用的服务，例如： Jellyfin(本地媒体库) Xiaomi(小米插件) Yeelight(易来插件) Matter(Matter 协议服务器) 除此之外还可以使用 HACS(Home Assistant Community Store) 社区版本的应用商店来安装新的应用： Xiaomi Miot Auto(小米独有规范集成插件) SSH 在 Home Assistant OS 中需要安装 SSH 插件才能通过 SSH 的方式链接进入 HomeAssistant。插件安装和配置流程如下： 进入设置 选择Add-ons 插件 点击右下角的 ADD-ON STORE 检索 SSH 即可得到 Advanced SSH &amp; Web Terminal 插件 点击安装即可 在插件导航栏中选择 Configuration 配置栏 填入如下配置信息即可 12345678username: hassiopassword: xxxxxauthorized_keys: []sftp: falsecompatibility_mode: falseallow_agent_forwarding: falseallow_remote_port_forwarding: falseallow_tcp_forwarding: false 返回 Info 配置页 开启 Start on boot Show in sidebar 选项 注：如果想在 SSH 中使用 docker 命令还需关闭 Protection mode 选项。 HAOS 获取 Root 权限 在安装完成 Advanced SSH &amp; Web Terminal 插件后还需要如下操作才能获取到 Root 权限。 准备一个格式为 FAT, ext4 或 NTFS 格式的 U 盘，将其重命名为 CONFIG (注意大小写) 生成一个 ssh 公钥，并将其复制出来，写入一个换行符为 LF 文件编码为 ISO 8859-1 文件名为 authorized_keys 的文件将其放置在 U 盘中 将 U 盘从电脑上拔出，插入 Homeassistant 设备上 使用 ha os import 命令引入配置文件，之后即可使用如下命令 ssh 链接至 Homeassistant 了 1ssh root@homeassistant.local -p 22222 自动化设置 蓝图 样例 自定义面板 在 GitHub 上有很多的 Home-Assistant 仪表盘配置样例，可以美化面板。 例如： hass-config-lajv 教学视频地址如下： video 常见问题 DNS 服务 1.1.1.1:853 或 1.0.0.1:853 connection refused 首先使用如下命令确认自己的网络情况： 1ha resolution info 如果出现 unhealthy: [] 则证明当前网络环境是正常的，即可运行如下配置关闭 dns fallback 1ha dns options --fallback=false 注：关闭之后 hassio-dns 容器就会重新启动，且不会再遇到此报错了。 参考资料 官方文档 HACS 配置方式 Debugging the Home Assistant Operating System Supervised 安装文档 os-agent 安装文档","categories":[],"tags":[{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://wangqian0306.github.io/tags/Home-Assistant/"}]},{"title":"Spring for Apache Kafka","slug":"spring/spring-kafka","date":"2022-11-09T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spring-for-apache-kafka/","permalink":"https://wangqian0306.github.io/2022/spring-for-apache-kafka/","excerpt":"","text":"Spring for Apache Kafka 简介 Spring 官方封装了 Kafka 相关的 API，可以让 Spring 应用程序更方便的使用 Kafka。 使用 引入依赖 maven 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt; gradle 123dependencies &#123; compileOnly &#x27;org.springframework.kafka:spring-kafka&#x27;&#125; 链接配置 1234567spring: kafka: bootstrap-servers: &lt;host_1&gt;:&lt;port_1&gt;;&lt;host_2&gt;:&lt;port_2&gt; listener: ack-mode: manual producer: retries: 6 注：可以根据需求调整 ack-mode 如需手动则可进行上述配置，默认则无需配置此项。producer.retries 代表生产者遇到故障时的重试次数。 Producer 123456789101112131415161718192021222324252627282930313233@Slf4j@RestController@RequestMapping(&quot;/kafka&quot;)public class KafkaDemoController &#123; @Resource KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Bean public NewTopic topic() &#123; return TopicBuilder.name(&quot;topic1&quot;) .partitions(10) .replicas(1) .build(); &#125; @GetMapping(&quot;/sync&quot;) public HttpEntity&lt;String&gt; syncSend() &#123; try &#123; kafkaTemplate.send(&quot;wq&quot;, &quot;asd&quot;, &quot;123&quot;).get(); &#125; catch (InterruptedException | ExecutionException e) &#123; throw new ResponseStatusException(HttpStatus.BAD_REQUEST, e.getMessage()); &#125; return new HttpEntity&lt;&gt;(&quot;success&quot;); &#125; @GetMapping(&quot;/async&quot;) public HttpEntity&lt;String&gt; asyncSend() &#123; kafkaTemplate.send(&quot;wq&quot;, &quot;asd&quot;, &quot;123&quot;); return new HttpEntity&lt;&gt;(&quot;success&quot;); &#125; &#125; 注：异步方式可能会导致发送失败，建议在配置当中声明重试次数或者配置回调。 回调配置 123456789101112131415161718192021222324import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import org.springframework.kafka.support.ProducerListener;import org.springframework.stereotype.Component;@Slf4j@Componentpublic class CustomProducerListener implements ProducerListener&lt;String, String&gt; &#123; @Override public void onSuccess(ProducerRecord&lt;String, String&gt; producerRecord, RecordMetadata recordMetadata) &#123; log.error(&quot;success callback&quot;); &#125; @Override public void onError(ProducerRecord&lt;String, String&gt; producerRecord, RecordMetadata recordMetadata, Exception exception) &#123; log.error(&quot;error callback&quot;); &#125;&#125; 1234567891011121314151617181920212223242526import lombok.extern.slf4j.Slf4j;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.core.DefaultKafkaProducerFactory;import org.springframework.kafka.core.KafkaTemplate;import jakarta.annotation.Resource;@Slf4j@Configurationpublic class CustomKafkaConf &#123; @Resource DefaultKafkaProducerFactory&lt;String, String&gt; defaultKafkaProducerFactory; @Resource CustomProducerListener customProducerListener; @Bean public KafkaTemplate&lt;String, String&gt; kafkaTemplate() &#123; KafkaTemplate&lt;String, String&gt; kafkaTemplate = new KafkaTemplate&lt;&gt;(defaultKafkaProducerFactory); kafkaTemplate.setProducerListener(customProducerListener); return kafkaTemplate; &#125;&#125; Consumer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Slf4j@Componentpublic class CustomKafkaListener implements ConsumerSeekAware &#123; public Boolean seekFlag = false; private final ThreadLocal&lt;ConsumerSeekCallback&gt; seekCallBack = new ThreadLocal&lt;&gt;(); @KafkaListener(topics = &quot;topic&quot;, groupId = &quot;group&quot;, concurrency = &quot;$&#123;consumer-concurrency:2&#125;&quot;) public void listenTestTopic(ConsumerRecord&lt;String, String&gt; record) &#123; log.error(record.key() + &quot; &quot; + record.value() + &quot; &quot; + record.partition() + &quot; &quot; + record.offset()); &#125; @KafkaListener(topics = &quot;topic&quot;, groupId = &quot;group&quot;, concurrency = &quot;$&#123;consumer-concurrency:2&#125;&quot;) public void listenTestTopicAndCommit(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack) &#123; log.error(record.key() + &quot; &quot; + record.value() + &quot; &quot; + record.partition() + &quot; &quot; + record.offset()); ack.acknowledge(); &#125; @KafkaListener(topics = &quot;topic&quot;, groupId = &quot;#&#123;T(java.util.UUID).randomUUID().toString()&#125;&quot;) public void listenBroadcast(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack) &#123; log.error(record.key() + &quot; &quot; + record.value() + &quot; &quot; + record.partition() + &quot; &quot; + record.offset()); ack.acknowledge(); &#125; @KafkaListener(topics = &quot;topic&quot;, groupId = &quot;group&quot;) public void seekListener(ConsumerRecord&lt;String, String&gt; record) &#123; if (seekFlag) &#123; seekToOffset(&quot;topic&quot;, null, 0L); this.seekFlag = false; &#125; &#125; @Override public void registerSeekCallback(ConsumerSeekCallback callback) &#123; this.seekCallBack.set(callback); &#125; public void seekToOffset(String topic, Integer partition, Long offset) &#123; if (partition == null) &#123; Map&lt;String, TopicDescription&gt; result = kafkaAdmin.describeTopics(topic); TopicDescription topicDescription = result.get(topic); List&lt;TopicPartitionInfo&gt; partitions = topicDescription.partitions(); for (TopicPartitionInfo topicPartitionInfo : partitions) &#123; this.seekCallBack.get().seek(topic, topicPartitionInfo.partition(), offset); &#125; &#125; else &#123; this.seekCallBack.get().seek(topic, partition, offset); &#125; &#125;&#125; 注: consumer-concurrency 可以配置 Consumer 的线程数。且 seek 操作需要在有数据消费时才能触发。 单元测试 在单元测试中可以加入 @EmbeddedKafka 注解进行单元测试，样例如下 12345678910111213141516171819202122@RunWith(SpringRunner.class)@SpringBootTest@EmbeddedKafka(partitions = 1, brokerProperties = &#123; &quot;listeners=PLAINTEXT://localhost:9092&quot;, &quot;port=9092&quot;&#125;)public class KafkaConsumerTest &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Autowired private KafkaConsumer kafkaConsumer; @Test public void testReceive() throws Exception &#123; String message = &quot;Hello, world!&quot;; kafkaTemplate.send(&quot;test-topic&quot;, message); kafkaConsumer.getLatch().await(10000, TimeUnit.MILLISECONDS); assertThat(kafkaConsumer.getLatch().getCount()).isEqualTo(0); assertThat(kafkaConsumer.getPayload()).isEqualTo(message); &#125;&#125; 参考资料 官方文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spring Cloud Consul 动态配置","slug":"spring/consul-config","date":"2022-11-04T13:05:12.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/consul-config/","permalink":"https://wangqian0306.github.io/2022/consul-config/","excerpt":"","text":"Spring Cloud Consul 动态配置 简介 如同 Nacos 一样 Consul 也支持配置存储，并且也提供了配置自动更新的机制。 项目搭建 创建 Spring Cloud 项目，然后按照如下配置引入依赖包 123456789101112131415161718192021ext &#123; set(&#x27;springCloudVersion&#x27;, &quot;2022.0.0-M5&quot;)&#125;dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.cloud:spring-cloud-starter-consul-config&#x27; implementation &#x27;org.springframework.cloud:spring-cloud-starter-consul-discovery&#x27; implementation(&#x27;org.springframework.cloud:spring-cloud-starter-bootstrap&#x27;) compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.springframework.boot:spring-boot-configuration-processor&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27;&#125;dependencyManagement &#123; imports &#123; mavenBom &quot;org.springframework.cloud:spring-cloud-dependencies:$&#123;springCloudVersion&#125;&quot; &#125;&#125; 编辑 Spring 配置文件 application.yaml： 123456789101112131415161718192021spring: application: name: &lt;name&gt; cloud: consul: host: $&#123;CONSUL_HOST:localhost&#125; port: $&#123;CONSUL_PORT:8500&#125; discovery: prefer-ip-address: true tags: version=1.0 instance-id: $&#123;spring.application.name&#125;:$&#123;spring.cloud.client.hostname&#125;:$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; healthCheckInterval: 15sserver: port: 8080 error: include-message: always include-exception: true servlet: encoding: charset: UTF-8 编辑引入配置文件 bootstrap.yaml： 1234567891011spring: cloud: consul: config: enabled: true defaultContext: &lt;name&gt; profileSeparator: &#x27;-&#x27; prefixes: config format: properties watch: enabled: true 注：此处支持的 format 有 yaml, properties, key_value, files 可以根据实际情况进行选用。 编辑 Spring 主类并添加如下注解： 123456789import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.scheduling.annotation.EnableScheduling;@EnableDiscoveryClient@EnableScheduling@SpringBootApplicationpublic class DynApplication &#123;&#125; 编辑 Spring 配置类 123456789101112131415161718import lombok.Getter;import lombok.Setter;import lombok.extern.slf4j.Slf4j;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.context.annotation.Configuration;@Slf4j@Getter@Setter@RefreshScope@Configuration@ConfigurationProperties(prefix = &quot;my&quot;)public class MyProperties &#123; private String prop;&#125; 编辑调试接口类： 12345678910111213141516171819202122232425262728293031323334353637import com.example.dyn.conf.MyProperties;import jakarta.annotation.Resource;import org.springframework.http.HttpEntity;import org.springframework.web.bind.annotation.*;@RestController@RequestMapping(&quot;/demo&quot;)public class DemoController &#123; @Value(&quot;http://$&#123;spring.cloud.consul.host&#125;:$&#123;spring.cloud.consul.port&#125;/v1/kv/$&#123;spring.cloud.consul.config.prefixes&#125;/$&#123;spring.cloud.consul.config.defaultContext&#125;-$&#123;spring.profiles.active&#125;/data&quot;) String consulUrl; @Resource MyProperties myProperties; @GetMapping public HttpEntity&lt;String&gt; get() &#123; return new HttpEntity&lt;&gt;(myProperties.getProp()); &#125; @PutMapping public HttpEntity&lt;String&gt; put(@RequestParam String prop) &#123; RestTemplate restTemplate = new RestTemplate(); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); MultiValueMap&lt;String, String&gt; content = new LinkedMultiValueMap&lt;&gt;(); content.put(&quot;my.prop&quot;, Collections.singletonList(prop)); HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; entity = new HttpEntity&lt;&gt;(content, headers); ResponseEntity&lt;Object&gt; response = restTemplate.exchange(consulUrl, HttpMethod.PUT, entity, Object.class); if (response.getStatusCode().is2xxSuccessful()) &#123; return new HttpEntity&lt;&gt;(prop); &#125; else &#123; throw new ResponseStatusException(HttpStatus.BAD_REQUEST, &quot;consul set error&quot;); &#125; &#125; &#125; 启动程序即可 流程测试 如需手动编辑可以访问 consul webUI 的 Key/Value 选项中编辑配置向相关配置： 配置目录如下: 1config/&lt;name&gt;/data 配置内容如下： 1my.prop=demo-1 也可以通过 REST API 进行编辑，请求样例如下： request1234567891011121314### 写入配置项PUT http://localhost:8080/demo?prop=edit### 获取配置项GET http://localhost:8080/demo### 使用 Consul HTTP 接口读取配置项GET http://localhost:8500/v1/kv/config/dyn-default/data### 使用 Consul HTTP 接口编辑配置项PUT http://localhost:8500/v1/kv/config/dyn-default/dataContent-Type: application/x-www-form-urlencodedmy.prop=idea-http 注：配置项的空间上限是 512 kb。 配置变更监听 在配置变更的时候可以采用如下的监听器，针对变更的配置完成业务逻辑。 123456789101112131415import lombok.extern.slf4j.Slf4j;import org.springframework.cloud.endpoint.event.RefreshEvent;import org.springframework.context.ApplicationListener;import org.springframework.stereotype.Component;@Slf4j@Componentpublic class CustomRefreshEventListener implements ApplicationListener&lt;RefreshEvent&gt; &#123; @Override public void onApplicationEvent(RefreshEvent event) &#123; log.error(&quot;checked&quot;); &#125;&#125; 参考资料 Spring Cloud Consul 官方文档 Consul KV store HTTP 接口手册","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Consul","slug":"Consul","permalink":"https://wangqian0306.github.io/tags/Consul/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud/"}]},{"title":"Ceph","slug":"tools/ceph","date":"2022-10-28T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/ceph/","permalink":"https://wangqian0306.github.io/2022/ceph/","excerpt":"","text":"Ceph 简介 Ceph 是一个开源的分布式存储系统。 从安装方式上来讲可以才用 Cephadmin 来安装在容器和 systemd 中，也可以使用 Rook 安装在 Kubernetes 上。 关键概念 Ceph 集群有如下组件： Ceph 组件图 Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH 算法，Ceph 计算应计算应选择哪个归置组(placement group,PG)包含此对象，以及哪个 OSD 应存储归置组。这 CRUSH 算法使 Ceph 存储集群能够方便扩展、重新平衡以及动态恢复。 Monitors Ceph Monitor(监视器，ceph-mon) 维护着集群状态的 Map，此 Map 包括了Monitors Map，Manager Map，OSD Map，MDS Map 和 CRUSH Map。这些 Map 是集群用于协调 Ceph 守护进程之间关键集群状态。监视器还负责管理客户端与守护进程之间的授权。通常需要至少三个 Ceph Monitor 实现冗余和高可用性。 Manager Ceph Manager 守护进程(管理器，ceph-mgr) 负责跟踪运行时指标和当前 Ceph 集群的状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管了一些基于 Python 的模块包括 Ceph Dashboard 和 REST API。为了高可用最少需要引入两个 Ceph Manager。 Ceph OSDs Ceph OSD(ceph-osd,Object Storage Daemon) 是对象存储的守护进程，负责存储数据，处理数据副本，数据恢复，数据重平衡，并且通过查询其余 Ceph OSD 心跳数据从而向 Ceph Monitors 和 Ceph Manager 提供监控数据。为了高可用最少需要引入三个 Ceph Manager。 MDSs Ceph Metadata Server(元数据服务器,ceph-mds)负责代表 Ceph File System(Ceph 文件系统)存储元数据(例如 Ceph Block Devices 和 Ceph Object Storage 是不使用 MDS 的)。Ceph Metadata Server 允许符合 POSIX 规范的文件系统用户执行基本的命令(例如 ls,find 等)而不会给 Ceph 存储集群带来巨大负担。 Ceph File System Ceph File System(Ceph FS)，是一个符合 POSIX 规范的文件系统，构建在 Ceph 的分布式对象存储 RADOS 上。CephFS 的目标是构建一款最先进的，多用途，高可用，为大量应用程序提供高性能存储，其中用例包括像目录分享，HPC 暂存空间和分布式工作流所需的共享存储。 CephFS 通过使用一些新颖的架构实现这些目标。尤其重要的是，文件元数据与实际数据分离存储在一个 RADOS 池中并通过一个可靠的元数据服务器集群(MDS)提供服务，其还可以通过缩放的方式来应对更好的工作负载。文件系统的客户端可以直接访问 RADOS 以读取和写入文件数据块。因此，工作负载可能会随着底层RADOS 对象存储的大小而线性扩展；也就是说，没有网关或 broker 代理客户端的数据 I/O。 对数据的访问通过 MDS 集群进行协调，MDS 集群作为客户端和 MDS 协作维护的分布式元数据缓存状态的权威机构。元数据的变更由每个 MDS 聚合成一系列写入 RADOS 上的日志；MDS 本地不存储元数据状态。该模型允许在 POSIX 文件系统的上下文中客户机之间进行一致和快速的协作。 CephFS 架构图 CephFS 是 Ceph 中最旧的存储接口，曾经是 RADOS 的主要用例。但现在，它由另外两个存储接口连接起来，形成了一个现代的统一存储系统：RBD(Ceph Block Device)和 RGW(Ceph Object Storage Gateway)。 Ceph Block Device Block 是一系列的字节(通常是 512)。基于 Block 的存储接口是在包括 HDD、SSD、CD、软盘甚至磁带在内的介质上存储数据的一种成熟而常见的方式。Block Device 接口的普遍性非常适合与包括 Ceph 在内的海量数据存储交互。 Ceph Block Device 的特点是简单的配置，可调整存储大小，将数据切分存储在多个 OSD 上。Ceph Block Device 利用了 RADOS 包括快照、复制和强一致性的功能。Ceph Block 存储客户端可以通过 内核模组或者 librbd 库读取 Ceph 集群。 使用方式 在安装完成 Ceph 存储集群之后就可以使用 Ceph FS 作为文件系统进行挂载使用。 如需在 Kubernetes 中使用则可以创建 Ceph PV，又或者使用 RBD 开关集群 集群关闭官方文档 使用如下命令即可： 123456ceph osd set nooutceph osd set norecoverceph osd set norebalanceceph osd set nobackfillceph osd set nodownceph osd set pause 然后关闭 OSD 节点： 1systemctl stop ceph-osd.target 关闭监控节点： 1systemctl stop ceph-mon.target 在开机时反向启动节点即可： 1systemctl start ceph-mon.target 1systemctl start ceph-osd.target 等待所有节点出现，然后运行如下命令： 123456ceph osd unset nooutceph osd unset norecoverceph osd unset norebalanceceph osd unset nobackfillceph osd unset nodownceph osd unset pause 参考资料 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://wangqian0306.github.io/tags/Ceph/"}]},{"title":"Linux 环境清理","slug":"linux/cleanup","date":"2022-10-27T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/cleanup/","permalink":"https://wangqian0306.github.io/2022/cleanup/","excerpt":"","text":"Linux 环境清理 简介 随着系统使用时间变长，日志和缓存等文件会占据很多的存储空间。故环境清理方式整合如下： 清理内容 用户缓存 1rm -r /home/&lt;user&gt;/.cache/* journal 日志 1rm -r /var/log/journal/* dnf/yum 缓存 1dnf clean all docker 数据清除 1docker system prune -a --volumes 清除 docker volumes 1docker volume prune 清除 mvn repository 1rm -rf ~/.m2/repository/* 清除 python package 12pip freeze &gt; modules.txtpip uninstall -r modules.txt -y 清除 go pkg 1rm -rf ~/.m2/pkg/* 参考资料 Cleaning system","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Spring Cloud Gateway","slug":"spring/gateway","date":"2022-10-25T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spring-cloud-gateway/","permalink":"https://wangqian0306.github.io/2022/spring-cloud-gateway/","excerpt":"","text":"Spring Cloud Gateway 简介 Spring Cloud Gateway 是 Spring Cloud 框架中的网关模块，负责转发请求至对应服务。 使用方式 正常编写一个空的 Spring Cloud 应用程序，并且引入 Spring Cloud Gateway 相关包即可。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud-gateway.version&#125;&lt;/version&gt;&lt;/dependency&gt; 123456789import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class,args); &#125;&#125; 根据需求编写重定向配置即可，具体配置样例请参照官方文档。 12345678910server: port: 8080spring: application: name: gateway cloud: gateway: routes: - id: demo uri: https://httpbin.org 参考资料 官方文档 样例项目","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud/"},{"name":"Gateway","slug":"Gateway","permalink":"https://wangqian0306.github.io/tags/Gateway/"}]},{"title":"KRaft","slug":"reference/kraft","date":"2022-10-17T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/kraft/","permalink":"https://wangqian0306.github.io/2022/kraft/","excerpt":"","text":"KRaft 简介 最新的 Kafka 版本已经可以采用 KRaft 替代 ZooKeeper 了，所以再此进行一些知识整理。 原因及改善方式 根本原因：随着 Broker 数量和集群中主题(topic)分区(partition)的增加，仍然存在一些在伸缩情况下的读写 ZooKeeper 产生的流量瓶颈。 细节原因： 大多数的元数据更改传播都发生在 Controller 与 Broker 之间，并且与主题分区数量呈线性关系。 Controller 需要向 ZooKeeper 写入更新的元数据信息来进行持久化，所以最终所花的时间也与写入持久化的数据量成线性关系。 使用 ZooKeeper 作为元数据存储工具还有一些挑战，例如 Znodes 的数量限制，watcher 的最大数量限制，对 Broker 额外的合规性检查来保证每个 Broker 维护了自己的元数据视图。当更新延迟或重新排序时，这些元数据视图可能会有所不同。 保护、升级和调试是一件很痛苦的事情。简化分布式系统可延长其使用寿命和稳定性。 改善方式： 使用 Controller 直接维护元数据日志，并将其作为内部主题进行存储。 注：这样一来也就意味着操作会被有序的追加记录在元数据中，并与异步日志 I/O 一起批处理，以实现更好的性能。 元数据更改传播会被 Broker 通过复制元数据变更日志而不是 RPC 的方式完成。 注：这意味着不需要再担心一致性的问题，每个 Broker 的元数据视图最终都会保持一致，因为它们来自与同一日志，并且当前所处的状态与 offset 相关。另一个好处是，这将 Controller 的元数据日志与其他日志分离进行管理。(具有单独的端口，请求处理队列，指标，线程等)。 通过将一小部分 Broker 同步元数据日志的方式，我们可以得到另一个 quorum 的 Controller 而不是单个 Controller。 实现细节 数据同步方式 原先的 Kafka 采用了主备复制算法(Primary-Backup)： Leader 接收到数据会将其复制到其他 Follower 副本，在 Follower 副本承认写入之后会写回信。具体流程如下： 最新的复制算法是仲裁复制(quorum replication)： Leader 接收到数据会将其复制到其他 Follower 副本，在 Follower 副本副本承认写入之后会写回信。在大部分 Follower 返回回信之后 Leader 将认为写入已提交，并将返回到写入客户端。 KRaft 选用仲裁复制算法的原因是： 为了可用性，可以负担核心元数据日志的更多副本 因为系统对元数据的追加写入延迟很敏感，这样的情况会是集群的热点 选举方式 集群内共有三种角色：Leader，Voter 和 Observer。Leader 和其他 Voter 共同组成 Quorum，并负责保持复制的日志的共识，并在需要时选举新的 Leader。在集群内的其他 Broker 都是 Observer，它们只负责被动的读取复制日志来跟随 Quorum。每条记录在写入日志时都会加上 Leader 的 Epoch。 在集群启动后，所有 Broker 会按照初始配置的 Quorum 成 Voter，并且从本地日志中读取当前 Epoch(Raft 原文中称为 term 即任期)。在下图中，让我们假设我们的 Quorum 为三名 Voter。每个记录在其本地日志中都有六条记录，分别带有绿色和黄色： 经过一段时间而没有找到 Leader 后，Voter 可能会进入一个新的 Epoch，并过渡到作为 Leader 候选人的临时角色。然后，它将向 Quorum 中的所有其他 Broker 发送请求，要求他们投票支持它作为这个 Epoch 的新 Leader。 在投票请求中含有两部分的关键信息：其他 Broker 投票的 Epoch，和候选人本地日志的 offset。在接受投票的过程中，每个 Voter 会检查请求的 Epoch 是否大于其自己的 Epoch；如果已经投票过该 Epoch；或者它本地日志已经超出了给定的 offset。如果上述内容都是否，才会将此投票视为真实的。投票会被本地持久化存储，以便在 Quorum 中的 Broker 不会忘记投票的情况，即便是在刚刚启动时。当候选人收集到过半 Quorum 的选票(包括他自己)，就可以认为投票已经完成了。 需要注意的是如果在之前设置的时间期限内候选者没能获取到足够的投票，它将认为投票程序失败，并将尝试再次提高其 Epoch 并重试。为了避免任何僵局情况，例如多个候选人同时要求投票，从而防止另一个人获得足够的选票以应对颠簸的 Epoch，我们还引入了重试前的随机备份时间。 结合所有这些条件检查和投票超时机制，我们可以保证在 KRaft 上，在给定的 Epoch 最多有一个 Leader 当选，并且这个当选的 Leader 将拥有所有选票的记录。 日志复制 与 Kafka 一样，KRaft 才有用拉取的机制来保持复制内容一致，而不是原始的 Raft 论文引入的基于推送的模型。在下图中，假设 Leader-1 在 Epoch 3 中有两条记录(红色的)，而 Voter-2 正在拉取此数据。 与 Kafka 中现有的复制副本获取逻辑一样，Voter-2 将在其提取请求中编码两条信息：要从中获取的 Epoch 及其日志和偏移量。收到请求后，Leader-1 将首先检查 Epoch，如果它有效，将返回从给定 offset 开始的数据。读取的 Voter-2 会将返回的数据追加到其本地日志中，然后使用新的偏移量再次开始读取。这里没有什么新东西，只是普通的复制副本获取协议。 但是，假设另一个 Voter 已经偏离了日志记录。在样例中，Voter-3 是 Epoch 2 上的旧领导者，其本地日志上有一些附加的记录，这些记录尚未复制到 Quorum。当意识到 Epoch 以 Leader-1 作为前导起始集时，它将向 Leader-1 发送一个包含 Epoch 2 以及日志和 offset 的提取请求。Leader-1 将验证并发现此 Epoch 和 offset 不匹配，因此将在响应中返回错误代码，告诉 Voter-3 Epoch 2 仅提交了 offset 为 6 的记录。然后，Voter-3 将截断其本地日志以达到 offset 6。 然后，Voter-3 将再次重新发送拉取请求，这次是 Epoch 2 和偏移量 6。然后，Leader-1 可以将 Epoch 中的数据返回到 Voter-3，后者将在附加到其本地日志时从返回的数据中了解此新 Epoch。 请注意，如果 Voter-2 和 Voter-3 无法在预定义的时间内成功从 Leader-1 中获取响应，则它可能会增加其 Epoch 并尝试选举为 Epoch 4 的新 Leader。因此，我们可以看到，这个 fetch 请求也被用作心跳来确定 Leader 的活跃度。 拉取与推送方式复制数据的对比 与 Raft 文献中基于推送的模型相比，KRaft 中基于拉取的日志复制在日志协调方面更有效，因为 Voter 在提取操作时可以在重新发送下一次提取之前直接截断到可行的偏移量。在基于推送的模型中，需要很多的 “ping-pong” 交互(确认链接)，自从 Leader 推送数据开始就需要确认正确的目标日志的位置。 基于拉取的 KRaft 也更不易于收到服务器损坏的影响，例如：旧的 Voter 不知道它已被移除出 Quorum ，比方说经历了成员的重新配置。如果这些旧 Voter 继续向基于拉取的模型中的 Leader 发送 fetch 请求，则 Leader 可以使用特殊的错误代码进行响应，告诉他们他们已从仲裁中删除，并且可以转换为观察者。在原始推送模型的 Raft 中则正相反，推送数据的 Leader 并不知道哪一个 Voter 会变成 disruptive servers 。自从被移除的节点没有获取到 Larder 任何推送的数据后，他们就会尝试选举自己作为新的 Leader，从而中断原始进程。 另一个选用拉取模型的 Raft 协议的动机是 Kafka 作为基石的日志复制层已经是基于拉取模型建立的，因此可以更多的实现重用。 但是这样做的好处是有代价的：新的 Leader 需要声明一个新的起始 Epoch API 来通知 Quorum 用于区分。在 Raft 模型中，此推送可以由 Leader 的 push data API 携带。此外为了向 Quorum 提交记录且被大多数节点认可，Leader 需要等待下一个 fetch 请求来推进 offset。这对于解决 disruptive servers 问题来说都是值得的。此外，利用现有的基于 Kafka 拉取的数据复制模型(“不造轮子”)节省了数千行代码。 要了解有关 KRaft 实现设计的其他详细信息（如元数据快照和基于 KRaft 日志构建的状态机 API）的更多信息，请务必阅读 KIP-500、KIP-595 和 KIP-630 的参考文档。 Controller 使用 KRaft 之后和 ZooKeeper 一样选举出的 Leader 也会作为 Controller。其会负责接受新的 Broker 注册，检测 Broker 故障以及接收所有会更改集群元数据的请求。所有这些操作都可以按其相应的更改事件追加到元数据日志的时间进行管道传输和排序。Quorum 中的其他 Voter 会主动复制元数据日志，以便提交新追加的记录。 注：使用 KRaft 之后就不再有 ISR 的概念，因为所有数据都会经过 Quorum 的仲裁。且元数据也会生成快照用于快速恢复。 参考资料 Why ZooKeeper Was Replaced with KRaft – The Log of All Logs Getting Started with the KRaft Protocol KIP-500 KIP-595 KIP-630","categories":[{"name":"参考资料","slug":"参考资料","permalink":"https://wangqian0306.github.io/categories/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"Anomaly detection","slug":"reference/anomaly-detection","date":"2022-10-12T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/anomaly-detection/","permalink":"https://wangqian0306.github.io/2022/anomaly-detection/","excerpt":"","text":"异常检测 简介 在数据分析中，异常检测(也称为异常值检测，有时也称为新颖性检测)通常被理解为识别罕见项目，事件或观察结果，这些项目，事件或观察结果与大多数数据明显偏离并且不符合明确定义的正常行为概念。这些例子可能会引起人们对由不同机制产生的怀疑，或看起来与该组数据的其余部分不一致。 存在三大类异常检测技术。监督式异常检测技术需要一个被标记为“正常”和“异常”的数据集，并涉及训练分类器。但是，由于标记数据的一般不可用以及类固有的不平衡性质，此方法很少用于异常检测。半监督异常检测技术假定数据的某些部分已标记。这可能是正常或异常数据的任意组合，但通常情况下，技术会从给定的正常训练数据集构造表示正常行为的模型，然后测试模型生成测试实例的可能性。无监督异常检测技术假设数据是未标记的，并且由于其更广泛和相关的应用，因此是迄今为止最常用的。 异常检测可识别异常值，以回答“发生了什么通常不会发生？&quot;而预测则的目标则是发现 “未来将会怎样”，例如 Amazon QuickSight 就使用随机切割森林(RCF)算法的内置版本。 试用 创建数据模型 12345678910111213141516PUT sensor&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;voltage&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;status&quot; : &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125;&#125; 插入样本数据 1234567891011121314151617181920212223242526272829303132import timefrom elasticsearch import ElasticsearchELASTIC_PASSWORD = &quot;Demo123..&quot;VOLTAGE_MIN = 1.0VOLTAGE_MAX = 5.0VOLTAGE_STEP = 0.5def build_voltage_array(voltage_min: float, voltage_max: float, step: float): result = [] for i in range(int((voltage_max - voltage_min) / step) + 1): result.append(voltage_min + i * step) return resultif __name__ == &#x27;__main__&#x27;: client = Elasticsearch( &quot;https://192.168.2.77:9200&quot;, ca_certs=&quot;ca/ca.crt&quot;, basic_auth=(&quot;elastic&quot;, ELASTIC_PASSWORD) ) voltage_array = build_voltage_array(VOLTAGE_MIN, VOLTAGE_MAX, VOLTAGE_STEP) while True: for voltage in voltage_array: data = &#123; &quot;voltage&quot;: voltage, &quot;@timestamp&quot;: int(time.time()) &#125; time.sleep(10) client.index(index=&#x27;sensor&#x27;, document=data) 创建任务并进持续进行监测 在 Kibana 中创建 Data Views 创建 Anomaly detection 任务 将 voltage 参数作为监控参数 注：在创建任务的过程中需要注意，历史数据的持续时间需要超过两个小时才能完成训练。 在 Single Metric Viewer 中即可查看检测结果，或者根据数据进行预测 之后修改插入逻辑，插入异常值然后观测结果即可 参考资料 Elasticsearch Anomaly detection Amazon QuickSight OpenSearch 文档","categories":[{"name":"参考资料","slug":"参考资料","permalink":"https://wangqian0306.github.io/categories/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"}],"tags":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"}]},{"title":"IK 分词器","slug":"database/ik","date":"2022-10-11T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/ik/","permalink":"https://wangqian0306.github.io/2022/ik/","excerpt":"","text":"IK 分词器 简介 IK 分词器(IK Analysis) 是针对于 Elasticsearch 的一款分词插件。该插件提供了如下内容： Analyzer ik_smart ik_max_word Tokenizer ik_smart ik_max_word 安装 方式一： 在 官方网站 上下载对应版本的压缩包 创建 &lt;es_path&gt;/plugins/ik 文件夹 将压缩包里的内容解压至此文件夹中 重启 elasticsearch 方式二： 进入 elasticsearch 安装根目录运行如下命令： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/&lt;version&gt;/elasticsearch-analysis-ik-&lt;version&gt;.zip 试用 检测分词结果 12345GET &lt;index&gt;/_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: [&quot;白日依山尽，黄河入海流。&quot;]&#125; 存储并查询数据 1234567891011121314151617181920212223242526272829303132PUT demo&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_smart&quot; &#125; &#125; &#125;&#125;POST demo/_doc&#123; &quot;content&quot;:&quot;白日依山尽，黄河入海流。&quot;&#125;POST demo/_doc&#123; &quot;content&quot;:&quot;黄河西来决昆仑，咆哮万里触龙门。&quot;&#125;POST demo/_doc&#123; &quot;content&quot;:&quot;倒泻银河事有无，掀天浊浪只须臾。&quot;&#125;POST demo/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;content&quot; : &quot;黄河&quot; &#125;&#125;&#125; 配置字典 在 &#123;plugins&#125;/elasticsearch-analysis-ik-*/config/IKAnalyzer.cfg.xml 配置文件中： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;location&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;http://xxx.com/xxx.dic&lt;/entry&gt;&lt;/properties&gt; 注：词典返回需要有 Header Last-Modified 和 ETag，文件编码是 UTF-8，一行一个词且换行符为 \\n。 参考资料 官方文档","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wangqian0306.github.io/tags/Elasticsearch/"},{"name":"IK Analysis plugin","slug":"IK-Analysis-plugin","permalink":"https://wangqian0306.github.io/tags/IK-Analysis-plugin/"}]},{"title":"Nginx 代理","slug":"tools/nginx","date":"2022-10-08T15:09:32.000Z","updated":"2025-01-22T08:54:15.932Z","comments":true,"path":"2022/nginx/","permalink":"https://wangqian0306.github.io/2022/nginx/","excerpt":"","text":"Nginx 简介 Nginx 是一个高性能的 HTTP 和反向代理 web 服务器。 部署 容器部署 12345678services: nginx: image: nginx:latest ports: - &quot;8080:80&quot; volumes: - &lt;path&gt;/nginx.conf:/etc/nginx/nginx.conf - &lt;path&gt;/html:/usr/share/nginx/html:ro 注：如果部署之后出现访问权限异常，则最好先检查下文件，重新构建一次前端项目试试。 常见使用模式 代理加密 Nginx 提供了 ngx_http_auth_basic_module 模块实现让用户只有输入正确的用户名密码才允许访问 web。可以通过如下步骤，完成此需求： 生成用户名密码文件 12yum install -y httpd-toolshtpasswd -bc &lt;file_path&gt; &lt;username&gt; &lt;password&gt; 注：htpasswd命令选项参数说明: -c 创建一个加密文件 -n 不更新加密文件，只将htpasswd命令加密后的用户名密码显示在屏幕上 -m 默认 htpassswd 命令采用 MD5 算法对密码进行加密 -d htpassswd 命令采用 CRYPT 算法对密码进行加密 -p htpassswd 命令不对密码进行进行加密，即明文密码 -s htpassswd 命令采用 SHA 算法对密码进行加密 -b htpassswd 命令行中一并输入用户名和密码而不是根据提示输入密码 -D 删除指定的用户 部署 Nginx 服务 编写配置文件，并将密码文件放置在 /etc/nginx/passwd 即可： 1234567891011121314151617181920212223242526272829303132333435363738user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; server &#123; listen 80; listen [::]:80; server_name _; location / &#123; auth_basic &quot;closed site&quot;; auth_basic_user_file /etc/nginx/passwd; proxy_pass http://xxx.xxx.xxx; client_max_body_size 10m; &#125; &#125;&#125; CORS 跨域配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; server &#123; listen 80; listen [::]:80; server_name _; location / &#123; if ($request_method = &#x27;OPTIONS&#x27;) &#123; add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET, POST, OPTIONS&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;Authorization, Content-Type&#x27;; add_header &#x27;Access-Control-Max-Age&#x27; 1728000; add_header &#x27;Content-Type&#x27; &#x27;text/plain; charset=utf-8&#x27;; add_header &#x27;Content-Length&#x27; 0; return 204; &#125; add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET, POST&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;Authorization, Content-Type&#x27;; add_header &#x27;Access-Control-Expose-Headers&#x27; &#x27;Authorization&#x27;; proxy_pass http://xxx.xxx.xxx; &#125; &#125;&#125; HTTPS 配置 12345678910111213141516171819202122232425262728293031323334353637383940414243user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; server &#123; listen 80; server_name xxx.xxx.xxx; return 301 https://$host$request_uri; &#125; server &#123; listen 443 ssl; server_name xxx.xxx.xxx; ssl_certificate /etc/nginx/cert.pem; ssl_certificate_key /etc/nginx/key.pem; location / &#123; proxy_pass http://xxx.xxx.xxx/; &#125; &#125;&#125; 代理具有密码的网页 浏览器访问带有密码的网页会使用 base64 运算过的头。使用如下命令即可获得令牌： 1echo -n &#x27;&lt;user&gt;:&lt;password&gt;&#x27; | base64 然后编写如下配置即可： 12345678910server &#123; listen 80; listen [::]:80; server_name _; location / &#123; proxy_pass http://xxxx; proxy_set_header Authorization &quot;Basic &lt;token&gt;&quot;; &#125;&#125; 缓存配置 12345678910111213141516171819202122232425262728293031323334353637383940user nginx;worker_processes auto;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; # 缓存路径和key以及清除时间 proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=my_api_cache:10m inactive=60m max_size=1g; proxy_temp_path /data/nginx/cache/tmp; server &#123; listen 80; server_name localhost; location / &#123; proxy_cache my_api_cache; # 缓存的方法和时间长度 proxy_cache_valid 200 301 302 10m; proxy_cache_valid 404 1m; proxy_cache_key &quot;$scheme$request_method$host$request_uri&quot;; proxy_cache_revalidate on; add_header X-Proxy-Cache $upstream_cache_status; add_header ETag $upstream_http_etag; proxy_pass http://xxx.xxx.xxx.xxx/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; &#125; &#125;&#125; SSE 配置 1234567891011121314151617181920212223server &#123; listen 443 ssl; server_name xxx.xxx.xxx; client_max_body_size 5M; location /xxx &#123; # 代理到你的后端服务器 proxy_pass http://xxx.xxx.xxx.xxx/xxx; # 保持连接 proxy_http_version 1.1; proxy_set_header Connection &#x27;&#x27;; proxy_buffering off; # 添加适当的超时设置 proxy_connect_timeout 60s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; gzip off; &#125;&#125; 参考资料 官方文档 容器页","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://wangqian0306.github.io/tags/Nginx/"}]},{"title":"Redis Stack","slug":"database/redis-stack","date":"2022-09-29T14:12:59.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/redis-stack/","permalink":"https://wangqian0306.github.io/2022/redis-stack/","excerpt":"","text":"Redis Stack 简介 Redis Stack 是 Redis 关于数据模型和处理引擎的扩展。包括了一些扩展模块和 RedisInsight 。 使用 Docker 安装 服务器版 123456services: redis-stack-server: image: redis/redis-stack-server:latest container_name: redis-stack-server ports: - &quot;6379:6379&quot; 本地测试 1234567services: redis-stack: image: redis/redis-stack:latest container_name: redis ports: - &quot;6379:6379&quot; - &quot;8001:8001&quot; 注：8001 端口是 RedisInsight 客户端管理工具的端口。 增强功能 Redis Stack 与原版 Redis 相比有如下的增强： 设计方面： 支持使用 Hashset 和 JSON 两种基本数据类型，且可以使用索引、全文检索、聚合查询等功能 功能方面： 图数据存储和检索 时序型数据存储和检索 矢量相似性搜索 概率数据结构 布隆过滤器(Bloom Filter) 布谷过滤器(Cuckoo Filter) Count-min Sketch 算法 Top-K 工具 使用样例 检索数组 可以使用如下方式插入样例数据： 1JSON.SET demo $ &#x27;&#123;&quot;name&quot;:&quot;Paul John&quot;,&quot;email&quot;:&quot;paul.john@example.com&quot;,&quot;age&quot;:42,&quot;city&quot;:&quot;London&quot;,&quot;temp&quot;:[1.1,2.2,3.3,4.4,5.5,6.6,7.7,8.8,9.9,10,11.11,12.12,13.13,14.14,15.15,16.16,17.17,18.18]&#125;&#x27; 使用如下命令即可获取数组中取样步长为2的所有元素： 1JSON.GET demo $.temp[0:-1:2] 参考资料 官方文档","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"MySQL 虚拟列","slug":"database/mysql-virtual","date":"2022-09-28T14:12:59.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/mysql-virtual/","permalink":"https://wangqian0306.github.io/2022/mysql-virtual/","excerpt":"","text":"MySQL 虚拟列 简介 在创建表的时候 MySQL 还支持虚拟列。虚拟列的值是根据列定义中包含的表达式计算得出的。 使用方式 按照如下方式建表和查询即可： 123456CREATE TABLE triangle ( sidea DOUBLE, sideb DOUBLE, sidec DOUBLE AS (SQRT(sidea * sidea + sideb * sideb)));INSERT INTO triangle (sidea, sideb) VALUES(1,1),(3,4),(6,8); 1SELECT * FROM triangle; 1234567+-------+-------+--------------------+| sidea | sideb | sidec |+-------+-------+--------------------+| 1 | 1 | 1.4142135623730951 || 3 | 4 | 5 || 6 | 8 | 10 |+-------+-------+--------------------+ 虚拟列有以下语法： 1234col_name data_type [GENERATED ALWAYS] AS (expr) [VIRTUAL | STORED] [NOT NULL | NULL] [UNIQUE [KEY]] [[PRIMARY] KEY] [COMMENT &#x27;string&#x27;] 其中的关键在于 [VIRTUAL | STORED] 选项： VIRTUAL：列值不存储，但在读取时，最终值在任何BEFORE 触发器之后计算。虚拟列不占用存储空间。InnoDB支持虚拟列的二级索引 STORED：插入或更新行时，将计算和存储列值。存储的列需要存储空间，可以进行索引。 注：如果不进行声明默认会采用 VIRTUAL 方式。 虚拟列的表达式必须遵守以下规则： 必须使用使用文字、确定性内置函数和运算符。如果给定表中的相同数据，多次调用产生相同的结果，而不依赖于连接的用户，则函数是确定的。不确定且未通过此定义的函数示例：CONNECTION_ID()、CURRENT_USER()、NOW()。 不允许使用存储函数和可加载函数。 不允许使用存储过程和函数参数。 不允许使用变量（系统变量、用户定义变量和存储的程序局部变量）。 不允许子查询。 生成的列定义可以引用其他生成的列，但只能引用表定义中前面出现的列。生成的列定义可以引用表中的任何基（非生成的）列，无论其定义出现在前面还是后面。 AUTO_INCREMENT 属性不能在生成的列定义中使用。 AUTO_INCREMENT 列不能用作生成的列定义中的基列。 如果表达式求值导致错误或向函数提供不正确的输入，CREATE TABLE 语句将终止并返回错误，DDL 操作将被拒绝。 参考资料 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"}]},{"title":"元数据注册系统","slug":"theory/metadata","date":"2022-09-23T13:32:58.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/metadata/","permalink":"https://wangqian0306.github.io/2022/metadata/","excerpt":"","text":"元数据注册系统(Metadata registries,MDR) 简介 最近在看 DAMA 的时候知道了 ISO/IEC 11179 标准，所以针对相关内容进行一些学习和记录。 注：GB/T 18391 等同于 ISO/IEC 11179 仅有编辑性修改。关键术语请参照原文。 概念 通常，将描述性的数据称为元数据，即，元数据是用于描述其他数据的数据。元数据注册系统是支持注册功能的一个元数据的数据库。系统的主要目的为：标识、来源和质量监控。标识由赋予(注册系统内)每个注册对象一个唯一的标识符来实现；来源指明元数据及其描述对象的来源；质量监控确保元数据完成其被赋予的任务。 元数据注册用于管理数据的语义。对数据的理解是设计、协调、标准化、应用、重用以及交换数据的基础。设计元数据注册系统的基本模型，目的在于获取数据语义的所有基本成分，而与应用或主题域无关。 元数据注册系统的目标是，各类应用能够确定在现有的元数据注册系统中是否存在合适的对象。如果确认需要一个新对象，鼓励通过适当修改现有描述来派生，以免类似的描述产生不必要的差异。注册也可以辨别两个或多个管理项描述的是同一对象，更为重要的是，可以发现在一个或多个方面存在显著差异的管理项是否使用了相似或相同的名称。 数据的基本容器也称为数据元(Data Element)。它可以抽象地存在，也可存在于某个应用系统中，但其描述是相同的。数据元描述既有语义成分，也有表示成分。语义可以进一步细分为语境型和符号型。 语境语义由数据元概念(DEC)描述。数据源概念描述数据所指的对象种类和用于度量该对象的独特特征。符号语义由概念域(CD)描述。概念域是种类的集合，它不必是有限集合。在此，种类表示数据元值域中允许值的含义。 框架 数据元的基本模型如下： 数据元概念：数据源概念是一个数据元的形式表示的概念，其描述与任何特定表示法无关。 表示：表示由值域、数据类型、计量单位(如果需要)、表示类(可选)。组成 数据元概念由以下两部分构成： 对象类：可以对其界限和含义进行明确的标识，且特性和行为遵循相同规则的观念、抽象概念或现实世界中事务的集合。 特性：一个对象类所有成员所共有的特征。 对象类是我们希望采集和存储数据的事务。对象类是概念，在面向对象的模型中与类相对应，在实体关系模型中与实体对应，例如轿车、人、家庭、雇员和订单等。特性用来区别和描述对象，是对象类的特征，但不一定是本质特征，它们构成对象类的内涵。特征也是概念，对应于面向对象模型或实体关系模型中的属性(不包括相关的数据类型)，例如颜色、模型、性别、年龄、收入、地址、价格等。 对象类可能是一般概念。当对象类所对应的对象集有两个或多个元素时，就是一般概念。上段的例子就是一般概念。记录级数据以这种方式描述。对象类也可以是个别概念。当对象类对应的对象集仅有一个元素时，就是个别概念，例如 ’美国自然人集合‘ 或 ’澳大利亚服务行业公司集合‘。聚合数据以这种方式描述。特性作为个别概念的例子有：平均收入或总收入。 在组织内部，数据库或文件由记录、段和元组等组成，而记录、段和元组则由数据元组成。数据元本身包含有字符、图像、声音等多累数据。 组织需要将数据传输给其他组织时，数据元是构成事务集的基本单元。事务主要发生于数据库间或文件间，单文件和数据库结构(如记录或元组)在不同组织间并不一定相同。因此，信息(数据加上理解)传输的公共单元就是数据元。 值域是允许值的集合，其有以下子类： 可枚举值域：由允许值(值和它们的含义)列表规定的值域； 不可枚举值域：由描述规定的值域。 概念的外延构成了概念域，每个值域都是概念域的一个元素。一个概念域是一个值含义的集合。一个概念域的内涵是它的值含义。多个值域可能是同一个概念域的外延。但一个值域只与一个概念域关联。概念域之间可以存在关系，所以可能创建概念域的一个概念体系。值域之间也可以存在关系，根据这些关系提供的框架，就能够获得相关值域和它们关联概念的结构。 概念域也有两种子类： 可枚举概念域：由值含义列表规定的概念域； 不可枚举概念域：由描述规定的概念域。 分类 注：此章节大部分内容都是图，暂时没理解清楚，之后补充吧。 注册系统元模型(metamodel)与基本属性 元模型是描述其他模型的一个模型。一个元模型为理解特定模型的准确结构及其成分提供一种机制，它对于用户和/或软件工具成功地共用该特定模型来说是必需的。 在样例中会用一个元模型来描述元数据注册系统的结构。该注册系统依次将被用于描述和模拟其他数据，例如：关于企业，公共管理或商业应用方面的数据。该注册系统元模型是作为一个概念数据模型来被规定的，亦即描述自然世界中相关信息是如何构造为模型的。作为一个概念模型，不需要把模型中的属性于数据库中的字段、列、对象以及其他事项作一对一地匹配。每一属性可对应的不只是一个字段，并且某些实体和关系可以由多个字段实现，而不必在意一个实现对于每个关系或实体是否都有一个表。该元模型不需要在物理上作特定的实现。 由这种元模型描述的框架结构可以分为多种实现。这些实现可以是数据库、数据仓库、元数据注册簿、元数据注册系统、词典及其他等等。 数据定义的形成 数据定义应： 用单数形式阐述； 要阐述其概念是什么，而不是仅阐述其概念不是什么； 用描述性的短语或句子阐述； 仅可使用人们普遍理解的缩略语； 表述不应包括其他数据或基本概念的定义。 数据定义宜： 阐述概念的基本含义； 准确而无歧义； 简练； 能单独成立； 表述中不应加入理由、功能用法、领域信息或程序信息； 避免循环定义； 对相关定义使用相同的术语和一致的逻辑结构； 适合被定义的元数据项的类型。 命名和标识原则 命名由描述性命名和规定性命名两种。命名约定也可以在一个引用文件中规定。命名约定应涵盖命名的所有方面。包括： 命名约定的范围，例如，已确定的行业(产业名称)； 建立机构的名称； 管理那些在名称中使用的术语的来源和内容的语义规则，例如：从数据模型中得到的术语、在学科中公共使用的术语等； 涵盖所需的术语顺序的句法规则； 涵盖所管理的术语列表，名称长度，字符集和语言的词法规则； 确定名称是否必须唯一的规则。 描述性的命名约定可使用于那些不受注册系统管理或早期进入注册系统的其他机构控制的管理项。描述性命名约定至少应记录范围和机构规则，适当的时候也可以记录语义、句法、词法和唯一性规则。 规定性的命名除了记录描述性的命名约定所需的范围和机构规则外，还应记录语义、句法、词法和唯一性规则： 语义规则使得含义可被传达。 句法规则使这些项按一致的、规定的顺序相关联、 词法(词的构成和词汇)规则减少了项的冗余并增加了准确性。 唯一性规则记录了如何防止在命名约定范围内同名现象的出现。 语义规则：语义涉及到名称各部分和它们幻定界限的分隔符语义。语义规则记录了名称是否传达了含义，如果转达，则记录如何转达。 句法规则：句法规定了一个名称和各部分的排列。这个排列可以按相对或绝对，或两者的组合来规定。相对排列按照其他部分来规定某部分的位置，例如，一个命名约定中的规则可以要求一个限定词术语应总是出现在所限定词术语的前面。绝对排列规定某部分固定出现的位置。例如规则可能要求特性术语总是放置在名称的最后部分。 词法原则：词法问题涉及到名称的表现形式：首选和非首选的术语，同义词，缩写词，各部分长度，拼写，允许的字符集以及大小写敏感性等等。应用词法规则的结果应是由特定命名约定管理的所有名称都有一致的表现形式。 唯一性原则：在名称的范围中，可以有或没有对名称的唯一性要求。 注册 整个注册流程有两类状态：注册状态是指注册、元数据的质量或者管理项的进程的级别。管理状态是指注册机构处理注册请求的管理流程中的状态。进入到元数据注册系统中的每个管理项都应当有一个注册状态。 管理状态详述了某个注册状态下的管理项所经历的过程。它标识出管理项在某一注册状态中发展的过程。管理状态的允许值很可能依赖于该管理项当前所处的注册状态。注册机构负责管理状态的设置和使用，并决定管理状态的允许值。注册机构负责挂你状态的细化、发布和实施。 注册状态详细说明了元数据注册系统中的管理项的情况。注册状态类别应当应用到已经进入元数据注册簿中的单个管理项。管理项的注册状态分两类，生命周期型和记录型。生命周期型注册状态处理的是管理项的元数据的质量逐渐提高、完善的过程、以及管理项使用上的优先选择问题。记录型注册状态用于表明元数据的质量不再继续优化或者管理项不再被推荐使用。这些状态类的关系，以及管理项要达到某个特定的注册状态级别的准则： 管理项注册状态类别 状态准则 生命周期状态 首选 注册机构确认该管理项： 1.在使用该注册系统的团体中被优先选用 标准 注册机构确认该管理项：在使用该注册系统的团体中， 1. 质量合格； 2. 且得到广泛应用 合格 注册机构确认： 1. 必须元数据属性完整， 2. 且必须元数据属性符合应用质量要求 已记录 注册机构确认： 1. 所有必选元数据属性完整 候选 管理项被提议按照注册级别逐渐完善 未完成 提交者期望使用该元数据注册系统的团体意识到其本领域内的一个管理项的存在 失效 注册机构批准该管理项： 1. 不再推荐给该元数据注册系统的团体使用； 2. 且不应当继续被使用 被替代 注册机构确认该管理项： 1. 不再推荐给该元数据注册系统的团体使用； 2. 且一个继承的管理项被优先选用 记录状态 历史 提交者希望使用该元数据注册系统的团体，意识到在其本领域内一个管理项曾经存在过 应用 注册机构希望使用该元数据注册系统的团体，意识到其本领域内一个管理项的存在，该管理项存在于一个应用系统中，尚未在逻辑层次上进行规范。有可能会对该管理项进行规范描述 参考资料 GB/T 18391 国标","categories":[{"name":"大数据概念","slug":"大数据概念","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5/"}],"tags":[{"name":"元数据","slug":"元数据","permalink":"https://wangqian0306.github.io/tags/%E5%85%83%E6%95%B0%E6%8D%AE/"},{"name":"Metadata","slug":"Metadata","permalink":"https://wangqian0306.github.io/tags/Metadata/"}]},{"title":"DAMA","slug":"theory/dama","date":"2022-09-22T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/dama/","permalink":"https://wangqian0306.github.io/2022/dama/","excerpt":"","text":"DAMA 简介 DAMA 国际是一个全球性数据管理和业务专业志愿人士组成的非营利协会，致力于数据管理的研究和实践。此协会编写了一本介绍数据管理知识体系的书名为：《DAMA数据管理知识体系指南（原书第2版）》(DAMA-DMBOK2)。 注：也被称为 DAMA 2.0，全书大篇幅的内容都在讲理论与实现方式，并没有大篇幅涉及到具体实现。 主要讲述的内容 数据治理(Data Governance)通过建立一个能够满足企业数据需求的决策体系，为数据管理提供指导和监督 数据架构(Data Architecture)定义了与组织战略协调的管理数据资产蓝图，以建立战略性数据需求及满足需求的总体设计 数据建模和设计(Data Modeling and Design)以数据模型的精确形式，进行发现、分析、展示和沟通数据需求的过程 数据存储和操作(Data Storage and Operations)以数据价值最大化为目标，在整个数据生命周期中，从计划到销毁的各种操作活动 数据安全(Data Security)确保数据隐私和机密性得到维护，数据不被破坏，数据被适当访问 数据集成和互操作(Data Integration and Interoperability)包括与数据存储、应用程序和组织之间的数据移动和整合相关的过程 文档和内容管理(Document and Content Management)用于管理非结构化媒体数据和信息的生命周期过程，包括计划、实施和控制活动，尤其是指支持法律法规遵从性要求所需的文档 参考数据和主数据(Reference and Master Data)包括核心共享数据的持续协调和维护，使关键业务实体的真实信息，以准确、及时和相关联的方式在各系统间得到一致使用 数据仓库和商务智能(Data Warehousing and Business Intelligence)包括计划、实施和控制流程来管理决策支持数据，并使知识工作者通过分析报告从数据中获得价值 元数据(Metadata)包括规划、实施和控制活动，以便能够访问高质量的集成元数据，包括定义、模型、数据流和其他至关重要的信息(对理解数据及其创建、维护和访问系统有帮助) 数据质量(Data Quality)包括规划和实施质量管理技术，以测量、评估和提高数据在组织内的适用性 参考资料 官方网站","categories":[{"name":"大数据概念","slug":"大数据概念","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5/"}],"tags":[{"name":"DAMA","slug":"DAMA","permalink":"https://wangqian0306.github.io/tags/DAMA/"}]},{"title":"Flink Table API","slug":"flink/table","date":"2022-09-20T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_table/","permalink":"https://wangqian0306.github.io/2022/flink_table/","excerpt":"","text":"Flink Table API 简介 Apache Flink 提供了两种处理关系型内容的 API，分别是 Table API 和 SQL。 样例 1234567891011121314151617181920212223// for batch programs use ExecutionEnvironment instead of StreamExecutionEnvironmentval env = StreamExecutionEnvironment.getExecutionEnvironment// create a TableEnvironmentval tableEnv = TableEnvironment.getTableEnvironment(env)// register a TabletableEnv.registerTable(&quot;table1&quot;, ...) // ortableEnv.registerTableSource(&quot;table2&quot;, ...) // ortableEnv.registerExternalCatalog(&quot;extCat&quot;, ...)// register an output TabletableEnv.registerTableSink(&quot;outputTable&quot;, ...);// create a Table from a Table API queryval tapiResult = tableEnv.scan(&quot;table1&quot;).select(...)// Create a Table from a SQL queryval sqlResult = tableEnv.sqlQuery(&quot;SELECT ... FROM table2 ...&quot;)// emit a Table API result Table to a TableSink, same for SQL resulttapiResult.insertInto(&quot;outputTable&quot;)// executeenv.execute() 参考资料 Table API &amp; SQL","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Apache BookKeeper","slug":"bigdata/bookkeeper","date":"2022-09-15T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/bookkeeper/","permalink":"https://wangqian0306.github.io/2022/bookkeeper/","excerpt":"","text":"Apache BookKeeper 简介 BookKeeper 是一种服务，它提供日志条目流(记录)的持久存储在名为 ledgers 的序列中。BookKeeper 会将数据分布式的复制到多台设备中存储。 基本元素 在 BookKeeper 中： 每个日志记录单元被称为 entry 又名 record 由日志记录单元的数据流被称为 ledger 每个存储 ledger 的服务器被称为 bookie BookKeeper 被设计为可靠且具有弹性以应对各种故障。Bookies 可能会崩溃、损坏数据或丢弃数据，但只要在 ensemble 中有足够多的 bookie 行为正确，整个服务就会正确运行。 每个 entry 包含了写入 ledger 的字节序列。每个条目都有以下字段： 列名 Java 类型 描述 Ledger number Long entry 写入的 ledger ID Entry number Long entry ID Last confirmed (LC) Long 最后一个 entry 的 ID Data Byte[] 数据 Authentication code Byte[] 消息认证编号 每个 ledger 会按顺序存储 entry ,并且遵循以下规则： 序列化写入 至多写入一次 这代表了 ledger 的语义为只做追加。entry 写入之后就无法被修改。正当的写入顺序需要客户端负责。 每个 bookie 服务器存储 ledgers 的一些片段，它们是 BookKeeper 存储服务器的组成元素。 为了性能考虑对于给定 L ledger 来说都有一组 bookie 服务器负责存储。 当一些 entrie 写入 ledger 时，这些 entrie 会被分散存储到 bookie 组中(写入一个子组肯定比写入所有的服务器更好)。 数据存储 注：BookKeeper 收到了 HDFS NameNode 的启发。 目前 BookKeeper 使用 ZooKeeper 存储元数据，例如：ledger 元数据，目前可用的 bookie 服务器，等等。 实际数据则按照日志结构进行存储，分为如下三种： journals entry logs index files Journals 一个 journal file 存储了 BookKeeper 的事务日志。在 ledger 更新发生之前，bookie 会确保此更新事务的相关描述会被写入一个暂未合规的存储中。一个新的 journal file 会在 bookie 启动或者旧的 journal file 文件大小达到设定阈值的时候创建。 Entry logs entry log file 管理了 BookKeeper 客户端发送回来已经完成写入的 entry。来自不同 ledger 的 entry 会经过聚合然后顺序写入，而它们的 offset 会作为指针缓存在 ledger 中来支持快速检索。 一个新的 entry log file 会在 bookie 启动或者旧的 entry log file 文件大小达到设定阈值的时候创建。 旧的 entry log file 一旦与 ledger 断开关联就会被垃圾回收线程收集。 Index files index file 是为了每个 ledger 创建的，其中包含了文件头和一些固定长度的索引页，其中记录了 entry log file 中存储数据的 offset。 由于更新 index file 会引入随机磁盘I/O，因此 index file 由后台运行的同步线程进行延迟更新。这确保了更新的快速性能。在索引页被持久化到磁盘之前，它们被收集在 ledger 缓存中进行查找。 注：ledger 缓存页存储在内存池中，这可以让磁盘头调度更有效。 写入顺序 当客户端指示 bookie 写入 ledger 一项 entry 时，会遵循如下步骤持久写入磁盘： 写入 entry log 将 entry 索引写入 ledger 缓存 对应于此 entry 的写入事件会追加写入 journal 返回响应至客户端 注: 出于性能原因，entry log 会缓存一些到本地内存中，然后批量的写入磁盘。当 ledger 缓存到足够的索引页就会将它们刷写至磁盘。 ledger 缓存页会在如下两种情况下写入 index files： ledger 缓存的内存达到阈值。没有空间存储新的缓存页了。旧的缓存页会被驱逐出缓存然后持久化到磁盘中。 一个后台同步线程会在周期性的运行负责将 ledger 缓存页刷写到 index files。 除了刷新 ledger 缓存页之外，此同步线程还负责滚动 journal file 并通过此方式防止 journal file 使用太多的磁盘空间。在同步线程中的数据刷写流程如下： 将 LastLogMark 记录在内存中。LastLogMark 表示了此 entriy 在持久化之前(存储到 index file 和 entry log file)的如下两种信息： txnLogId(journal file ID) txnLogPos(journal 的 offset) 旧的缓存页会从 ledger 缓存页刷写到 index files 并且 entry log file 也会被刷写，来确保所有缓存在 entry log file 的 entry 都被持久化在磁盘上。 注：理想情况下 bookie 只需要刷写索引页和 entry log file 其中先于 LastLogMark 的 entry。但是在 ledger 和 entry log 映射到 journal files 中的内容并没有这样的信息。因此，线程会在此处完全刷写 ledger 缓存和 entry log，并且可能会刷写 LastLogMark 之后的数据。不过，刷写更多数据没什么大问题，只是有些多余。 将 LastLogMark 持久化到磁盘，这可以让在 LastLogMark 之前的 entry 数据和索引页写入磁盘。这样可以让 journal file 更安全的移除早于 txnLogId 的数据。 如果 bookie 在持久化 LastLogMark 之前故障了，它还是会有 journal file 其中存储了 索引页中可能没有持久化的 entry。因此，当 bookie 重启的时候，它会检查 journal file 并且重新存储这些 entry，数据不会丢失。 使用上述数据刷写机制，当 bookie 关闭时，同步线程跳过数据刷写是安全的。然而在 entry logger 中会使用缓存的频道(channel)批量写入数据，在关闭时它可能还缓冲了数据。 bookie 还需要确保在关闭期间 entry log 每次的刷写都是完成的。否则，entry log file 会因部分 entry 而损坏。 数据压缩 在 bookie 中， entry log file 含有不同的 ledger 它们的 entry 交错在一起。 bookie 为了清理磁盘空间会运行垃圾回收线程删除不相关的 entry log file。 如果一个给定的 entry log file 包含有尚未删除 ledger 中的 entry，则 entry log file 则永远不会被删除占用的磁盘空间也永远不会清空。为了避免此种情况，bookie 服务器会在垃圾回收线程中压缩 entry log file 并以此来节约空间。 压缩有两种不同的运行频率：小型压缩和大型压缩。小型压缩和大型压缩的区别在于它们的阈值和运行间隔。 垃圾收集阈值是那些未删除的 ledger 占用 entry log file 大小的百分比。默认的小型压缩阈值为 0.2，大型压缩为 0.8。 垃圾收集运行间隔是运行压缩的频率。默认的小型压缩间隔为1小时，而大型压缩阈值为1天。 注：如果阈值或间隔设置为小于或等于零，则禁用压缩。 垃圾收集器线程中的数据压缩流程如下： 此线程会扫描 entry log files 来获取 entry log 元数据，其中记录了 ledger 组成的 entry log 和它对应的百分比 在正常的垃圾收集流程中，一旦 bookie 确定已删除 ledger，它将从 entry log 元数据中删除并减小存储空间 如果 entry log file 的剩余大小达到指定阈值，则 entry log 中 ledger 的活动的 entry 将被复制到新的 entry log file 复制所有有效 entry 后，将删除旧 entry log file。f方式 部署需求 为了达到最佳性能 BookKeeper 部署需要至少四个节点，且每台服务器至少要有两个硬盘。分别负责： journalDirectory 负责 journal 存储 ledgerDirectories 负责 entry 和部分 ledger 的存储 参考资料 官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"https://wangqian0306.github.io/tags/Pulsar/"},{"name":"BookKeeper","slug":"BookKeeper","permalink":"https://wangqian0306.github.io/tags/BookKeeper/"}]},{"title":"KMP 算法","slug":"algorithm/kmp","date":"2022-09-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/kmp/","permalink":"https://wangqian0306.github.io/2022/kmp/","excerpt":"","text":"KMP 算法 简介 1234567891011121314151617181920212223242526272829303132333435class Solution: def resolve(self, haystack: str, needle: str) -&gt; int: a = len(needle) b = len(haystack) if a == 0: return 0 next_array = self.get_next(a, needle) p = -1 for j in range(b): while p &gt;= 0 and needle[p + 1] != haystack[j]: p = next_array[p] if needle[p + 1] == haystack[j]: p += 1 if p == a - 1: return j - a + 1 return -1 @staticmethod def get_next(a, needle): next_array = [&#x27;&#x27; for i in range(a)] k = -1 next_array[0] = k for i in range(1, len(needle)): while k &gt; -1 and needle[k + 1] != needle[i]: k = next_array[k] if needle[k + 1] == needle[i]: k += 1 next_array[i] = k return next_arraytxt = &quot;ABABDABACDABABCABAB&quot;pat = &quot;ABABCABAB&quot;s = Solution()print(s.resolve(txt,pat)) 参考资料 视频图解 源码","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Kibana","slug":"database/kibana","date":"2022-09-07T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/kibana/","permalink":"https://wangqian0306.github.io/2022/kibana/","excerpt":"","text":"Kibana 简介 Kibana 是一款检索分析和展示 Elastic Stack 的仪表板工具。 容器部署 可以通过如下 docker-compose 部署 Kibana 12345678services: kibana: image: docker.elastic.co/kibana/kibana:8.4.1 environment: SERVER_NAME: kibana.example.org ELASTICSEARCH_HOSTS: &#x27;[&quot;http://es01:9200&quot;,&quot;http://es02:9200&quot;,&quot;http://es03:9200&quot;]&#x27; ports: - &quot;5601:5601&quot; 如与 Elastic Search 一同部署，则可以先编写 .env 文件然后再编写 docker-compose.yaml 即可： 123ELASTIC_PASSWORD=&lt;ELASTIC_PASSWORD&gt;KIBANA_PASSWORD=&lt;KIBANA_PASSWORD&gt;CLUSTER_NAME=&lt;CLUSTER_NAME&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394services: setup: image: docker.elastic.co/elasticsearch/elasticsearch:8.4.1 volumes: - certs:/usr/share/elasticsearch/config/certs user: &quot;0&quot; command: &gt; bash -c &#x27; if [ x$&#123;ELASTIC_PASSWORD&#125; == x ]; then echo &quot;Set the ELASTIC_PASSWORD environment variable in the .env file&quot;; exit 1; elif [ x$&#123;KIBANA_PASSWORD&#125; == x ]; then echo &quot;Set the KIBANA_PASSWORD environment variable in the .env file&quot;; exit 1; fi; if [ ! -f config/certs/ca.zip ]; then echo &quot;Creating CA&quot;; bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip; unzip config/certs/ca.zip -d config/certs; fi; if [ ! -f config/certs/certs.zip ]; then echo &quot;Creating certs&quot;; echo -ne \\ &quot;instances:\\n&quot;\\ &quot; - name: elasticsearch\\n&quot;\\ &quot; dns:\\n&quot;\\ &quot; - elasticsearch\\n&quot;\\ &quot; - localhost\\n&quot;\\ &quot; ip:\\n&quot;\\ &quot; - 127.0.0.1\\n&quot;\\ &quot; - &lt;remote_host&gt;\\n&quot;\\ &gt; config/certs/instances.yml; bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key; unzip config/certs/certs.zip -d config/certs; fi; echo &quot;Setting file permissions&quot; chown -R root:root config/certs; find . -type d -exec chmod 750 \\&#123;\\&#125; \\;; find . -type f -exec chmod 640 \\&#123;\\&#125; \\;; echo &quot;Waiting for Elasticsearch availability&quot;; until curl -s --cacert config/certs/ca/ca.crt https://elasticsearch:9200 | grep -q &quot;missing authentication credentials&quot;; do sleep 30; done; echo &quot;Setting kibana_system password&quot;; until curl -s -X POST --cacert config/certs/ca/ca.crt -u &quot;elastic:$&#123;ELASTIC_PASSWORD&#125;&quot; -H &quot;Content-Type: application/json&quot; https://elasticsearch:9200/_security/user/kibana_system/_password -d &quot;&#123;\\&quot;password\\&quot;:\\&quot;$&#123;KIBANA_PASSWORD&#125;\\&quot;&#125;&quot; | grep -q &quot;^&#123;&#125;&quot;; do sleep 10; done; echo &quot;All done!&quot;; &#x27; healthcheck: test: [ &quot;CMD-SHELL&quot;, &quot;[ -f config/certs/elasticsearch/elasticsearch.crt ]&quot; ] interval: 1s timeout: 5s retries: 120 elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:8.4.1 environment: - discovery.type=single-node - node.name=elasticsearch - cluster.name=$&#123;CLUSTER_NAME&#125; - ELASTIC_PASSWORD=$&#123;ELASTIC_PASSWORD&#125; - xpack.security.enabled=true - xpack.security.http.ssl.enabled=true - xpack.security.http.ssl.key=certs/elasticsearch/elasticsearch.key - xpack.security.http.ssl.certificate=certs/elasticsearch/elasticsearch.crt - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt - xpack.security.http.ssl.verification_mode=certificate - xpack.security.transport.ssl.enabled=true - xpack.security.transport.ssl.key=certs/elasticsearch/elasticsearch.key - xpack.security.transport.ssl.certificate=certs/elasticsearch/elasticsearch.crt - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt - xpack.security.transport.ssl.verification_mode=certificate volumes: - certs:/usr/share/elasticsearch/config/certs ports: - &quot;9200:9200&quot; - &quot;9300:9300&quot; depends_on: setup: condition: service_healthy kibana: image: docker.elastic.co/kibana/kibana:8.4.1 environment: - SERVERNAME=kibana - ELASTICSEARCH_HOSTS=https://elasticsearch:9200 - ELASTICSEARCH_USERNAME=kibana_system - ELASTICSEARCH_PASSWORD=$&#123;KIBANA_PASSWORD&#125; - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt ports: - &quot;5601:5601&quot; depends_on: - elasticsearch volumes: - certs:/usr/share/kibana/config/certsvolumes: certs: driver: local 注：容器启动之后需要等待 startup 服务完成配置。之后即可使用 elastic 账户和之前配置的密码访问 kibana RPM 部署 使用如下命令配置依赖： 12rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchvim /etc/yum.repos.d/kibana.repo 然后填入如下内容： 12345678[kibana-8.x]name=Kibana repository for 8.x packagesbaseurl=https://artifacts.elastic.co/packages/8.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md 使用如下命令进行安装： 1sudo yum install kibana 使用如下命令启动服务： 123sudo /bin/systemctl daemon-reloadsudo /bin/systemctl enable kibana.servicesudo systemctl start kibana.service 配置 软件配置在 /etc/kibana 目录中。 默认日志在 /var/log/kibana 目录中。 注：默认 kibana 不会开放外部访问，如有需求请参照注释编辑 kibana.yml 文件。 样例配置如下： 12server.host: &quot;xxx.xxx.xxx.xxx&quot;elasticsearch.ssl.certificate: /etc/kibana/certs/http_ca.crt 关闭服务 1sudo systemctl stop kibana.service 等待服务启动完成后即可访问 kibana 页面进行初始化配置。 在运行 ElasticSearch 的设备上运行如下命令获取 token： 1/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana 注：使用此 Token 可以快速初始化 Kibana ，需要注意的是目前页面上显示的链接地址如果错误也可以尝试下一步，遇到问题再去使用手动配置方式。 参考资料 官方文档 容器说明 安装手册","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"},{"name":"Kibana","slug":"Kibana","permalink":"https://wangqian0306.github.io/tags/Kibana/"}]},{"title":"monitor","slug":"java/monitor","date":"2022-09-07T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/monitor/","permalink":"https://wangqian0306.github.io/2022/monitor/","excerpt":"","text":"性能监控 简介 为了监控线上环境运行情况可以采用如下组件： Elastic Stack Elastic Stack 在 8 版本之前提供了 APM Server 和 Beats 作为性能监控工具，而在 8 版本之后则采用了 Fleet Server 替代了 APM Server。目前 APM agent 可以兼容如下语言的性能指标采集： GO iOS JAVA .NET Node.js PHP Python Ruby RUM OpenTelemetry 可以使用如下新版的 Elastic Agent 收集数据： 12345678910111213141516services: agent: image: docker.elastic.co/beats/elastic-agent:8.4.1 container_name: elastic-agent user: root environment: - FLEET_ENROLLMENT_TOKEN=&lt;enrollment_token&gt; - FLEET_ENROLL=1 - FLEET_URL=&lt;server_url&gt; - FLEET_SERVER_ENABLE=true - FLEET_SERVER_ELASTICSEARCH_HOST=&lt;es_host&gt; - FLEET_SERVER_SERVICE_TOKEN=&lt;service_token&gt; - FLEET_SERVER_POLICY=&lt;policy&gt; ports: - &quot;8200:8200&quot; - &quot;8220:8220&quot; Prometheus Prometheus 有如下采集的方式： Client Library Exporter 注：针对与 JVM 的性能都可以通过 JMX exporter 完成。 其中有一些内容是官方编写的一些是第三方编写的，在使用时需要格外注意。 SkyWalking SkyWalking 则依赖于 K8s，相较于以上两种来说更针对与微服务之间的数据流监控。目前提供如下语言的监控： Java Python NodeJS Nginx LUA Kong Client JavaScript Rust PHP 参考资料 Elastic Stack Prometheus SkyWalking","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"},{"name":"Go","slug":"Go","permalink":"https://wangqian0306.github.io/tags/Go/"},{"name":"SkyWalking","slug":"SkyWalking","permalink":"https://wangqian0306.github.io/tags/SkyWalking/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"}]},{"title":"网络爬虫","slug":"tmp/spider","date":"2022-09-05T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/spider/","permalink":"https://wangqian0306.github.io/2022/spider/","excerpt":"","text":"网络爬虫 简介 爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。 管理平台 目前市面上有一些管理平台可以方便的管理爬虫： crawlab 可以通过如下 docker-compose 快速启动社区单节点版本： 12345678910111213services: master: image: crawlabteam/crawlab:0.6.0 container_name: crawlab_master environment: CRAWLAB_NODE_MASTER: &quot;Y&quot; CRAWLAB_MONGO_HOST: &quot;mongo&quot; ports: - &quot;8080:8080&quot; depends_on: - mongo mongo: image: mongo:4.2 注：由于最新版无法正常登录，所以采用了最新 release 版。默认账户和密码都是 admin 参考资料 crawlab","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wangqian0306.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"Spark 任务调度","slug":"spark/task","date":"2022-09-01T14:26:13.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spark-task/","permalink":"https://wangqian0306.github.io/2022/spark-task/","excerpt":"","text":"Spark 任务调度拆解 梳理 Spark 的任务调度流程可以概括为下图： 即在提交作业之后进行如下操作： 通过给定的行动算子来进行任务(Job)划分 通过 Shuffle 操作来划分阶段(Stage) 通过每个阶段中最后一个 RDD 的分区数来确定任务(Task)的数量 根据给定的任务调度器(TaskScheduler)来决定任务的调度方式，对任务进行排序 根据指定的本地化级别将任务分发到 Worker 上 参考资料 spark-dagscheduler Spark job submission breakdown spark源码-任务提交流程之-7-流程梳理总结","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"Swagger2Markup","slug":"java/swagger2markup","date":"2022-08-26T13:32:58.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/swagger2markup/","permalink":"https://wangqian0306.github.io/2022/swagger2markup/","excerpt":"","text":"Swagger2Markup 简介 此项目是 swagger 转 AsciiDoc 或 Markdown 的一款开源工具，通过将手写文档与自动生成的 API 文档相结合，简化了 RESTful API 文档的生成方式。 使用方式 首先需要引入如下依赖包： 123456&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132333435&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;systemPropertyVariables&gt; &lt;io.springfox.staticdocs.outputDir&gt;$&#123;swagger.output.dir&#125;&lt;/io.springfox.staticdocs.outputDir&gt; &lt;io.springfox.staticdocs.snippetsOutputDir&gt;$&#123;swagger.snippetOutput.dir&#125; &lt;/io.springfox.staticdocs.snippetsOutputDir&gt; &lt;/systemPropertyVariables&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;io.github.swagger2markup&lt;/groupId&gt; &lt;artifactId&gt;swagger2markup-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;swagger2markup.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;swaggerInput&gt;$&#123;swagger.input&#125;&lt;/swaggerInput&gt; &lt;outputDir&gt;$&#123;generated.asciidoc.directory&#125;&lt;/outputDir&gt; &lt;config&gt; &lt;swagger2markup.markupLanguage&gt;ASCIIDOC&lt;/swagger2markup.markupLanguage&gt; &lt;swagger2markup.pathsGroupedBy&gt;TAGS&lt;/swagger2markup.pathsGroupedBy&gt; &lt;/config&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;convertSwagger2markup&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt; 注：此处需要读取 spring 的 repo 建议使用 Nexus 做个本地源，或者手动添加也可。 并进行如下配置： 123456789101112&lt;properties&gt; &lt;asciidoctor.input.directory&gt;$&#123;project.basedir&#125;/src/docs/asciidoc&lt;/asciidoctor.input.directory&gt; &lt;swagger.output.dir&gt;$&#123;project.build.directory&#125;/swagger&lt;/swagger.output.dir&gt; &lt;swagger.snippetOutput.dir&gt;$&#123;project.build.directory&#125;/asciidoc/snippets&lt;/swagger.snippetOutput.dir&gt; &lt;generated.asciidoc.directory&gt;$&#123;project.build.directory&#125;/asciidoc/generated&lt;/generated.asciidoc.directory&gt; &lt;asciidoctor.html.output.directory&gt;$&#123;project.build.directory&#125;/asciidoc/html&lt;/asciidoctor.html.output.directory&gt; &lt;asciidoctor.pdf.output.directory&gt;$&#123;project.build.directory&#125;/asciidoc/pdf&lt;/asciidoctor.pdf.output.directory&gt; &lt;swagger.input&gt;$&#123;swagger.output.dir&#125;/swagger.json&lt;/swagger.input&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;asciidoctor-plugin.version&gt;1.5.6&lt;/asciidoctor-plugin.version&gt;&lt;/properties&gt; 编写如下文档配置和测试用例： 12345678910111213141516171819202122232425262728293031import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;@Configurationpublic class OpenapiConf &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title(&quot;demo&quot;) .description(&quot;demo&quot;) .version(&quot;0.1.0&quot;) .build(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.http.MediaType;import org.springframework.mock.web.MockHttpServletResponse;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.MvcResult;import java.io.BufferedWriter;import java.nio.charset.StandardCharsets;import java.nio.file.Files;import java.nio.file.Paths;import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;@SpringBootTest@AutoConfigureMockMvcpublic class Swagger2MarkupTest &#123; @Autowired private MockMvc mockMvc; @Test public void createSpringfoxSwaggerJson() throws Exception &#123; String outputDir = System.getProperty(&quot;io.springfox.staticdocs.outputDir&quot;); MvcResult mvcResult = this.mockMvc.perform(get(&quot;/v2/api-docs&quot;) .accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andReturn(); MockHttpServletResponse response = mvcResult.getResponse(); String swaggerJson = response.getContentAsString(); Files.createDirectories(Paths.get(outputDir)); try (BufferedWriter writer = Files.newBufferedWriter(Paths.get(outputDir, &quot;swagger.json&quot;), StandardCharsets.UTF_8)) &#123; writer.write(swaggerJson); &#125; &#125;&#125; 在主类中新增如下注解： 123456789101112import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.autoconfigure.domain.EntityScan;import org.springframework.data.jpa.repository.config.EnableJpaRepositories;import springfox.documentation.oas.annotations.EnableOpenApi;@EntityScan@EnableJpaRepositories@EnableOpenApi@SpringBootApplicationpublic class TestApplication &#123;&#125; 然后使用如下命令即可在 target/asciidoc/generated 文件夹下即可获得如下文档： 1mvn test definitions.adoc overview.adoc paths.adoc security.adoc 注：目前尚且不支持 openapi(swagger3) 参考资料 官方项目 使用手册 样例代码","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Swagger2Markup","slug":"Swagger2Markup","permalink":"https://wangqian0306.github.io/tags/Swagger2Markup/"}]},{"title":"OCR 工具","slug":"ai/ocr","date":"2022-08-17T13:41:32.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/ocr/","permalink":"https://wangqian0306.github.io/2022/ocr/","excerpt":"","text":"OCR 工具整理 第三方工具 国外 DocTranslator 注：此网站支持在线将文档进行翻译并转化为 Word。 onlineocr(网页) 注：此工具支持中文解析，但是效果不是特别的好。 Adobe Acrobat DC(软件) 注：此工具可以使用 OCR 功能编辑 PDF。 国内 百度智慧云 注：目前免费，但是调用需求比较怪，需要 url 或者 base64 的流，且大小不能超过 4 M。 夸克浏览器 注：手机端使用，识别效果很好，但是需要付费，图片和文档分开收费，分开收取会员。 开源库 PaddleOCR PaddleOCR 官方项目 车牌识别应用场景 一种基于PaddleOCR的轻量级车牌识别模型 Surya Surya 官方项目","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wangqian0306.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"大气层折腾教程","slug":"tmp/atmosphere","date":"2022-08-08T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/atmosphere/","permalink":"https://wangqian0306.github.io/2022/atmosphere/","excerpt":"","text":"大气层折腾教程 简介 大气层只是一个 Switch 的固件，还需要配合 hekate(bootloader) 和相关 Patch 包，才能代替 SX OS。 而为了管理这些软件和依赖的就产生了一堆的整合包，例如 DeepSea 安装软件 除了复制文件到内存卡上之外还可以通过 NS-USBloader 软件配合大气层携带的安装软件使用 USB 线的方式进行软件安装。安装软件可以选择很久没有更新的 awoo 或 DBI 等。 注：在安装软件时，需要保持按住 R 键打开任意一款游戏，然后等待弹出系统软件列表然后选择安装软件进行安装即可。 系统更新 在安装完成大气层之后，可以使用 DayBreak 软件升级虚拟系统版本。 注：系统版本与大气层版本直接相关。大气层版本决定了最高的系统版本，如需升级系统则首先需要升级大气层，建议采用整合包完成。 常见问题 报错 010041544d530000 此问题是由于默认分区方式采用了 exFAT 而导致的系统异常，需要下载磁盘管理工具将内存卡格式化为 FAT 32(族大小为 64k) 即可。 注：这样一来单个文件无法超过 4G 但可以通过其他方式安装软件。 参考资料 参考文档 Atmosphere hekate NS-USBloader DeepSea 整合包","categories":[],"tags":[{"name":"Atmosphere","slug":"Atmosphere","permalink":"https://wangqian0306.github.io/tags/Atmosphere/"}]},{"title":"Git 常见问题","slug":"windows/git","date":"2022-08-05T12:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2022/terminal/","permalink":"https://wangqian0306.github.io/2022/terminal/","excerpt":"","text":"Git Bash 中文乱码的问题 简介 在运行 Git Bash 的时候会出现命令行中无法正常显示中文，但是在文件部分却可以正常显示的问题。此问题可以通过如下方法修复： 打开 Git Bash 右键 Bash 窗口中的任意位置，选择 Options 选项 在 Text 栏中选择 Locale 选项为 zh_CN 将 Character set 选项设定为 GBK 即可。 配置 git 代理 12git config --global http.proxy http://host:portgit config --global https.proxy http://host:port 取消 git 代理 12git config --global --unset http.proxygit config --global --unset https.proxy","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"Windows Terminal","slug":"Windows-Terminal","permalink":"https://wangqian0306.github.io/tags/Windows-Terminal/"},{"name":"Git Bash","slug":"Git-Bash","permalink":"https://wangqian0306.github.io/tags/Git-Bash/"}]},{"title":"Bigtop","slug":"bigdata/bigtop","date":"2022-07-27T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/bigtop/","permalink":"https://wangqian0306.github.io/2022/bigtop/","excerpt":"","text":"Bigtop 简介 Apache Bigtop 是一个用于开发 Apache Hadoop 生态系统的打包和测试的项目。可以方便的使用虚拟机(vagrant)或容器(docker)部署 Hadoop 集群。 Bigtop 包括用于在各个级别(打包、平台、运行时等)进行测试的工具和框架，用于整个数据平台的初始部署和升级场景，而不仅仅是单个组件。 注：可以使用 Bigtop 快速的测试软件之间的兼容性，打包 RPM/Deb 包，运行冒烟测试。目前 Ambari 已经在 RoadMap 当中说明 Bigtop mpack 会在后期做接入，但是目前的版本。 参考资料 官网 官方文档 支持组件 Ambari Roadmap","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Bigtop","slug":"Bigtop","permalink":"https://wangqian0306.github.io/tags/Bigtop/"}]},{"title":"Spring Boot Configuration Metadata","slug":"spring/conf","date":"2022-07-22T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/conf/","permalink":"https://wangqian0306.github.io/2022/conf/","excerpt":"","text":"Spring Boot Configuration Metadata 简介 除了使用 @Value 注解之外还可以引入此包来实现配置项的引入功能。 注：这样使用还可以避免 IDEA 读取配置项报警的问题。 使用方式 首先需要引入依赖包 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 123dependencies &#123; annotationProcessor &quot;org.springframework.boot:spring-boot-configuration-processor&quot;&#125; 然后需要编写配置项文件 1234567891011121314import lombok.Getter;import lombok.Setter;import org.springframework.boot.context.properties.ConfigurationProperties;@Setter@Getter@ConfigurationProperties(prefix = &quot;xxx.xxx&quot;)public class XxxProperties &#123; private Integer xxx = 10; private Integer yyy = 10;&#125; 最后要在启动类中标识配置项文件 12345678910111213import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.context.properties.EnableConfigurationProperties;@SpringBootApplication@EnableConfigurationProperties(&#123;XxxProperties.class&#125;)public class XxxApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Xxx.class, args); &#125;&#125; 配置 IDEA 进入如下配置项 Settings -&gt; Build,Execution,Deployment -&gt; Compiler -&gt; Annotation Processors 打开如下配置即可 Enable annotation processing 参考资料 Properties with Spring and Spring Boot SpringBoot 中的 ConfigurationProperties 官方网站","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Spark Partitioner","slug":"spark/partitioner","date":"2022-07-19T14:26:13.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/partitioner/","permalink":"https://wangqian0306.github.io/2022/partitioner/","excerpt":"","text":"Spark Partitioner 简介 Spark 中的数据可以被分区器(Partitioner)重新分配分区，解决数据倾斜等问题。 分区函数 Spark 内置了 HashPartitioner 和 RangePartitioner 两种分区器，并且用户可以编写一个 Partitioner 的子类完成自定义分区器。 HashPartitioner HashPartitioner 为默认分区器 分区方式： 哈希取模，具体源码如下 1234567891011121314151617class HashPartitioner(partitions: Int) extends Partitioner &#123; def numPartitions: Int = partitions def getPartition(key: Any): Int = key match &#123; case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) &#125; override def equals(other: Any): Boolean = other match &#123; case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false &#125; override def hashCode: Int = numPartitions&#125; 注：此方式可能导致数据偏移。 RangePartitioner 分区方式： 根据父 RDD 的数据特征，确定子 RDD 分区的边界 给定一个键值对数据，能够快速根据键值定位其所应该被分配的分区编号 注：通过水塘抽样算法确定边界数组，再根据 key 来获取所在的分区索引。具体实现细节参见源码。 自定义分区器 123456789101112class TestPartitioner(Partitions:Int) extends Partitioner &#123; override def numPartitions: Int = Partitions override def getPartition(key: Any):Int = &#123; val a = if (&lt;xxx&gt;) &#123; 1 &#125;else if (&lt;xxx&gt;)&#123; 2 &#125;else&#123; 0 &#125; a&#125; 参考资料 官方文档 Apache Spark 源码阅读 源码","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"lambda","slug":"scala/lambda","date":"2022-07-18T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/lambda/","permalink":"https://wangqian0306.github.io/2022/lambda/","excerpt":"","text":"Lambda 表达式 简介 在 Scala 中 Lambda 表达式还有一些特殊的用法。 样例 可以使用 _ 代替唯一参数： 123val ints = List(1, 2, 3)val doubledIntsFull = ints.map(i =&gt; i * 2)val doubledInts = ints.map(_ * 2) 可以直接传入函数： 123val ints = List(1, 2, 3)ints.foreach((i: Int) =&gt; println(i))ints.foreach(println) 可以快速访问元组中的某个元素： 123val tuplesList = List((&quot;wq&quot;,1),(&quot;nice&quot;,2))tuplesList.map(tuple =&gt; tuple._1).foreach(println)tuplesList.map(_._1).foreach(println) 参考资料 匿名函数","categories":[{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/tags/Scala/"}]},{"title":"Spark DataSet 和 DataFrame","slug":"spark/dataframe","date":"2022-07-14T14:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/dataframe/","permalink":"https://wangqian0306.github.io/2022/dataframe/","excerpt":"","text":"Spark DataSet 和 DataFrame 简介 DataSet 是一个分布式数据的集合。 DataFrame 则是按照列名进行整理后的 DataSet，在概念上更贴近于传统关系型数据库中的表或是 R/Python 中的 DataFrame 但在底层进行了更丰富的优化。 简单使用 DataFrame 读取数据 从 parquet 导入 DataFrame 1val usersDF = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;) 从 json 导入 DataFrame 1val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;) 从 csv 导入 DataFrame 12345val peopleDFCsv = spark.read.format(&quot;csv&quot;) .option(&quot;sep&quot;, &quot;;&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .option(&quot;header&quot;, &quot;true&quot;) .load(&quot;examples/src/main/resources/people.csv&quot;) 从 orc 导入 DataFrame 1val parDF=spark.read.orc(&quot;/tmp/orc/data.orc/gender=M&quot;) 从 rdd 导入 DataFrame 1val rdd = spark.sparkContext.makeRDD(List(1,2,3)) 从 DataSet 转为 DataFrame 1val df = ds.toDF() 从 Hive 读取数据 1val df = spark.read().table(&quot;person&quot;); 从 JDBC 读取数据 1234567val jdbcDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;) .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;) .option(&quot;user&quot;, &quot;username&quot;) .option(&quot;password&quot;, &quot;password&quot;) .load() 输出 写入 Parquet 1peopleDF.write.parquet(&quot;people.parquet&quot;) 写入 ORC 12345usersDF.write.format(&quot;orc&quot;) .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;) .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;) .option(&quot;orc.column.encoding.direct&quot;, &quot;name&quot;) .save(&quot;users_with_options.orc&quot;) 写入 JSON 1allDF.write.json(&quot;src/main/other_resources/all_json_file.json&quot;) 写入 CSV 1df.write.format(&quot;csv&quot;).save(&quot;/tmp/spark_output/datacsv&quot;) 写入文本文件 1df.write.text(&quot;output&quot;) 写入 Hive 表 1df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;) 写入 JDBC 链接的数据库 1234567jdbcDF.write .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;) .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;) .option(&quot;user&quot;, &quot;username&quot;) .option(&quot;password&quot;, &quot;password&quot;) .save() 写入 Avro 1df.write.format(&quot;avro&quot;).save(&quot;namesAndFavColors.avro&quot;) DataSet 从其他源转换 从集合创建 123case class Person(name: String, age: Long)val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()caseClassDS.show() 从 DataFrame 创建 12case class Person(name: String, age: Long)val ds: DataSet[Person] = df.as[User] 从 RDD 创建 123456case class Person(name: String, age: Long)val ds: DataSet[Person] = rdd.map =&gt; &#123; case (name,age) =&gt; &#123; Person(name,age) &#125;&#125;.toDS() 参考资料 官方文档 样例教程","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"Spark RDD","slug":"spark/rdd","date":"2022-07-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/rdd/","permalink":"https://wangqian0306.github.io/2022/rdd/","excerpt":"","text":"Spark RDD 简介 Spark 的设计围绕弹性分布式数据集(RDD) 的概念展开，RDD 是可以并行操作的元素的容错集合。 它是一个不可变的分布式对象集合。 RDD 中的每个数据集都被划分为逻辑分区，可以在集群的不同节点上进行计算。 RDD 可以包含任何类型的 Python、Java 或 Scala 对象，包括用户定义的类。 有两种方法可以创建 RDD：并行化 驱动程序中的现有集合，或引用外部存储系统中的数据集，例如共享文件系统、HDFS、HBase 或任何提供 Hadoop InputFormat 的数据源。 在高层次上，每个 Spark 应用程序都包含一个驱动程序(driver program)，该驱动程序运行用户的 main 函数并在集群上执行各种并行操作。 Spark 提供的主要抽象是弹性分布式数据集(RDD resilient distributed dataset)，它是跨集群节点分区的元素集合，可以并行操作。 RDD 是通过从 Hadoop 文件系统(或任何其他 Hadoop 支持的文件系统)中的文件或驱动程序中现有的 Scala 集合开始并对其进行转换来创建的。 用户还可以要求 Spark 将 RDD 持久化到内存中，以便在并行操作中有效地重用它。最后，RDD 会自动从节点故障中恢复。 Spark 中除了 RDD 之外还有一个抽象即：可以在并行操作中使用的共享变量。 默认情况下，当 Spark 在不同节点上并行运行一个函数作为一组任务时，它会将函数中使用的每个变量的副本发送到每个任务。 有时，需要在任务之间或在任务和驱动程序之间共享变量。 Spark 支持两种类型的共享变量：广播变量，可用于在所有节点的内存中缓存值，以及累加器，它们是仅“添加”到的变量，例如计数器和求和计算。 操作类型 RDD 支持两种类型的操作： transformations(从现有数据集创建新数据集) actions(在对数据集运行计算后将值返回给驱动程序) 例如，map 是一种通过函数传递每个数据集元素并返回表示结果的新 RDD 的转换。另一方面，reduce 是使用某个函数聚合 RDD 的所有元素并将最终结果返回给驱动程序的操作(尽管也有 reduceByKey 返回分布式数据集的并行操作)。 Spark 中的所有转换都是惰性的，因为它们不会立即计算结果。 相反，他们只记得应用于某些基础数据集（例如文件）的转换。仅当操作需要将结果返回给驱动程序时才计算转换。 这种设计使 Spark 能够更高效地运行。例如，我们可以意识到通过创建的数据集map将在 a 中使用，reduce并且仅将结果返回reduce给驱动程序，而不是更大的映射数据集。 默认情况下，每个转换后的 RDD 可能会在您每次对其运行操作时重新计算。但是，您也可以使用(or) 方法将 RDD 持久化到内存中，在这种情况下，Spark 会将元素保留在集群上，以便下次查询时更快地访问它。还支持在磁盘上持久化 RDD，或跨多个节点复制。 Shuffle Spark 中的某些操作会触发一个称为 shuffle 的事件。shuffle 是 Spark 用于重新分配数据的机制，以便跨分区以不同方式分组。这通常涉及跨执行器和机器复制数据，使 Shuffle 成为一项复杂且成本高昂的操作。 要了解在 shuffle 期间发生了什么，我们可以考虑 reduceByKey 操作。reduceByKey 操作会生成一个新的 RDD，返回其中单个键的所有值都组合成一个元组-键和针对与该键关联的所有值执行 reduce 函数的结果。 挑战在于，并非单个键的所有值都必须位于同一分区甚至同一台机器上，但它们必须位于同一位置才能得出计算结果。 在 Spark 中，数据通常不会跨分区分布在特定操作的必要位置。在计算过程中，单个任务将在单个分区上运行 - 因此，为了组织单个 reduceByKey 的 reduce 任务执行的所有数据，Spark 需要执行 all-to-all 操作。 它必须从所有分区中读取以找到所有键的所有值，然后将跨分区的值组合在一起以计算每个键的最终结果 - 这称为 shuffle。 尽管新混洗数据的每个分区中的元素集合是确定性的，分区本身的顺序也是确定性的，但这些元素的顺序不是。如果希望在 shuffle 之后可预测有序的数据，那么可以使用： mapPartitions 对每个分区进行排序，例如，.sorted repartitionAndSortWithinPartitions 在重新分区的同时有效地对分区进行排序 sortBy 制作一个全局有序的 RDD 可能导致洗牌的操作包括重新分区操作，如 repartition 和 coalesce，'ByKey操作(计数除外)，如 groupByKey 和 reduceByKey，以及 join 操作，如 cogroup 和 join。 持久性 Spark 中最重要的功能之一是跨操作将数据集持久化(或缓存)在内存中。 当你持久化一个 RDD 时，每个节点都会将它计算的任何分区存储在内存中，并在对该数据集(或从它派生的数据集)的其他操作中重用它们。 这使得未来的行动更快(通常超过 10 倍)。缓存是迭代算法和快速交互使用的关键工具。 可以使用 persist() 或 cache() 方法将 RDD 标记为持久化。第一次计算时，它将保存在节点的内存中。 Spark 的缓存是容错的——如果 RDD 的任何分区产生了丢失，它将使用最初创建它的转换自动重新计算。 此外，每个持久化的 RDD 可以使用不同的存储级别存储，例如，允许您将数据集持久化到磁盘上，将其持久化在内存中，但它会作为序列化的 Java 对象(以节省空间和跨节点复制)。 这些级别是通过将 StorageLevel 对象传递给 persist()。该 cache() 方法是使用默认存储级别的简写，即 StorageLevel.MEMORY_ONLY(将反序列化的对象存储在内存中)。 共享变量 通常，当传递给 Spark 操作的函数（例如map或reduce）在远程集群节点上执行时，它会在函数中使用的所有变量的单独副本上工作。这些变量被复制到每台机器上，并且对远程机器上的变量的更新不会传播回驱动程序。支持跨任务的通用读写共享变量效率低下。 然而，Spark 确实为两种常见的使用模式提供了两种有限类型的共享变量：广播变量(broadcast variables)和累加器(accumulators)。 广播变量 广播变量允许程序员在每台机器上缓存一个只读变量，而不是随任务一起发送它的副本。 例如，它们可用于以有效的方式为每个节点提供大型输入数据集的副本。Spark 还尝试使用高效的广播算法来分发广播变量，以降低通信成本。 Spark 动作通过一组阶段执行，由分布式 “shuffle” 操作隔离。Spark 自动广播每个阶段内任务所需的公共数据。 以这种方式广播的数据以序列化形式缓存，并在运行每个任务之前进行反序列化。 这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 可以通过 sc.broadcast() 方法构建广播变量。 累加器 累加器是仅通过关联和交换操作“添加”到的变量，因此可以有效地支持并行操作。它们可用于实现计数器（如在 MapReduce 中）或求和。Spark 原生支持数值类型的累加器，如果需要特定类型的累加器需要自行实现。 可以通过分别调用 SparkContext.longAccumulator() 或 SparkContext.doubleAccumulator() 累加 Long 或 Double 类型的值来创建数值累加器。 然后可以使用该 add 方法将在集群上运行的任务添加到其中。但是，程序无法读取其值。只有驱动程序(Driver)可以使用其 value 方法读取累加器的值。 使用 Spark 官方已经不建议采用 RDD 这样的低级 API 了，如有需求应该选择 DataSet 或 DataFrame 这样的高级 API。 参考资料 官方文档","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"sbt 工具","slug":"scala/sbt","date":"2022-07-08T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/sbt/","permalink":"https://wangqian0306.github.io/2022/sbt/","excerpt":"","text":"sbt 工具 简介 sbt 是 Scala 的包管理工具。 使用及配置 可以使用 IDEA 工具轻松的创建由 sbt 管理的 Scala 项目，此处不在对环境安装和配置进行赘述。 配置依赖项 123456name := &quot;&lt;name&gt;&quot;version := &quot;&lt;version&gt;&quot;scalaVersion := &quot;&lt;scalaVersion&gt;&quot;//使用阿里云的仓库resolvers+=&quot;aliyun Maven Repository&quot;at&quot;http://maven.aliyun.com/nexus/content/groups/public&quot;libraryDependencies += &lt;groupID&gt; % &lt;artifactID&gt; % &lt;revision&gt; 注：也可以使用 organization %% moduleName % version 的方式引入依赖包，这样一来 sbt 则会将项目 Scala 的二进制版本添加到引入中。 打包 fat jar 在创建项目之后，整体的目录结构如下： 123456project-root |----build.sbt |----src |----project |----plugins.sbt |----build.properties 如需打包 fatjar 则需要在 plugins.sbt 中写入如下内容： 1addSbtPlugin(&quot;com.eed3si9n&quot; % &quot;sbt-assembly&quot; % &quot;0.15.0&quot;) 然后运行 sbt assembly 即可 如果需要移除某些依赖则在 build.sbt 文件中采用如下写法： 1libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % sparkVersion % &quot;provided&quot; 如需剔除了 Scala 相关依赖和指定主类则需要在 build.sbt 文件中进行如下配置： 123456lazy val root = (project in file(&quot;.&quot;)) .settings( assemblyPackageScala / assembleArtifact := false, assembly / mainClass := Some(&quot;&lt;groupID&gt;.&lt;artifactID&gt;.&lt;mainClass&gt;&quot;), assembly / assemblyJarName := &quot;&lt;jarName&gt;.jar&quot;, ) 在 Spark 项目中使用 需要在 build.sbt 文件中进行如下配置，修改合并策略： 1234567891011121314151617181920ThisBuild / assemblyMergeStrategy := &#123; case PathList(&quot;org&quot;,&quot;aopalliance&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;javax&quot;, &quot;inject&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;javax&quot;, &quot;servlet&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;javax&quot;, &quot;activation&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;org&quot;, &quot;apache&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;com&quot;, &quot;google&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;com&quot;, &quot;esotericsoftware&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;com&quot;, &quot;codahale&quot;, xs @ _*) =&gt; MergeStrategy.last case PathList(&quot;com&quot;, &quot;yammer&quot;, xs @ _*) =&gt; MergeStrategy.last case &quot;about.html&quot; =&gt; MergeStrategy.rename case &quot;META-INF/ECLIPSEF.RSA&quot; =&gt; MergeStrategy.last case &quot;META-INF/mailcap&quot; =&gt; MergeStrategy.last case &quot;META-INF/mimetypes.default&quot; =&gt; MergeStrategy.last case &quot;plugin.properties&quot; =&gt; MergeStrategy.last case &quot;log4j.properties&quot; =&gt; MergeStrategy.last case x =&gt; val oldStrategy = (ThisBuild / assemblyMergeStrategy).value oldStrategy(x)&#125; 参考资料 官方文档 sbt-assembly 插件源码及文档","categories":[{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/tags/Scala/"},{"name":"sbt","slug":"sbt","permalink":"https://wangqian0306.github.io/tags/sbt/"}]},{"title":"Spark 知识整理","slug":"spark/init","date":"2022-07-06T14:26:13.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spark_init/","permalink":"https://wangqian0306.github.io/2022/spark_init/","excerpt":"","text":"Spark 知识整理 简介 Apache Spark 是用于大规模数据处理的统一分析引擎。 它提供 Java、Scala、Python 和 R 中的高级 API，以及支持通用执行图(DAG)的优化引擎。 Spark 还支持一组丰富的高级工具，包括: 用于 SQL 和结构化数据处理的 Spark SQL 用于 Pandas 工作负载的 Pandas API on Spark 用于机器学习的 MLlib、用于图形处理的 GraphX 以及用于增量计算和流处理的结构化流(Spark Streaming) 而在部署模式上则分为： 独立部署 Apache Mesos(已弃用) Hadoop Yarn Kubernetes 使用 交互式 Spark 提供了 Scala 和 Python 两种语言的交互式命令行，可以使用如下命令开启交互式命令行： Scala 1./bin/spark-shell 注：使用 :help 可以查看帮助，:quit 退出交互式命令行 Python 1./bin/pyspark 注：使用 help() 可以查看帮助，exit() 退出交互式命令行 在启动后会自动初始化 spark-session 对象，可以使用 spark 变量进行访问。 独立应用程序 对于 Java 和 Scala 这样的应用程序来说可以使用 maven 或 sbt 这样的包管理工具来制作可执行 jar 包，然后使用 spark-submit 命令提交即可。 Python 程序则需要打包成 zip 文件才能提交，详情请参照 官方文档 。 关键对象说明 在 Spark 2.0 之前，SparkContext 是任何 Spark 应用程序的入口点，用于访问所有 Spark 功能，并且需要具有所有集群配置和参数的 SparkConf 来创建 SparkContext 对象，并在为其他交互创建特定的 SparkContext。 而现在有了 SparkSession 对象，它是这些不同 Context 对象的组合，在任何情况下都可以使用 SparkSession 对象访问 Spark 资源。 参考资料 官方文档","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"MapProxy","slug":"ocean/mapproxy","date":"2022-07-05T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/mapproxy/","permalink":"https://wangqian0306.github.io/2022/mapproxy/","excerpt":"","text":"MapProxy 简介 MapProxy 是地理空间数据的开源代理。它可以缓存、加速和转换现有地图服务中的数据，并为任何桌面或 web GIS 客户端提供服务。 安装和使用 本地运行 依赖包安装： 12dnf install -y python3-pyyaml python3-pyproj python3-lxml geos-devel gdal-devel python3-shapely pip install Pillow six MapProxy 服务检测： 1mapproxy-util --version 创建配置样例： 1mapproxy-util create -t base-config mapproxy 运行服务： 1mapproxy-util serve-develop mapproxy/mapproxy.yaml 容器安装 Dockerfile 123456789101112FROM python:alpineWORKDIR /optRUN apk add py3-yaml py3-build geos-dev py3-lxml gdal-dev py3-shapely proj-util gcc g++RUN pip install Pillow pyproj six MapProxyRUN mapproxy-util create -t base-config mapproxyEXPOSE 8080ENTRYPOINT [&quot;mapproxy-util&quot;,&quot;serve-develop&quot;,&quot;-b&quot;,&quot;0.0.0.0&quot;]CMD [&quot;/opt/mapproxy/mapproxy.yaml&quot;] Docker-compose file 123456789services: mapproxy: build: . image: mapproxy:latest ports: - &quot;8080:8080&quot; volumes: - ./xxx.yaml:/opt/mapproxy/mapproxy.yaml - &lt;path&gt;:/tmp/mapcenter/cache 样例配置 代理服务可以进行如下配置： 12345678910111213141516171819202122232425262728293031323334353637services: demo: wmts: md: title: demo abstract: demo online_resource: http://demo:8080layers: - name: demo title: EPSG:3857 sources: [ demo_cache ]caches: demo_cache: sources: [ demo_tiles ] format: image/png grids: [ osm_grid ]sources: demo_tiles: type: tile url: http://xxx.xxx.xxx:xxxx/xxxx/%(z)s/%(y)s/%(x)s.png grid: osm_gridgrids: osm_grid: name: EPSG:3857 srs: EPSG:3857 origin: nw num_levels: 19 bbox: [ -20037508.3427892,-20037508.3427892,20037508.3427892,20037508.3427892 ]globals: cache: base_dir: &#x27;/tmp/mapcenter/cache&#x27; lock_dir: &#x27;/tmp/mapcenter/cache/locks&#x27; 手动缓存 如需缓存数据可以编写如下配置和命令： 123456789101112seeds: demo_seed: caches: [ demo_cache ] levels: to: 2 refresh_before: mtime: ./reseed.timecleanups: demo_remove: caches: [ demo_cache ] remove_all: true 1mapproxy-seed -f mapcenter.yaml -s seed.yaml -c 1 --reseed-interval 14d --reseed-file reseed.time --progress-file .mapproxy_seed_progress 注：此命令代表重新缓存需要间隔 14 天，且保存执行过程和时间到缓存文件中，如果有需要可以配合 --continue 命令继续执行。 独立部署 如果需要将缓存之后的文件单独部署为服务可以使用如下配置： 12345678910111213141516171819202122232425262728293031services: demo: wmts: md: title: demo abstract: demo online_resource: http://demo:8080layers: - name: demo title: EPSG:3857 sources: [ demo_cache ]caches: demo_cache: sources: [] format: image/png grids: [ osm_grid ]grids: osm_grid: name: EPSG:3857 srs: EPSG:3857 origin: nw num_levels: 19 bbox: [ -20037508.3427892,-20037508.3427892,20037508.3427892,20037508.3427892 ]globals: cache: base_dir: &#x27;/tmp/mapcenter/cache&#x27; lock_dir: &#x27;/tmp/mapcenter/cache/locks&#x27; 注：在缓存目录中应该存在如下名称的数据文件夹 demo_cache_EPSG3857,tile_locks 参考资料 项目原文 配置样例","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"NetCDF","slug":"ocean/netCDF","date":"2022-07-05T15:09:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2022/netcdf/","permalink":"https://wangqian0306.github.io/2022/netcdf/","excerpt":"","text":"NetCDF 简介 NetCDF(Network Common Data Form)是一种自描述、与机器无关、基于数组的科学数据格式，同时也是支持创建、访问和共享这一数据格式的函数库。 此格式是由美国大气科学研究大学联盟(UCAR) 针对科学是数据的特点进行开发的，常见的文件后缀名为 .nc。 此种数据格式以已经广泛的应用于大气科学、水文、海洋学、环境模拟、地球物理等诸多领域。 读取方式 Java 可以使用 Maven 或 Gradle 来引入依赖包： 1234567&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;unidata-all&lt;/id&gt; &lt;name&gt;Unidata All&lt;/name&gt; &lt;url&gt;https://artifacts.unidata.ucar.edu/repository/unidata-all/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;edu.ucar&lt;/groupId&gt; &lt;artifactId&gt;cdm-core&lt;/artifactId&gt; &lt;version&gt;$&#123;netcdfJavaVersion&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678repositories &#123; maven &#123; url &quot;https://artifacts.unidata.ucar.edu/repository/unidata-all/&quot; &#125;&#125;dependencies &#123; implementation &#x27;edu.ucar:netcdfAll:5.4.1&#x27;&#125; 1234567891011121314151617181920212223242526import ucar.ma2.Array;import ucar.nc2.NetcdfFile;import ucar.nc2.NetcdfFiles;import ucar.nc2.Variable;import ucar.nc2.write.Ncdump;import java.io.IOException;public class SimpleRead &#123; private static final String PATH = &quot;&lt;path&gt;&quot;; public static void main(String[] args) throws IOException &#123; try (NetcdfFile ncfile = NetcdfFiles.open(PATH)) &#123; Variable v = ncfile.findVariable(&quot;&lt;xxx&gt;&quot;); if (v == null) &#123; return; &#125; Array data = v.read(); String arrayStr = Ncdump.printArray(data, &quot;&lt;xxx&gt;&quot;, null); System.out.println(arrayStr); &#125; catch (IOException ioe) &#123; System.out.println(ioe.getMessage()); &#125; &#125;&#125; Python 1pip install netCDF4 123456789101112import netCDF4from netCDF4 import Datasetnc_obj=Dataset(&#x27;20200809_prof.nc&#x27;)# 查看参数列表print(nc_obj.variables.keys())# 查看变量信息print(nc_obj.variables[&#x27;&lt;xxx&gt;&#x27;])# 查看变量属性print(nc_obj.variables[&#x27;&lt;xxx&gt;&#x27;].ncattrs())#读取数据值arr_xxx=(nc_obj.variables[&#x27;&lt;xxx&gt;&#x27;][:]) 有的 nc 文件会采用不同的文件结构，例如 NOAA 的 RTOFS nc 文件可以通过如下方式进行读取： 1pip install xarray 1234567891011121314151617181920212223242526import xarray as xrimport pandas as pd# 打开NetCDF文件dataset = xr.open_dataset(&#x27;rtofs_glo_2ds_f000_prog.nc&#x27;)# 获取sst、经度和纬度变量sst = dataset[&#x27;sst&#x27;]lon = dataset[&#x27;Longitude&#x27;]lat = dataset[&#x27;Latitude&#x27;]# 将数据转换为numpy数组sst_values = sst.valueslon_values = lon.valueslat_values = lat.values# 创建一个DataFrame，将经纬度和SST值对应起来# 这里假设sst只有一个时间点sst_df = pd.DataFrame(&#123; &#x27;Latitude&#x27;: lat_values.flatten(), &#x27;Longitude&#x27;: lon_values.flatten(), &#x27;SST&#x27;: sst_values[0, :, :].flatten()&#125;)# 打印前几行查看print(sst_df.head()) 在这种情况下经纬度的变化是没有明显规律的，需要使用时多加关注。 参考资料 维基百科-NetCDF netCDF-Java netCDF4-Python","categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Spring Native","slug":"spring/native","date":"2022-07-04T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/native/","permalink":"https://wangqian0306.github.io/2022/native/","excerpt":"","text":"Spring Native 简介 Spring Boot 可以通过默认的 gradle 或 maven 插件省略 Dockerfile 的形式构建容器，而且在 3.0 版本之后也支持使用 GraalVM native-image 编译器将 Spring 应用程序编译为本机可执行文件和容器，非常适合容器云平台。 使用 项目初始化 建议访问 Spring Initializr ，创建初始化 gradle 项目，(可选)引入 GraalVM Native Support Dependencies (可选)在 build.gradle 中可以配置加速和镜像名： 12345tasks.named(&quot;bootBuildImage&quot;) &#123; environment[&quot;HTTP_PROXY&quot;] = &quot;http://&lt;host&gt;:&lt;port&gt;&quot; environment[&quot;HTTPS_PROXY&quot;] = &quot;https://&lt;host&gt;:&lt;port&gt;&quot; imageName = &quot;&lt;repository&gt;/&lt;dir&gt;/$&#123;rootProject.name&#125;:$&#123;project.version&#125;&quot;&#125; 注: 如果引入了 GraalVM Native Support 则会默认使用 paketobuildpacks/builder:tiny 否则使用 paketobuildpacks/builder:base，且首次拉取镜像所花的时间会比较长。 运行打包命令 ./gradlew bootBuildImage 等待镜像打包完成即可。 注：打包时间较长，且针对网络和内存需求比较严苛，建议提前配置好 github 加速。 修改配置文件 项目可执行文件和根目录都位于 /workspace 路径下，可以挂载配置文件至此位置。 测试构建的二进制包 在引入 GraalVM Native Support 之后会将程序打包成二进制包，然后再引入 Docker 中，构建时间较长，为了解决频繁打包的问题建议在本地打包二进制程序进行测试： 首先可以安装如下 c++ 依赖： 1dnf insatll gcc -y 注：目前版本好像这个就够了，且由于 ld 包更新的原因导致最新版的 fedora 无法构建 native 容器。 然后使用如下命令即可完成构建： 1./gradlew nativeCompile 若构建失败则可以尝试安装如下包： 123dnf install libstdc++ libstdc++-docs libstdc++-static -ydnf install zlib zlib-static -ydnf install freetype freetype-devel -y 生成的二进制文件位于 build/native/nativeCompile 路径下，可以直接运行，用于检测打包是否完全。 注：如果遇到 137 代表构建内存不足，需要尝试清下内存或者使用如下环境变量限制内存：export JAVA_TOOL_OPTIONS=-Xmx&#123;size&#125;m。 自定义引入类 在引入 GraalVM Native Support 之后，打包过程中可能会忽略部分未使用的类，所以建议在运行时新增如下配置，手动标识引入内容。 ImportRuntimeHints 1234567891011121314151617181920212223242526272829303132import org.springframework.aot.hint.ExecutableMode;import org.springframework.aot.hint.RuntimeHints;import org.springframework.aot.hint.RuntimeHintsRegistrar;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.ImportRuntimeHints;import org.springframework.util.ReflectionUtils;import java.lang.reflect.Method;@Configuration@ImportRuntimeHints(MyRuntimeHints.MyRuntimeRegistrar.class)public class MyRuntimeHints &#123; static class MyRuntimeRegistrar implements RuntimeHintsRegistrar &#123; @Override public void registerHints(RuntimeHints hints, ClassLoader classLoader) &#123; Method method = ReflectionUtils.findMethod(MyClass.class, &quot;sayHello&quot;, String.class); hints.reflection().registerMethod(method, ExecutableMode.INVOKE); // Register resources hints.resources().registerPattern(&quot;my-resource.txt&quot;); // Register serialization hints.serialization().registerType(MySerializableClass.class); // Register proxy hints.proxies().registerJdkProxy(MyInterface.class); &#125; &#125;&#125; 注：生成的 hint 文件可以在 Maven 项目的 target/spring-aot/main/resources Gradle 项目的 build/generated/aotResources 目录中。 RegisterReflectionForBinding 在类上直接使用 @RegisterReflectionForBinding 注解。 Reflective 在方法上使用 @Reflective 注解。 安装其他命令 在项目中需要额外的软件或命令的时候，使用此方式就很烦了。spring 官方使用了 Paketo Buildpacks 作为 builder 但是它采用了 buildpack 弃用的 stack 方式来构建项目。自定义构建包难度好大，但是可以通过挂载卷的方式将命令挂载到容器中进行执行。 构建失败解决方案 由于 GraalVM Native Support 插件目前还没有正式完成，所以在容器构建时容易遇到很多小错误。此时就可以通过这些办法来得到类似的效果。 自定义 jre 在本地构建项目，生成 jar 文件之后就可以通过此种方式来得到小型镜像了。 注：此处以 Java 23 和 gradle 作为构建工具。如果需要使用 maven 建议寻找参考资料中优化镜像的文档。 123456789101112131415161718192021222324252627282930313233343536FROM eclipse-temurin:23-jdk-alpine AS jre-builderWORKDIR /appCOPY build/libs/*.jar /app/app.jarRUN apk update &amp;&amp; \\ apk add --no-cache tar binutilsRUN jar xvf app.jarRUN jdeps --ignore-missing-deps -q \\ --recursive \\ --multi-release 23 \\ --print-module-deps \\ --class-path &#x27;BOOT-INF/lib/*&#x27; \\ app.jar &gt; modules.txtRUN $JAVA_HOME/bin/jlink \\ --verbose \\ --add-modules $(cat modules.txt) \\ --strip-debug \\ --no-man-pages \\ --no-header-files \\ --compress=2 \\ --output /optimized-jdk-23FROM alpine:latestWORKDIR /appENV JAVA_HOME=/opt/jdk/jdk-23ENV PATH=&quot;$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot;COPY --from=jre-builder /optimized-jdk-23 $JAVA_HOMEARG APPLICATION_USER=springRUN addgroup --system $APPLICATION_USER &amp;&amp; adduser --system $APPLICATION_USER --ingroup $APPLICATION_USERRUN mkdir /app &amp;&amp; chown -R $APPLICATION_USER /appCOPY build/libs/*.jar /app/app.jarRUN chown $APPLICATION_USER:$APPLICATION_USER /app/app.jarUSER $APPLICATION_USEREXPOSE 8080ENTRYPOINT [ &quot;java&quot;, &quot;-jar&quot;, &quot;/app/app.jar&quot; ] 参考资料 Spring Boot Gradle Plugin 官方文档 官方文档 Gradle 插件官方文档 成功优化！Java 基础 Docker 镜像从 674MB 缩减到 58MB 的经验分享","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Linux 系统性能排查","slug":"linux/monitor","date":"2022-06-29T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/monitor/","permalink":"https://wangqian0306.github.io/2022/monitor/","excerpt":"","text":"Linux 系统性能排查 整体情况排查 htop 注：此软件需要在软件管理工具中独立安装，例：yum install -y htop 可以使用 htop 命令获取当前系统的整体情况，在此页面可以对如下参数项进行进行一些基本排查。 软件上半部分为系统基础信息： CPU 核心使用率(数字编号的图形) 内存使用情况(Mem) 交换内存使用情况(Swp) 进程总览(Tasks 即总进程数，thr 线程数，running 代表处于此状态的进程数) 系统平均负载倍数(平均负载部分有三个值分别为：最近 1 分钟负载，最近 5 分钟负载，最近 15 分钟负载) 系统持续运行时间(Uptime) 软件下半部分为进程详细清单，参数解释如下： PID – 描述进程的ID号 USER – 描述进程的所有者（谁跑的） PRI – 描述Linux内核查看的进程优先级 NI – 描述由用户或root重置的进程优先级 VIR – 它描述进程正在使用的虚拟内存 （virtual memory） RES – 描述进程正在消耗的物理内存（physical memory） SHR – 描述进程正在使用的共享内存（shared memory） S – 描述流程的当前状态 (state) CPU％ – 描述每个进程消耗的CPU百分比 MEM％ – 描述每个进程消耗的内存百分比 TIME+ – 显示自流程开始执行以来的时间 Command –它与每个进程并行显示完整的命令执行 (比如/usr/lib/R) CPU 核心使用率图示 蓝色：显示低优先级(low priority)进程使用的CPU百分比。 绿色：显示用于普通用户(user)拥有的进程的CPU百分比。 红色：显示系统进程(kernel threads)使用的CPU百分比。 橙色：显示IRQ时间使用的CPU百分比。 洋红色(Magenta)：显示Soft IRQ时间消耗的CPU百分比。 灰色：显示IO等待时间消耗的CPU百分比。 青色：显示窃取时间(Steal time)消耗的CPU百分比。 内存及交换内存使用率图示 绿色：显示内存页面占用的 RAM 百分比 蓝色：显示缓冲区页面占用的 RAM 百分比 橙色：显示缓存页面占用的 RAM 百分比 进程状态汇总 R: Running：表示进程(process)正在使用CPU S: Sleeping: 通常进程在大多数时间都处于睡眠状态，并以固定的时间间隔执行小检查，或者等待用户输入后再返回运行状态。 T/S: Traced/Stopped: 表示进程正在处于暂停的状态 Z:Zombie or defunct:已完成执行但在进程表中仍具有条目的进程。 dstat 注：此软件需要在软件管理工具中独立安装，例：yum install -y dstat 可以使用 dstat -vtns [&lt;option&gt;] &lt;second&gt; 命令，查看当前的 CPU 和一些 IO 状况，其中 option 参数表示需要查看的内容 second 参数代表每隔多少秒进行一次统计，参数详解如下： CPU 使用率(total-cpu-usage)，可以在 option 参数中添加 -C &lt;core_num&gt;,total 来查看某个核心及总量的使用情况 用户时间占比(usr) 系统时间占比(sys) 空闲时间占比(idl) 等待时间占比(wai) 硬中断次数(hiq) 软中断次数(siq) 内存使用率(memory-usage) 已用内存(used) 缓冲区(buff) 缓存(cache) 可用内存(free) 交换内存使用率(swap) 已用内存(used) 可用内存(free) 磁盘使用率(dsk)，可以在 option 参数中添加 -D &lt;disk_name&gt;,total 参数查看某个磁盘的使用情况 读取带宽(read) 写入带宽(writ) 网络使用率(net)，可以在 option 参数中添加 -N &lt;network_name&gt;,total 参数查看某个网卡的使用情况 输入带宽(recv) 输出带宽(send) 系统使用情况(system) 中断数量(int) 上下文切换次数(csw) 系统的分页活动(paging) 换入次数(in) 换出次数(out) 除此之外 dstat 命令还可以监控如下内容： aio 状态：--aio 文件系统状态：--fs ipc 状态：--ipc 锁状态：--lock Socket 状态(raw)：--raw Socket 状态：--socket TCP 状态：--tcp UPD 状态：--udp unix 状态：--unix 虚拟内存状态：--vm CPU 检测 具体指标的分析方式参见下表： 性能指标 分析工具 说明 平均负载 uptime htop /proc/loadavg 最后一个常用于监控系统 系统 CPU 使用率 vmstat mpstat htop sar /proc/stat sar 还可以记录历史数据，/proc/stat 常用于监控系统 进程 CPU 使用率 htop ps pidstat pidstat 只显示实际使用了 CPU 的进程 系统上下文切换 vmstat 除了上下文切换次数，还提供运行状态和不可中断状态进程的数量 进程上下文切换 pidstat 注意加上 -w 选项 软中断 htop mpstat /proc/softirqs htop 提供使用率，其他内容则提供了在每个 CPU 上的运行次数 硬中断 vmstat /proc/interrupts vmstat 提供总的中断次数，而 /proc/interrupts 提供各种中断在每个 CPU 上运行的累计次数 CPU 缓存 pref 使用 pref stat 子命令 CPU 基础信息 lscpu /proc/cpuinfo lscpu 更直观 事件剖析 pref 火焰图 execsnoop pref 和火焰图用户来分析热点函数以及调用栈，execsnoop 用来监测短时进程 动态追踪 ftrace bcc systemtap ftrace 用于跟踪内核函数调用栈，而 bcc 和 systemtap 则用于跟踪内核或应用程序的执行过程 内存检测 具体指标的分析方式参见下表： 性能指标 分析工具 说明 系统已用，可用，剩余内存 free vmstat dstat sar /proc/meminfo /proc/meminfo 常用于监控系统 进程虚拟内存，常驻内存，共享内存 ps htop pidstat /proc/&lt;pid&gt;/stat /proc/&lt;pid&gt;/status pidstat 命令需要加上 -r 选项，/proc/&lt;pid&gt;/stat 和 /proc/&lt;pid&gt;/status 常用于监控系统中 进行内存分布 pmap /proc/&lt;pid&gt;/maps /proc/&lt;pid&gt;/maps 是 pmap 的数据来源，常用于监控系统中 进程 swap 换出内存 htop /proc/&lt;pid&gt;/status /proc/&lt;pid&gt;/status 是 htop 的数据来源，常用于监控系统中 进程缺页异常 ps htop pidstat pidstat 命令需要加上 -r 选项 系统换页情况 sar sar 命令需要加上 -B 选项 缓存/缓冲区用量 free vmstat dstat sar cachestat cachestat 需要安装 bcc 缓存缓冲区命中率 cachetop 需要安装 bcc swap 已用和剩余空间 free sar sar 可以记录历史 swap 换入换出 vmstat sar sar 可以记录历史 内存泄漏检测 memleak memleak 需要安装 bcc 磁盘检测 性能指标 分析工具 说明 文件系统空间容量，使用量以及剩余空间 df 索引节点容量，使用量以及剩余量 df 需要加上 -r 选项 页缓存和可回收 Slab 缓存 /proc/meminfo sar vmstat sar 命令需要加上 -r 选项，/proc/meminfo 常用于监控系统中 缓冲区 /proc/meninfo sar vmstat sar 命令需要加上 -r 选项，/proc/meminfo 常用于监控系统中 目录项，索引节点以及文件系统的缓存 /proc/slabinfo slabtop /proc/slabinfo 常用于监控系统中 磁盘 I/O 使用率，IOPS，吞吐量，响应时间，I/O 平均大小以及等待队列长度 iostat sar dstat /proc/diskstatus sar 命令需要加上 -d 选项，/proc/diskstatus 常用于监控系统中 进程 I/O 大小以及 I/O 延迟 pidstat iotop pidstat 命令需要加上 -d 选项 块设备 I/O 事件跟踪 blktrace 需要跟 blkparse 配合使用，例如 blktrace -d /dev/&lt;disk_name&gt; -o- &amp;#124; blkparse -i- 进程 I/O 系统调用跟踪 strace pref trace strace 只可以跟踪单个进程，pref 和 trace 还可以跟踪所有进程的系统调用 进程块设备 I/O 大小跟踪 biosnoop biotop 需要安装 bcc 动态追踪 ftrace bcc systemtap ftrace 用于跟踪内核函数调用栈，而 bcc 和 systemtap 则用于跟踪内核或应用程序的执行过程 网络检测 性能指标 分析工具 说明 吞吐量(BPS) sar nethogs iftop /proc/net/dev 分别可以查看网络接口，进程以及 IP 地址的网络吞吐量；/proc/net/dev 常用于监控系统 吞吐量(PPS) sar /proc/net/dev sar 命令需要加上 -n DEV 选项 网络连接数 netstat ss ss 速度更快 网络错误数 netstat sar 注意使用 netstat -s 或 sar -n EDEV/EIP 网络延迟 ping hping3 ping 基于 ICMP，而 hping3 基于 TCP 协议 连接跟踪数 conntrack /proc/sys/net/netfilter/nf_connectrack_count /proc/sys/net/netfilter/nf_connectrack_max conntrack 可以查看所有连接 路由 mtr traceroute route route 用于查询路由表，而 mtr 和 traceroute 通常用于排查定位路由问题 DNS dig nslookup 用于排查 DNS 解析的问题 防火墙和 NAT iptables 用于排查防火墙和 NAT 问题 网卡选项 ethtool 用于查看和配置网络接口的功能选项 网络抓包 tcpdump 通常用 tcpdump 抓包后再使用 Wireshark 工具进行分析 动态追踪 ftrace bcc systemtap ftrace 用于跟踪内核函数调用栈，而 bcc 和 systemtap 则用于跟踪内核或应用程序的执行过程 参考资料 Linux性能问题分析流程与性能优化思路 Linux htop 详解 dstat Command Examples in Linux","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Heroku","slug":"tmp/heroku","date":"2022-06-28T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/heroku/","permalink":"https://wangqian0306.github.io/2022/heroku/","excerpt":"","text":"Heroku 简介 Heroku 是一个基于托管容器系统的平台即服务(PaaS)，具有集成的数据服务和强大的生态系统，用于部署和运行现代应用程序。 使用 注：本文采用免费的账户进行测试。 需要确定应用程序的语言，Heroku 支持以下语言: Node.js Ruby Java PHP Python Go Scala Clojure 登陆 Heroku 官网进行注册并完成登陆及创建服务 选择应用程序的接入方式： 使用 Heroku 作为版本管理工具 安装 Heroku 客户端，并按照官网样例进行操作 使用 GitHub 仓库作为版本管理工具 配置 GitHub 地址，及程序运行配置项 配置自动部署或手动部署服务 之后 Heroku 会自动下载项目依赖并将服务打包为以 Debian 为基础系统的容器将其部署在公网上，可以通过页面跳转快速进行访问。 注：试用的容器仅有 512m 内存且仅能有 2 个进程，在闲置 30 分钟后会进入睡眠状态。 插件 Heroku 提供了一些存储后端，可以作为插件的形式引入到项目中。 目前的插件大致分为以下类： Postgres Redis Kafka IPFS MongoDB (收费) MySQL (收费) 当项目需要以上依赖时，可以快速将项目运行起来。 参考资料 Heroku 官网","categories":[],"tags":[{"name":"Heroku","slug":"Heroku","permalink":"https://wangqian0306.github.io/tags/Heroku/"}]},{"title":"record","slug":"java/record","date":"2022-06-27T13:32:58.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/record/","permalink":"https://wangqian0306.github.io/2022/record/","excerpt":"","text":"Record 简介 在 Java 16 中有一个新的 record 关键字用于生成 record 类。 简单使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344public final class Range &#123; private final int start; private final int end; public Range(int start, int end) &#123; // 参数检查 this.start = start; this.end = end; &#125; public int start() &#123; return start; &#125; public int end() &#123; return end; &#125; @Override public boolean equals(Object obj) &#123; if (obj == this) &#123; return true; &#125; if (obj == null || obj.getClass() != this.getClass()) &#123; return false; &#125; var that = (Range) obj; return this.start == that.start &amp;&amp; this.end == that.end; &#125; @Override public int hashCode() &#123; return Objects.hash(start, end); &#125; @Override public String toString() &#123; return &quot;Range[&quot; + &quot;start=&quot; + start + &quot;, &quot; + &quot;end=&quot; + end + &#x27;]&#x27;; &#125;&#125; 等同于 public record Range(int start, int end) &#123; public Range&#123; // 参数检查 &#125; &#125;```","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"}]},{"title":"Java 序列化工具","slug":"java/serialization","date":"2022-06-27T13:32:58.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/serialization/","permalink":"https://wangqian0306.github.io/2022/serialization/","excerpt":"","text":"Java 序列化工具 简介 序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。Java 本身提供了 Serializable 类，通过继承此类可以方便的将数据进行序列化。 除此之外可以使用如下提到的第三方框架。 Kryo Kryo 是一个快速高效的 Java 二进制对象图序列化框架。该项目的目标是高速、小尺寸和易于使用的 API。该项目在需要持久化对象的任何时候都很有用，无论是构建文件、存储至数据库还是通过网络传输对象。 Kryo 还可以执行自动深浅复制/克隆。这是从对象到对象的直接复制，而不是从对象到字节到对象的直接复制。 简单使用 首先需要引入依赖包： 123456&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;5.3.0&lt;/version&gt;&lt;/dependency&gt; 然后可以参照此样例完成代码编写： 123456789101112131415161718192021222324252627import com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.io.Input;import com.esotericsoftware.kryo.io.Output;import java.io.*;public class HelloKryo &#123; static public void main(String[] args) throws Exception &#123; Kryo kryo = new Kryo(); kryo.register(SomeClass.class); SomeClass object = new SomeClass(); object.value = &quot;Hello Kryo!&quot;; Output output = new Output(new FileOutputStream(&quot;file.bin&quot;)); kryo.writeObject(output, object); output.close(); Input input = new Input(new FileInputStream(&quot;file.bin&quot;)); SomeClass object2 = kryo.readObject(input, SomeClass.class); input.close(); &#125; static public class SomeClass &#123; String value; &#125;&#125; Avro Apache Avro 是一个数据序列化系统。 Avro 提供： 丰富的数据结构。 一种紧凑、快速的二进制数据格式。 一个容器文件，用于存储持久数据。 远程过程调用 (RPC)。 与动态语言的简单集成。代码生成不需要读取或写入数据文件，也不需要使用或实现 RPC 协议。代码生成作为一种可选的优化，只为静态类型语言实现。 简单使用 首先需要引入依赖包： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;1.11.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.11.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;schema&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sourceDirectory&gt;$&#123;project.basedir&#125;/src/main/avro/&lt;/sourceDirectory&gt; &lt;outputDirectory&gt;$&#123;project.basedir&#125;/src/main/java/&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt;&lt;/plugins&gt; Avro 在序列化时是需要提前定义结构： 123456789&#123;&quot;namespace&quot;: &quot;example.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User&quot;, &quot;fields&quot;: [ &#123;&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;&#125;, &#123;&quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: [&quot;int&quot;, &quot;null&quot;]&#125;, &#123;&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]&#125; ]&#125; 然后根据所指定的结构编译 Java 文件，将目标文件放在代码中： 1java -jar /path/to/avro-tools-1.11.0.jar compile schema &lt;schema file&gt; &lt;destination&gt; 编写样例代码 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main(String[] args) &#123; User user1 = new User(); user1.setName(&quot;Alyssa&quot;); user1.setFavoriteNumber(256); User user2 = new User(&quot;Ben&quot;, 7, &quot;red&quot;); User user3 = User.newBuilder() .setName(&quot;Charlie&quot;) .setFavoriteColor(&quot;blue&quot;) .setFavoriteNumber(null) .build(); DatumWriter&lt;User&gt; userDatumWriter = new SpecificDatumWriter&lt;User&gt;(User.class); DataFileWriter&lt;User&gt; dataFileWriter = new DataFileWriter&lt;User&gt;(userDatumWriter); dataFileWriter.create(user1.getSchema(), new File(&quot;users.avro&quot;)); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.append(user3); dataFileWriter.close(); DatumReader&lt;User&gt; userDatumReader = new SpecificDatumReader&lt;User&gt;(User.class); DataFileReader&lt;User&gt; dataFileReader = new DataFileReader&lt;User&gt;(file, userDatumReader); User user = null; while (dataFileReader.hasNext()) &#123; user = dataFileReader.next(user); System.out.println(user); &#125; &#125;&#125; 参考资料 Kryo Avro","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"Avro","slug":"Avro","permalink":"https://wangqian0306.github.io/tags/Avro/"},{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Kryo","slug":"Kryo","permalink":"https://wangqian0306.github.io/tags/Kryo/"}]},{"title":"磁盘管理","slug":"linux/disk","date":"2022-06-24T12:04:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/disk/","permalink":"https://wangqian0306.github.io/2022/disk/","excerpt":"","text":"磁盘管理 简介 之前一直没有记录过相关文档，所以做个补充。 命令 使用此命令查看挂载情况，及分区格式： 1lsblk -f 针对未分区磁盘进行分区 1fdisk &lt;disk_name&gt; m 显示命令列表 p 显示磁盘分区 n 新增分区 d 删除分区 w 写入并退出 磁盘格式化(配置文件系统) 1mkfs -t &lt;fs_type&gt; &lt;disk_name&gt; fs_type 类型如下： xfs ext4 tmpfs 挂载磁盘 1mount &lt;disk_name&gt; &lt;path&gt; 取消挂载磁盘 1umount &lt;disk_name&gt; &lt;path&gt; 查看磁盘 UUID 1blkid &lt;disk_name&gt; 开机自动挂载 1vim /etc/fstab 按照如下规则填写即可： 第一列-磁盘 UUID 第二列-需要挂载的目录 第三列-文件系统格式 第四列-系统的默认参数，一般填 defaults 第五列-是否做 dump 备份，0 表示不备份，1 表示每天备份，2 表示不定期备份 第六列-是否开机检查扇区：0 表示不检查，1 表示最早检验，2 表示在1之后开始检验","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"安装 fcitx5 输入法","slug":"linux/fcitx5","date":"2022-06-24T12:04:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/fcitx/","permalink":"https://wangqian0306.github.io/2022/fcitx/","excerpt":"","text":"安装 fcitx5 输入法 简介 Fcitx5 是一个轻量级核心的输入法框架，通过插件提供额外的语言支持。它是 Fcitx 的继任者。 安装 单次使用 可以使用如下命令在 Fedora 平台上安装 Fcitx5 输入法： 12dnf install fcitx5 kcm-fcitx5 fcitx5-chinese-addons fcitx5-table-extra fcitx5-zhuyin fcitx5-configtoolfcitx5 另起一个终端开启图形化配置 1fcitx5-config-qt 永久使用 在单次使用配置完成后，使用如下命令： 1alternatives --config xinputrc 注：然后选择 fcitx5 项即可。 添加自启动 1ln -s /usr/share/applications/org.fcitx.Fcitx5.desktop ~/.config/autostart/ 参考资料 fedora 35 安装 fcitx5 拼音输入法","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Consul","slug":"kubernetes/consul","date":"2022-06-23T15:09:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/consul/","permalink":"https://wangqian0306.github.io/2022/consul/","excerpt":"","text":"Consul 简介 Consul 是一种多网络管理工具，提供功能齐全的服务网格(ServiceMesh)解决方案，可解决运营微服务和云基础设施（多云和混合云）的网络和安全挑战。 安装与部署 在本机上安装 123sudo dnf install -y dnf-plugins-coresudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.reposudo dnf -y install consul 注：其他平台请参照 官方文档 在 Kubernetes 上部署 注：在部署 Consul 之前需要安装 Helm 并在 Kubernetes 上配置 StorageClass。 生成配置文件： 12helm repo add hashicorp https://helm.releases.hashicorp.comhelm inspect values hashicorp/consul &gt; values.yaml 配置下面的内容： 123456789101112131415161718192021222324global: enabled: true logLevel: &quot;info&quot; logJSON: false name: consulserver: enabled: true storageClass: nfs-clientui: enbaled: true ingress: enabled: true ingressClassName: &quot;nginx&quot; pathType: Prefix hosts: [ &#123; host:&quot;&lt;host&gt;&quot; &#125; ]connectInject: enabled: true transparentProxy: defaultEnabled: true cni: enabled: true logLevel: info cniBinDir: &quot;/opt/cni/bin&quot; cniNetDir: &quot;/etc/cni/net.d&quot; 然后使用下面的命令部署集群： 1helm install consul hashicorp/consul --create-namespace --namespace consul --values values.yaml 注: 在配置 API Consul 服务若出现异常可以通过此种方式进行删除： request12345678### 检查服务状态GET http://&lt;host&gt;:8500/v1/agent/checks### 查看服务列表GET http://&lt;host&gt;:8500/v1/agent/services### 刪除空服务PUT http://&lt;host&gt;:8500/v1/agent/service/deregister/&lt;service-name&gt; 在 Kubernetes 上开发与部署服务 流程及插件文档详见 整合文档 部署样例参见： Nginx Ingress Consul 服务部署 参考资料 官方文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Consul","slug":"Consul","permalink":"https://wangqian0306.github.io/tags/Consul/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"},{"name":"Helm","slug":"Helm","permalink":"https://wangqian0306.github.io/tags/Helm/"}]},{"title":"Spring 定时任务","slug":"spring/task","date":"2022-06-22T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/task/","permalink":"https://wangqian0306.github.io/2022/task/","excerpt":"","text":"Spring 定时任务 简介 Spring 对于定时任务的支持有以下方式： 可以使用 spring-boot-starter-quartz 快速对接 Quartz 自带的 @Scheduled 注解配置调度任务 @Secheduled 注解 使用 @Secheduled 注解应当遵循下面的两个简单的规则： 该方法的返回类型为 void (如果有返回值则将被忽略) 该方法应当没有任何入参 在使用时需要在入口类中启用调度器 12345678910111213import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.scheduling.annotation.EnableScheduling;@SpringBootApplication@EnableSchedulingpublic class TestApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Test.class, args); &#125;&#125; 在需要定时运行的方法上可以使用如下注解： 12345678910111213141516171819202122232425import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Component;@Componentpublic class Test &#123; // 此方法为固定延时，以任务运行结束的时间点为间隔 @Scheduled(fixedDelayString = &quot;$&#123;fixedDelay.in.milliseconds&#125;&quot;) void runByDelay() &#123; System.out.println(&quot;runByDelay At：&quot; + System.currentTimeMillis()); &#125; // 此方法为固定延时，以任务运行的开始时间为间隔 @Scheduled(fixedRateString = &quot;$&#123;fixedRate.in.milliseconds&#125;&quot;) void runByRate() &#123; System.out.println(&quot;runByRate At：&quot; + System.currentTimeMillis()); &#125; // 此方法以 cron 表达式作为运行条件 @Scheduled(cron = &quot;$&#123;cron.expression&#125;&quot;) void runByCorn() &#123; System.out.println(&quot;runByCorn At：&quot; + System.currentTimeMillis()); &#125;&#125; 注: 如果任务需要异步执行，则需要引入 @Async 注解。 如果有多个任务可能涉及到同时运行还需要在配置文件中写入线程池设置： 1spring.task.scheduling.pool.size=5 Quartz 首先需要引入下面的依赖包： Maven 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Gradle 1234dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-quartz&#x27; runtimeOnly &#x27;mysql:mysql-connector-java&#x27;&#125; 注: Quartz 需要依赖于存储后端 编写配置项如下： Yaml 12345678910spring: datasource: driver-class-name: $&#123;JDBC_DRIVER:com.mysql.cj.jdbc.Driver&#125; url: $&#123;MYSQL_URI:jdbc:mysql://xxx.xxx.xxx.xxx:xxxx/xxxx&#125; username: $&#123;MYSQL_USERNAME:xxxx&#125; password: $&#123;MYSQL_PASSWORD:xxxx&#125; quartz: job-store-type: jdbc jdbc: initialize-schema: always Properties 123456spring.datasource.driver-class-name=$&#123;JDBC_DRIVER:com.mysql.cj.jdbc.Driver&#125;spring.datasource.url=$&#123;MYSQL_URI:jdbc:mysql://xxx.xxx.xxx.xxx:xxxx/xxxx&#125;spring.datasource.username=$&#123;MYSQL_USERNAME:xxxx&#125;spring.datasource.password=$&#123;MYSQL_PASSWORD:xxxx&#125;spring.quartz.job-store-type=$&#123;QUARTZ_STORE_TYPE:jdbc&#125;spring.quartz.jdbc.initialize-schema=$&#123;QUARTZ_INIT_SCHEMA:always&#125; 编写定时任务如下： 1234567891011121314151617import lombok.extern.slf4j.Slf4j;import org.quartz.JobExecutionContext;import org.quartz.JobExecutionException;import org.springframework.scheduling.quartz.QuartzJobBean;import org.springframework.stereotype.Component;@Slf4j@Componentpublic class SampleJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; log.info(context.toString()); log.error(&quot;execute :&quot; + System.currentTimeMillis()); &#125;&#125; 编写定时任务配置如下： 1234567891011121314151617181920212223242526import org.quartz.*;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class QuartzConf &#123; @Bean public JobDetail jobDetail() &#123; return JobBuilder.newJob().ofType(SampleJob.class) .storeDurably() .withIdentity(&quot;Qrtz_Job_Detail&quot;) .withDescription(&quot;Invoke Sample Job service...&quot;) .build(); &#125; @Bean public Trigger trigger(JobDetail job) &#123; return TriggerBuilder.newTrigger().forJob(job) .withIdentity(&quot;Qrtz_Trigger&quot;) .withDescription(&quot;Sample trigger&quot;) .withSchedule(CronScheduleBuilder.cronSchedule(&quot;0/10 * * * * ? &quot;)) .build(); &#125;&#125; 如需动态编辑任务，则可以依照如下代码： 123456789101112131415161718192021222324252627282930313233import org.quartz.*;import org.springframework.http.HttpEntity;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import jakarta.annotation.Resource;@RestController@RequestMapping(&quot;/quartz&quot;)public class DemoController &#123; @Resource Scheduler scheduler; @GetMapping public HttpEntity&lt;String&gt; get() throws SchedulerException &#123; JobDetail job = JobBuilder.newJob().ofType(SampleJob.class) .storeDurably() .withIdentity(&quot;Qrtz_Job_Detail&quot;) .withDescription(&quot;Invoke Sample Job service...&quot;) .build(); Trigger trigger = TriggerBuilder.newTrigger().forJob(job) .withIdentity(&quot;Qrtz_Trigger&quot;) .withDescription(&quot;Sample trigger&quot;) .withSchedule(CronScheduleBuilder.cronSchedule(&quot;0/10 * * * * ? &quot;)) .build(); scheduler.scheduleJob(job, trigger); scheduler.start(); return new HttpEntity&lt;&gt;(&quot;success&quot;); &#125;&#125; 参考资料 Quartz 官网 Quartz Scheduler Spring 中的 @Scheduled 注解 Spring 定时任务参考代码 Spring Quartz 参考代码 Spring 定时任务多线程配置","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Redis 客户端缓存","slug":"database/redis-client-side-caching","date":"2022-06-21T13:41:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/redis-client-side-caching/","permalink":"https://wangqian0306.github.io/2022/redis-client-side-caching/","excerpt":"","text":"Redis 客户端缓存 简介 Redis 客户端缓存的技术通常被用来构建高性能的服务。在一般情况下服务与 Redis 的请求与响应情况如下： 12345+-------------+ +----------+| | ------- GET user:1234 -------&gt; | || Application | | Database || | &lt;---- username = Alice ------- | |+-------------+ +----------+ 而在客户端缓存技术则会将响应内容缓存至客户端中，样例如下： 1234567891011+-------------+ +----------+| | | || Application | ( No chat needed ) | Database || | | |+-------------+ +----------+| Local cache || || user:1234 = || username || Alice |+-------------+ 应用程序的内存可能不是很大相应用于本地缓存的空间也会收到很大的限制，但与访问数据库等网络服务相比，访问本地计算机内存所需的时间要小几个数量级。 由于经常访问相同比例的小部分数据，这种模式可以大大减少应用程序获取数据的延迟，同时减少数据库端的负载。这种模式的优点在于： 数据的延迟会非常小 数据库系统接收的查询更少，允许它以更少的节点为相同的数据集提供服务。 实现方式 Redis 采用了名为 Tracking 的技术实现了客户端缓存： 在默认模式下，服务器会记录客户端访问的 key 并在发生变动时将无效请求(使客户端缓存失效)发送至客户端。这样做会损耗服务器内存，但是能确保客户端可以精准的收到无效请求。 在广播模式下，客户端会订阅一些 key 的前缀，例如 object: 或 user: 然后在这些 key 经过改动后收到广播通知。这样不会损耗服务器内存，但是所有客户端都可能经常收到不相关的通知。 在默认模式中的读写流程如下： 客户端根据需求启用 Tracking 功能。(在连接开始时未启用 Tracking) 启用 Tracking 后，服务器会记录每个客户端在连接生命周期内请求的密钥(通过发送有关此类密钥的读取命令) 当客户端修改某个 key 或是它过期或是因清除策略被删除时，所有启用了追踪并可能缓存了该 key 的客户端都会拿到一条无效请求通知。 当客户端收到无效消息时，他们需要删除相应的密钥，以避免提供过时的数据。 在广播模式下的主要行为如下： 客户端使用该选项启用客户端缓存 BCAST 使用该选项指定一个或多个前缀 PREFIX。例如：CLIENT TRACKING on REDIRECT 10 BCAST PREFIX object: PREFIX user:如果根本没有指定前缀，则假定前缀为空字符串，因此客户端将收到每个被修改的键的失效消息。相反，如果使用一个或多个前缀，则只有与指定前缀之一匹配的键才会在失效消息中发送。 服务器不会在无效表中存储任何内容。相反，它使用不同的 Prefixes Table，其中每个前缀都与客户端列表相关联。 没有两个前缀可以跟踪键空间的重叠部分。例如，不允许使用前缀 foo 和 foob，因为它们都会触发键 foobar 的失效。但是，仅使用前缀 foo 就足够了。 每次修改匹配任何前缀的键时，所有订阅该前缀的客户端都会收到失效消息。 服务器将消耗与注册前缀数量成正比的 CPU。如果你只有几个，很难看出任何区别。使用大量前缀，CPU 成本会变得非常大。 在这种模式下，服务器可以优化为订阅给定前缀的所有客户端创建单个回复，并向所有客户端发送相同的回复。这有助于降低 CPU 使用率。 实现原理 从表面上看，这看起来很棒，但是如果您想象 10k 个连接的客户端都要求通过长期连接请求数百万个密钥，那么服务器最终会存储太多信息。 出于这个原因，Redis 使用两个关键思想来限制服务器端使用的内存量和处理实现该功能的数据结构的 CPU 成本。 服务器会维护一个由客户端和它所缓存的 key 构成的全局表，这个表被称为无效表(Invalidation Table)。此表可以容纳最大数量的元素。如果有新的 key 插入，服务器会将旧的 key 视作已经改动，并向客户端发送无效请求。通过此种方式可以使得客户端释放此 key 的内存，即使这会使拥有此 key 的本地客户端将其逐出。 在无效表中，我们实际上不需要存储指向客户端的指针，这样会使客户端断开连接时强制执行垃圾收集过程：相反，我们仅仅存储客户端 ID（每个 Redis 客户端都有一个唯一的数字 ID）。如果客户端断开俩连接，则随着该缓存插槽位置的失效，信息将被增量垃圾收集。 还存在一个 key 的命名空间，它不被数据库编号分割。所以如果一个客户端在 2 号库缓存了 foo，而另一个客户端在 3 号库更新了 foo 则无效请求依然会被发送。这样一来我们可以减少内存负载并且减少实现的复杂性。 缓存排除 默认情况下，客户端跟踪将向修改密钥的客户端发送失效消息。有时客户端需要这样做，因为它们实现了非常基本的逻辑，不涉及在本地自动缓存写入。但是，更高级的客户端甚至可能希望缓存他们在本地内存表中所做的写入。在这种情况下，在写入后立即接收无效消息是一个问题，因为它会强制客户端驱逐它刚刚缓存的值。 在这种情况下，可以使用该 NOLOOP 选项：它可以在正常模式和广播模式下工作。使用此选项，客户端可以告诉服务器他们不想接收他们修改的密钥的无效消息。 避免竞争条件 在实现客户端缓存将失效消息重定向到不同的连接时，您应该知道可能存在竞争条件。请参阅以下示例交互，其中我们将调用数据连接“D”和无效连接“I”： 123[D] client -&gt; server: GET foo[I] server -&gt; client: Invalidate foo (somebody else touched it)[D] server -&gt; client: &quot;bar&quot; (the reply of &quot;GET foo&quot;) 如您所见，由于对 GET 的回复到达客户端的速度较慢，因此我们在已经不再有效的实际数据之前收到了无效消息。因此，我们将继续提供 foo 密钥的陈旧版本。为了避免这个问题，当我们发送带有占位符的命令时填充缓存是一个好主意： 123456Client cache: set the local copy of &quot;foo&quot; to &quot;caching-in-progress&quot;[D] client-&gt; server: GET foo.[I] server -&gt; client: Invalidate foo (somebody else touched it)Client cache: delete &quot;foo&quot; from the local cache.[D] server -&gt; client: &quot;bar&quot; (the reply of &quot;GET foo&quot;)Client cache: don&#x27;t set &quot;bar&quot; since the entry for &quot;foo&quot; is missing. 当对数据和失效消息使用单个连接时，这种竞争条件是不可能的，因为在这种情况下消息的顺序总是已知的。 内存限制 请务必为 Redis 记住的最大键数配置一个合适的值，或者在 Redis 端使用完全不消耗内存的 BCAST 模式。请注意，不使用 BCAST 时 Redis 消耗的内存与跟踪的键数和请求此类键的客户端数成正比。 简单试用 注：在测试时采用的 Redis 版本为 6.2.5 打开第一个 REDIS 客户端，然后输入下面的命令查看客户端 ID： 1CLIENT ID 注：之后会返回此客户端的 ID 订阅无效请求话题： 1SUBSCRIBE __redis__:invalidate 注：此后客户端会无法操作，仅会输出无效请求相关信息。 打开第二个 Redis 客户端，然后输入下面的命令打开客户端缓存： 1CLIENT TRACKING on REDIRECT &lt;Client_1 ID&gt; 注：此命令会打开客户端缓存，并将无效请求重定位至客户端 1 中。 将任意内容写入客户端缓存，然后将其修改即可 12get wqset wq 1 之后在第一个客户端内应当见到如下输出： 1234561) &quot;subscribe&quot;2) &quot;__redis__:invalidate&quot;3) (integer) 11) &quot;message&quot;2) &quot;__redis__:invalidate&quot;3) 1) &quot;wq&quot; 参考资料 Redis 官方文档 Redis 命令手册","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"Spring Data Redis","slug":"spring/data-redis","date":"2022-06-21T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/spring-data-redis/","permalink":"https://wangqian0306.github.io/2022/spring-data-redis/","excerpt":"","text":"Spring Data Redis 简介 Spring Data Redis 是更大的 Spring Data 系列的一部分，它提供了从 Spring 应用程序对 Redis 的轻松配置和访问。 使用 引入依赖包： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 123dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-data-redis&#x27;&#125; 配置连接地址： 1234567spring: redis: host: $&#123;REDIS_HOST:xxx.xxx.xxx.xxx&#125; port: $&#123;REDIS_PORT:xxxx&#125; database: $&#123;REDIS_DB:xx&#125; username: $&#123;REDIS_USERNAME:xxxx&#125; password: $&#123;REDIS_PASSWORD:xxxx&#125; 注: 此处为单节点模式，其他模式请参照官方文档进行配置。 之后参照 Spring Data 其他组件的使用方式进行使用即可，例如： 1234567891011121314151617181920import jakarta.annotation.Resource;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource private StringRedisTemplate redisTemplate; @GetMapping public Boolean test(@RequestParam String name) &#123; return redisTemplate.hasKey(name); &#125;&#125; 分布式锁 实现分布式锁有两种方案： 使用 Spring Integration 实现 使用 Redission 的 Redlock 实现 Spring Integration 引入如下依赖： 12345678910111213dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-integration&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-data-redis&#x27; implementation &#x27;org.springframework.integration:spring-integration-redis&#x27; implementation &#x27;org.springframework.integration:spring-integration-http&#x27; testImplementation &#x27;org.springframework.integration:spring-integration-test&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testRuntimeOnly &#x27;org.junit.platform:junit-platform-launcher&#x27;&#125; 编写如下配置 application.yaml ： 1234spring: data: redis: host: &lt;redis_host&gt; 编写 RedisLockConfig.java ： 1234567891011121314import org.springframework.context.annotation.Bean;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.integration.redis.util.RedisLockRegistry;import org.springframework.stereotype.Component;@Componentpublic class RedisLockConfig &#123; @Bean public RedisLockRegistry redisLockRegistry(RedisConnectionFactory redisConnectionFactory) &#123; return new RedisLockRegistry(redisConnectionFactory, &quot;distributedLock&quot;); &#125;&#125; 编写 TestService.java : 12345678910111213141516171819202122232425262728293031323334353637import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.springframework.integration.redis.util.RedisLockRegistry;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;@Slf4j@Servicepublic class TestService &#123; @Resource private RedisLockRegistry redisLockRegistry; public String test(String id, Long time) throws InterruptedException &#123; log.info(&quot;Attempting to acquire lock with id: &#123;&#125;&quot;, id); Lock lock = redisLockRegistry.obtain(id); boolean lockAcquired = lock.tryLock(2, TimeUnit.SECONDS); if (lockAcquired) &#123; try &#123; log.info(&quot;Lock acquired for id: &#123;&#125;, performing critical operation...&quot;, id); Thread.sleep(time); &#125; finally &#123; lock.unlock(); log.info(&quot;Lock released for id: &#123;&#125;&quot;, id); &#125; return &quot;Operation completed successfully&quot;; &#125; else &#123; log.warn(&quot;Unable to acquire lock for id: &#123;&#125;&quot;, id); return &quot;Unable to acquire lock, please try again later.&quot;; &#125; &#125;&#125; 编写 TestController.java : 12345678910111213141516171819import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource TestService testService; @GetMapping(&quot;/&#123;id&#125;/&#123;time&#125;&quot;) public String test(@PathVariable String id, @PathVariable Long time) throws InterruptedException &#123; return testService.test(id, time); &#125;&#125; 之后启动服务，然后使用如下地址进行测试即可： http://localhost:8080/test/1/20000 http://localhost:8080/test/1/1000 Redission 与 Spring 集成 引入如下依赖： 12345678910dependencies &#123; implementation &#x27;org.springframework.boot:spring-boot-starter-web&#x27; implementation &#x27;org.springframework.boot:spring-boot-starter-data-redis&#x27; compileOnly &#x27;org.redisson:redisson-spring-boot-starter:3.40.2&#x27; compileOnly &#x27;org.projectlombok:lombok&#x27; developmentOnly &#x27;org.springframework.boot:spring-boot-devtools&#x27; annotationProcessor &#x27;org.projectlombok:lombok&#x27; testImplementation &#x27;org.springframework.boot:spring-boot-starter-test&#x27; testRuntimeOnly &#x27;org.junit.platform:junit-platform-launcher&#x27;&#125; 编写如下配置 application.yaml ： 1234spring: data: redis: host: &lt;redis_host&gt; 编写 TestService.java : 12345678910111213141516171819202122232425262728293031323334353637import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.redisson.api.RLock;import org.redisson.api.RedissonClient;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;@Slf4j@Servicepublic class TestService &#123; @Resource RedissonClient redissonClient; public String test(String id, Long time) throws InterruptedException &#123; log.info(&quot;Attempting to acquire lock with id: &#123;&#125;&quot;, id); RLock lock = redissonClient.getLock(id); boolean lockAcquired = lock.tryLock(2, TimeUnit.SECONDS); if (lockAcquired) &#123; try &#123; log.info(&quot;Lock acquired for id: &#123;&#125;, performing critical operation...&quot;, id); Thread.sleep(time); &#125; finally &#123; lock.unlock(); log.info(&quot;Lock released for id: &#123;&#125;&quot;, id); &#125; return &quot;Operation completed successfully&quot;; &#125; else &#123; log.warn(&quot;Unable to acquire lock for id: &#123;&#125;&quot;, id); return &quot;Unable to acquire lock, please try again later.&quot;; &#125; &#125;&#125; 编写 TestController.java : 12345678910111213141516171819import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource TestService testService; @GetMapping(&quot;/&#123;id&#125;/&#123;time&#125;&quot;) public String test(@PathVariable String id, @PathVariable Long time) throws InterruptedException &#123; return testService.test(id, time); &#125;&#125; 之后启动服务，然后使用如下地址进行测试即可： http://localhost:8080/test/1/20000 http://localhost:8080/test/1/1000 客户端缓存 经过查找发现 Spring Data Redis 并不打算支持此功能。如需使用需要自行根据 Lettuce 实现。 拒绝原文 单元测试 在编写单元测试时需要加上 @DataRedisTest 注解，此注解会使用内存数据库进行测试，而不会产生多余的测试数据。 参考资料 官方文档 Jedis Lettuce Redssion","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"打包插件","slug":"java/assemble","date":"2022-06-10T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/maven-assemble/","permalink":"https://wangqian0306.github.io/2022/maven-assemble/","excerpt":"","text":"打包插件 简介 在需要将带有额外包的程序部署在集群上的时候，可以通过使用下面的方式将外部包与代码进行合并打包。 注：如果是 Spring Boot 项目则可以直接使用 GraalVM Native Support 打包成可执行文件。 Maven 样例如下： 123456789101112131415161718192021222324252627282930313233343536&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;xxx.xxx.xxx&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 如果需要移除某些包则可以新增下面的配置项： 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;xxx&lt;/groupId&gt; &lt;artifactId&gt;xxx&lt;/artifactId&gt; &lt;version&gt;xxx&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Gradle 在 build.gradle 文件中新增如下内容即可： 1234plugins &#123; id &#x27;com.github.johnrengelman.shadow&#x27; version &#x27;7.1.2&#x27; id &#x27;java&#x27;&#125; 参考资料 Apache Maven Assembly Plugin 官方文档 Gradle Shadow","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Maven","slug":"Maven","permalink":"https://wangqian0306.github.io/tags/Maven/"},{"name":"Gradle","slug":"Gradle","permalink":"https://wangqian0306.github.io/tags/Gradle/"}]},{"title":"Doris","slug":"bigdata/doris","date":"2022-06-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/doris/","permalink":"https://wangqian0306.github.io/2022/doris/","excerpt":"","text":"Doris 简介 Apache Doris 是一个现代化的 MPP 分析型数据库产品。 仅需亚秒级响应时间即可获得查询结果，有效地支持实时数据分析。 Apache Doris 的分布式架构非常简洁，易于运维，并且可以支持 10 PB 以上的超大数据集。 Doris 的组成架构如下所示： 关键概念 FE(Frontend)：前端进程，负责如下工作 存储，维护元数据 接收解析查询请求 规划查询计划 调度查询执行 返回查询结果 BE(Backend)：后端进程，负责如下工作 依据物理计划，分布执行查询 存储数据，保证可靠性 Broker：无状态进程，主要用于访问外部数据源 Tablet：表的实际存储单元。在由 BE 组成的分布式存储层中，经过分区和分桶后，会以 Tablet 为基本单元存储一张表。每个 Tablet 包括元信息和几个连续的 RowSet。 Rowset：Rowset 是 Tablet 中某次数据变化的数据集合，数据变化包括数据的导入、删除、更新。按版本信息记录的行集。每次更改都会生成一个版本。 Version：由 Start 和 End 两个属性组成，维护数据变化的记录信息。通常用于表示 Rowset 的版本范围，在新导入后会生成 Start 和 End 相等的 Rowset，Compaction 后会生成具有范围的 Rowset 版本。 Segment：表示 Rowset 中的数据段。多个 Segment 形成一个 Rowset。 Compaction：合并连续版本的 Rowset 的过程称为 Compaction，合并过程中数据会被压缩。 FE 中还具有 Leader，Follower 和 Observer 角色的不同进程： Leader 和 Follower 角色使用 Paxos 协议保证了高可用。 Observer 角色则是作为扩展的查询节点，可以增强集群在查询方面的能力。(Observer 不参与任何写入，只参与读取)。 安装 编辑最大打开的句柄数 1vim /etc/security/limits.conf 新增如下内容 12* soft nofile 65536* hard nofile 65536 注：此处内容需要重启才能生效 下载软件包，并解压至指定目录 1234567wget https://dist.apache.org/repos/dist/release/incubator/doris/&lt;version&gt;# 注：例如wget https://dist.apache.org/repos/dist/release/incubator/doris/1.0/1.0.0-incubating/apache-doris-1.0.0-incubating-bin.tar.gztar -zxvf apache-doris-1.0.0-incubating-bin.tar.gzmv apache-doris-1.0.0-incubating-bin /opt/doriscd /opt/doris 编辑 FE 配置 1vim fe/conf/fe.conf 修改如下内容 1priority_networks = 127.0.0.0/24 编辑 BE 配置 1vim be/conf/be.conf 修改如下内容 1priority_networks = 127.0.0.0/24 编辑环境变量 1vim /etc/profile.d/doris.sh 填入如下内容 12export DORIS_HOME=/opt/dorisexport PATH=$PATH:$DORIS_HOME/fe/bin:$DORIS_HOME/be/bin 1source /etc/profile.d/doris.sh 启动服务 12start_fe.sh --daemonstart_be.sh --daemon 检查服务启动情况 请根据返回内容进行判断 12curl http://fe_host:fe_http_port/api/bootstrapcurl http://be_host:be_http_port/api/health 注： fe_http_port 为 8030，WebUI 默认账号为 root，密码为空。 be_http_port 为 8040， 安装 MySQL 客户端 1yum install -y mysql 注册 BE 至 FE 登陆 Doris 1mysql -h 127.0.0.1 -P 9030 -uroot 注册 BE 1ALTER SYSTEM ADD BACKEND &quot;127.0.0.1:9050&quot;; 参考资料 官方文档 视频教程","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"https://wangqian0306.github.io/tags/OLAP/"},{"name":"Doris","slug":"Doris","permalink":"https://wangqian0306.github.io/tags/Doris/"}]},{"title":"Java 版本的图形算法","slug":"java/graph","date":"2022-06-06T15:09:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/java-graph/","permalink":"https://wangqian0306.github.io/2022/java-graph/","excerpt":"","text":"Java 版本的图形算法 简介 最近在工作中遇到了计算机图形学相关的问题，所以针对这些问题进行了一些整理。 计算机图形学教程 华中科技大学-计算机图形学教程 光线投射算法 光线投射算法可以判别单点是否在多边形区域中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import static java.lang.Math.*; public class RayCasting &#123; static boolean intersects(int[] A, int[] B, double[] P) &#123; if (A[1] &gt; B[1]) return intersects(B, A, P); if (P[1] == A[1] || P[1] == B[1]) P[1] += 0.0001; if (P[1] &gt; B[1] || P[1] &lt; A[1] || P[0] &gt;= max(A[0], B[0])) return false; if (P[0] &lt; min(A[0], B[0])) return true; double red = (P[1] - A[1]) / (double) (P[0] - A[0]); double blue = (B[1] - A[1]) / (double) (B[0] - A[0]); return red &gt;= blue; &#125; static boolean contains(int[][] shape, double[] pnt) &#123; boolean inside = false; int len = shape.length; for (int i = 0; i &lt; len; i++) &#123; if (intersects(shape[i], shape[(i + 1) % len], pnt)) inside = !inside; &#125; return inside; &#125; public static void main(String[] a) &#123; double[][] testPoints = &#123;&#123;10, 10&#125;, &#123;10, 16&#125;, &#123;-20, 10&#125;, &#123;0, 10&#125;, &#123;20, 10&#125;, &#123;16, 10&#125;, &#123;20, 20&#125;&#125;; for (int[][] shape : shapes) &#123; for (double[] pnt : testPoints) System.out.printf(&quot;%7s &quot;, contains(shape, pnt)); System.out.println(); &#125; &#125; final static int[][] square = &#123;&#123;0, 0&#125;, &#123;20, 0&#125;, &#123;20, 20&#125;, &#123;0, 20&#125;&#125;; final static int[][] squareHole = &#123;&#123;0, 0&#125;, &#123;20, 0&#125;, &#123;20, 20&#125;, &#123;0, 20&#125;, &#123;5, 5&#125;, &#123;15, 5&#125;, &#123;15, 15&#125;, &#123;5, 15&#125;&#125;; final static int[][] strange = &#123;&#123;0, 0&#125;, &#123;5, 5&#125;, &#123;0, 20&#125;, &#123;5, 15&#125;, &#123;15, 15&#125;, &#123;20, 20&#125;, &#123;20, 0&#125;&#125;; final static int[][] hexagon = &#123;&#123;6, 0&#125;, &#123;14, 0&#125;, &#123;20, 10&#125;, &#123;14, 20&#125;, &#123;6, 20&#125;, &#123;0, 10&#125;&#125;; final static int[][][] shapes = &#123;square, squareHole, strange, hexagon&#125;;&#125; 参考资料 多边形撒点算法 注：此章节中的参考资料都是前端代码 参考资料 使用四叉树撒点 参考资料","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"SeaTunnel","slug":"bigdata/seatunnel","date":"2022-06-06T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/seatunnel/","permalink":"https://wangqian0306.github.io/2022/seatunnel/","excerpt":"","text":"SeaTunnel 简介 SeaTunnel 是一个非常好用的超高性能分布式数据集成平台，支持海量数据的实时同步。 注：使用方式类似于 DataX 和 Logstash 可以读取多个源并将其进行处理之后存储到目标位置中，只不过其中的执行引擎可以是 Spark 或 Flink。而在数据处理方面目前还是建议使用 SQL 安装及配置 安装 从官网下载软件包，进行解压。 配置 注：SeaTunnel 默认运行需要指定本机中的 SPARK_HOME 或 FLINK_HOME 环境变量。 可以配置 config/seatunnel-env.sh 脚本来跳过环境变量。 样例 在 config 目录中有如下的样例： flink.batch.conf.template flink.sql.conf.template flink.streaming.conf.template spark.batch.conf.template spark.streaming.conf.template 注：每个插件都有自己的默认值，这些默认值需要到官网进行查询。 在 bin 目录中如下脚本： start-seatunnel-flink.sh start-seatunnel-spark.sh start-seatunnel-sql.sh 注：目前网站上仅存在 flink 和 spark 的使用说明，而 sql 脚本也是基于 flink 实现的。 运行方式： Spark 1234./bin/start-seatunnel-spark.sh \\--master local[4] \\--deploy-mode client \\--config ./config/spark.streaming.conf.template Flink 12./bin/start-seatunnel-flink.sh \\--config ./config/flink.streaming.conf.template 配置文件简介 一个基本的配置文件由以下几个部分组成： 123456789101112env &#123; # 设置 Spark 或 Flink 的环境参数&#125;source &#123; # 声明数据源(此处可声明多个 Source 插件)&#125;transform &#123; # 数据处理(可以为空，但必须存在)&#125;sink &#123; # 声明数据输出&#125; 插件简介 所有插件都需要去官网查看支持的运行引擎和参数及默认值。除了每个插件自带的参数之外，每种类型的插件还存在共用的参数。 目前的情况是： 在 source 块中使用 result_table_name 标识输出。 在 transform 块中使用 source_table_name 标识输入，result_table_name 标识输出。 在 sink 块中使用 source_table_name 标识输入。 注：SQL 无需标识输入参数。 参考资料 官网 视频教程","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"SeaTunnel","slug":"SeaTunnel","permalink":"https://wangqian0306.github.io/tags/SeaTunnel/"}]},{"title":"Spring Data JPA","slug":"spring/jpa","date":"2022-05-06T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2022/jpa/","permalink":"https://wangqian0306.github.io/2022/jpa/","excerpt":"","text":"Spring Data JPA 简介 Spring Data JPA 旨在通过减少实际需要的工作量来显着改进数据访问层的实现。 常见使用方式 Repository 中的返回对象及分页与排序参数 在查询大量数据时，Spring 提供了多种的返回对象与输入参数，可以在不同情况下进行采用。 12345678910111213141516171819import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123; // 较为适合 RESTful Page&lt;User&gt; findByLastname(String lastname, Pageable pageable); // 较为适合 GraphQL Slice&lt;User&gt; findByLastname(String lastname, Pageable pageable); Window&lt;User&gt; findTop10ByLastname(String lastname, ScrollPosition position, Sort sort); List&lt;User&gt; findByLastname(String lastname, Sort sort); List&lt;User&gt; findByLastname(String lastname, Pageable pageable);&#125; 自动获取更新时间创建时间等内容 首先需要按照如下样例创建模型类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import lombok.Getter;import lombok.RequiredArgsConstructor;import lombok.Setter;import lombok.ToString;import org.springframework.data.annotation.CreatedBy;import org.springframework.data.annotation.CreatedDate;import org.springframework.data.annotation.LastModifiedBy;import org.springframework.data.annotation.LastModifiedDate;import javax.persistence.Entity;import javax.persistence.Id;import java.util.Date;enum TypeEnum &#123; A, B&#125;@Getter@Setter@ToString@RequiredArgsConstructor@Entity@EntityListeners(AuditingEntityListener.class)public class Test &#123; @Id private String id; @Enumerated(EnumType.STRING) private TypeEnum type = TypeEnum.A; @CreatedBy private String createdBy; @LastModifiedBy private String lastModifiedBy; @CreatedDate private Date createdDate; @LastModifiedDate private Date updatedDate; @Transient private String cache;&#125; 创建如下的操作员获取类： 1234567891011121314151617181920212223import jakarta.validation.constraints.NotNull;import org.springframework.data.domain.AuditorAware;import org.springframework.security.core.context.SecurityContextHolder;import org.springframework.security.oauth2.jwt.Jwt;import org.springframework.stereotype.Component;import java.util.Optional;@Componentpublic class AuditorConfig implements AuditorAware&lt;String&gt; &#123; @NotNull @Override public Optional&lt;String&gt; getCurrentAuditor() &#123; Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal(); if (principal instanceof Jwt) &#123; return Optional.ofNullable(((Jwt) principal).getSubject()); &#125; else &#123; return Optional.empty(); &#125; &#125;&#125; 注：此样例为 SpringSecurity 启用 JWT 之后获取用户名的方式。 开启编辑审计功能(在启动类中加入下面的注解即可) 1234567891011121314import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.data.jpa.repository.config.EnableJpaAuditing;@SpringBootApplication@EnableJpaAuditingpublic class TestApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Test.class, args); &#125;&#125; 表名或列名大写 在配置中填入如下内容即可： 12345spring: jpa: hibernate: naming: physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl 自定义返回分页对象 1234567891011121314151617181920import org.springframework.data.domain.Page;import org.springframework.data.domain.PageImpl;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Pageable;import java.util.ArrayList;import java.util.List;class T &#123;&#125;public class Test &#123; public void test() &#123; long total = 1L; List&lt;T&gt; contentList = new ArrayList&lt;&gt;(); // 注意此处的页码是从 0 开始的 Pageable pageable = PageRequest.of(0, 10); Page&lt;T&gt; result = new PageImpl&lt;&gt;(contentList, pageable, total); &#125;&#125; 使用大写的表名和列名 新增如下配置即可： 123456789101112131415161718192021222324252627282930313233343536373839import org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy;import org.hibernate.boot.model.naming.Identifier;import org.hibernate.engine.jdbc.env.spi.JdbcEnvironment;import org.springframework.context.annotation.Configuration;@Configurationpublic class CustomNamingConf extends CamelCaseToUnderscoresNamingStrategy &#123; @Override public Identifier toPhysicalCatalogName(Identifier logicalName, JdbcEnvironment context) &#123; return logicalName; &#125; @Override public Identifier toPhysicalSchemaName(Identifier logicalName, JdbcEnvironment context) &#123; return logicalName; &#125; @Override public Identifier toPhysicalTableName(Identifier logicalName, JdbcEnvironment context) &#123; return logicalName; &#125; @Override public Identifier toPhysicalSequenceName(Identifier logicalName, JdbcEnvironment context) &#123; return logicalName; &#125; @Override public Identifier toPhysicalColumnName(Identifier logicalName, JdbcEnvironment context) &#123; return logicalName; &#125; @Override protected boolean isCaseInsensitive(JdbcEnvironment jdbcEnvironment) &#123; return false; &#125;&#125; 使用自定义的 SQL 文件 在初始化时可以采用自定义的 SQL 文件： 123456spring: sql: init: mode: always schema-locations: schema.sql data-locations: data.sql 使用 JPA 中的关系 使用如下模型类： 123456789101112131415@Entity@NamedEntityGraph(name = &quot;Item.characteristics&quot;, attributeNodes = @NamedAttributeNode(&quot;characteristics&quot;))public class Item &#123; @Id private Long id; private String name; @OneToMany(mappedBy = &quot;item&quot;) private List&lt;Characteristic&gt; characteristics = new ArrayList&lt;&gt;(); // getters and setters&#125; 12345678910111213@Entitypublic class Characteristic &#123; @Id private Long id; private String type; @ManyToOne(fetch = FetchType.LAZY) @JoinColumn private Item item; //Getters and Setters&#125; 然后在 Repository 中声明启用相关图即可： 12345public interface ItemRepository extends JpaRepository&lt;Item, Long&gt; &#123; @EntityGraph(value = &quot;Item.characteristics&quot;) Item findByName(String name);&#125; 动态构建查询 基本查询 首先需要在 Repository 中额外引入 JpaSpecificationExecutor 123@Repositorypublic interface TestRepository extends JpaRepository&lt;Test, String&gt;, JpaSpecificationExecutor&lt;Test&gt; &#123;&#125; 然后即可在查询中进行如下操作 1234567891011121314151617@Servicepublic class Test &#123; @Resource private TestRepository testRepository; public void test() &#123; testRepository.findAll((root, criteriaQuery, cb) -&gt; &#123; List&lt;Predicate&gt; predicate = new ArrayList&lt;&gt;(); // 此处可以添加条件，需要注意的是所填写的列名要和模型对象名一致 Path&lt;String&gt; testPath = root.get(&quot;test&quot;); predicate.add(cb.like(testPath, &quot;test&quot;)); Predicate[] pre = new Predicate[predicate.size()]; criteriaQuery.where(predicate.toArray(pre)); return criteriaQuery.getRestriction(); &#125;); &#125;&#125; 连表查询 如果需要联表查询则可使用如下的方式： 1234567891011121314151617import jakarta.persistence.Entity;import jakarta.persistence.GeneratedValue;import jakarta.persistence.GenerationType;import jakarta.persistence.Id;import lombok.Data;@Data@Entitypublic class Book &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String title;&#125; 123456789101112131415161718192021import jakarta.persistence.*;import lombok.Data;import java.util.List;@Data@Entitypublic class Author &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String firstName; private String lastName; @OneToMany(cascade = CascadeType.ALL) private List&lt;Book&gt; books;&#125; 123@Repositorypublic interface AuthorsRepository extends JpaRepository&lt;Author, Long&gt;, JpaSpecificationExecutor&lt;Author&gt; &#123;&#125; 12345678910111213141516import jakarta.persistence.criteria.Predicate;import org.springframework.data.jpa.domain.Specification;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.List;@Servicepublic class TestService &#123; public static Specification&lt;Author&gt; hasBookWithTitle(String bookTitle) &#123; return (root, query, criteriaBuilder) -&gt; &#123; Join&lt;Book, Author&gt; authorsBook = root.join(&quot;books&quot;); return criteriaBuilder.equal(authorsBook.get(&quot;title&quot;), bookTitle); &#125;; &#125;&#125; QueryDSL 注：和 Specification 类似，但是对 GraphQL 更友好。 Maven 引入依赖： 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-jpa&lt;/artifactId&gt; &lt;version&gt;$&#123;querydsl.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.querydsl&lt;/groupId&gt; &lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt; &lt;version&gt;$&#123;querydsl.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 安装插件： 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;com.mysema.maven&lt;/groupId&gt; &lt;artifactId&gt;apt-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;process&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/generated-sources/java&lt;/outputDirectory&gt; &lt;processor&gt;com.querydsl.apt.jpa.JPAAnnotationProcessor&lt;/processor&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 编写模型，加上 @Table 与 @Entity 注解，然后使用 mvn compile 即可在对应目录找到查询类。 Gradle 引入依赖： 12345678dependencies &#123; implementation(&quot;com.querydsl:querydsl-core:5.0.0&quot;) implementation(&quot;com.querydsl:querydsl-jpa:5.0.0:jakarta&quot;) annotationProcessor( &quot;com.querydsl:querydsl-apt:5.0.0:jakarta&quot;, &quot;jakarta.persistence:jakarta.persistence-api:3.1.0&quot; )&#125; 编写模型，加上 @Table 与 @Entity 注解，然后使用 ./gradlew compileJava 即可在对应目录找到查询类。 如果准备与 SpringDataJpa 一起使用则可以依照如下方式编写 Repository : 1234567import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.querydsl.QuerydslPredicateExecutor;import org.springframework.stereotype.Repository;@Repositorypublic interface AuthorRepository extends JpaRepository&lt;Author, Long&gt;, QuerydslPredicateExecutor&lt;Author&gt; &#123;&#125; 然后按如下方式进行查询即可: 1234567891011121314151617181920212223import com.querydsl.core.types.Predicate;import jakarta.annotation.Resource;import org.springframework.graphql.data.method.annotation.Argument;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.stereotype.Controller;@Controllerpublic class TestController &#123; @Resource private AuthorRepository authorRepository; @QueryMapping Iterable&lt;Author&gt; authors(@Argument String name) &#123; if (name != null) &#123; QAuthor author = QAuthor.author; Predicate predicate = author.name.eq(name); return authorRepository.findAll(predicate); &#125; else &#123; return authorRepository.findAll(); &#125; &#125;&#125; 如果使用 QueryDSL 原始的查询方式则可以按照如下方式编写代码: 12345678910111213import com.querydsl.jpa.impl.JPAQueryFactory;import jakarta.persistence.EntityManager;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class QueryDSLConfig &#123; @Bean public JPAQueryFactory jpaQueryFactory(EntityManager entityManager) &#123; return new JPAQueryFactory(entityManager); &#125;&#125; 123456789101112131415161718192021import com.querydsl.core.types.Predicate;import com.querydsl.jpa.impl.JPAQueryFactory;import jakarta.annotation.Resource;import org.springframework.graphql.data.method.annotation.Argument;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.stereotype.Controller;@Controllerpublic class TestController &#123; @Resource private JPAQueryFactory queryFactory; @QueryMapping Iterable&lt;Author&gt; queryAuthor(@Argument String name, @Argument String publisher) &#123; QAuthor author = QAuthor.author; QBook book = QBook.book; return queryFactory.selectFrom(author).leftJoin(author.books, book).fetchJoin().where(author.name.like(name).and(book.publisher.eq(publisher))).fetch(); &#125;&#125; 分页和排序功能如下： 12345678910111213141516171819202122232425262728293031323334import com.querydsl.core.types.Predicate;import com.querydsl.core.types.dsl.PathBuilder;import com.querydsl.jpa.JPAExpressions;import com.querydsl.jpa.impl.JPAQuery;import com.querydsl.jpa.impl.JPAQueryFactory;import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.springframework.data.domain.*;import org.springframework.graphql.data.method.annotation.Argument;import org.springframework.graphql.data.method.annotation.QueryMapping;import org.springframework.stereotype.Controller;@Slf4j@Controllerpublic class TestController &#123; @Resource private JPAQueryFactory queryFactory; @QueryMapping Page&lt;Author&gt; pageAuthor(@Argument String name, @Argument String publisher) &#123; Pageable pageable = PageRequest.of(0, 2); Sort sort = Sort.by(Sort.Direction.DESC, &quot;id&quot;); QAuthor author = QAuthor.author; QBook book = QBook.book; List&lt;Long&gt; count = queryFactory.select(author.id.countDistinct()).from(author).leftJoin(book).on(book.author.id.eq(author.id)).where(author.name.like(name).and(book.publisher.eq(publisher))).fetch(); PathBuilder&lt;Author&gt; authorPathBuilder = new PathBuilder&lt;&gt;(Author.class, author.getMetadata().getName()); JPAQuery&lt;Author&gt; query = queryFactory.selectFrom(author).leftJoin(author.books, book).fetchJoin().where(author.name.like(name).and(book.publisher.eq(publisher))); sort.get().forEach(order -&gt; query.orderBy(QueryDSLUtil.toOrderSpecifier(order, authorPathBuilder))); query.offset(pageable.getOffset()); query.limit(pageable.getPageSize()); return new PageImpl&lt;&gt;(query.fetch(), pageable, count.get(0)); &#125;&#125; 注：此处由于检索逻辑复杂，所以 QueryDSL 在内存中做了数据合并，并不能依照正常的方式完成数据分页，在使用时需要尤其注意。 单元测试 在编写单元测试时需要加上 @DataJpaTest 注解，此注解会使用内存数据库进行测试，而不会产生多余的测试数据。 样例如下： 12345678import org.springframework.boot.test.autoconfigure.data.jdbc.DataJdbcTest;import org.springframework.boot.test.autoconfigure.jdbc.AutoConfigureTestDatabase;@DataJdbcTest@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE)public class DataTest &#123; &#125; 还可以在测试时指定独立的配置环境： 123456import org.springframework.test.context.ActiveProfiles;@ActiveProfiles(&quot;dev&quot;)public class DataTest &#123;&#125; 或是直接使用测试容器： 12345678910111213import org.springframework.boot.testcontainers.service.connection.ServiceConnection;import org.testcontainers.containers.PostgreSQLContainer;import org.testcontainers.junit.jupiter.Container;import org.testcontainers.junit.jupiter.Testcontainers;@Testcontainerspublic class DataTest &#123; @Container @ServiceConnection static PostgreSQLContainer&lt;?&gt; postgres = new PostgreSQLContainer&lt;&gt;(&quot;postgres:16.0&quot;); &#125; 使用样例数据查询(Query By Example,QBE) QBE 适合以下场景： 过滤多个条件 加速原型(初版)开发 简单的基本内容检索 编译时搜索条件未知 不适合以下场景： 复杂比较(&lt;,&gt;,BETWEEN) OR 查询条件 JOIN 查询条件 自定义 SQL 样例项目","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Spring Data JPA","slug":"Spring-Data-JPA","permalink":"https://wangqian0306.github.io/tags/Spring-Data-JPA/"}]},{"title":"Windows 远程链接","slug":"windows/remote","date":"2022-04-28T12:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2022/remote/","permalink":"https://wangqian0306.github.io/2022/remote/","excerpt":"","text":"Windows 远程链接 简介 Windows 内置了远程链接的官方软件。 启动远程链接 注：好久没关注，Windows 官方更新了远程桌面的配置方式。 点击设置按钮 进入系统设置 进入远程桌面 打开 ‘启用远程桌面’ 配置 '接通电源时，让我的电脑保持唤醒状态以进行链接 配置 ‘使我的电脑在专用网络上可被发现，以支持从远程设备进行自动链接’ 进入高级设置 配置 ‘需要计算机使用网络级别身份验证进行链接(建议)’ WOL 网络唤醒 右键我的电脑点击属性 在 ‘关于’ 项目的 ‘相关设置’ 中找到 ‘设备管理器’ 并将其打开 设备管理器 打开网络适配器选项 选择对应网卡，并点击右键选择属性 在属性中选择高级，并打开以下配置项 唤醒模式匹配 唤醒魔包 启用 PME 在电源管理中勾选 允许此设备唤醒计算机 注：此外还需要配合主板进行配置，就可以网络唤醒设备了 远程文件共享 打开控制面板 选择网络和共享中心 打开高级共享设置 选择当前配置文件所在的网络，并打开如下配置 启用网络发现功能 启用文件和打印机共享 选择所有网络，并打开如下配置 启用共享以便可以访问网络的用户可以读取和写入公共文件夹中的文件 使用 128 位加密帮助保护文件共享链接 无密码保护的共享 右键打开将要共享的文件夹或磁盘，选择属性 跳转至共享配置页面 选择共享并指定共享用户即可 从远程设备访问本机 使用 win + R 快捷键打开运行软件然后输入下面的命令： 1mstsc","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"remote","slug":"remote","permalink":"https://wangqian0306.github.io/tags/remote/"}]},{"title":"Windows 打开 WSL2","slug":"windows/wsl2","date":"2022-04-27T12:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2022/terminal/","permalink":"https://wangqian0306.github.io/2022/terminal/","excerpt":"","text":"Windows 打开 WSL2 简介 WSL 是 windows推出的可让开发人员不需要安装虚拟机或者设置双系统启动就可以原生支持运行 GNU/Linux 的系统环境，简称 WSL 子系统。 开启 wsl 在管理员模式下打开 PowerShell 并输入如下命令，待命令完成后重启计算机。 列出可用的 Linux 子系统 1wsl --list --online 安装 Linux 子系统 1wsl --install &lt;distribution&gt; 注：不要在 windows 更新的时候安装 Docker，更新完毕后重启电脑然后再安装。 常见问题 安装失败默认回退 在重启时显示遇到问题进行回滚 注：经过查看 windows 日志可能是由于系统版本和激活的问题引起的，具体问题有待进一步排查。 参考资料 官方文档","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"wsl2","slug":"wsl2","permalink":"https://wangqian0306.github.io/tags/wsl2/"}]},{"title":"Ingress-Nginx","slug":"kubernetes/kubernetes-ingress-nginx","date":"2022-04-25T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/kubernetes-ingress-nginx/","permalink":"https://wangqian0306.github.io/2022/kubernetes-ingress-nginx/","excerpt":"","text":"Ingress-Nginx 简介 Ingress-Nginx 是 Kubernetes 的 ingress controller，使用nginx 作为反向代理和负载均衡器。 部署 使用如下命令部署： 12345wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yamlmv deploy.yaml ingress-nginx-controller.yamlsed -i &#x27;s#registry.k8s.io/ingress-nginx#registry.aliyuncs.com/google_containers#g&#x27; ingress-nginx-controller.yamlsed -i &#x27;s#registry.aliyuncs.com/google_containers/controller#registry.aliyuncs.com/google_containers/nginx-ingress-controller#g&#x27; ingress-nginx-controller.yamlkubectl apply -f ingress-nginx-controller.yaml 检查 pod 和 svc 状态： 12kubectl get pods -n ingress-nginxkubectl get svc -n ingress-nginx 注：在单节点部署的时候出现了外部 IP 绑定处于 Pending 的状况，使用如下命令进行了配置 kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;LoadBalancer&quot;, &quot;externalIPs&quot;:[&quot;xxx.xxx.xxx.xxx&quot;]&#125;&#125;' 配置 有三种方法可以配置 NGINX： ConfigMap (全局) Annotations (独立) Custom template (高级) Annotations 配置 样例如下： 1234567891011121314151617181920212223apiVersion: networking.k8s.io/v1kind: Ingressmetadata: labels: app: &lt;name&gt; name: &lt;name&gt; namespace: &lt;namespace&gt; annotations: nginx.org/proxy-connect-timeout: &quot;75s&quot; nginx.org/proxy-read-timeout: &quot;75s&quot;spec: ingressClassName: nginx rules: - host: &lt;host&gt; http: paths: - backend: service: name: &lt;service_name&gt; port: number: &lt;port&gt; path: / pathType: Prefix 参考资料 官方文档 部署说明 配置文档","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"}]},{"title":"Ansible 安装","slug":"tools/ansible-install","date":"2022-03-28T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/ansible-install/","permalink":"https://wangqian0306.github.io/2022/ansible-install/","excerpt":"","text":"Ansible 安装 简介 Ansible 一款由 Python 编写的自动化工具。Ansible 可以从控制节点远程管理机器和其他设备(默认情况下，通过 SSH 协议)。 注：使用 Ansible 可以轻松的管理集群。使用一台控制节点管理很多台设备，非常适合部署集群。 安装 使用 Pip 安装 12python -m pip install ansiblepython -m pip install paramiko 在 CentOS Fedora 上安装 CentOS 12yum install epel-releaseyum install ansible Fedora 1dnf install ansible 在 Ubuntu 上安装 1234apt updateapt install software-properties-commonadd-apt-repository --yes --update ppa:ansible/ansibleapt install ansible 检测安装情况 1ansible all -m ping --ask-pass 安装命令补全 各版本的安装方式 Pip 1python -m pip install argcomplete Fedora 1dnf install python-argcomplete CentOS 1yum install python-argcomplete Ubuntu 1apt install python3-argcomplete 命令补全配置 全局配置 1activate-global-python-argcomplete 注：上述方式需要 bash 版本大于 4.2。可以使用 bash --version 命令查看 bash 版本、 独立命令配置 1vim /etc/profile.d/argcomplete.sh 然后新增如下内容 123456789$ eval $(register-python-argcomplete ansible)$ eval $(register-python-argcomplete ansible-config)$ eval $(register-python-argcomplete ansible-console)$ eval $(register-python-argcomplete ansible-doc)$ eval $(register-python-argcomplete ansible-galaxy)$ eval $(register-python-argcomplete ansible-inventory)$ eval $(register-python-argcomplete ansible-playbook)$ eval $(register-python-argcomplete ansible-pull)$ eval $(register-python-argcomplete ansible-vault) 注：此种方式暂未完成测试。 参考文档 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Ansible","slug":"Ansible","permalink":"https://wangqian0306.github.io/tags/Ansible/"}]},{"title":"Ansible 试用","slug":"tools/ansible-trial","date":"2022-03-28T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/ansible-trial/","permalink":"https://wangqian0306.github.io/2022/ansible-trial/","excerpt":"","text":"Ansible 试用 简介 本文会简述 Ansible 的使用方式。Ansible 项目的目录结构如下所示： 123456789101112131415161718# playbookssite.ymlwebservers.ymlfooservers.ymlroles/ common/ tasks/ handlers/ library/ files/ templates/ vars/ defaults/ meta/ webservers/ tasks/ defaults/ meta/ 默认情况下，Ansible 将在角色中的每个目录中main.yml查找相关内容的文件（也main.yaml和main）： tasks/main.yml- 角色执行的任务的主要列表。 handlers/main.yml- 处理程序，可以在此角色内部或外部使用。 library/my_module.py- 可在此角色中使用的模块（有关更多信息，请参阅在角色中嵌入模块和插件）。 defaults/main.yml- 角色的默认变量（有关更多信息，请参阅使用变量）。这些变量在所有可用变量中具有最低优先级，并且可以很容易地被任何其他变量（包括库存变量）覆盖。 vars/main.yml- 角色的其他变量（有关更多信息，请参阅使用变量）。 files/main.yml- 角色部署的文件。 templates/main.yml- 角色部署的模板。 meta/main.yml- 角色的元数据，包括角色依赖项。 配置目标地址 在项目中会存在 hosts 或 hosts.yaml 文档，通过此文档可以指定安装服务的位置。 在官方实例中提供了 hosts.yaml 的四种样例： 不指定组的主机 123456789# 样例 1: 对于这样的主机需要将其放在 &#x27;all&#x27; 或 &#x27;ungrouped&#x27; 参数下，在样例中定义了 4 个主机，有一个主机还被配置了两个参数all: hosts: green.example.com: ansible_ssh_host: 191.168.100.32 anyvariable: value blue.example.com: 192.168.100.1: 192.168.100.10: 指定组的主机 123456789# 样例 2: 4 个位于 webservers 组中的主机，并且它们全都具有相同的配置项webservers: hosts: alpha.example.org: beta.example.org: 192.168.1.100: 192.168.1.110: vars: http_port: 8080 使用子组的方式 1234567891011121314151617# 样例 3：可以选定范围的方式指定主机并将子组和变量添加到组中。子组和普通组一样可以定义全部内容，并且子组会从父组继承全部变量，同样父组也会包含子组中的所有主机。# testing 组是父组，webservers 组是它的子组。而且在 testing 组中已经指定了 www[001:006].example.com 的主机webservers: hosts: gamma1.example.org: gamma2.example.org:testing: hosts: www[001:006].example.com: vars: testing1: value1 children: webservers:other: children: webservers: gamma3.example.org 注：testing 组包含下面所有主机： gamma1.example.org gamma2.example.org gamma3.example.org www001.example.com www002.example.com www003.example.com www004.example.com www005.example.com www006.example.com 全局参数 1234# 样例 4：全局参数，在 `all` 组中的参数会具有最低级的优先级all: vars: commontoall: thisvar 配置运行文件 在项目中可以编写称为 playbook 的脚本文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517- hosts: all ########### # 关键参数: hosts # 是否必填: 是 # 简介: # 指明需要部署的主机或组。 # ## 样例: # hosts: all -- 所有主机 # hosts: host1 -- 主机 host1 单独运行 # hosts: group1 -- group1 中的所有主机 # hosts: group1,group2 -- group1 和 group2 中的所有主机 # hosts: group1,host1 -- hosts in group1 AND host # ## 表达式样例 # hosts: group1,!group3 -- 在 group1 但是不在 group3 中的主机 # hosts: group1,&amp;group3 -- 在 group1 和 group3 中的主机 # hosts: group1:&amp;group3 -- 同上但是使用 `:` 替代了 `,` # hosts: group1:!group2:&amp;group3 -- 在 group1 和 group3 但不在 group2 的主机 # ## 使用参数的形式传递 # # 可以通过如下的方式传递参数 # # hosts: &#x27;&#123;&#123;mygroups&#125;&#125;&#x27; -- 使用 mygroups 参数传递主机 # # 这对于测试 playbook 非常方便，在针对生产、偶尔的维护任务和其他情况运行 playbook 之前，在一个临时环境中运行相同的 playbook，在这些情况下，您只需要针对几个系统而不是整个组运行 playbook。 # 请注意，不能在清单中设置该变量，因为在使用清单变量之前，我们需要了解主机。所以通常会使用 “额外变量”，如下所示。 # # 如果如上所示设置主机，则可以指定在每次运行时应用 playbook 的主机，如下所示： # # ansible-playbook playbook.yml --extra-vars=&quot;mygroups=staging&quot; # # 使用 --extra vars 将变量设置为组、主机名或主机模式的任意组合，就像上一节中的示例一样。 roles: - foo - bar - foo ########### # 关键参数: roles # 默认: 无 # 是否必填: 否 # 简介: 需要执行任务的角色 name: my heavily commented play ########### # 关键参数: name # 默认: play # 是否必填: 否 # 简介: 需要执行命令的简介 gather_facts: yes ########### # 关键参数: gather_facts # 默认: None # 是否必填: 否 # 简介: # 此参数控制着程序是否触发 `fact gathering task` (也被叫做 `gather_facts` 或 `setup` 操作) 来获取远程执行的返回结果。 # 执行的返回值也通常在选择执行计划和作为参数输入的时候很有用。 # 例如我们想执行 `ansible_os_distribution` 命令来获取主机的系统类型到底是 RHEL, Ubuntu 或 FreeBSD 等, 以及 CPU, RAM 等硬件信息。 remote_user: login_user ########### # 关键参数: remote_user # 默认: 依赖于 `connection` 插件, 对于 ssh 来说就是 &#x27;执行当前命令的用户&#x27; # 是否必填: 否 # 简介: # 登录远程设备的用户，通常也是执行命令的用户 become: True ########### # 关键参数: become # 默认: False # 是否必填: 否 # 简介: # 如果设置为 True 就会提升命令的执行权限，像是在命令行中加入了 `--become` 参数一样。 become_user: root ########### # 关键参数: become_user # 默认: None # 是否必填: 否 # 简介: # 当使用权限升级时，这是您在与远程用户登录后 “become” 的用户。例如，您以 “login_user” 身份登录远程用户，然后您“become” root 用户以执行任务。 become_method: sudo ########### # 关键参数: become_method # 默认: sudo # 是否必填: 否 # 简介: # 当使用特权升级时，这会选择用于特权升级的插件。 # 使用 `ansible-doc -t become -l` 命令来提示更多内容。 connection: ssh ########### # 关键参数: connection # 默认: ssh # 是否必填: 否 # 简介: # 这将设置 Ansible 将使用哪个 `connection` 插件尝试与目标主机进行通信。 # 注意此处是由 paramiko (python 版本的 ssh, 在 ssh 命令行不能很好地与目标系统配合使用的情况下非常有用。 # 除此之外还有 “local”，强制 “local fork” 执行任务，但通常您真正想要的是 “delegate_to:localhost”，其余内容需要参见下面的实例。 # 使用 `ansible-doc -t connection -l` 命令来提示更多内容。 ignore_unreachable: yes ########### # 关键参数: ignore_unreachable # 默认: false # 是否必填: 否 # 简介: # 忽略无法链接的主机。 vars: ########### # 关键参数: vars # 默认: none # 是否必填: 否 # 简介: # 为该任务定义的变量键值对，通常用于模板或任务变量。 # 在使用时填写 &#123;&#123;color&#125;&#125; 即可引用变量 color: brown # 键值对的数据类型允许传入复杂的结构, 在使用时可以采用 &#123;&#123;web[&#x27;memcache&#x27;]&#125;&#125; 这样的方式来获取子项，或 &#123;&#123;web&#125;&#125; 来获取完整的对象 web: memcache: 192.168.1.2 httpd: apache # 列表型参数，使用 &#123;&#123; mylist[1] &#125;&#125; 可以得到 &#x27;b&#x27;, 索引从 0 开始. mylist: - a - b - c # 参数可以使用 Jinja 模板引擎进行动态配置, 直至使用的时候才会被读取. # # 在这个 playbook 中, 此表达式永远会返回 False, 因为 &#x27;color&#x27; 在上面被赋值为了 &#x27;brown&#x27;。 # # 当 ansible 转译如下内容时会首先将 &#x27;color&#x27; 赋值为 &#x27;brown&#x27; 然后依据 Jinja 表达式对比 &#x27;brown&#x27; == &#x27;blue&#x27; is_color_blue: &quot;&#123;&#123; color == &#x27;blue&#x27; &#125;&#125;&quot; my_version: 1.2.3 vars_files: ########## # 关键参数: vars_files # 是否必填: 否 # 简介: # 此处可以填写一个 YAML 格式的参数文件列表，这些参数会在 `vars` 之后载入，无论 `vars` 写在哪里。样例如下： # # --- # monitored_by: phobos.mars.nasa.gov # fish_sticks: &quot;good with custard&quot; # ... # (文件结束) # # 注： `---` 应该位于页面最左端 # # 使用绝对路径引入配置文件 - /srv/ansible/vars/vars_file.yml # 使用相对路径引入配置文件 - vars/vars_file.yml # 使用可变配置引入配置文件 - vars/&#123;&#123;something&#125;&#125;.yml # 也可以使用数组来引入配置文件 - [ &#x27;vars/&#123;&#123;platform&#125;&#125;.yml&#x27;, vars/default.yml ] # 文件会按照顺序进行引入，所以后面的配置文件可以写入更多的内容 - [ &#x27;vars/&#123;&#123;host&#125;&#125;.yml&#x27; ] # 但是如果在做主机特定的变量文件，可以考虑在你的库中设置一个组的变量，并把你的主机添加到那个组。 vars_prompt: ########## # 关键参数: vars_prompt # 是否必填: 否 # 简介: # Ansible 将在每次运行此 playbook 时提示手动输入的变量列表。用于敏感数据，也可用于不同部署的版本号等。 # # 如果已经提供了这个值，Ansible将不会提示输入，比如在传递时——额外的变量，但不是来自库存。 # # 如果检测到它是非交互式会话，它也不会提示。例如，当从cron调用时。 # - name: passphrase prompt: &quot;Please enter the passphrase for the SSL certificate&quot; private: yes # 在 private 参数为 yes 时输入不会回显到终端 # 在配置不敏感的内容时应该这样做. - name: release_version prompt: &quot;Please enter a release tag&quot; private: no # 配置默认值 - name: package_version prompt: &quot;Please enter a package version&quot; default: &#x27;1.0&#x27; # 可以在这个链接找到更多特性 https://docs.ansible.com/ansible/latest/user_guide/playbooks_prompts.html roles: ########## # 关键参数: roles # 是否必填: 否 # 简介: 在此 playbook 中的的权限列表。执行时间在 pre_tasks 和基础信息获取之后, 但在 &#x27;tasks&#x27; 之前. tasks: ########## # 关键参数: tasks # 是否必填: 否 # 简介: 此 playbook 中的任务列表. 在 roles 之后执行，在 post_tasks 之前执行 # 一个简单的任务 # 每个任务都必须有一个操作。&#x27;name&#x27; 是一个可选项，但是对说明任务的工作项非常有用 - name: Check that the target can execute Ansible tasks action: ping ########## # Ansible 的 modules 会完成业务工作!, &#x27;action&#x27; 并不是必须的, 当然也可以使用 &#x27;action itself&#x27; 作为 task 的一部分 - file: path=/tmp/secret mode=0600 owner=root group=root # # &#x27;action&#x27; 的格式如同上面的内容: # &lt;modulename&gt;: &lt;module parameters&gt; # # 可以使用如下的方式进行参数测试: # ansible -m &lt;module&gt; -a &quot;&lt;module parameters&gt;&quot; # # modules 文档如下: # http://ansible.github.com/modules.html # 通常情况下，大多数人希望使用“k:v”符号，而不是上面使用的“k=v”（但对临时执行有用）。 # 虽然这两种格式基本上都是可交换的，`k:v`更明确，类型更友好，并且更容易转义。 - name: Ensure secret is locked down file: path: /tmp/secret mode: &#x27;0600&#x27; owner: root group: root # 请注意 &#x27;action&#x27; 参数的缩进, 并且 &#x27;task&#x27; 要位于顶层 ########## # 在任务中使用变量 - name: Paint the server command: echo &#123;&#123;color&#125;&#125; # 您还可以在任务级别定义变量 - name: Ensure secret is locked down file: path: &#x27;&#123;&#123;secret_file&#125;&#125;&#x27; mode: &#x27;0600&#x27; owner: root group: root vars: secret_file: /tmp/secret # 在运行任务时进行检查 - debug: msg: &quot;my_version is higher than 1.0.0&quot; when: my_version is version(&#x27;1.0.0&#x27;, &#x27;&gt;&#x27;) # 捕捉任务运行中的异常 - name: Attempt and graceful roll back demo block: - name: Print a message ansible.builtin.debug: msg: &#x27;I execute normally&#x27; - name: Force a failure ansible.builtin.command: /bin/false - name: Never print this ansible.builtin.debug: msg: &#x27;I never execute, due to the above task failing, :-(&#x27; rescue: - name: Print when errors ansible.builtin.debug: msg: &#x27;I caught an error&#x27; - name: Force a failure in middle of recovery! &gt;:-) ansible.builtin.command: /bin/false - name: Never print this ansible.builtin.debug: msg: &#x27;I also never execute :-(&#x27; always: - name: Always do this ansible.builtin.debug: msg: &quot;This always executes&quot; # 渲染配置文件到指定位置 - name: Template configuration file ansible.builtin.template: src: template.j2 dest: /etc/foo.conf # 确定软件处于最新版本 - name: Ensure apache is at the latest version ansible.builtin.yum: name: httpd state: latest # 确定软件运行状态 - name: Ensure apache is running ansible.builtin.service: name: httpd state: started # 忽略错误，并且将结果进行存储 - name: Do not count this as a failure ansible.builtin.command: /bin/false ignore_errors: yes register: bass_result # 在返回值处于某些状态时进行异常抛出 - name: Fail task when both files are identical ansible.builtin.raw: diff foo/file1 bar/file2 register: diff_cmd failed_when: diff_cmd.rc == 0 or diff_cmd.rc &gt;= 2 # 使用代理完成任务 - name: Install cobbler ansible.builtin.package: name: cobbler state: present environment: http_proxy: http://proxy.example.com:8080 ########## # 事情发生变化时触发处理程序！ # # 大多数 Ansible 操作都可以在发生变化时检测并报告。 # 例如，如果文件权限与请求的权限不同，文件内容不同，或者在报告更改时安装（或删除）了包，则任务将假定为“已更改”状态。 # Ansible 可以选择通知一个或多个处理程序。 # 处理程序与普通任务类似，主要区别在于它们只在收到通知时运行。 # 常见的用法是在更新服务的配置文件后重新启动服务。 # https://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html#handlers-running-operations-on-change # 在拷贝 httpd.conf 文件之后触发重启 Apache - name: Update the Apache config copy: src: httpd.conf dest: /etc/httpd/httpd.conf notify: Restart Apache # 下面是如何指定多个处理程序的样例 - name: Update our app&#x27;s configuration copy: src: myapp.conf dest: /etc/myapp/production.conf notify: - Restart Apache - Restart Redis ########## # 引入来自其他文件的任务 # # Ansible 可以引入另一个文件中的任务列表。该文件必须表示任务列表。 # # 任务列表如下: # --- # - name: create user # user: name=&#123;&#123;myuser&#125;&#125; color=&#123;&#123;color&#125;&#125; # # - name: add user to group # user: name=&#123;&#123;myuser&#125;&#125; groups=&#123;&#123;hisgroup&#125;&#125; append=true # ... # (文件结束) # # &#x27;tasks&#x27; YAML 文件中的内容是任务列表. 不要用 playbook YAML 代替 &#x27;tasks&#x27; YAML 文件。 # 在本例 new_user.yml 里，用户将是 &#x27;sklar&#x27;，&#x27;color&#x27; 将是 &#x27;red&#x27; - import_tasks: tasks/new_user.yml vars: myuser: sklar color: red # 在本例 new_user.yml 里，用户将是 &#x27;mosh&#x27;，&#x27;color&#x27; 将是 &#x27;mauve&#x27; - import_tasks: tasks/new_user.yml vars: myuser: mosh color: mauve # 引入 playbook 文件 - import_playbook: webservers.yml ########## # 使用列表运行任务 # # Ansible 提供了一个基本的循环方式. 如果任务中引入了 &#x27;loop&#x27; 则会依据给定的内容循环运行。 - name: Create a file named via variable in /tmp file: path=/tmp/&#123;&#123;item&#125;&#125; state=touched loop: - tangerine - lemon - name: Loop using a variable file: path=/tmp/&#123;&#123;item&#125;&#125; state=touched loop: &#x27;&#123;&#123;mylist&#125;&#125;&#x27; vars: # 样例在此处定义，但实际使用中可以在任务运行前的任意位置进行定义。 # 请注意的是 YAML 的缩进，这样的方式也是正确的。 mylist: - tangerine - lemon ########## # 有条件地执行任务 # # 有时，您只想在特定条件下运行某个操作。 # Ansible 支持使用条件 Jinja 表达式，仅在“True”时执行任务。 # # 如果您试图仅在值更改时运行任务， # 请考虑将任务重写为处理程序并使用“notify”（见下文）。 # - name: &quot;shutdown all ubuntu&quot; command: /sbin/shutdown -t now when: &#x27;&#123;&#123;is_ubuntu|bool&#125;&#125;&#x27; - name: &quot;shutdown the if host is in the government&quot; command: /sbin/shutdown -t now when: &quot;&#123;&#123;inventory_hostname in groups[&#x27;government&#x27;]&#125;&#125;&quot; # 另外一种实现方式 - name: &quot;shutdown the if host is in the government&quot; command: /sbin/shutdown -t now when: &quot;&#123;&#123;&#x27;government&#x27; in group_names&#125;&#125;&quot; # Ansible 有一些内置变量，您可以在此处检查它们。 # inventory_hostname 是执行任务的当前主机的名称（源自hosts:关键字） # group_names 包含当前主机（inventory_hostname）所属的组列表 # 组是库存组与属于它们的主机列表的映射 ########## # 使用其他用户运行 # # 每个任务都有可选的关键字来控制用户，以及是否使用权限提升（如sudo或su）切换到该用户。 - name: login in as postgres and dump all postgres databases shell: pg_dumpall -w -f /tmp/backup.psql remote_user: postgres become: False - name: login normally, but sudo to postgres to dump all postgres databases shell: pg_dumpall -w -f /tmp/backup.psql become: true become_user: postgres become_method: sudo ########## # 本地运行! # # 任务也可以委派给控制主机 - name: create tempfile local_action: shell dd if=/dev/urandom of=/tmp/random.txt count=100 # 这相当于如下操作 - name: create tempfile shell: dd if=/dev/urandom of=/tmp/random.txt count=100 delegate_to: localhost # “delegate_to”可以使用任何目标主机，但对于上述情况，它与使用“local_action”相同 handlers: ########## # 关键参数: handlers # 是否必填: 否 # 简介: # Handlers 是在另一个任务更改某些内容时运行的任务。 # 有关如何触发它们的示例，请参见上文。 # 定义 handlers 的格式与任务的格式完全相同。 # 请注意，如果多个任务在playbook运行中通知同一个处理程序，则该处理程序将只为该主机运行一次。 # # 通过名称或使用 listen 关键字来引用处理程序。 # 它们将按照 playbook 中宣布的顺序运行。 # 例如：如果任务要以相反的顺序通知处理程序，如下所示： # # - task: ensure file does not exist # file: # name: /tmp/lock.txt # state: absent # notify: # - Restart application # - Restart nginx # # 因为声明时的顺序 &quot;Restart nginx&quot; handler 会在 &quot;Restart application&quot; 之前运行 # 使用名称触发的样例 - name: Restart nginx service: name: nginx state: restarted # 使用 listen 或名称触发 - name: redis restarter service: name: redis state: restarted listen: - Restart redis # 任何模块都可以用于处理程序操作，尽管这可以通过多种方式和多次触发，但每个主机只能执行一次 - name: restart application that should really be a service command: /srv/myapp/restart.sh listen: - Restart application - restart myapp # 同样还可以包含来自另一个文件的处理程序。结构与任务文件相同，有关示例，请参见上面的任务部分。 - import_tasks: handlers/site.yml# 请注意: 这不是一个 playbook 或任务中所有可能关键字的完整列表，只是一个非常常见的选项示例。# 下面的内容是文档的结束标记... 使用自带的工具进行加密 1234567ansible-vault [create(创建新)|decrypt(解密)edit(编辑加密文件encrypt(加密)rekey(修改口令)view(查看)] [options(选项)] [vaultfile.yml] 例如可以将主机与密码文件单独写出然后进行加密 1ansible-vault encrypt hello.yml 配置用户的方式 可以使用如下的方式覆写： 在运行时使用 -u 参数 将用户相关信息存储在库中 将用户信息存储在配置文件中 设置环境变量 参考资料 官方文档 官方样例","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Ansible","slug":"Ansible","permalink":"https://wangqian0306.github.io/tags/Ansible/"}]},{"title":"线程池","slug":"java/thread-pool","date":"2022-03-25T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/thread-pool/","permalink":"https://wangqian0306.github.io/2022/thread-pool/","excerpt":"","text":"线程池 简介 看到阿里巴巴 “开发军规” 里有这样的内容，所以针对此内容进行了整理。 样例 引入依赖包 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;31.1-jre&lt;/version&gt;&lt;/dependency&gt; 编写线程池服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import com.google.common.util.concurrent.ThreadFactoryBuilder;import java.util.concurrent.*;public class ThreadPoolService &#123; private static final ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;TEST-POOL-%d&quot;).build(); /** * corePoolSize 线程池核心池的大小 * maximumPoolSize 线程池中允许的最大线程数量 * keepAliveTime 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间 * unit keepAliveTime 的时间单位 * workQueue 用来储存等待执行任务的队列 * threadFactory 创建线程的工厂类 * handler 拒绝策略类,当线程池数量达到上线并且workQueue队列长度达到上限时就需要对到来的任务做拒绝处理 * 原理： * 有请求时，创建线程执行任务，当线程数量等于 corePoolSize 时，请求加入阻塞队列里，当队列满了时，接着创建线程， * 线程数等于 maximumPoolSize。当任务处理不过来的时候，线程池开始执行拒绝策略。 * 换言之，线程池最多同时并行执行 maximumPoolSize 的线程，最多处理 maximumPoolSize+workQueue.size() 的任务。多余的默认采用 AbortPolicy 会丢弃。 * 阻塞队列： * ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 * LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 * PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 * DelayQueue： 一个使用优先级队列实现的无界阻塞队列。 * SynchronousQueue： 一个不存储元素的阻塞队列。 * LinkedTransferQueue： 一个由链表结构组成的无界阻塞队列。 * LinkedBlockingDeque： 一个由链表结构组成的双向阻塞队列。 * 拒绝策略： * ThreadPoolExecutor.AbortPolicy: 丢弃任务并抛出 RejectedExecutionException 异常。 (默认) * ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 * ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务。（重复此过程） * ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务。 */ private static final ExecutorService service = new ThreadPoolExecutor( 4, 10, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;&gt;(4), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy() ); public static ExecutorService getExecutorService() &#123; return service; &#125; public static void newTask(Runnable r) &#123; service.execute(r); &#125; public static void shutdown() &#123; service.shutdown(); &#125;&#125; 参考资料 阿里巴巴规范创建 Java 线程池","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"异步与同步的转化","slug":"java/sync-to-async","date":"2022-03-22T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2022/sync-and-async/","permalink":"https://wangqian0306.github.io/2022/sync-and-async/","excerpt":"","text":"异步与同步的转化 简介 在实际的项目开发中经常会用到异步与同步的调用方式。这两种调用方式的区别如下： 同步调用：调用方在调用过程中，持续等待返回结果。 异步调用：调用方在调用过程中，不直接等待返回结果，而是执行其他任务，结果返回形式通常为回调函数。 本文会针对需要转化这两种请求方式的特殊需求进行初步分析。 异步转同步 在进行异步转同步调用的时候通常有如下方式 wait 和 notify 条件锁 Future 包 CountDownLatch CyclicBarrier 前置条件 新建异步调用类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.Random;import java.util.concurrent.Callable;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class AsyncCall &#123; private final Random random = new Random(System.currentTimeMillis()); private final ExecutorService tp = Executors.newSingleThreadExecutor(); // wait 和 notify // 条件锁 // CountDownLatch // CyclicBarrier public void call(final BaseDemo demo) &#123; new Thread(new Runnable() &#123; public void run() &#123; long res = random.nextInt(10); try &#123; Thread.sleep(res * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; demo.callback(res); &#125; &#125;).start(); &#125; // Future 包 public Future&lt;Long&gt; futureCall() &#123; return tp.submit(new Callable&lt;Long&gt;() &#123; public Long call() throws Exception &#123; long res = random.nextInt(10); try &#123; Thread.sleep(res * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return res; &#125; &#125;); &#125; public void shutdown() &#123; tp.shutdown(); &#125;&#125; 新建调用抽象类 12345678910111213public abstract class BaseDemo &#123; protected AsyncCall asyncCall = new AsyncCall(); public abstract void callback(Long response); public void call() &#123; System.out.println(&quot;发起调用&quot;); asyncCall.call(this); System.out.println(&quot;结束调用&quot;); &#125;&#125; wait 和 notify 1234567891011121314151617181920212223242526public class WaitNotifyDemo extends BaseDemo &#123; private final Object lock = new Object(); public void callback(Long response) &#123; System.out.println(&quot;响应回调&quot;); System.out.println(&quot;返回等待时长: &quot; + response); System.out.println(&quot;回调结束&quot;); synchronized (lock) &#123; lock.notifyAll(); &#125; &#125; public static void main(String[] args) &#123; WaitNotifyDemo demo = new WaitNotifyDemo(); demo.call(); synchronized (demo.lock) &#123; try &#123; demo.lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;主线程结束&quot;); &#125;&#125; 条件锁 1234567891011121314151617181920212223242526272829303132333435363738import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;// 条件锁版样例代码public class ConditionVariableDemo extends BaseDemo &#123; private final Lock lock = new ReentrantLock(); private final Condition con = lock.newCondition(); @Override public void callback(Long response) &#123; System.out.println(&quot;响应回调&quot;); System.out.println(&quot;返回等待时长: &quot; + response); lock.lock(); try &#123; con.signal(); &#125; finally &#123; lock.unlock(); &#125; System.out.println(&quot;回调结束&quot;); &#125; public static void main(String[] args) &#123; ConditionVariableDemo demo = new ConditionVariableDemo(); demo.call(); demo.lock.lock(); try &#123; demo.con.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; demo.lock.unlock(); &#125; System.out.println(&quot;主线程结束&quot;); &#125;&#125; Future 包 12345678910111213141516171819202122232425262728import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;// Future 包版样例代码public class FutureDemo &#123; private static final AsyncCall asyncCall = new AsyncCall(); public Future&lt;Long&gt; call() &#123; Future&lt;Long&gt; future = asyncCall.futureCall(); asyncCall.shutdown(); return future; &#125; public static void main(String[] args) &#123; FutureDemo demo = new FutureDemo(); System.out.println(&quot;发起调用&quot;); Future&lt;Long&gt; future = demo.call(); System.out.println(&quot;结束调用&quot;); try &#123; System.out.println(&quot;返回等待时长: &quot; + future.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程结束&quot;); &#125;&#125; CountDownLatch 12345678910111213141516171819202122232425// CountDownLaunch 版样例代码public class CountDownLaunchDemo extends BaseDemo &#123; private final CountDownLatch countDownLatch = new CountDownLatch(1); @Override public void callback(Long response) &#123; System.out.println(&quot;响应回调&quot;); System.out.println(&quot;返回等待时长: &quot; + response); System.out.println(&quot;调用结束&quot;); countDownLatch.countDown(); &#125; public static void main(String[] args) &#123; CountDownLaunchDemo demo = new CountDownLaunchDemo(); demo.call(); try &#123; demo.countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程内容&quot;); &#125;&#125; CyclicBarrier 123456789101112131415161718192021222324252627282930313233import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;// CyclicBarrier 版样例代码public class CyclicBarrierDemo extends BaseDemo &#123; private final CyclicBarrier cyclicBarrier = new CyclicBarrier(2); @Override public void callback(Long response) &#123; System.out.println(&quot;响应回调&quot;); System.out.println(&quot;返回等待时长: &quot; + response); System.out.println(&quot;回调结束&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; CyclicBarrierDemo demo = new CyclicBarrierDemo(); demo.call(); try &#123; demo.cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;主线程内容&quot;); &#125;&#125; 同步转异步 使用多线程即可，具体使用方式参见线程池部分。 参考资料 异步调用转同步","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Flink 侧输出流","slug":"flink/side-output","date":"2022-03-02T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_side_output/","permalink":"https://wangqian0306.github.io/2022/flink_side_output/","excerpt":"","text":"Flink 侧输出流 简介 在 DataStream API 中除了通常操作产生的主流之外，还可以生成任意数量的侧输出流，且主流于侧输出流的数据类型可以不同。可以生成侧输出流的函数如下： ProcessFunction KeyedProcessFunction CoProcessFunction KeyedCoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 使用示例 向侧输出流中输入内容的示例如下： 12345678910111213141516171819DataStream&lt;Integer&gt; input = ...;final OutputTag&lt;String&gt; outputTag = new OutputTag&lt;String&gt;(&quot;side-output&quot;)&#123;&#125;;SingleOutputStreamOperator&lt;Integer&gt; mainDataStream = input .process(new ProcessFunction&lt;Integer, Integer&gt;() &#123; @Override public void processElement( Integer value, Context ctx, Collector&lt;Integer&gt; out) throws Exception &#123; // emit data to regular output out.collect(value); // emit data to side output ctx.output(outputTag, &quot;sideout-&quot; + String.valueOf(value)); &#125; &#125;); 还可以通过如下方式处理侧输出流： 12345final OutputTag&lt;String&gt; outputTag = new OutputTag&lt;String&gt;(&quot;side-output&quot;)&#123;&#125;;SingleOutputStreamOperator&lt;Integer&gt; mainDataStream = ...;DataStream&lt;String&gt; sideOutputStream = mainDataStream.getSideOutput(outputTag); 参考资料 官方文档","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink 数据类型和序列化","slug":"flink/data-type","date":"2022-02-28T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_data_type/","permalink":"https://wangqian0306.github.io/2022/flink_data_type/","excerpt":"","text":"Flink 数据类型和序列化 简介 Flink 处于对执行性能的考量对可以在 DataStream 中的元素类型进行了一些限制。目前支持的类型如下： Java Tuples and Scala Case Classes Java POJOs Primitive Types 原始数据类型及包装类 Regular Classes 即大多数的 Java 和 Scala 类，限制适用于包含无法序列化的字段的类，这样的类通常使用序列化框架 Kryo 进行序列化/反序列化。 Values 即 ByteValue, ShortValue, IntValue, LongValue, FloatValue, DoubleValue, StringValue, CharValue, BooleanValue Hadoop Writables 实现了 org.apache.hadoop.Writable 接口的类 Special Types Scala 中的 Either, Option 和 Try 以及 Java 中的 Either POJO 常见使用方式 在使用 KeyBy 处理 POJO 类的时候需要重写 hashCode 方法，具体样例如下： 1234567891011121314151617181920212223242526272829public class Color &#123; private String name; Color(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return this.name; &#125; @Override public int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + ((name == null) ? 0 : name.hashCode()); return result; &#125;&#125; 常见问题 注册子类型：在程序声明中只包含了父类型，但是在使用中需要使用子类，此时注册子类可以让 Flink 提高性能。(为子类调用 .registerType(clazz) 方法) 注册自定义序列化器：Flink 使用 Kryo 作为默认序列化器。如果需要使用其他序列化方式则需要进行独立配置。(第三方序列化工具) 新增类型提示：在 Java 程序中返回类型不确定时需要指定返回类型。 手动创建 TypeInformation：在 Flink 无法推断数据类型时需要配置此项。 参考资料 官方文档","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink Broadcast State","slug":"flink/broadcast-state","date":"2022-02-23T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_broadcast_state/","permalink":"https://wangqian0306.github.io/2022/flink_broadcast_state/","excerpt":"","text":"Flink Broadcast State 简介 Broadcast State 意为将输入的内容广播至所有 Operator 中。在官方文档当中描述了这样的使用场景，输入流有以下两种： 由颜色和形状组成的 KeyedStream 由规则组成的普通流 在程序运行时需要将这两种流 JOIN 起来，输出符合特定规则的统计结果。 实现 官网示例程序实现要求如下： 输入形状和颜色的数据流。 输入由名称，前一个数据的形状，最新数据的形状构成的数据流。 寻找到颜色相同且形状符合规则的数据。 样例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.state.MapState;import org.apache.flink.api.common.state.MapStateDescriptor;import org.apache.flink.api.common.typeinfo.BasicTypeInfo;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.typeutils.ListTypeInfo;import org.apache.flink.streaming.api.datastream.BroadcastStream;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.co.KeyedBroadcastProcessFunction;import org.apache.flink.util.Collector;import java.util.ArrayList;import java.util.Collection;import java.util.List;import java.util.Map;enum Shape &#123; SQUARE, CIRCLE, TRIANGLE;&#125;class Item &#123; Shape shape; String color; Item(Shape shape, String color) &#123; this.shape = shape; this.color = color; &#125; public Shape getShape() &#123; return shape; &#125; public String getColor() &#123; return color; &#125; @Override public String toString() &#123; return &quot;&#123;&#x27;shape&#x27;:&#x27;&quot; + this.shape.toString() + &quot;&#x27;,&#x27;color&#x27;: &#x27;&quot; + this.color + &quot;&#x27;&#125;&quot;; &#125;&#125;class Rule &#123; String name; Shape first; Shape second; Rule(String name, Shape first, Shape second) &#123; this.name = name; this.first = first; this.second = second; &#125; @Override public String toString() &#123; return &quot;&#123;&#x27;name&#x27;:&#x27;&quot; + this.name + &quot;&#x27;,&#x27;first&#x27;: &#x27;&quot; + this.first + &quot;&#x27;,&#x27;second&#x27;:&#x27;&quot; + this.second + &quot;&#x27;&#125;&quot;; &#125;&#125;public class WQTest &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); /* Source */ Collection&lt;String&gt; originCollection = new ArrayList&lt;&gt;(); originCollection.add(&quot;SQUARE BLUE&quot;); originCollection.add(&quot;SQUARE BLUE&quot;); originCollection.add(&quot;CIRCLE BLUE&quot;); originCollection.add(&quot;CIRCLE RED&quot;); originCollection.add(&quot;CIRCLE BLUE&quot;); Collection&lt;String&gt; ruleCollection = new ArrayList&lt;&gt;(); ruleCollection.add(&quot;1 SQUARE CIRCLE&quot;); /* Item Stream */ DataStream&lt;String&gt; originStream = env.fromCollection(originCollection); DataStream&lt;Item&gt; itemStream = originStream.map(new MapFunction&lt;String, Item&gt;() &#123; @Override public Item map(String s) throws Exception &#123; String[] content = s.split(&quot; &quot;); return new Item(Shape.valueOf(content[0]), content[1]); &#125; &#125;); KeyedStream&lt;Item, String&gt; colorPartitionedStream = itemStream.keyBy(new KeySelector&lt;Item, String&gt;() &#123; @Override public String getKey(Item item) throws Exception &#123; return item.getColor(); &#125; &#125;); /* Rule Stream */ DataStream&lt;Rule&gt; ruleStream = env.fromCollection(ruleCollection).map(new MapFunction&lt;String, Rule&gt;() &#123; @Override public Rule map(String s) throws Exception &#123; String[] content = s.split(&quot; &quot;); return new Rule(content[0], Shape.valueOf(content[1]), Shape.valueOf(content[2])); &#125; &#125;); MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = new MapStateDescriptor&lt;&gt;( &quot;RulesBroadcastState&quot;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint&lt;Rule&gt;() &#123; &#125;)); BroadcastStream&lt;Rule&gt; ruleBroadcastStream = ruleStream.broadcast(ruleStateDescriptor); /* Broadcast Stream Process */ DataStream&lt;String&gt; output = colorPartitionedStream.connect(ruleBroadcastStream).process( new KeyedBroadcastProcessFunction&lt;String, Item, Rule, String&gt;() &#123; private final MapStateDescriptor&lt;String, List&lt;Item&gt;&gt; mapStateDesc = new MapStateDescriptor&lt;&gt;( &quot;items&quot;, BasicTypeInfo.STRING_TYPE_INFO, new ListTypeInfo&lt;&gt;(Item.class)); private final MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = new MapStateDescriptor&lt;&gt;( &quot;RulesBroadcastState&quot;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint&lt;Rule&gt;() &#123; &#125;)); @Override public void processBroadcastElement(Rule value, Context ctx, Collector&lt;String&gt; out) throws Exception &#123; ctx.getBroadcastState(ruleStateDescriptor).put(value.name, value); &#125; @Override public void processElement(Item value, ReadOnlyContext ctx, Collector&lt;String&gt; out) throws Exception &#123; final MapState&lt;String, List&lt;Item&gt;&gt; state = getRuntimeContext().getMapState(mapStateDesc); final Shape shape = value.getShape(); for (Map.Entry&lt;String, Rule&gt; entry : ctx.getBroadcastState(ruleStateDescriptor).immutableEntries()) &#123; final String ruleName = entry.getKey(); final Rule rule = entry.getValue(); List&lt;Item&gt; stored = state.get(ruleName); if (stored == null) &#123; stored = new ArrayList&lt;&gt;(); &#125; if (shape == rule.second &amp;&amp; !stored.isEmpty()) &#123; for (Item i : stored) &#123; out.collect(&quot;MATCH: &quot; + i + &quot; - &quot; + value); &#125; stored.clear(); &#125; if (shape.equals(rule.first)) &#123; stored.add(value); &#125; if (stored.isEmpty()) &#123; state.remove(ruleName); &#125; else &#123; state.put(ruleName, stored); &#125; &#125; &#125; &#125; ); output.print(); env.execute(); &#125;&#125; 注意事项 不能跨任务通信：用户必须确保所有任务对每个传入元素都以相同的方式修改 Broadcast State 的内容。否则，不同的任务可能会有不同的内容，导致结果不一致。 事件顺序因任务而异：尽管广播流的元素可以保证所有元素(最终)都会到达所有下游任务，但元素可能以不同的顺序到达每个任务。因此，每个传入元素的状态更新不得依赖于传入事件的顺序。 所有任务都会检查其 Broadcast State：这是一个设计决策，以避免在还原期间从同一文件读取所有任务(从而避免热点)，尽管它的代价是将检查点状态的大小增加了 p 倍(并行度)。Flink 保证在恢复/重新缩放时不会有重复和丢失数据。在以相同或更小的并行度进行恢复的情况下，每个任务都会读取其检查点状态。扩大规模后，每个任务读取自己的状态，其余任务(p_new-p_old) 以循环方式读取先前任务的检查点。 不适用 RocksDB State Backend：Broadcast State 在运行时保存在内存中，并且应该相应地进行内存配置。 参考资料 官方文档","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink 容错","slug":"flink/fault-tolerance","date":"2022-02-21T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_fault_tolerance/","permalink":"https://wangqian0306.github.io/2022/flink_fault_tolerance/","excerpt":"","text":"Flink 容错 概念 在 Flink 中的关于容错的专业术语及解释如下： Snapshot(快照) 这是一个通用术语，指的是 Flink 作业状态的全局一致性镜像。快照包括指向每个数据源的指针(例如，文件或 Kafka 分区的偏移量)，以及来自每个作业的有状态操作符的状态副本，这些保存的内容中含有之前已经处理的结果和处理条目的指针。 Checkpoint(检查点) Flink 为能够从故障中恢复而自动拍摄的快照。检查点可以是增量的，并针对快速恢复进行了优化。 Externalized Checkpoint(外部检查点) 通常检查点不需要用户操作。Flink 在作业运行时仅保留最近的 n 个检查点，并在作业取消时删除它们。但是您可以将它们配置为保留，在这种情况下，您可以手动从它们恢复。 Savepoint(保存点) 由用户处于某种目的触发的快照。使用场景可能是重新部署/升级/缩放操作。保存点始终是完整的，并且针对灵活性进行了优化。 在 Flink 中当检查点协调器(作业管理器的一部分)指示任务管理器开始检查点时，它会拿到所有的元数据的偏移量(offset) 并将编号后的保存点隔离屏障插入数据流中。 这些屏障(barriers)贯穿整个处理图，通过这一特点可以来判断每段流位相对于检查点的位置。 如上图所示检查点 N 将包含每个操作的 State，而这些 State 是由屏障 N 之前的每个事件运算生成的，并且在此屏障之后没有任何事件。 当作业图中的每个操作收到这些屏障之一时，它会记录其状态。具有两个输入流(例如 CoProcessFunction)的运算符执行屏障对齐，以便快照将反映从两个输入流中消费事件所产生的状态，直到(但不超过)第二个屏障到来。 Flink 的状态后端使用写时复制机制来允许流处理继续畅通无阻，同时对旧版本的状态进行异步快照。只有当快照被持久保存时，这些旧版本的 State 才会被垃圾回收机制收集。 关键元素介绍 Checkpoint Flink 定期对每个算子的所有状态进行持久化快照，并将这些快照复制到更持久的地方(例如分布式文件系统)。如果发生故障，Flink 可以恢复应用程序的完整状态并恢复处理程序，就好像没有出现任何问题一样。快照的存储位置就是通过 Checkpoint 机制进行指定的。 存储位置及优势如下： FileSystemCheckpointStorage(分布式文件系统存储) 支持大量的 State 很高的可靠性 建议在生产环境中使用 JobManagerCheckpointStorage(JobManager JVM Heap 中存储) 建议少量 State 的情况下在测试和实验环境中使用 Savepoint Savepoint 是流作业执行状态的一致镜像，通过 Flink 的检查点机制创建。您可以使用 Savepoint 来停止和恢复、fork 或更新您的 Flink 作业。 保存点由两部分组成：在稳定存储(例如 HDFS、S3 等)上包含(通常较大的)二进制文件的目录和(相对较小的)元数据文件。稳定存储上的文件代表作业执行状态镜像的数据。保存点的元数据文件(主要)包含指向稳定存储上所有文件的指针，这些文件以相对路径的形式作为保存点的一部分。 使用 Checkpoint 存储 ： 可以在全局配置文件中声明 Checkpoint 的存储位置: 1state.checkpoints.dir: hdfs:///checkpoints/ 也可以在代码中配置： 1env.getCheckpointConfig().setCheckpointStorage(&quot;hdfs:///checkpoints-data/&quot;); 配置 : 关于检查点建立模式的配置如下： EXACTLY_ONCE 先缓冲等到下一个 barrier 到来之后进行处理。 AT_LEAST_ONCE 来就处理 注: 其他详细配置参见 官方文档 恢复 : 直接重启即可 Savepoint 创建 : 手动在页面当中创建 Savepoint 使用命令行创建运行中程序的 Savepoint 1flink savepoint [OPTIONS] &lt;Job ID&gt; [&lt;target directory&gt;] 使用命令行创建保存点然后停止程序 1flink stop --savepointPath [:targetDirectory] :jobId 使用 : 从 Savepoint 恢复运行 1flink run -s :savepointPath -n [:runArgs] 参考资料 容错 官方文档 Checkpoint 官方文档 Savepoint 官方文档","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink State","slug":"flink/state","date":"2022-02-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_state/","permalink":"https://wangqian0306.github.io/2022/flink_state/","excerpt":"","text":"Flink State 简介 State 是指流计算过程中计算节点的中间计算结果或元数据属性，比如在 aggregation 过程中要在 state 中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的 offset，这些 State 数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据(计算数据和元数据属性)的快照。 Flink 内部按照算子和数据分组角度将 State 划分为如下两类： KeyedState 这里面的 key 是我们在 SQL 语句中对应的 GroupBy/PartitionBy 里面的字段，key 的值就是 GroupBy/PartitionBy 字段组成的 Row 的字节数组，每一个 key 都有一个属于自己的 State，key 与 key 之间的 State 是不可见的； OperatorState Flink 内部的 Source Connector 的实现中就会用 OperatorState 来记录 source 数据读取的 offset。 OperatorState 的作用范围限定为单个任务，由同一并行任务所处理的所有数据都可以访问到相同的 State OperatorState 对于同一子任务而言是共享的 OperatorState 不能由相同或不同算子的另一个子任务访问 自带数据存储后端有如下两种： HashMapStateBackend EmbeddedRocksDBStateBackend 如果没有其他配置，系统将使用 HashMapStateBackend。 通常情况下应该采用 HashMapStateBackend 仅在处理大量 State，超大窗口及大量键值对 State 时应当选择 HashMapStateBackend。 配置 使用 HashMapStateBackend 可以进行如下配置 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(new HashMapStateBackend()); 如果使用 EmbeddedRocksDBStateBackend 则需要额外引入如下包： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt; &lt;version&gt;1.14.3&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(new EmbeddedRocksDBStateBackend()); 样例 KeyedState 在经过 keyBy 操作之后就可以获得 KeyedStream 对象，针对 KeyedStream 对象可以使用如下 State： ValueState :储存单个值的 State MapState&lt;UK,UV&gt; :储存 Map 形式的 State ListState :储存 List 形式的 State ReducingState :只存储一个元素，而不是一个列表。它的运行方式是将新元素通过 add(value: T) 加入后，与已有的状态元素使用 ReduceFunction 合并为一个元素，并更新至 State。 AggregatingState&lt;IN,OUT&gt; :运行方式与 ReducingState 类似，但是可以指定不同的输入输出类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; // access the state value Tuple2&lt;Long, Long&gt; currentSum = sum.value(); // update the count currentSum.f0 += 1; // add the second field of the input value currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; @Override public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( &quot;average&quot;, // the state name TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;) // type information sum = getRuntimeContext().getState(descriptor); if (sum == null)&#123; sum = Tuple2.of(0L, 0L); &#125; &#125;&#125;// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(value -&gt; value.f0) .flatMap(new CountWindowAverage()) .print(); 此外还可以为 State 设置 TTL 123456789101112import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;(&quot;text state&quot;, String.class);stateDescriptor.enableTimeToLive(ttlConfig); OperatorState 通常无须使用 OperatorState。它主要是一种特殊类型的状态，主要用在对接数据源或没有可以对状态进行分区的键的场景。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;, CheckpointedFunction &#123; private final int threshold; private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState; private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements; public BufferingSink(int threshold) &#123; this.threshold = threshold; this.bufferedElements = new ArrayList&lt;&gt;(); &#125; @Override public void invoke(Tuple2&lt;String, Integer&gt; value, Context contex) throws Exception &#123; bufferedElements.add(value); if (bufferedElements.size() &gt;= threshold) &#123; for (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; checkpointedState.clear(); for (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123; checkpointedState.add(element); &#125; &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( &quot;buffered-elements&quot;, TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;)); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) &#123; for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123; bufferedElements.add(element); &#125; &#125; &#125;&#125; 参考资料 state 官方文档 state_backends 官方文档 Apache Flink 漫谈系列(04) - State Flink状态管理详解：Keyed State和Operator List State深度解析","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink Window","slug":"flink/window","date":"2022-02-14T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_window/","permalink":"https://wangqian0306.github.io/2022/flink_window/","excerpt":"","text":"Flink Window 简介 Window 即窗口操作把一个数据集切分为有限的数据片以便于聚合处理。当面对无边界的数据时，有些操作需要窗口(以定义大多数聚合操作需要的边界：汇总，外链接，以时间区域定义的操作；如最近 5 分钟 xx等)。 另一些则不需要(如过滤，映射，内链接等)。对有边界的数据，窗口是可选的，不过很多情况下仍然是一种有效的语义概念(如回填一大批的更新数据到之前读取无边界数据源处理过的数据，译者注：类似于 Lambda 架构)。 窗口基本上都是基于时间的；不过也有些系统支持基于记录数的窗口。这种窗口可以认为是基于一个逻辑上的时间域，该时间域中的元素包含顺序递增的逻辑时间戳。 窗口可以是对齐的，也就是说窗口应用于所有落在窗口时间范围内的数据。也可以是非对齐的，也就是应用于部分特定的数据子集(如按某个键值筛选的数据子集)。窗口可以分为如下的几种类型，详情如图所示： 固定窗口(Fixed) (有时叫滚动窗口 Tumbling)是按固定窗口大小定义的，比如说小时窗口或天窗口。它们一般是对齐窗口，也就是说，每个窗口都包含了对应时间段范围内的所有数据。有时为了把窗口计算的负荷均匀分摊到整个时间范围内，有时固定窗口会做成把窗口的边界的时间加上一个随机数，这样的固定窗口则变成了不对齐窗口。 滑动窗口(Sliding) 按窗口大小和滑动周期大小来定义，比如说小时窗口，每一分钟滑动一次。这个滑动周期一般比窗口大小小，也就是说窗口有相互重合之处。滑动窗口一般也是对齐的；尽管上面的图为了画出滑动的效果窗口没有遮盖到所有的键，但其实五个滑动窗口其实是包含了所有的3个键，而不仅仅是窗口3包含了所有的3个键。固定窗口可以看做是滑动窗口的一个特例，即窗口大小和滑动周期大小相等。 会话窗口(Sessions) 是在数据的子集上捕捉一段时间内的活动。一般来说会话按超时时间来定义，任何发生在超时时间以内的事件认为属于同一个会话。会话是非对齐窗口。如上图，窗口 2 只包含 key1，窗口 3 则只包含 key2。而窗口 1 和 4 都包含了 key3。 而在 Flink 中的窗口可以进行如下的划分： 键控窗口(Keyed Window) 非键控窗口(Non-Keyed Window) 窗口分类 键控窗口 123456789stream .keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness(...)] &lt;- optional: &quot;lateness&quot; (else zero) [.sideOutputLateData(...)] &lt;- optional: &quot;output tag&quot; (else no side output for late data) .reduce/aggregate/apply() &lt;- required: &quot;function&quot; [.getSideOutput(...)] &lt;- optional: &quot;output tag&quot; 非键控窗口 12345678stream .windowAll(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness(...)] &lt;- optional: &quot;lateness&quot; (else zero) [.sideOutputLateData(...)] &lt;- optional: &quot;output tag&quot; (else no side output for late data) .reduce/aggregate/apply() &lt;- required: &quot;function&quot; [.getSideOutput(...)] &lt;- optional: &quot;output tag&quot; 注：[] 代表此操作为可选，建议采用键控窗口否则并行度会降低。 例程 滚动窗口 12345678910111213141516171819DataStream&lt;T&gt; input = ...;// tumbling event-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// tumbling processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 滑动窗口 12345678910111213141516171819DataStream&lt;T&gt; input = ...;// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); 会话窗口 1234567891011121314151617181920212223242526272829DataStream&lt;T&gt; input = ...;// event-time session windows with static gapinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); // event-time session windows with dynamic gapinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123; // determine and return session gap &#125;)) .&lt;windowed transformation&gt;(&lt;window function&gt;);// processing-time session windows with static gapinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); // processing-time session windows with dynamic gapinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withDynamicGap((element) -&gt; &#123; // determine and return session gap &#125;)) .&lt;windowed transformation&gt;(&lt;window function&gt;); 全局窗口 123456DataStream&lt;T&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;); 注：此窗口方案仅在您还指定自定义触发器时才有效。 Window Function 在定义了窗口分配器之后，我们需要指定我们想要在每个窗口上执行的计算被称为 Window Function 即窗口函数。 窗口函数分为以下几种： ReduceFunction AggregateFunction ProcessWindowFunction 其中的前两种可以更有效地执行。 ReduceFunction 合并相同 key 的内容 12345678910DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) &#123; return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1); &#125; &#125;); AggregateFunction 1234567891011121314151617181920212223242526272829private static class AverageAggregate implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0L, 0L); &#125; @Override public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); &#125; @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return ((double) accumulator.f0) / accumulator.f1; &#125; @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125;&#125;DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate()); ProcessWindowFunction ProcessWindowFunction 获得一个包含窗口所有元素的可迭代对象，以及一个可以访问时间和状态信息的 Context 对象，这使得它能够提供比其他窗口函数更大的灵活性。这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口被认为准备好处理。 12345678910111213141516171819DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(t -&gt; t.f0) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction());public class MyProcessWindowFunction extends ProcessWindowFunction&lt;Tuple2&lt;String, Long&gt;, String, String, TimeWindow&gt; &#123; @Override public void process(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) &#123; long count = 0; for (Tuple2&lt;String, Long&gt; in: input) &#123; count++; &#125; out.collect(&quot;Window: &quot; + context.window() + &quot;count: &quot; + count); &#125;&#125; 具有增量聚合的 ProcessWindowFunction ProcessWindowFunction 可以与 ReduceFunction 或 AggregateFunction 组合 注：此处暂略，参见官方文档。 参考资料 官方文档","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink Watermark","slug":"flink/watermark","date":"2022-02-10T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_watermark/","permalink":"https://wangqian0306.github.io/2022/flink_watermark/","excerpt":"","text":"Flink Watermark 简介 Flink 中有三种不同的时间概念： 事件时间(event time)：EventTime 是事件在设备上产生时候携带的。在进入框架之前 EventTime 通常要嵌入到记录中，并且 EventTime 也可以从记录中提取出来。 摄取时间(ingestion time)：IngestionTime 是数据进入框架的时间，是在 Source Operator 中设置的。与 ProcessingTime 相比可以提供更可预测的结果，因为 IngestionTime 的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于 ProcessingTime 同一数据在流经不同窗口算子会有不同的处理时间戳。 处理时间(processing time)：数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中 ProcessingTime 具有不确定性，相同数据流多次运行有可能产生不同的计算结果。 比如我们需要统计某天中的某个小时的股票最高价格时就会选择采用事件时间的机制进行聚合操作。 但是我们发现输入系统中的数据不是按序的，某些数据由于某种原因(如：网络原因，外部存储自身原因)产生了一些延迟。 此时 Watermark 机制就可以处理上述问题，Flink 将使用它来跟踪事件时间的进度。 实现方式 假设需要对数据进行排序，数据输入方式如下： 14 2 7 11 9 15 12 13 17 14 21 24 22 19 23 ... 首先拿到的数据是 4 但是并不能将其作为已排序流的第一个元素释放，因为它可能并不是起点。 等待必须是有限的，加入获取到 2 之后如果持续等待则可能等不到 1。 现在你需要一个策略在给定时间戳之后将数据进行输出，并防止更早的数据损害之前输出的结果。 策略需要是可变的，将时间戳的间隔设短则可以经常拿到结果，设置的长则可以获得更精准的结果。 在 Flink 中 Watermark 实际上也是一种时间戳，并且这个时间戳会被 Source 或者自定义的 Watermark 生成器按照特定的方式编码为系统 Event，与普通的数据流 Event 一起流转至下游，而在下游的算子则会以此不断调整自己管理的 EventTime clock。 Flink 框架保证 Watermark 单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该 Watermark 的时间戳的数据元素到来了，所以 Watermark 可以看做是告诉 Flink 框架数据流已经处理到什么位置(时间维度)的方式。 Watermark 的产生和 Flink 内部处理逻辑如下图所示: 注：Watermark 的数据模型为毫秒级时间戳。 产生方式 使用 WatermarkStrategy 我们可以在应用程序中选择 Watermark 的生成位置： 在数据源上生成 在非源操作上生成 通常来说第一种方式更好，因为它允许 Source 利用 Watermark 逻辑中的分片/分区/拆分的相应信息。 在流中可以使用如下代码添加 WatermarkStrategy 12DataStream&lt;String&gt; stream = ...stream.assignTimestampsAndWatermarks(WatermarkStrategy&lt;T&gt; watermarkStrategy) 这种方式获取一个流并生成一个带有时间戳元素和水印的新流。如果原始流已经具有时间戳和/或 Watermark，则时间戳分配器会覆盖它们。 但是如果在一段时间内没有新的 Event 就会形成空闲输入或空闲源。在这种情况下 Watermark 会失效。需要采用如下代码： 123WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)); 编写 WatermarkGenerators 略 使用内置的 WatermarkGenerators 单调递增 1WatermarkStrategy.forMonotonousTimestamps(); 固定延迟量 1WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); 参考资料 Streaming Analytics Apache Flink 漫谈系列(03) - Watermark Flink 的 Watermark 机制 The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing 流计算精品翻译: The Dataflow Model","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink Sink","slug":"flink/sink","date":"2022-02-09T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_sink/","permalink":"https://wangqian0306.github.io/2022/flink_sink/","excerpt":"","text":"Flink Sink 简介 Sink 即 Flink 输出数据的方式。因为 Flink 弃用了 DataSet API 所以本文只针对 DataStream API 进行说明。 DataStream 类提供了如下的输出格式: 基于 Socket writeToSocket() 将数据流写入 Socket addSink() 调用自定义接收器功能。Flink 与其他系统(例如 Apache Kafka)的连接器捆绑在一起，这些连接器被实现为接收器功能。 在生产情况下建议采用 StreamingFileSink 来进行输出。 StreamingFileSink StreamingFileSink 支持逐行和批量编码格式，例如 Apache Parquet。这两个变体带有各自的构建器，可以使用以下静态方法创建： 行编码：StreamingFileSink.forRowFormat(basePath, rowEncoder) 批量编码：StreamingFileSink.forBulkFormat(basePath, bulkWriterFactory) 行编码格式 行编码格式需要指定一个 Encoder 用于将各个行序列化为 OutputStream。 除了存储桶分配器之外，还RowFormatBuilder允许用户指定： 自定义 RollingPolicy：滚动策略以覆盖 DefaultRollingPolicy bucketCheckInterval (default = 1 min): 检查基于时间的滚动策略的毫秒间隔 样例如下： 1234567891011121314151617181920212223242526272829/* 指定source */DataStream&lt;String&gt; stream = .../* 自定义滚动策略 */DefaultRollingPolicy&lt;String, String&gt; rollPolicy = DefaultRollingPolicy.builder() .withRolloverInterval(TimeUnit.MINUTES.toMillis(2)) /* 每隔多长时间生成一个文件 */ .withInactivityInterval(TimeUnit.MINUTES.toMillis(5)) /* 过去 5 分钟(默认是 60 秒)内没有收到新记录就滚动生成新文件 */ .withMaxPartSize(128 * 1024 * 1024) /* 设置每个文件的最大大小(默认是 128 M) */ .build();/*输出文件的前、后缀配置*/OutputFileConfig config = OutputFileConfig .builder() .withPartPrefix(&quot;prefix&quot;) .withPartSuffix(&quot;.txt&quot;) .build();StreamingFileSink&lt;String&gt; streamingFileSink = StreamingFileSink /*forRowFormat指定文件的跟目录与文件写入编码方式，这里使用SimpleStringEncoder 以UTF-8字符串编码方式写入文件*/ .forRowFormat(new Path(&quot;hdfs://localhost:8020/cache&quot;), new SimpleStringEncoder&lt;String&gt;(&quot;UTF-8&quot;)) /*这里是采用默认的分桶策略DateTimeBucketAssigner，它基于时间的分配器，每小时产生一个桶，格式如下yyyy-MM-dd--HH*/ .withBucketAssigner(new DateTimeBucketAssigner&lt;&gt;()) /*设置上面指定的滚动策略*/ .withRollingPolicy(rollPolicy) /*桶检查间隔，这里设置为1s*/ .withBucketCheckInterval(1) /*指定输出文件的前、后缀*/ .withOutputFileConfig(config) .build();/*指定sink*/stream.addSink(streamingFileSink); 批量编码格式 批量编码格式指定了一个 BulkWriter.Factory 由BulkWriter 逻辑定义了如何添加和刷新元素，以及如何最终确定一批记录以进行进一步的编码。 Flink 提供了如下 BulkWriter Factory: ParquetWriterFactory AvroWriterFactory SequenceFileWriterFactory CompressWriterFactory OrcBulkWriterFactory 注：Bulk Formats 只能为 OnCheckpointRollingPolicy 在每个检查点生成。 样例如下(Hadoop SequenceFile)： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-sequence-file&lt;/artifactId&gt; &lt;version&gt;1.13.5&lt;/version&gt;&lt;/dependency&gt; 123456import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;import org.apache.flink.configuration.GlobalConfiguration;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.SequenceFile;import org.apache.hadoop.io.Text; 123456789DataStream&lt;Tuple2&lt;LongWritable, Text&gt;&gt; input = ...;Configuration hadoopConf = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration());final StreamingFileSink&lt;Tuple2&lt;LongWritable, Text&gt;&gt; sink = StreamingFileSink .forBulkFormat( outputBasePath, new SequenceFileWriterFactory&lt;&gt;(hadoopConf, LongWritable.class, Text.class)) .build();input.addSink(sink); 写入 Kafka 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.14.3&lt;/version&gt;&lt;/dependency&gt; 12345678910111213DataStream&lt;String&gt; stream = ... KafkaSink&lt;String&gt; sink = KafkaSink.&lt;String&gt;builder() .setBootstrapServers(brokers) .setRecordSerializer(KafkaRecordSerializationSchema.builder() .setTopic(&quot;topic-name&quot;) .setValueSerializationSchema(new SimpleStringSchema()) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build() ) .build(); stream.sinkTo(sink); 参考资料 Sink StreamingFileSink StreamingFileSink相关特性及代码实战","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"mysqlbinlog","slug":"database/mysqlbinlog","date":"2022-01-20T14:12:59.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2022/mysqlbinlog/","permalink":"https://wangqian0306.github.io/2022/mysqlbinlog/","excerpt":"","text":"mysqlbinlog 简介 MySQL 的二进制日志 binlog 可以说是 MySQL 最重要的日志，它记录了所有的 DDL 和 DML 语句(除了数据查询语句select、show等)，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。 每次 MySQL 启动都会默认生成一个新的 binlog 文件，所以如果在数据库启动过程中出现了问题可以使用原始的 binlog 文件进行数据恢复。 使用 开启 binlog 在 my.cnf 配置文件中填写如下信息： 12server_id=1log-bin= mysql-bin 之后重启集群即可。 查询 binlog 可以在 MySQL 在交互式 mysql 命令行工具 和 mysqlbinlog 命令行工具中进行查询 mysql 命令 命令 说明 SHOW MASTER LOGS; 查看日志文件列表 SHOW MASTER STATUS; 查看 master 状态，即最新的日志文件信息 FLUSH LOGS; 刷新日志，并生成新编号的日志文件 RESET MASTER; 清空日志 SHOW BINLOG EVENTS [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count] \\G 查看日志细节 mysqlbinlog 命令 mysqlbinlog 工具的官方描述是：转储 MySQL 二进制日志，其格式可用于查看或传送到 MySQL 命令行客户端。 常见参数 作用 --database=&lt;db&gt; 显示指定数据库的相关操作 --offset=&lt;offset&gt; 跳过条目数 -s 使用简单格式 --start-datetime=&lt;'yyyy-MM-dd HH:mm:ss'&gt; 指定起始时间 --stop-datetime=&lt;'yyyy-MM-dd HH:mm:ss'&gt; 指定结束时间 --server-id=&lt;id&gt; 显示指定服务器 ID 的数据 注：此工具需要读取 my.cnf 配置，如果跨设备使用需要关注服务器配置。 使用样例如下： 1mysqlbinlog --no-defaults -v -v --base64-output=DECODE-ROWS --database &lt;database&gt; mysql-bin.&lt;file&gt; 注：建议采用上述的解析方式并将解析内容存储至文件中进行检索。 使用 binlog 恢复数据 1mysqlbinlog &lt;bin_path&gt; | mysql -u &lt;user&gt; -p -v 参考资料 MySQL 官方文档 MySQL Binlog 介绍","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"}]},{"title":"Flink DataSource","slug":"flink/source","date":"2022-01-18T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_data_source/","permalink":"https://wangqian0306.github.io/2022/flink_data_source/","excerpt":"","text":"Flink DataSource 简介 DataSource 即 Flink 读取数据的来源。因为 Flink 弃用了 DataSet API 所以本文只针对 DataStream API 进行说明。 在 DataStream API 中可以使用 StreamExecutionEnvironment.addSource(sourceFunction) 将 Data Source 附加到程序中。 StreamExecutionEnvironment 可以访问几个预定义的 DataSource： 基于文件： readTextFile(path) 读取文本文件 readFile(fileInputFormat, path) 根据指定的文件输入格式读取文件 readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) 这是前两个内部调用的方法。它根据给定的 fileInputFormat 读取路径中的文件。根据提供的 watchType，此 DataSource 可以定期监视(每间隔 ms)新数据的路径(FileProcessingMode.PROCESS_CONTINUOUSLY)，或者处理当前路径中的数据并退出(FileProcessingMode.PROCESS_ONCE)。使用 pathFilter，用户可以进一步排除正在处理的文件。 注： 在底层，Flink 将文件读取过程拆分为两个子任务，即目录监控和数据读取。这些子任务中的每一个都由一个单独的实体实现。监控由单个非并行(并行度 = 1)的任务实现，而读取由并行运行的多个任务执行。后者的并行度等于作业并行度。单个监控任务的作用是扫描目录(定期或只扫描一次，视 watchType 情况而定)，找到要处理的文件，将它们进行分割，并将这些分片分配给下游阅读器。读取端是那些将读取实际数据的人。每个分片仅由一个读取端接收，而读取端可以一个接一个地读取多个分片。 值得注意的是： (1) 如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则在修改文件时，将完全重新处理其内容。这可能会破坏 “exactly-once” 语义，因为在文件末尾附加数据将导致其所有内容被重新处理。 (2) 如果 watchType 设置为FileProcessingMode.PROCESS_ONCE，则源扫描路径一次并退出，而不等待读者完成文件内容的读取。当然，读者将继续阅读，直到所有文件内容都被读取。关闭源会导致在该点之后不再有检查点。这可能会导致节点故障后恢复速度较慢，因为作业将从最后一个检查点继续读取。 基于 Socket： socketTextStream 从 Socket 读取。元素可以用分隔符分隔。 基于集合： fromCollection(Collection) 集合中的所有元素必须属于同一类型。 fromCollection(Iterator, Class) 从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。 fromElements(T …) 从给定的对象序列创建数据流。所有对象必须属于同一类型。 fromParallelCollection(SplittableIterator, Class) 从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。 generateSequence(from, to) 并行生成给定区间内的数字序列。 自定义 addSource 附加一个新的 DataSource 函数。(例如各种 connector ) 官方 connector 列表 类型 名称 source sink 捆绑 Apache Kafka √ √ 捆绑 Apache Cassandra √ 捆绑 Apache Kinesis Streams √ √ 捆绑 Elasticsearch √ 捆绑 FileSystem(Hadoop included)-Streaming only sink √ 捆绑 FileSystem(Hadoop included)-Streaming and Batch sink √ 捆绑 RabbitMQ √ √ 捆绑 Google PubSub √ √ 捆绑 Hibrid Source √ 捆绑 Apache NiFi √ √ 捆绑 Apache Pulsar √ 捆绑 Twitter Streaming API √ 捆绑 JDBC √ Apache Bahir Apache ActiveMQ √ √ Apache Bahir Apache Flume √ Apache Bahir Redis √ Apache Bahir Akka √ Apache Bahir Netty √ √ 例程 读取文本文件 1DataStream&lt;String&gt; stream = env.readTextFile(&quot;file:///path/to/file or hdfs://host:port/file&quot;); 读取 Socket 1DataStream&lt;String&gt; stream = env.socketTextStream(&quot;localhost&quot;, 6666); 读取集合 123Collection&lt;String&gt; collection = new ArrayList&lt;&gt;();collection.add(&quot;hello world&quot;);DataStream&lt;String&gt; stream = env.fromCollection(collection); 读取 Kafka 12345678KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder() .setBootstrapServers(&quot;host:port&quot;) .setTopics(&quot;input-topic&quot;) .setGroupId(&quot;my-group&quot;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .build();DataStream&lt;String&gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), &quot;Kafka Source&quot;); 参考资料 官方文档(1.14 版本)","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Flink 执行环境","slug":"flink/init","date":"2022-01-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2022/flink_init/","permalink":"https://wangqian0306.github.io/2022/flink_init/","excerpt":"","text":"Flink 执行环境 简介 在 Flink 架构中有两个核心 API：用于处理有限数据集(通常称为批处理)的 DataSet API，以及用于处理潜在无界数据流(通常称为流处理)的 DataStream API。 DataStream API 在 Flink 源码中是这样描述： StreamExecutionEnvironment 是执行流程序的上下文。 LocalStreamEnvironment 将导致在当前 JVM 中执行，RemoteStreamEnvironment 将导致在远程设置上执行。 该环境提供了控制作业执行(例如设置并行度或容错/检查点参数)以及与外部世界交互(访问数据)的方法。 相关类如下： LocalStreamEnvironment 本地模式执行 RemoteStreamEnvironment 提交到远程集群执行 StreamPlanEnvironment 执行计划 在编写流处理程序时可以使用如下代码创建执行环境： 1StreamExecutionEnvironment.getExecutionEnvironment(); 注：由于 DataSet API 被弃用，所以在使用 DataStream API 运行批处理时需要额外配置运行参数 execution.runtime-mode=BATCH (为了保持灵活建议在运行时指定此参数)。 使用 DataStream API 程序由以下几部分构成： 获取执行环境 载入或创建初始化数据 声明数据的转化方式 声明数据的存储位置 触发程序执行 Table API Table API 是构建在 Stream API 和 DataSet API 之上的 API。该 API 的核心概念是 Table 用作查询的输入和输出。 在 Flink 源码中是这样描述： TabelEnvironment 是用于创建表和 SQL API 程序的基类、入口和核心上下文。 在编写流处理程序时可以使用如下代码创建执行环境： 1234567EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build();TableEnvironment tEnv = TableEnvironment.create(settings); 或者可以从 StreamExecutionEnvironment 进行创建： 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); DataSet API 注：DataSet API 在 Flink 1.12 之后已经被 软弃用，不建议使用此 API。 在 Flink 源码中是这样描述： ExecutionEnvironment 是执行程序的上下文。LocalEnvironment 将导致在当前 JVM 中执行，RemoteEnvironment 将导致在远程设置上执行。 该环境提供了控制作业执行(例如设置并行度)和与外界交互(访问数据)的方法。 请注意，执行环境需要强类型信息，用于所有执行的操作的输入和返回类型。这意味着环境需要知道操作的返回值是例如字符串和整数的元组。因为 Java 编译器丢弃了大部分泛型类型信息，所以大多数方法都尝试使用反射重新获取该信息。在某些情况下，可能需要手动将该信息提供给某些方法。 相关类如下： LocalEnvironment 本地模式执行 RemoteEnvironment 提交到远程集群执行 CollectionEnvironment 集合数据集模式执行 OptimizerPlanEnvironment 不执行作业，仅创建优化的计划 PreviewPlanEnvironment 提取预先优化的执行计划 在编写批处理程序时可以使用如下代码创建执行环境： 1ExecutionEnvironment.getExecutionEnvironment(); 参考资料 Flink 源码 Flink Environment 概览","categories":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"}]},{"title":"Ray casting 算法","slug":"algorithm/ray-casting","date":"2022-01-12T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2022/ray-casting/","permalink":"https://wangqian0306.github.io/2022/ray-casting/","excerpt":"","text":"Ray casting 算法 简介 光线投射算法是基于图像序列的直接体绘制算法。从图像的每一个像素，沿固定方向（通常是视线方向）发射一条光线，光线穿越整个图像序列，并在这个过程中，对图像序列进行采样获取颜色信息，同时依据光线吸收模型将颜色值进行累加，直至光线穿越整个图像序列，最后得到的颜色值就是渲染图像的颜色。 简单实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import sysfrom collections import namedtuplePt = namedtuple(&#x27;Pt&#x27;, &#x27;x, y&#x27;) # PointEdge = namedtuple(&#x27;Edge&#x27;, &#x27;a, b&#x27;) # Polygon edge from a to bPolygon = namedtuple(&#x27;Polygon&#x27;, &#x27;name, edges&#x27;) # Polygon_eps = 0.00001_huge = sys.float_info.max_tiny = sys.float_info.mindef ray_intersect_seg(p, edge): &quot;&quot;&quot; takes a point p=Pt() and an edge of two endpoints a,b=Pt() of a line segment returns boolean &quot;&quot;&quot; a, b = edge if a.y &gt; b.y: a, b = b, a if p.y == a.y or p.y == b.y: p = Pt(p.x, p.y + _eps) if (p.y &gt; b.y or p.y &lt; a.y) or ( p.x &gt; max(a.x, b.x)): return False if p.x &lt; min(a.x, b.x): intersect = True else: if abs(a.x - b.x) &gt; _tiny: m_red = (b.y - a.y) / float(b.x - a.x) else: m_red = _huge if abs(a.x - p.x) &gt; _tiny: m_blue = (p.y - a.y) / float(p.x - a.x) else: m_blue = _huge intersect = m_blue &gt;= m_red return intersectdef _odd(x): return x % 2 == 1def is_point_inside(p, poly_list): ln = len(poly_list) return _odd(sum(ray_intersect_seg(p, edge) for edge in poly_list.edges))def poly_print(polygon): print(&quot;\\n Polygon(name=&#x27;%s&#x27;, edges=(&quot; % polygon.name) print(&#x27; &#x27;, &#x27;,\\n &#x27;.join(str(e) for e in polygon.edges) + &#x27;\\n ))&#x27;)if __name__ == &#x27;__main__&#x27;: polys = [ Polygon(name=&#x27;square&#x27;, edges=( Edge(a=Pt(x=0, y=0), b=Pt(x=10, y=0)), Edge(a=Pt(x=10, y=0), b=Pt(x=10, y=10)), Edge(a=Pt(x=10, y=10), b=Pt(x=0, y=10)), Edge(a=Pt(x=0, y=10), b=Pt(x=0, y=0)) )), Polygon(name=&#x27;square_hole&#x27;, edges=( Edge(a=Pt(x=0, y=0), b=Pt(x=10, y=0)), Edge(a=Pt(x=10, y=0), b=Pt(x=10, y=10)), Edge(a=Pt(x=10, y=10), b=Pt(x=0, y=10)), Edge(a=Pt(x=0, y=10), b=Pt(x=0, y=0)), Edge(a=Pt(x=2.5, y=2.5), b=Pt(x=7.5, y=2.5)), Edge(a=Pt(x=7.5, y=2.5), b=Pt(x=7.5, y=7.5)), Edge(a=Pt(x=7.5, y=7.5), b=Pt(x=2.5, y=7.5)), Edge(a=Pt(x=2.5, y=7.5), b=Pt(x=2.5, y=2.5)) )), Polygon(name=&#x27;strange&#x27;, edges=( Edge(a=Pt(x=0, y=0), b=Pt(x=2.5, y=2.5)), Edge(a=Pt(x=2.5, y=2.5), b=Pt(x=0, y=10)), Edge(a=Pt(x=0, y=10), b=Pt(x=2.5, y=7.5)), Edge(a=Pt(x=2.5, y=7.5), b=Pt(x=7.5, y=7.5)), Edge(a=Pt(x=7.5, y=7.5), b=Pt(x=10, y=10)), Edge(a=Pt(x=10, y=10), b=Pt(x=10, y=0)), Edge(a=Pt(x=10, y=0), b=Pt(x=2.5, y=2.5)) )), Polygon(name=&#x27;exagon&#x27;, edges=( Edge(a=Pt(x=3, y=0), b=Pt(x=7, y=0)), Edge(a=Pt(x=7, y=0), b=Pt(x=10, y=5)), Edge(a=Pt(x=10, y=5), b=Pt(x=7, y=10)), Edge(a=Pt(x=7, y=10), b=Pt(x=3, y=10)), Edge(a=Pt(x=3, y=10), b=Pt(x=0, y=5)), Edge(a=Pt(x=0, y=5), b=Pt(x=3, y=0)) )), ] test_points = (Pt(x=5, y=5), Pt(x=5, y=8), Pt(x=-10, y=5), Pt(x=0, y=5), Pt(x=10, y=5), Pt(x=8, y=5), Pt(x=10, y=10)) print(&quot;\\n TESTING WHETHER POINTS ARE WITHIN POLYGONS&quot;) for poly in polys: poly_print(poly) print(&#x27; &#x27;, &#x27;\\t&#x27;.join(&quot;%s: %s&quot; % (p, is_point_inside(p, poly)) for p in test_points[:3])) print(&#x27; &#x27;, &#x27;\\t&#x27;.join(&quot;%s: %s&quot; % (p, is_point_inside(p, poly)) for p in test_points[3:6])) print(&#x27; &#x27;, &#x27;\\t&#x27;.join(&quot;%s: %s&quot; % (p, is_point_inside(p, poly)) for p in test_points[6:])) 参考资料 Ray casting Algorithm","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Impala: A Modern, Open-Source SQL Engine for Hadoop 中文翻译","slug":"treatise/impala_a_modern_open_source_sql_engine_for_hadoop","date":"2022-01-12T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2022/impala_a_modern_open_source_sql_engine_for_hadoop/","permalink":"https://wangqian0306.github.io/2022/impala_a_modern_open_source_sql_engine_for_hadoop/","excerpt":"","text":"Impala: A Modern, Open-Source SQL Engine for Hadoop 作者：Marcel Kornacker, Alexander Behm, Victor Bittorf, Taras Bobrovytsky, Casey Ching, Alan Choi, Justin Erickson, Martin Grund, Daniel Hecht, Matthew Jacobs, Ishaan Joshi, Lenni Kuff, Dileep Kumar, Alex Leblang, Nong Li, Ippokratis Pandis, Henry Robinson, David Rorke, Silvius Rus, John Russell, Dimitris Tsirogiannis, Skye Wanderman-Milne, Michael Yoder 版权说明 1This article is published under a Creative Commons Attribution License(http://creativecommons.org/licenses/by/3.0/), which permits distribution and reproduction in any medium as well as allowing derivative works, provided that you attribute the original work to the author(s) and CIDR 2015. 7th Biennial Conference on Innovative Data Systems Research (CIDR’15) January 4-7, 2015, Asilomar, California, USA 简介 Cloudera Impala 是一个现代的、开源的 MPP SQL 引擎，专为 Hadoop 数据处理环境而设计。 Impala 为 Hadoop 上的 BI/分析以读取为主的查询提供低延迟和高并发，而不是由 Apache Hive 等批处理框架提供。 本文从用户的角度介绍 Impala，概述其架构和主要组件，并简要展示其与其他流行的 SQL-on-Hadoop 系统相比的卓越性能。 1 引言 Impala 是一个开源的、完全集成的、先进的 MPP SQL 查询引擎，专为利用 Hadoop 的灵活性和可扩展性而设计。 Impala 的目标是将熟悉的 SQL 支持和传统分析数据库的多用户性能与 Apache Hadoop 的可扩展性和灵活性以及 Cloudera Enterprise 的生产级安全和管理扩展相结合。 Impala 的 beta 版本于 2012 年 10 月发布，并于 2013 年 5 月 GA 发布。最新版本 Impala 2.0 于 2014 年 10 月发布。Impala 的生态系统势头继续加速，自 GA 以来下载量接近 100 万次。 与其他系统(通常是 Postgres 的分支)不同，Impala 是一个全新的引擎，使用 C++ 和 Java 从头开始编写。 它通过使用标准组件(HDFS、HBase、Metastore、YARN、Sentry)来保持 Hadoop 的灵活性，并且能够读取大多数广泛使用的文件格式(例如 Parquet、Avro、RCFile)。 为了减少延迟，例如使用 MapReduce 或远程读取数据所产生的延迟，Impala 实现了一个基于守护进程的分布式架构，这些进程负责查询执行的所有方面，并且与 Hadoop 基础架构的其余部分在同一台机器上运行。 结果是性能相当或超过商业 MPP 分析 DBMS，具体取决于特定的工作负载。 本文讨论 Impala 为用户提供的服务，然后概述其架构和主要组件。 当今可实现的最高性能需要使用 HDFS 作为底层存储管理器，因此这是本文的重点；当某些技术方面如何与 HBase 一起处理方面存在显着差异时，我们会在文本中说明而不会深入细节。 Impala 是性能最高的 SQL-on-Hadoop 系统，尤其是在多用户工作负载下。 如第 7 节所示，对于单用户查询，Impala 比替代方案快 13 倍，平均快 6.7 倍。 对于多用户查询，差距越来越大：Impala 比替代方案快 27.4 倍，平均快 18 倍——多用户查询的平均速度是单用户查询的近三倍。 本文的其余部分结构如下：下一节从用户的角度对 Impala 进行概述，并指出它与传统 RDBMS 的不同之处。 第 3 节介绍了系统的整体架构。第 4 节介绍了前端组件，其中包括基于成本的分布式查询优化器，第 5 节介绍了负责查询执行并使用运行时代码生成的后端组件，第 6 节介绍了资源/工作负载管理组件。 7 节简要评估了 Impala 的性能。第 8 节讨论了未来的路线图，第 9 节得出结论。 2 IMPALA 的用户视图 Impala 是一个集成到 Hadoop 环境中的查询引擎，它利用许多标准 Hadoop 组件(Metastore、HDFS、HBase、YARN、Sentry)来提供类似 RDBMS 的体验。 但是，本节的其余部分将提出一些重要的区别。 Impala 专门针对与标准商业智能环境的集成，并为此支持大多数相关的行业标准：客户端可以通过 ODBC 或 JDBC 进行连接；使用 Kerberos 或 LDAP 完成身份验证；授权遵循标准的 SQL 角色和权限。 为了查询 HDFS 驻留的数据，用户通过熟悉的 CREATE TABLE 语句创建表，该语句除了提供数据的逻辑模式外，还指示物理布局，例如文件格式和在 HDFS 目录结构。 然后可以使用标准 SQL 语法查询这些表。 2.1 物理结构设计 创建表时，用户还可以指定一些分区列： 1CREATE TABLE T (...) PARTITIONED BY (day int, month int) LOCATION ’&lt;hdfs-path&gt;’ STORED AS PARQUET; 对于未分区的表，数据文件默认直接存储在根目录中&lt;3&gt;。对于分区表，数据文件放置在其路径反映分区列值的子目录中。 例如，对于表 T 的第 2 个月第 17 天，所有数据文件都将位于目录 &lt;root&gt;/day=17/month=2/ 中。 请注意，这种形式的分区并不意味着单个分区的数据的搭配：一个分区的数据文件块随机分布在 HDFS 数据节点上。 注 3：但是，位于根目录下的任何目录中的所有数据文件都是表数据集的一部分。这是处理未分区表的常用方法，Apache Hive 也采用了这种方法。 在选择文件格式时，Impala 还为用户提供了很大的灵活性。 它目前支持压缩和未压缩的文本文件、序列文件(文本文件的可拆分形式)、RCFile(传统的列格式)、Avro(二进制行格式)和 Parquet，这是最高性能的存储选项(第 5.3 节会更详细的要论文件类型)。 如上例，用户在 CREATE TABLE 或 ALTER TABLE 语句中指明存储格式。也可以为每个分区单独选择单独的格式。 例如，可以使用以下命令专门将特定分区的文件格式设置为 Parquet：ALTER TABLE PARTITION(day=17, month=2) SET FILEFORMAT PARQUET。 作为一个有用的例子，考虑一个按时间顺序记录数据的表，例如点击日志。当天的数据可能以 CSV 文件的形式出现，并在每天结束时批量转换为 Parquet。 2.2 SQL 支持 Impala 支持大多数 SQL-92 SELECT 语句语法，以及附加的 SQL-2003 分析函数，以及大多数标准标量数据类型：整型和浮点型、STRING、CHAR、VARCHAR、TIMESTAMP 和精度高达 38 位的 DECIMAL。 自定义应用程序逻辑可以通过 Java 和 C++ 中的用户定义函数 (UDF) 以及当前仅在 C++ 中的用户定义聚合函数 (UDA) 进行合并。 由于 HDFS 作为存储管理器的限制，Impala 不支持 UPDATE 或 DELETE，本质上只支持批量插入(INSERT INTO … SELECT …) &lt;4&gt;。 与传统 RDBMS 不同，用户只需使用 HDFS 的 API 将数据文件复制/移动到该表的目录位置，即可将数据添加到表中。 或者，同样可以使用 LOAD DATA 语句来完成。 注 4：我们还应该注意 Impala 支持 VALUES 子句。但是，对于 HDFS 支持的表，这将在每个 INSERT 语句生成一个文件，这会导致大多数应用程序的性能非常差。对于 HBase 支持的表，VALUES 变体通过 HBase API 执行单行插入。 与批量插入类似，Impala 通过删除表分区(ALTER TABLE DROP PARTITION)支持批量数据删除。因为不可能就地更新 HDFS 文件，所以 Impala 不支持 UPDATE 语句。 相反，用户通常会重新计算部分数据集以合并更新，然后替换相应的数据文件，通常是通过删除和重新添加分区。 在初始数据加载之后，或者每当表的大部分数据发生变化时，用户应该运行 COMPUTE STATS &lt;table&gt; 语句，该语句指示 Impala 收集表的统计信息。这些统计信息随后将在查询优化期间使用。 3 架构 Impala 是一个大规模并行查询执行引擎，它运行在现有 Hadoop 集群中的数百台机器上。 它与底层存储引擎分离，与传统的关系数据库管理系统不同，后者的查询处理和底层存储引擎是单个紧密耦合系统的组件。Impala 的高级架构如图 1 所示。 Impala 部署由三个服务组成。Impala 守护进程 (impalad) 服务双重负责接受来自客户端进程的查询并协调它们在集群中的执行，以及代表其他 Impala 守护进程执行单个查询片段。 当 Impala 守护进程通过管理查询执行以第一个角色运行时，它被称为该查询的协调者。然而，所有 Impala 守护进程都是对称的；他们可能都扮演所有角色。此属性有助于容错和负载平衡。 一个 Impala 守护进程部署在集群中的每台机器上，这些机器也运行一个 datanode 进程——底层 HDFS 部署的块服务器——因此每台机器上通常都有一个 Impala 守护进程。 这使 Impala 可以利用数据局部性，并从文件系统中读取块，而无需使用网络。 Statestore 守护进程(statestored)是 Impala 的元数据发布订阅服务，它将集群范围的元数据传播到所有 Impala 进程。 有一个单独的 statestored 实例，在下面的 3.1 节中有更详细的描述。 最后，3.2 节中描述的目录守护进程（catalogd）用作 Impala 的目录存储库和元数据访问网关。 通过 catalogd，Impala 守护程序可以执行反映在外部目录存储（例如 Hive Metastore）中的 DDL 命令。对系统目录的更改通过 statestore 广播。 所有这些 Impala 服务以及几个配置选项，例如资源池的大小、可用内存等。 (有关资源和工作负载管理的更多详细信息，请参阅第 6 节)也暴露给 Cloudera Manager，这是一个复杂的 集群管理应用。 Cloudera Manager 不仅可以管理 Impala，还可以管理几乎所有服务，以全面了解 Hadoop 部署。 3.1 状态分发 旨在在数百个节点上运行的 MPP 数据库设计中的一个主要挑战是集群范围元数据的协调和同步。 Impala 的对称节点架构要求所有节点都必须能够接受和执行查询。因此，所有节点都必须拥有最新版本的系统目录和 Impala 集群成员的最新视图，以便可以正确安排查询。 我们可以通过部署一个单独的集群管理服务来解决这个问题，其中包含所有集群范围元数据的真实版本。 Impala 守护进程可以延迟查询这个存储(即仅在需要时)，这将确保所有查询都得到最新的响应。 然而，Impala 设计的一个基本原则是在任何查询的关键路径上尽可能避免同步 RPC。 在没有密切关注这些成本的情况下，我们发现查询延迟通常会因建立 TCP 连接或加载某些远程服务所花费的时间而受到影响。 相反，我们将 Impala 设计为向所有相关方推送更新，并设计了一个名为 statestore 的简单发布-订阅服务来将元数据更改传播给一组订阅者。 statestore 维护一组主题，它们是(键、值、版本)三元组的数组，称为条目，其中“键”和“值”是字节数组，“版本”是 64 位整数。 主题由应用程序定义，因此 statestore 不了解任何主题条目的内容。主题在 statestore 的整个生命周期中都是持久的，但不会在服务重新启动时持久。 希望接收任何主题更新的进程称为订阅者，并通过在启动时向 statestore 注册并提供主题列表来表达他们的兴趣。 statestore 通过向订阅者发送每个已注册主题的初始主题更新来响应注册，其中包含当前在该主题中的所有条目。 注册后，statestore 会定期向每个订阅者发送两种消息。第一种消息是主题更新，包括自上次更新成功发送给订阅者以来对主题的所有更改(新条目、修改的条目和删除)。 每个订阅者都维护一个每个主题的最新版本标识符，它允许 statestore 仅在更新之间发送增量。作为对主题更新的响应，每个订阅者都会发送一份希望对其订阅主题进行更改的列表。 保证在收到下一次更新时已应用这些更改。 第二种 statestore 消息是 keepalive。statestore 使用 keepalive 消息来维护与每个订阅者的连接，否则订阅会超时并尝试重新注册。 以前版本的 statestore 将主题更新消息用于这两个目的，但随着主题更新规模的增长，很难确保及时向每个订阅者传递更新，从而导致订阅者的故障检测过程中出现误报。 如果 statestore 检测到失败的订阅者(例如，通过重复失败的 keepalive 交付)，它将停止发送更新。 一些主题条目可能被标记为“临时”，这意味着如果他们的“拥有”订阅者失败，他们将被删除。 这是一个自然的原语，用于在专用主题中维护集群的活跃度信息，以及每个节点的负载统计信息。 statestore 提供了非常弱的语义：订阅者可能以不同的速率更新(尽管 statestore 尝试公平地分发主题更新)，因此可能对主题内容有非常不同的看法。 但是，Impala 仅使用主题元数据在本地做出决策，而无需跨集群进行任何协调。 例如，查询计划是基于目录元数据主题在单个节点上执行的，一旦计算出完整的计划，执行该计划所需的所有信息都会直接分发给执行节点。 执行节点不需要知道目录元数据主题的相同版本。 尽管现有 Impala 部署中只有一个 statestore 进程，但我们发现它可以很好地扩展到中型集群，并且通过一些配置，可以服务于我们最大的部署。 statestore 不会将任何元数据保存到磁盘：所有当前元数据都由实时订阅者推送到 statestore(例如加载信息)。 因此，如果 statestore 重新启动，它的状态可以在初始订阅者注册阶段恢复。 或者，如果运行 statestore 的机器发生故障，则可以在其他地方启动新的 statestore 进程，并且订阅者可能会故障转移到它。 Impala 中没有内置的故障转移机制，而是部署通常使用可重定向的 DNS 条目来强制订阅者自动移动到新的流程实例。 3.2 目录服务 Impala 的目录服务通过 statestore 广播机制为 Impala 守护进程提供目录元数据，并代表 Impala 守护进程执行 DDL 操作。 目录服务从第三方元数据存储(例如，Hive Metastore 或 HDFS Namenode)中提取信息，并将该信息聚合到 Impala 兼容的目录结构中。 这种架构允许 Impala 对其所依赖的存储引擎的元数据存储相对不可知，这允许我们相对快速地将新的元数据存储添加到 Impala(例如 HBase 支持)。 对系统目录的任何更改(例如，当加载新表时)都会通过 statestore 传播。 目录服务还允许我们使用特定于 Impala 的信息来扩充系统目录。 例如，我们仅向目录服务注册用户定义的函数(例如，不将其复制到 Hive Metastore)，因为它们特定于 Impala。 由于目录通常非常大，而且对表的访问很少是统一的，因此目录服务只会为它在启动时发现的每个表加载一个框架条目。 更详细的表元数据可以从其第三方商店在后台延迟加载。如果在完全加载之前需要一个表，Impala 守护程序将检测到这一点并向目录服务发出优先级请求。此请求会阻塞，直到表完全加载。 4 前端 Impala 前端负责将 SQL 文本编译成 Impala 后端可执行的查询计划。它是用 Java 编写的，由功能齐全的 SQL 解析器和基于成本的查询优化器组成，所有这些都是从头开始实现的。 除了基本的 SQL 功能(select、project、join、group by、order by、limit)之外，Impala 还支持内联视图、不相关和相关子查询(被重写为连接)、外连接的所有变体以及显式左/右 半链接和反连接，以及分析窗口函数。 查询编译过程遵循传统的分工：查询解析、语义分析和查询规划/优化。我们将专注于查询编译中最具挑战性的后者部分。 Impala 查询计划器作为输入提供一个解析树以及在语义分析期间组装的查询全局信息(表/列标识符、等价类等)。 可执行查询计划的构建分为两个阶段：（1）单节点计划和（2）计划并行化和分片。 第一阶段，解析树被翻译成一个不可执行的单节点计划树，由以下计划节点组成：HDFS/HBase scan、hash join、cross join、union、hash aggregation、sort、top-n、分析评估。 此步骤负责在可能的最低损耗的节点分配谓词、基于等价类推断谓词、修剪表分区、设置限制/偏移量、应用列投影，以及执行一些基于成本的计划优化，例如排序和合并分析 窗口函数和连接重新排序以最小化总评估成本。 成本估算基于表/分区基数加上每列的不同值计数&lt;6&gt;；直方图目前不是统计数据的一部分。Impala 使用简单的启发式方法来避免在常见情况下详尽地枚举和计算整个连接顺序空间。 注 6：我们使用 HyperLogLog 算法 [5] 进行不同的估值。 第二阶段将单节点计划作为输入，并产生一个分布式执行计划。总体目标是最小化数据移动并最大化扫描局部性：在 HDFS 中，远程读取比本地读取慢得多。 通过根据需要在计划节点之间添加交换节点，并通过添加额外的非交换计划节点来最小化跨网络的数据移动(例如，本地聚合节点)，从而使计划成为分布式的。 在第二阶段，我们决定每个连接节点的连接策略(此时连接顺序是固定的)。支持的连接策略是广播和分区的。 前者将连接的整个构建端复制到所有执行探测的集群机器上，后者在连接表达式上重新分配构建端和探测端。 Impala 选择估计的任何策略以最小化通过网络交换的数据量，同时利用连接输入的现有数据分区。 所有聚合当前都作为本地预聚合执行，然后合并聚合操作。对于分组聚合，预聚合输出在分组表达式上进行分区，合并聚合在所有参与节点上并行完成。 对于非分组聚合，合并聚合在单个节点上完成。sort 和 top-n 以类似的方式并行化：分布式本地排序/top-n 之后是单节点合并操作。 分析表达式评估是基于 partition-by 表达式并行化的。它依赖于根据 partition-by/order-by 表达式对输入进行排序。 最后，分布式计划树在交换边界处被拆分。计划的每个这样的部分都放置在计划片段中，即 Impala 的后端执行单元。 计划片段封装了计划树的一部分，该部分在单台机器上的相同数据分区上运行。 图 2 举例说明了查询计划的两个阶段。该图的左侧显示了一个查询的单节点计划，该查询连接了两个 HDFS 表(t1，t2)和一个 HBase 表(t3)，然后是一个聚合和 order by with limit(top-n)。 右侧显示了分散的、分散的计划。圆角矩形表示片段边界和箭头数据交换。表 t1 和 t2 通过分区策略连接。 扫描位于它们自己的片段中，因为它们的结果会立即交换给消费者(连接节点)，消费者(连接节点)在基于散列的数据分区上运行，而表数据是随机分区的。 以下与 t3 的连接是广播连接，与 t1 和 t2 之间的连接放置在同一片段中，因为广播连接保留了现有的数据分区(连接 t1、t2 和 t3 的结果仍然是基于连接键的哈希分区 t1 和 t2)。 在连接之后，我们执行两阶段分布式聚合，其中预聚合在与最后一个连接相同的片段中计算。预聚合结果基于分组键进行哈希交换，然后再次聚合以计算最终聚合结果。 相同的两阶段方法应用于 top-n，最后的 top-n 步骤在协调器处执行，协调器将结果返回给用户。 5 后端 Impala 的后端从前端接收查询片段并负责它们的快速执行。它旨在充分利用现代硬件。 后端是用 C++ 编写的，并在运行时使用代码生成来生成有效的代码路径(相对于指令数)和较小的内存开销，尤其是与用 Java 实现的其他引擎相比。 Impala 利用了数十年来对并行数据库的研究。执行模型是具有 Exchange 运算符的传统 Volcano 样式 [7]。处理是分批执行的：每个 GetNext() 调用对成批的行进行操作，类似于 [10]。 除了“走走停停(stop-and-go)”操作符(例如排序)外，执行是完全可流水线的，这最大限度地减少了存储中间结果的内存消耗。 可能需要消耗大量内存的操作员被设计为能够在需要时将部分工作集溢写到磁盘。可溢出的运算符是散列连接、(基于散列的)聚合、排序和分析函数的评估。 Impala 对散列连接和聚合运算符采用分区方法。也就是说，每个元组的哈希值的一些位确定目标分区，其余位用于哈希表探测。 在正常操作期间，当所有哈希表都适合内存时，分区步骤的开销最小，在不可溢出的非基于分区的实现的性能的 10% 以内。 当存在内存压力时，“受害者”分区可能会溢出到磁盘，从而释放内存以供其他分区完成处理。 当为哈希连接构建哈希表并且构建端关系的基数减少时，我们构建了一个布隆过滤器，然后将其传递给探测端扫描器，实现半连接的简单版本。 5.1 运行时代码生成 使用 LLVM [8] 生成运行时代码是 Impala 后端广泛采用的用于缩短执行时间的技术之一。典型工作负载通常会获得 5 倍或更多的性能提升。 LLVM 是一个编译器库和相关工具的集合。与作为独立应用程序实现的传统编译器不同，LLVM 被设计为模块化和可重用的。 它允许像 Impala 这样的应用程序在正在运行的进程中执行即时(JIT)编译，具有现代优化器的全部优势以及为多种架构生成机器代码的能力，通过为编译的所有步骤公开单独的 API 过程。 Impala 使用运行时代码生成来生成对性能至关重要的特定查询版本的函数。特别是，代码生成应用于“内循环”函数，即在给定查询中多次执行(对于每个元组)的函数，因此构成了查询执行总时间的很大一部分。 例如，用于将数据文件中的记录解析为 Impala 内存中元组格式的函数必须为扫描的每个数据文件中的每条记录调用。对于扫描大型表的查询，这可能是数十亿或更多的记录。 因此，该函数必须非常高效才能获得良好的查询性能，即使从函数执行中删除一些指令也会导致查询速度大幅提升。 如果没有代码生成，函数执行时为了处理程序编译时未知的运行时信息而导致低效率几乎总是必定的。 例如，仅处理整数类型的记录解析函数在解析仅整数文件时将比处理其他数据类型(如字符串和浮点数)的函数更快。 但是，要扫描的文件的模式在编译时是未知的，因此必须使用通用函数，即使在运行时已知更有限的功能就足够了。 大量运行时的开销都来源于虚函数。虚函数调用会导致很大的性能损失，特别是当被调用函数非常简单时，因为调用不能内联。 如果对象实例的类型在运行时已知，我们可以使用代码生成将虚函数调用替换为直接调用正确的函数，然后可以内联。 这在评估表达式树时特别有价值。在 Impala 中(就像在许多系统中一样)，表达式由单个运算符和函数的树组成，如图 3 的左侧所示。 可以出现在树中的每种类型的表达式都是通过重写表达式基类中的虚函数来实现的，该虚函数递归地调用其子表达式。其中许多表达式函数非常简单，例如，将两个数字相加。 因此，调用虚函数的成本往往远远超过实际评估函数的成本。如图 3 所示，通过使用代码生成解析虚函数调用，然后内联生成的函数调用，可以直接评估表达式树，而无需函数调用开销。 此外，内联函数增加了指令级并行性，并允许编译器进行进一步的优化，例如跨表达式消除子表达式。 总的来说，JIT 编译的效果类似于自定义编码查询。例如，它消除了分支、展开循环、传播常量、偏移量和指针、内联函数。代码生成对性能有显着影响，如图 4 所示。 例如，在一个 10 节点集群中，每个节点有 8 个内核、48GB RAM 和 12 个磁盘，我们测量了运行时代码生成造成的影响。 我们正在使用比例因子为 100 的 Avro TPC-H 数据库，并运行简单的聚合查询。代码生成将执行速度提高了 5.7 倍，速度随着查询复杂度的增加而增加。 5.2 I/O 管理 高效地从 HDFS 检索数据是所有 SQL-on-Hadoop 系统的挑战。为了以或接近硬件速度从磁盘和内存执行数据扫描，Impala 使用称为短路本地读取 [3] 的 HDFS 功能在从本地磁盘读取时绕过 DataNode 协议。 Impala 几乎可以以磁盘带宽(每个磁盘大约 100MB/s)进行读取，并且通常能够使所有可用磁盘饱和。我们测量了 12 个磁盘，Impala 能够维持 1.2GB/秒的 I/O。 此外，HDFS 缓存 [2] 允许 Impala 以内存总线速度访问内存驻留数据，并且还节省了 CPU 周期，因为不需要复制数据块和/或校验它们。 从/向存储设备读取/写入数据是 I/O 管理器组件的职责。 I/O 管理器为每个物理磁盘分配固定数量的工作线程(每个旋转磁盘一个线程，每个 SSD 八个)，为客户端提供异步接口(例如扫描程序线程)。 Impala 的 I/O 管理器的有效性最近得到了 [6] 的证实，这表明 Impala 的读取吞吐量比其他测试系统高 4 倍到 8 倍。 5.3 存储格式 Impala 支持最流行的文件格式：Avro、RC、Sequence、纯文本和 Parquet。这些格式可以与不同的压缩算法结合使用，例如 snappy、gzip、bz2。 在大多数用例中，我们建议使用 Apache Parquet，这是一种最先进的开源列式文件格式，可提供高压缩和高扫描效率。 它由 Twitter 和 Cloudera 在 Criteo、Stripe、Berkeley AMPlab 和 LinkedIn 的贡献下共同开发。 除了 Impala，大多数基于 Hadoop 的处理框架，包括 Hive、Pig、MapReduce 和 Cascading 都能够处理 Parquet。 简单地说，Parquet 是一种可定制的类 PAX [1] 格式，针对大型数据块(数十、数百、数千兆字节)进行了优化，并内置了对嵌套数据的支持。 受 Dremel 的 ColumnIO 格式 [9] 的启发，Parquet 按列存储嵌套字段，并用最少的信息对其进行扩充，以便在扫描时从列数据中重新组装嵌套结构。 Parquet 具有一组可扩展的列编码。1.2 版支持运行长度和字典编码，2.0 版增加了对增量和优化字符串编码的支持。 最新版本(Parquet 2.0)还实现了嵌入式统计：内联列统计以进一步优化扫描效率，例如最小/最大索引。 如前所述，Parquet 提供高压缩和扫描效率。图 5 (左)比较了比例因子为 1,000 的 TPC-H 数据库的 Lineitem 表在以一些流行的文件格式和压缩算法组合存储时的磁盘大小。 采用 snappy 压缩的 Parquet 实现了其中最好的压缩。同样，图 5 (右)显示了当数据库以纯文本、序列、RC 和 Parquet 格式存储时，来自 TPC-DS 基准的各种查询的 Impala 执行时间。 Parquet 的性能始终优于所有其他格式的 5 倍。 6 资源/负载管理 任何集群框架的主要挑战之一是仔细控制资源消耗。Impala 通常在繁忙的集群环境中运行，其中 MapReduce 任务、摄取作业和定制框架竞争有限的 CPU、内存和网络资源。 困难在于在不影响查询延迟或吞吐量的情况下协调查询之间以及可能在框架之间的资源调度。 Apache YARN [12] 是 Hadoop 集群上资源中介的当前标准，它允许框架共享 CPU 和内存等资源，而无需对集群进行分区。 YARN 有一个集中式架构，其中框架请求 CPU 和内存资源，这些资源由中央资源管理器服务进行仲裁。 这种架构的优点是允许在完全了解集群状态的情况下做出决策，但它也对资源获取造成了很大的延迟。 由于 Impala 以每秒数千次查询的工作负载为目标，我们发现资源请求和响应周期过长。 我们解决这个问题的方法有两个：首先，我们实现了一个互补但独立的准入控制机制，允许用户控制他们的工作负载，而无需昂贵的集中决策。 其次，我们在 Impala 和 YARN 之间设计了一个中介服务，目的是纠正一些阻抗不匹配。 该服务称为 Llama for Low-Latency Application MAster，它实现了资源缓存、组调度和增量分配更改，同时仍将实际调度决策推迟到 YARN 以处理未命中 Llama 缓存的资源请求。 本节的其余部分描述了使用 Impala 进行资源管理的两种方法。我们的长期目标是通过支持准入控制的低延迟决策制定和 YARN 的跨框架支持的单一机制来支持混合工作负载资源管理。 6.1 Llama 和 YARN Llama 是一个独立的守护进程，所有 Impala 守护进程都向其发送每个查询的资源请求。每个资源请求都与一个资源池相关联，该资源池定义了查询可能使用的集群可用资源的公平份额。 如果资源池的资源在 Llama 的资源缓存中可用，Llama 会立即将它们返回给查询。这种快速路径允许 Llama 在资源争用较低时绕过 YARN 的资源分配算法。 否则，Llama 将请求转发给 YARN 的资源管理器，并等待所有资源返回。这与 YARN 的“滴灌”分配模型不同，后者在分配资源时返回资源。 Impala 的流水线执行模型要求所有资源同时可用，以便所有查询片段可以并行进行。 由于查询计划的资源估计，特别是在非常大的数据集上，通常是不准确的，我们允许 Impala 查询在执行期间调整其资源消耗估计。 YARN 不支持这种模式，相反，我们让 Llama 向 YARN 发出新的资源请求(例如，要求每个节点多 1GB 的内存)，然后从 Impala 的角度将它们聚合到单个资源分配中。 这种适配器架构允许 Impala 与 YARN 完全集成，而无需自己承担处理不合适的编程接口的复杂性。 6.2 准入控制 除了与 YARN 集成以进行集群范围的资源管理之外，Impala 还具有内置的准入控制机制来限制传入请求。 请求被分配到资源池，并根据定义每个池的最大并发请求数和请求的最大内存使用量限制的策略来接受、排队或拒绝。 准入控制器被设计为快速和分散的，因此可以准入任何 Impala 守护程序的传入请求，而无需向中央服务器发出同步请求。 做出准入决定所需的状态通过 statestore 在 Impala 守护进程之间传播，因此每个 Impala 守护进程都能够基于其对全局状态的聚合视图做出准入决定，而无需在请求执行路径上进行任何额外的同步通信。 然而，由于共享状态是异步接收的，Impala 守护进程可能会在本地做出导致超出策略指定限制的决策。 实际上，这并没有问题，因为状态通常比非平凡查询更新得更快。此外，准入控制机制主要被设计为一种简单的节流机制，而不是像 YARN 这样的资源管理解决方案。 资源池是分层定义的。传入的请求根据放置策略分配给资源池，并且可以使用 ACL 控制对资源池的访问。 配置使用 YARN 公平调度器分配文件和 Llama 配置指定，Cloudera Manager 提供简单的用户界面来配置资源池，无需重新启动任何正在运行的服务即可对其进行修改。 7 评估 本节的目的不是详尽地评估 Impala 的性能，而主要是给出一些指示。有独立的学术研究得出了类似的结论，例如 [6]。 7.1 搭建实验环境 所有实验都在同一个 21 节点集群上运行。集群中的每个节点都是 2 插槽计算机，具有 2.00GHz 的 6 核 Intel Xeon CPU E5-2630L。 每个节点都有 64GB RAM 和 12 个 932GB 磁盘驱动器(一个用于操作系统，其余用于 HDFS)。 我们在 15TB 比例因子数据集上运行了一个决策支持风格基准测试，该基准测试由 TPC-DS 查询的子集组成。 在下面的结果中，我们根据查询访问的数据量将查询分类为交互式、报告和深度分析查询。 具体来说，交互式存储桶包含查询：q19、q42、q52、q55、q63、q68、q73 和 q98； 报告桶包含查询：q27、q3、q43、q53、q7 和 q89； 深度分析桶包含查询：q34、q46、q59、q79 和 ss max。 我们用于这些测量的套件是公开可用的。 对于我们的比较，我们使用了最流行的 SQL-onHadoop 系统，比如说：Impala、Presto、Shark、SparkSQL 和 Hive 0.13。 由于在除 Impala 之外的所有测试引擎中都缺少基于成本的优化器，我们测试了所有引擎的查询已转换为 SQL-92 样式连接。 为了保持一致性，我们对 Impala 运行了相同的查询，尽管 Impala 在没有这些修改的情况下产生了相同的结果。 每个引擎都根据其表现最佳的文件格式进行评估，同时始终使用 Snappy 压缩来确保公平比较：Apache Parquet 上的 Impala、ORC 上的 Hive 0.13、RCFile 上的 Presto 和 Parquet 上的 SparkSQL。 7.2 单用户性能 图 6 比较了四个系统在单用户运行时的性能，其中单个用户以零思考时间重复提交查询。在所有查询运行的单用户工作负载上，Impala 的性能优于所有替代方案。 Impala 的性能优势从 2.1 倍到 13.0 倍不等，平均快 6.7 倍。实际上，这与早期版本 Impala 的 Hive 0.13(从平均 4.9 倍到 9 倍)和 Presto(从平均 5.3 倍到 7.5 倍)相比，性能优势差距更大。 7.3 多用户性能 Impala 的卓越性能在多用户工作负载中变得更加明显，这在现实世界的应用程序中无处不在。图 7(左) 显示了当有 10 个并发用户从交互式类别提交查询时四个系统的响应时间。 在这种情况下，从单用户到并发用户工作负载时，Impala 的性能优于其他系统，从 6.7 倍到 18.7 倍。 根据比较，加速从 10.6 倍到 27.4 倍不等。请注意，Impala 在 10 用户负载下的速度几乎是单用户负载下的一半——而替代方案的平均值仅为单用户负载下的五分之一。 同样，图 7 (右)比较了四个系统的吞吐量。当 10 个用户从交互式存储桶提交查询时，Impala 的吞吐量比其他系统高 8.7 倍至 22 倍。 7.4 与商业 RDBMS 比较 从以上比较可以看出，Impala 在性能方面在 SQL-on Hadoop 系统中处于领先地位。但 Impala 也适合部署在传统的数据仓库设置中。 在图 8 中，我们将 Impala 的性能与流行的商业柱状分析 DBMS 进行了比较，由于有限制性的专有许可协议，此处称为“DBMS-Y”。 我们使用比例因子为 30,000(30TB 原始数据)的 TPC-DS 数据集，并根据前面段落中介绍的工作负载运行查询。 我们可以看到 Impala 的性能比 DBMS-Y 高出 4.5 倍，平均高出 2 倍，只有三个查询执行得更慢。 8 路线图 在本文中，我们概述了 Cloudera Impala。 尽管 Impala 已经对现代数据管理产生了影响，并且是 SQL-on-Hadoop 系统中的性能领导者，但仍有许多工作要做。 我们的路线图项目大致分为两类：添加更传统的并行 DBMS 技术，这是为了解决越来越多的现有数据仓库工作负载所必需的，以及解决 Hadoop 环境所特有的问题的解决方案。 8.1 额外的 SQL 支持 Impala 对 SQL 的支持在 2.0 版本中已经相当完善了，但是仍然缺少一些标准的语言特性：set MINUS 和 INTERSECT；ROLLUP 和 GROUPING 集；动态分区修剪；日期/时间/日期时间数据类型。 我们计划在下一个版本中添加这些内容。 Impala 目前仅限于平面关系模式，虽然这通常足以满足预先存在的数据仓库工作负载，但我们看到更多使用更新的文件格式，这些格式允许本质上是嵌套关系模式，并添加了复杂的列类型(结构 ，数组，地图)。 Impala 将被扩展为以对嵌套级别或可以在单个查询中处理的嵌套元素的数量没有限制的方式处理这些模式。 8.2 其他性能增强 计划中的性能增强包括连接、聚合和排序的节点内并行化，以及更普遍地使用运行时代码生成来完成诸如网络传输的数据准备、查询输出的具体化等任务。 我们还考虑将在查询处理期间需要物化的数据切换到列式规范内存格式，以便利用 SIMD 指令 [11, 13]。 另一个计划改进的领域是 Impala 的查询优化器。 它探索的计划空间目前被故意限制健壮性/可预测性，部分原因是缺乏复杂的数据统计信息(例如直方图)和额外的模式信息(例如主/外键约束、列的可空性)，这些信息可以实现更准确的成本计算生成计划替代方案。 我们计划在近期内将直方图添加到表/分区元数据中，以纠正其中的一些问题。利用这些额外的元数据并以稳健的方式合并复杂的计划重写是一项具有挑战性的持续任务。 8.3 元数据和统计收集 在 Hadoop 环境中收集元数据和表统计信息很复杂，因为与 RDBMS 不同，新数据可以通过将数据文件移动到表的根目录中简单地显示出来。 目前，用户必须发出命令来重新计算统计数据并更新物理元数据以包含新的数据文件，但这已经证明是有问题的：用户经常忘记发出该命令，或者在确切需要发出该命令时感到困惑。 该问题的解决方案是通过运行后台进程自动检测新数据文件，该进程还更新元数据并安排计算增量表统计信息的查询。 8.4 自动数据转换 允许并排使用多种数据格式的更具挑战性的方面之一是从一种格式转换为另一种格式。数据通常以结构化的面向行的格式(例如 Json、Avro 或 XML)或作为文本添加到系统中。 另一方面，从性能的角度来看，Parquet 等面向列的格式是理想的。在生产环境中，让用户管理从一个到另一个的转换通常是一项不平凡的任务：它本质上需要建立一个可靠的数据管道(识别新数据文件，在转换过程中合并它们等)，这本身就需要大量的工程。 我们正计划增加转换过程的自动化，以便用户可以标记表进行自动转换；转换过程本身依赖于后台元数据和统计数据收集过程，该过程另外安排在新数据文件上运行的转换查询。 8.5 资源管理 开放多租户环境中的资源管理，其中 Impala 与其他处理框架(如 MapReduce、Spark 等)共享集群资源，目前仍是一个未解决的问题。 与 YARN 的现有集成目前并未涵盖所有用例，并且 YARN 专注于拥有具有同步资源预留的单个预留注册表，这使得难以适应低延迟、高吞吐量的工作负载。我们正在积极研究这个问题的新解决方案。 8.6 支持远程数据存储 Impala 目前依靠存储和计算的搭配来实现高性能。然而，像亚马逊的 S3 这样的云数据存储正变得越来越流行。 此外，基于 SAN 的传统存储基础架构需要将计算和存储分离。我们正在积极致力于扩展 Impala 以访问 Amazon S3(计划用于 2.2 版)和基于 SAN 的系统。 除了简单地将本地存储替换为远程存储之外，我们还计划研究允许本地处理而不增加额外操作负担的自动缓存策略。 9 结论 在本文中，我们介绍了 Cloudera Impala，这是一个开源 SQL 引擎，旨在将并行 DBMS 技术引入 Hadoop 环境。 我们的性能结果表明，尽管 Hadoop 起源于批处理环境，但可以在其上构建一个分析 DBMS，其性能与当前的商业解决方案一样好或更好，但同时保留了灵活性和成本效益的 Hadoop。 在目前的状态下，Impala 已经可以取代传统的、单一的分析 RDBMS 来处理许多工作负载。 我们预测，这些系统在 SQL 功能方面的差距将随着时间的推移而消失，并且 Impala 将能够承担越来越多的现有数据仓库工作负载。 然而，我们相信 Hadoop 环境的模块化特性(其中 Impala 利用了跨平台共享的许多标准组件)带来了一些传统的单体 RDBMS 无法复制的优势。 特别是，混合文件格式和处理框架的能力意味着单个系统可以处理更广泛的计算任务，而无需数据移动，这本身通常是组织使用数据做某事的最大障碍之一。 Hadoop 生态系统中的数据管理仍然缺乏过去几十年为商业 RDBMS 开发的一些功能； 尽管如此，我们预计这种差距会迅速缩小，并且开放模块化环境的优势将使其在不久的将来成为主导的数据管理架构。 参考资料 [1] A. Ailamaki, D. J. DeWitt, M. D. Hill, and M. Skounakis. Weaving relations for cache performance. In VLDB,2001. [2] Apache. Centralized cache management in HDFS. Available at https://hadoop.apache.org/docs/r2.3.0/hadoopproject-dist/hadoop-hdfs/CentralizedCacheManagement.html. [3] Apache. HDFS short-circuit local reads. Available at http://hadoop.apache.org/docs/r2.5.1/hadoop-projectdist/hadoop-hdfs/ShortCircuitLocalReads.html. [4] Apache. Sentry. Available at http://sentry.incubator.apache.org/. [5] P. Flajolet, E. Fusy, O. Gandouet, and F. Meunier. HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm. In AOFA, 2007. [6] A. Floratou, U. F. Minhas, and F. Ozcan. SQL-onHadoop: Full circle back to shared-nothing database architectures. PVLDB, 2014. [7] G. Graefe. Encapsulation of parallelism in the Volcano query processing system. In SIGMOD, 1990. [8] C. Lattner and V. Adve. LLVM: A compilation framework for lifelong program analysis &amp; transformation. In CGO, 2004. [9] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton, and T. Vassilakis. Dremel: Interactive analysis of web-scale datasets. PVLDB, 2010. [10] S. Padmanabhan, T. Malkemus, R. C. Agarwal, and A. Jhingran. Block oriented processing of relational database operations in modern computer architectures. In ICDE, 2001. [11] V. Raman, G. Attaluri, R. Barber, N. Chainani, D. Kalmuk, V. KulandaiSamy, J. Leenstra, S. Lightstone, S. Liu, G. M. Lohman, T. Malkemus, R. Mueller, I. Pandis, B. Schiefer, D. Sharpe, R. Sidle, A. Storm, and L. Zhang. DB2 with BLU Acceleration: So much more than just a column store. PVLDB, 6, 2013. [12] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, B. Saha, C. Curino, O. O’Malley, S. Radia, B. Reed, and E. Baldeschwieler. Apache Hadoop YARN: Yet another resource negotiator. In SOCC, 2013. [13] T. Willhalm, N. Popovici, Y. Boshmaf, H. Plattner, A. Zeier, and J. Schaffner. SIMD-scan: ultra fast inmemory table scan using on-chip vector processing units. PVLDB, 2, 2009.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Impala","slug":"Impala","permalink":"https://wangqian0306.github.io/tags/Impala/"}]},{"title":"Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing 中文翻译","slug":"treatise/apache_hive_from_mapreduce_to_enterprise_grade_big_data_warehousing","date":"2022-01-10T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2022/apache_hive_from_mapreduce_to_enterprise_grade_big_data_warehousing/","permalink":"https://wangqian0306.github.io/2022/apache_hive_from_mapreduce_to_enterprise_grade_big_data_warehousing/","excerpt":"","text":"Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing 中文翻译 作者：Jesús Camacho-Rodríguez, Ashutosh Chauhan, Alan Gates, Eugene Koifman, Owen O’Malley, Vineet Garg, Zoltan Haindrich, Sergey Shelukhin, Prasanth Jayachandran, Siddharth Seth, Deepak Jaiswal, Slim Bouguerra, Nishant Bangarwa, Sankar Hariappan, Anishek Agarwal, Jason Dere, Daniel Dai, Thejas Nair, Nita Dembla, Gopal Vijayaraghavan, Günther Hagleitner 摘要 Apache Hive 是一个用于分析大数据工作负载的开源关系型数据库。本文中，我们描述了从批处理工具到成熟的企业数据仓库系统的关键创新。 我们提出了一种混合架构，它将传统的 MPP 技术与最新的大数据和云概念相结合，以实现当今分析应用程序所需的规模和性能。 我们通过沿四个主轴详细说明增强功能来探索系统：事务、优化器、运行时(runtime)和联邦集群(federation)。然后，我们提供实验结果来展示系统在典型工作负载下的性能，并以查看社区路线图作为总结。 1. 简介 10 多年前首次引入 Hive 时 [55]，作者的动机是在 Hadoop MapReduce 之上公开一个类似 SQL 的接口，以将用户从处理并行批处理作业的低级实现细节中抽象出来。 Hive 主要专注于 ExtractTransform-Load (ETL) 或批处理报告工作负载，这些工作负载包括 (i) 读取大量数据，(ii) 对该数据执行转换(例如，数据整理、整合、聚合)以及最后 (iii) 加载(输出)到其他系统用于进一步分析。 随着 Hadoop 成为使用 HDFS 进行廉价数据存储的无处不在的平台，开发人员专注于增加可以在平台内高效执行的工作负载范围。 在 Hadoop 引入 YARN [56] 资源管理框架之后不久 Spark [14,59] 或 Flink [5,26] 等 MapReduce 之外的数据处理引擎也可以在支持 YARN 之后直接在 Hadoop 上运行。 用户也越来越关注将他们的数据仓库工作负载从其他系统迁移到 Hadoop。这些工作负载包括交互式和临时报告、仪表板和其他商业智能用例。 有一个共同要求使这些工作负载对 Hadoop 构成挑战：它们需要低延迟 SQL 引擎。实现这一目标的多项努力是并行启动的，并且出现了与 YARN 兼容的新 SQL MPP 系统，例如 Impala [8, 42] 和 Presto [48]。 Hive 社区没有实施新系统，而是得出结论，该项目的当前实施为支持这些工作负载提供了良好的基础。 Hive 是为 Hadoop 中的大规模可靠计算而设计的，它已经提供了 SQL 兼容性(只有一部分)和与其他数据管理系统的连接。 然而，Hive 需要发展并进行重大改造，采用多年来广泛研究的通用数据仓库技术以满足这些新用例的要求。 以前向研究界展示的关于 Hive 的工作主要集中在 (i) 它在 HDFS 和 MapReduce [55] 之上的初始架构和实现，以及 (ii) 改进以解决原始系统中的多个性能缺点，包括引入优化的列文件格式，物理优化以减少查询计划中 MapReduce 阶段的数量，以及矢量化执行模型以提高运行时效率 [39]。 相反，本文描述了在上一篇文章发表之后在 Hive 中引入的重要创新。特别是，它侧重于沿四个不同主轴改进系统的相关工作： SQL 和 ACID 支持(第 3 节) SQL 合规性是数据仓库的一项关键要求。因此，Hive 中的 SQL 支持得到了扩展，包括相关的子查询、完整性约束和扩展的 OLAP 操作等。 反过来，仓库用户需要支持来按需插入、更新、删除和合并他们的个人记录。Hive 使用构建在 Hive Metastore 之上的事务管理器通过快照隔离策略来提供 ACID 保证。 优化技术(第 4 节) 查询优化与使用声明性查询语言(如 SQL)的数据管理系统尤其相关。Hive 没有从头开始实现自己的优化器，而是选择与 Calcite [3, 19] 集成，并将其优化功能带入系统。 此外，Hive 还包括数据仓库环境中常用的其他优化技术，例如查询重新优化、查询结果缓存和物化视图重写。 运行时的延迟(第 5 节) 为了涵盖更广泛的用例，包括交互式查询，改善延迟至关重要。Hive 支持优化的列数据存储方式和运算符的矢量化在之前的文章中已经描述过了 [39]。 除了这些改进之外，Hive 已经从 MapReduce 转移到 Tez [15, 50]，这是一个与 YARN 兼容的运行时，它比 MapReduce 提供了更多的灵活性来实现任意数据处理应用程序。 此外，Hive 包括 LLAP，这是一个额外的持久性长期运行的执行程序层，可提供数据缓存、设施运行时优化，并避免启动时的 YARN 容器分配开销。 联邦集群性能(第 6 节) Hive 最重要的特性之一是能够在过去几年出现的许多专业数据管理系统之上提供统一的 SQL 层。 由于其 Calcite 集成和存储处理程序的改进，Hive 可以无缝地推送计算并从这些系统读取数据。反过来，该实现很容易扩展以支持将来的其他系统。 本文的其余部分安排如下。第 2 节提供了 Hive 架构和主要组件的背景。第 3-6 节描述了我们对沿上述主轴改进系统的主要贡献。第 7 节介绍了 Hive 的实验评估。 第 8 节讨论了新功能的影响，而第 9 节简要介绍了项目的路线图。最后，第 10 节总结了我们的结论。 2 系统架构 在本节中，我们将简要介绍 Hive 的架构。图 1 描述了系统中的主要组件。 数据存储 Hive 中的数据可以使用任何支持的文件格式存储在与 Hadoop 兼容的任何文件系统中。 截至今天，最常见的文件格式是 ORC [10] 和 Parquet [11]。反过来，兼容的文件系统包括最常用的分布式文件系统实现 HDFS，以及所有主要的商业云对象存储，如 AWS S3 和 Azure Blob 存储。 此外，Hive 还可以读取和写入数据到其他独立处理系统，例如 Druid [4, 58] 或 HBase [6]，我们将在第 6 节中详细讨论。 数据目录 Hive 使用 Hive Metastore(简称 HMS)存储有关其数据源的所有信息。简而言之，HMS 是 Hive 可查询的所有数据的目录。 它使用 RDBMS 来持久化信息，并依赖 Java 对象关系映射实现 DataNucleus [30] 来简化后端对多个 RDBMS 的支持。对于需要低延迟的调用，HMS 可以绕过 DataNucleus 直接查询 RDBMS。 HMS API 支持多种编程语言，该服务使用 Thrift [16] 实现，这是一个提供接口定义语言、代码生成引擎和二进制通信协议实现的软件框架。 可更换的数据处理引擎 Hive 已成为 Hadoop 之上最受欢迎的 SQL 引擎之一，它已逐渐远离 MapReduce，以支持与 YARN [50] 兼容的更灵活的处理运行时。 虽然仍然支持 MapReduce，但目前 Hive 最流行的运行时是 Tez [15, 50]。 Tez 通过将数据处理建模为 DAG 提供比 MapReduce 更大的灵活性，其中顶点表示应用程序逻辑，边表示数据传输，类似于 Dryad [40] 或 Hyracks [22] 等其他系统。 此外，Tez 与第 5 节中介绍的持久执行和缓存层 LLAP 兼容。 查询服务器 HiveServer2(或简称 HS2)允许用户在 Hive 中执行 SQL 查询。HS2 支持本地和远程 JDBC 和 ODBC 连接；Hive 发行版中包括一个名为 Beeline 的 JDBC 简易客户端。 图 2 描述了 SQL 查询在 HS2 中经过的各个阶段，以成为可执行计划。一旦用户向 HS2 提交查询，该查询由驱动程序处理，驱动程序解析语句并从其 AST 生成 Calcite [3, 19] 逻辑计划。 然后优化 Calcite 计划。请注意，HS2 访问有关 HMS 中数据源的信息以进行验证和优化。随后，计划被转换为物理计划，可能会引入额外的操作符用于数据分区、排序等。 HS2 对物理计划 DAG 执行额外的优化，如果支持计划中的所有运算符和表达式，则可以从中生成矢量化计划 [39]。 物理计划被传递给任务编译器，它将操作符树分解为可执行任务的 DAG。Hive 为每个支持的处理运行时实现了一个单独的任务编译器，即 Tez、Spark 和 MapReduce。 生成任务后，驱动程序将它们提交给 YARN 中的运行时应用程序管理器，由其处理执行。对于每个任务，首先初始化该任务中的物理操作符，然后它们以流水线方式处理输入数据。 执行完成后，驱动程序获取查询结果并将其返回给用户。 3 SQL 和 ACID 支持 标准 SQL 和 ACID 事务是企业数据仓库中的关键要求。在本节中，我们将介绍 Hive 更广泛的 SQL 支持。 此外，我们描述了对 Hive 所做的改进，以便在 Hadoop 之上提供 ACID 保证。 3.1 SQL 支持 为了替代传统数据仓库，需要扩展 Hive 以支持标准 SQL 的更多功能。Hive 使用嵌套数据模型，支持所有主要的原子 SQL 数据类型以及非原子类型，例如 STRUCT、ARRAY 和 MAP。 此外，每个新的 Hive 版本都增加了对作为 SQL 规范一部分的重要构造的支持。 例如，扩展了对相关子查询的支持，即引用外部查询列的子查询、高级 OLAP 操作(如分组或窗口函数)、集合操作和完整性约束等。 另一方面，Hive 保留了其原始查询语言的多个功能，这些功能对其用户群很有价值。最流行的功能之一是能够在创建表时使用 PARTITIONED BY 列子句指定物理存储布局。 简而言之，该子句允许用户对表进行水平分区。然后 Hive 将每组分区值的数据存储在文件系统的不同目录中。为了说明这个想法，请考虑下表定义和图 3 中描述的相应物理布局： 1234CREATE TABLE store_sales ( sold_date_sk INT, item_sk INT, customer_sk INT, store_sk INT, quantity INT, list_price DECIMAL(7,2), sales_price DECIMAL(7,2)) PARTITIONED BY (sold_date_sk INT); 使用 PARTITIONED BY 子句的优点是 Hive 将能够轻松跳过扫描完整分区以查找过滤这些值的查询。 3.2 ACID 支持 最初，Hive 仅支持从表中插入和删除完整分区 [55]。尽管对于 ETL 工作负载来说缺乏行级操作是可以接受的，但随着 Hive 演变为支持许多传统的数据仓库工作负载，对完全 DML 支持和 ACID 事务的需求越来越大。 因此，Hive 现在支持执行 INSERT、UPDATE、DELETE 和 MERGE 语句。 它通过 Snapshot Isolation [24] 为读取和定义明确的语义提供 ACID 保证，以防使用构建在 HMS 之上的事务管理器发生故障。 目前事务只能跨越一个语句； 我们计划在不久的将来支持多语句事务。但是，可以使用 Hive 多插入语句 [55] 在单个事务中写入多个表。 在 Hive 中支持行级操作需要克服的主要挑战是 (i) 系统中缺少事务管理器，以及 (ii) 底层文件系统中缺少文件更新支持。在下文中，我们提供了有关在 Hive 中实施 ACID 以及如何解决这些问题的更多详细信息。 事务和锁管理 Hive 在 HMS 中存储事务和锁定信息状态。它为系统中运行的每个事务使用全局事务标识符或 TxnId，即 Metastore 生成的单调递增值。 反过来，每个 TxnId 映射到一个或多个写入标识符或 WriteId。WriteId 也是由 Metastore 生成的单调递增值，但在表范围内。 WriteId 与事务写入的每条记录一起存储；同一个事务写入同一个表的所有记录共享同一个 WriteId。 反过来，共享相同 WriteId 的文件使用 FileId 唯一标识，而文件中的每条记录由 RowId 字段唯一标识。 请注意，WriteId、FileId 和 RowId 的组合唯一标识了表中的每条记录。Hive 中的删除操作被建模为标记记录的插入，该记录指向被删除记录的唯一标识符。 为了实现快照隔离，HS2 在执行查询时获取需要读取的数据的逻辑快照。快照由一个事务列表表示，该列表包含当时分配的最高 TxnId，即高水位线，以及它下面的一组打开和中止的事务。 对于查询需要读取的每张表，HS2 首先通过联系 HMS 从事务列表中生成 WriteId 列表； WriteId 列表类似于事务列表，但在单个表的范围内。 计划中的每个扫描操作在编译期间都绑定到一个 WriteId 列表。该扫描中的读取器将跳过 WriteId (i) 高于高水位线或 (ii) 是打开和中止事务集的一部分的行。 保留全局和每个表标识符的原因是每个表的读取器保持较小的状态，当系统中有大量打开的事务时，这对性能至关重要。 对于分区表，锁定粒度是分区，而对于未分区表，需要锁定全表。HS2 只需要为破坏读写器的操作获取排他锁，例如 DROP PARTITION 或 DROP TABLE 语句。 所有其他常见操作只是获取共享锁。更新和删除通过跟踪其写入集并在提交时解决冲突来使用乐观冲突解决，让第一次提交获胜。 数据和文件布局 Hive 将每个表和分区的数据存储在不同的目录中(回忆图 3)。 与 [45] 类似，我们在每个表或分区中使用不同的存储或目录来支持并发读写操作：base 和 delta，它们又可能包含一个或多个文件。 基本存储中的文件包含直到某个 WriteId 的所有有效记录。例如，文件夹 base_100 包含直到 WriteId 100 的所有记录。另一方面，增量目录包含具有 WriteId 范围内的记录的文件。 Hive 为插入和删除的记录保留单独的增量目录；更新操作分为删除和插入操作。插入或删除事务创建一个增量目录，其中记录绑定到单个 WriteId，例如 delta_101_101 或 delete_delta_102_102。 包含多个 WriteId 的增量目录是作为压缩过程的一部分创建的(如下所述)。 如前所述，查询中的表扫描具有与之关联的 WriteId 列表。扫描中的读取器丢弃完整目录以及基于当前快照无效的单个记录。 当增量文件中存在删除时，基本和插入增量文件中的记录需要与适用于其 WriteId 范围的删除增量反连接。 由于删除记录的增量文件通常很小，因此它们可以大部分时间保存在内存中，从而加速合并阶段。 数据压缩(compaction) 压缩是 Hive 中的过程，它将 delta 目录中的文件与 delta 目录中的其他文件合并(称为次要压缩)，或者将 delta 目录中的文件与基本目录中的文件合并(称为主要压缩)。 定期运行压缩的关键原因是(i) 减少表中目录和文件的数量，否则会影响文件系统性能，(ii)减少读取端在查询执行时合并文件的工作量，以及 (iii) 缩短与每个快照关联的打开和中止 TxnId 和 WriteId 的集合，即主要压缩删除历史记录，增加已知表中所有记录有效的 TxnId。 当超过某些阈值时，HS2 会自动触发压缩，例如，表中的 delta 文件数或 delta 文件中的记录与基本文件的比率。最后，请注意，压缩不需要对表进行任何锁定。 实际上，清理阶段与合并阶段是分开的，因此任何正在进行的查询都可以在文件从系统中删除之前完成其执行。 4 查询优化 虽然在 Hive 的初始版本中对优化的支持是有限的，但显然其执行内部的开发不足以保证高效的性能。因此，目前该项目包括关系数据库系统中通常使用的许多复杂技术。 本节介绍最重要的优化功能，这些功能可帮助系统生成更好的计划并改进查询执行，包括它与 Apache Calcite 的基本集成。 4.1 基于规则和成本的优化器 最初，Apache Hive 在解析输入 SQL 语句时执行了多次重写以提高性能。此外，它包含一个基于规则的优化器，该优化器将简单的转换应用于查询生成的物理计划。 例如，许多优化的目标是尽量减少数据混洗的成本，这是 MapReduce 引擎中的一项关键操作。还有其他优化来下推过滤器谓词、投影未使用的列和修剪分区。 虽然这对某些查询很有效，但使用物理计划表示使得实现复杂的重写(例如连接重新排序、谓词简化和传播，或基于物化视图的重写)变得过于复杂。 因此，引入了由 Apache Calcite [3, 19] 提供支持的新计划表示和优化器。 Calcite 是一个模块化和可扩展的查询优化器，具有内置元素，可以以不同的方式组合以构建您自己的优化逻辑。其中包括不同的重写规则(rewriting rules)、计划者(planner)和成本模型(cost model)。 Calcite 提供了两种不同的规划器引擎：(i) 基于成本的规划器，它触发重写规则以降低整体表达式成本，以及 (ii) 穷举计划器，它穷举地触发规则，直到它生成不再被任何规则修改。转换规则对这两个规划引擎都不起作用。 Hive 实现类似于其他查询优化器 [52] 的多阶段优化，其中每个优化阶段使用一个计划器和一组重写规则。这允许 Hive 通过指导搜索不同的查询计划来减少整体优化时间。 Apache Hive 中启用的一些 Calcite 规则是连接重新排序、多个运算符重新排序和消除、常量折叠和传播以及基于约束的转换。 统计数据 表统计信息存储在 HMS 中，并在计划时提供给 Calcite。这些包括表基数、不同值的数量、每列的最小值和最大值。 存储统计信息以便它们可以以附加方式组合，即未来插入以及跨多个分区的数据可以添加到现有统计信息中。范围和基数可以简单地合并。 对于不同值的数量，HMS 使用基于 HyperLogLog++ [38] 的位数组表示，可以在不损失近似精度的情况下进行组合。 4.2 查询重新优化 当执行期间抛出某些错误时，Hive 支持查询重新优化。特别是，它实现了两个独立的重新优化策略。 第一个策略，覆盖，更改所有查询重新执行的某些配置参数。例如，用户可以选择强制查询重新执行中的所有连接使用某种算法，例如，带有排序合并的哈希分区。当已知某些配置值可以使查询执行更加健壮时，这可能很有用。 第二种策略，重新优化，依赖于运行时捕获的统计信息。在计划查询时，优化器会根据从 HMS 检索到的统计信息来估计计划中中间结果的大小。如果这些估计不准确，优化器可能会犯计划错误，例如错误的连接算法选择或内存分配。 这反过来可能导致性能不佳和执行错误。Hive 为计划中的每个运算符捕获运行时统计信息。如果在查询执行期间检测到任何上述问题，则使用运行时统计信息重新优化查询并再次执行。 4.3 查询结果缓存 仓库的事务一致性允许 Hive 通过使用参与表的内部事务状态来重用先前执行的查询的结果。在处理生成重复相同查询的 BI 工具时，查询缓存提供了可扩展性优势。 每个 HS2 实例都保留自己的查询缓存组件，该组件又保留从查询 AST 表示到条目的映射，该条目包含结果位置和回答查询的数据快照的信息。该组件还负责清除过时的条目并清理这些条目使用的资源。 在查询编译期间，HS2 在初步步骤中使用输入查询 AST 检查其缓存。查询中不合格的表引用在 AST 用于证明缓存之前被解析，因为根据查询执行时的当前数据库，具有相同文本的两个查询可能访问来自不同数据库的表。 如果缓存命中并且查询使用的表不包含新的或修改的数据，则查询计划将包含从缓存位置获取结果的单个任务。 如果该条目不存在，则查询照常运行，如果查询满足某些条件，则为查询生成的结果保存到缓存中；例如，查询不能包含非确定性函数(rand)、运行时常量函数(current_date、current_timestamp)等。 查询缓存有一个挂起的条目模式，当数据更新并且其中几个同时观察到缓存未命中时，它可以防止雷鸣般的相同查询群。 缓存将被第一个进入的查询重新填充。此外，该查询可能会获得更多的集群容量，因为它会将结果提供给所有其他遭受缓存未命中的并发相同查询。 4.4 物化视图与重写 传统上，用于加速数据仓库中查询处理的最强大的技术之一是相关物化视图的预计算 [28,31,35-37]。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849(a)CREATE MATERIALIZED VIEW mat_view ASSELECT d_year, d_moy, d_dom, SUM(ss_sales_price) AS sum_salesFROM store_sales, date_dimWHERE ss_sold_date_sk = d_date_sk AND d_year &gt; 2017GROUP BY d_year, d_moy, d_dom;(b)q1:SELECT SUM(ss_sales_price) AS sum_salesFROM store_sales, date_dimWHERE ss_sold_date_sk = d_date_sk AND d_year = 2018 AND d_moy IN (1,2,3);(b)q&#x27;1:SELECT SUM(sum_sales)FROM mat_viewWHERE d_year = 2018 AND d_moy IN (1,2,3);(c)q2:SELECT d_year, d_moy, SUM(ss_sales_price) AS sum_salesFROM store_sales, date_dimWHERE ss_sold_date_sk = d_date_sk AND d_year &gt; 2016GROUP BY d_year, d_moy;q&#x27;2:SELECT d_year, d_moy, SUM(sum_sales)FROM ( SELECT d_year, d_moy, SUM(sum_sales) AS sum_sales FROM mat_view GROUP BY d_year, d_moy UNION ALL SELECT d_year, d_moy, SUM(ss_sales_price) AS sum_sales FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_year &gt; 2016 AND d_year &lt;= 2017 GROUP BY d_year, d_moy) subqGROUP BY d_year, d_moy; 图 4：物化视图定义 (a) 和完整样本 (b) 和部分样本 © 包含重写。 Apache Hive 支持物化视图和基于这些物化的自动查询重写。特别的是，物化视图只是语义丰富的表。因此，它们可以由 Hive 或其他支持的系统本地存储(参见第 6 节)，并且它们可以无缝利用 LLAP 加速等功能(参见第 5.1 节)。 优化器依靠 Calcite 自动生成对 SelectProject-Join-Aggregate (SPJA)查询表达式的全部和部分包含的重写(参见图 4)。 重写算法利用 Hive 中声明的完整性约束信息，例如主键、外键、唯一键和非空值，以产生额外的有效转换。该算法封装在规则中，由基于成本的优化器触发，该优化器负责决定是否应该使用重写来回答查询。 请注意，如果多次重写适用于查询的不同部分，优化器最终可能会选择多个视图替换。 物化视图维护 当物化视图使用的源表中的数据发生更改时，例如，插入新数据或修改现有数据，我们将需要刷新物化视图的内容以使其与这些更改保持同步。 目前，物化视图的重建操作需要由用户使用 REBUILD 语句手动触发。 默认情况下，Hive 尝试增量重建物化视图 [32, 34]，如果失败，则回退到完全重建。 当前实现仅在对源表进行 INSERT 操作时支持增量重建，而 UPDATE 和 DELETE 操作将强制物化视图的完全重建。 一个有趣的方面是增量维护依赖于重写算法本身。由于查询与数据的快照相关联，因此物化视图定义通过扫描每个表的 WriteId 列值的过滤条件来丰富(参见第 3.2 节)。 这些过滤条件反映了创建或最后刷新实体化视图时的数据快照。当触发维护操作时，重写算法可能会产生部分包含的重写，该重写从源表中读取物化视图和新数据。 这个重写的计划又被转换为 (i) 如果它是 SPJ 物化视图，则为 INSERT 操作，或者 (ii) 如果它是 SPJA 物化视图，则为 MERGE 操作。 物化视图生命周期 默认情况下，一旦物化视图内容过时，物化视图将不会用于查询重写。 但是，在某些情况下，在以微批次更新物化视图的同时接受对陈旧数据的重写可能会很好。 对于这些情况，Hive 允许用户组合定期运行的重建操作，例如每 5/10 分钟一次，并使用表属性定义物化视图定义中允许的数据陈旧窗口。 注：表属性允许 Hive 中的用户使用元数据键值对标记表和物化视图定义 4.5 共享工作优化 Hive 能够识别给定查询的执行计划中的重叠子表达式，只计算一次并重用它们的结果。 共享工作优化器不会触发转换以在计划中找到等效的子表达式，而是仅合并计划的相等部分，类似于先前工作中提出的其他基于重用的方法 [25, 41]。 它在执行之前应用重用算法：它开始合并相同表上的扫描操作，然后继续合并计划运算符，直到发现差异。来自合并表达式的新共享边的数据传输策略的决定留给底层引擎，即 Apache Tez。 基于重用的方法的优点是它可以加速执行多次计算相同子表达式的查询，而不会引入太多优化开销。尽管如此，由于共享工作优化器没有探索等效计划的完整搜索空间，Hive 可能无法检测到现有的重用机会。 4.6 动态半连接(semijoin)缩减 半连接缩减是一种传统技术，用于在查询执行期间减少中间结果的大小 [17, 20]。该优化对于具有一个或多个维度表的星型数据库特别有用。 对这些数据库的查询通常会连接事实表和维度表，这些表使用一个或多个列上的谓词进行过滤。但是，这些列并未在连接条件中使用，因此无法静态创建对事实表的过滤器。 以下 SQL 查询显示了 store_sales 和 store_returns 事实表与商品维度表之间的这种连接示例： 12345678SELECT ss_customer_sk, SUM(ss_sales_price) AS sum_salesFROM store_sales, store_returns, itemWHERE ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number AND ss_item_sk = i_item_sk AND i_category = &#x27;Sports&#x27;GROUP BY ss_customer_skORDER BY sum_sales DESC; 通过应用半连接缩减，Hive 评估被过滤的子表达式(在上面的示例中，项目表上的过滤器)，随后，表达式产生的值用于跳过从其余表中读取记录。 半连接缩减由优化器引入，并被推送到计划中的扫描运算符中。根据数据布局，Hive 实现了两种优化变体。 动态分区修剪 如果表会被半链接缩减并且被目标链接列分组的话就会应用它。评估过滤子表达式后，Hive 使用生成的值在查询运行时动态跳过读取不需要的分区。 回想一下，每个表分区值都存储在文件系统中的不同文件夹中，因此跳过它们很简单。 索引半连接 如果表被半链接缩减但没有被目标链接列分组的话，Hive 可以使用过滤子表达式生成的值来 (i) 创建具有最小值和最大值的范围过滤条件，(ii) 使用子表达式产生的值创建一个布隆过滤器。 Hive 使用这两个过滤器填充半连接缩减器，这可用于避免在运行时扫描整个行组，例如，如果数据存储在 ORC 文件中 [39]。 5 执行查询 查询执行内部的改进，例如从 MapReduce 到 Apache Tez [50] 的转换以及基于列的存储格式和矢量化运算符 [39] 的实现，将 Hive 中的查询延迟降低了几个数量级。 但是，为了进一步改进执行运行时，Hive 需要额外的增强功能来克服其针对长时间运行的查询量身定制的初始架构固有的限制。 其中，(i) 执行需要在启动时分配 YARN 容器，这很快成为低延迟查询的关键瓶颈，(ii) JustIn-Time (JIT) 编译器优化没有预想中有效，因为容器在查询执行后被简单地杀死，(iii) Hive 无法利用查询内部和查询之间的数据共享和缓存可能性，导致不必要的 IO 开销。 5.1 LLAP: Live Long and Process Live Long and Process，也称为 LLAP，是一个可选层，它提供持久的多线程查询执行器和内存缓存中的多租户，以在 Hive 中大规模提供更快的 SQL 处理。 LLAP 不会取代 Hive 使用的现有执行运行时，例如 Tez，而是对其进行了增强。特别是，Hive 查询协调器在 LLAP 节点和常规容器上透明地调度和监控执行。 LLAP 的数据 I/O、缓存和查询片段执行能力被封装在守护进程中。守护进程设置为在集群的工作节点中连续运行，促进 JIT 优化，同时避免任何启动开销。 YARN 用于粗粒度的资源分配和调度。它为守护进程保留内存和 CPU 并处理重启和重定位。 守护进程是无状态的：每个都包含许多执行程序以并行运行多个查询片段和一个本地工作队列。 故障和恢复很简单，因为如果 LLAP 守护程序失败，任何节点仍可用于处理输入数据的任何片段。 I/O 调度算法 守护进程使用单独的线程来卸载数据 I/O 和解压缩，我们将其称为 I/O 调度算法。 数据分批读取并转换为内部游程编码 (RLE) 列格式，可用于矢量化处理。一个列批次在读取后立即进入执行阶段，这允许在准备后续批次的同时处理先前的批次。 使用特定于每种格式的插件完成从底层文件格式到 LLAP 内部数据格式的转换。目前 LLAP 支持 ORC [10]、Parquet [11] 和文本文件格式的转译。 I/O 调度算法可以将投影、sargable 谓词和布隆过滤器下推到文件读取器(如果提供的话)。例如，ORC 文件可以利用这些结构跳过读取整个列和行组。 注：sargable 谓词代表如下操作 =, &gt;, &lt;, &gt;=, &lt;=, BETWEEN, LIKE, IS [NOT] NULL 数据缓存 LLAP 将堆外缓存作为其主要缓冲池，用于保存进入 I/O 调度算法的数据。对于每个文件，缓存由 I/O 调度算法沿两个维度寻址：行组和列组。 一组行和列形成一个行列块(参见图 5)。I/O 调度算法重新组装选定的投影并评估谓词以提取块，这些块可以重组为操作员管道的向量批次。在缓存未命中的情况下，在执行重构之前，将用丢失的块重新填充缓存。 结果是当用户沿着非规范化维度导航数据集时缓存的增量填充，这是缓存优化的常见模式。 LLAP 缓存输入文件中的元数据和数据。为了在存在文件更新的情况下保持缓存的有效性，LLAP 使用分配给存储在 HDFS 中的每个文件的唯一标识符以及有关文件长度的信息。 这类似于 blob 存储(如 AWS S3 或 Azure Blob 存储)中的 ETag 字段。 即使对于从未在缓存中的数据，元数据(包括索引信息)也会被缓存。特别是，第一次扫描会批量填充元数据，该元数据用于确定必须加载的行组并在确定缓存未命中之前评估谓词。 这种方法的优点是 LLAP 不会加载对给定查询实际上不必要的块，因此将避免破坏缓存。 缓存是增量可变的，即向表中添加新数据不会导致缓存完全失效。具体来说，缓存仅在查询处理该文件时才获取文件，这会将可见性的控制权转移回查询事务状态。 具体来说，缓存仅在查询处理该文件时才获取文件，这会将可见性的控制权转移回查询事务状态。 由于 ACID 实现通过调整文件级别的可见性来处理事务(回忆第 3.2 节)，缓存变成了数据的 MVCC 视图，为可能处于不同事务状态的多个并发查询提供服务。 缓存中数据的驱逐策略是可交换的。目前，默认使用针对具有频繁完整和部分扫描操作的分析工作负载调整的简单 LRFU (最近最少/经常使用)替换策略。 驱逐的数据单位是块。这种选择代表了低开销处理和存储效率之间的折衷。 查询片段执行 LLAP 守护进程执行任意查询计划片段，其中包含过滤器、投影、数据转换、连接、部分聚合和排序等操作。 它们允许并行执行来自不同查询和会话的多个片段。片段中的操作是矢量化的，它们直接在内部 RLE 格式上运行。 对 I/O、缓存和执行使用相同的格式可以最大限度地减少所有这些之间交互所需的工作。 如上所述，LLAP 不包含自己的执行逻辑。相反，它的执行器基本上可以复制 YARN 容器的功能。 但是，出于稳定性和安全性原因，LLAP 仅接受 Hive 代码和静态指定的用户定义函数 (UDF)。 5.2 工作负载管理方面的改进 工作负载管理器控制 Hive 执行的每个查询对 LLAP 资源的访问。管理员可以创建资源计划，即自包含的资源共享配置，以通过在 LLAP 上运行的并发查询来提高执行可预测性和集群共享。 这些因素在多租户环境中至关重要。尽管可以在系统中定义多个资源计划，但在给定时间，对于给定的部署，只有其中一个是活动的。资源计划在 HMS 中由 Hive 持久化。 资源计划包括 (i) 一个或多个资源池，每个池具有最大资源量和并发查询数，(ii) 映射会被当做根，根据特定查询属性(例如用户、组或应用程序)将传入查询到池中，(iii) 根据在运行时收集的查询指标启动操作的触发器，例如终止池中的查询或将查询从一个池移动到另一个池。 尽管查询获得了池中定义的集群资源的保证部分，但工作负载管理器试图防止集群未被充分利用。具体来说，可以为查询分配尚未分配到的池中的空闲资源，直到映射到该池的后续查询声明它们为止。 举个例子，请考虑生产集群的以下资源计划定义： 1234567891011CREATE RESOURCE PLAN daytime;CREATE POOL daytime.bi WITH alloc_fraction=0.8, query_parallelism=5;CREATE POOL daytime.etl WITH alloc_fraction=0.2, query_parallelism=20;CREATE RULE downgrade IN daytime WHEN total_runtime &gt; 3000 THEN MOVE etl;ADD RULE downgrade TO bi;CREATE APPLICATION MAPPING visualization_app IN daytime TO bi;ALTER PLAN daytime SET DEFAULT POOL = etl;ALTER RESOURCE PLAN daytime ENABLE ACTIVATE; 第 1 行为日常创建资源计划。第 2-3 行使用集群中 80% 的 LLAP 资源创建一个池 bi。这些资源可用于同时执行多达 5 个查询。 类似地，第 4-5 行创建了一个池 etl，其中包含可用于同时执行多达 20 个查询的其余资源。第 6-8 行创建一个规则，当查询运行超过 3 秒时，将查询从 bi 移动到 etl 资源池。 注意前面的操作可以执行，因为查询分片比容器更容易抢占。第 9 行为名为 interactive_bi 的应用程序创建了一个映射，即由 interactive_bi 触发的所有查询最初都将从 bi 池中获取资源。 反过来，第 10 行将系统中其余查询的默认池设置为 etl。最后，第 11 行启用并激活集群中的资源计划。 6 联邦集群仓库系统 在过去的十年中，专业数据管理系统 [54] 越来越多，这些系统变得流行，因为它们在特定用例中实现了比传统 RDBMS 更好的成本效益性能。 除了其本机处理能力外，Hive 还可以充当中介 [23、27、57]，因为它旨在支持对多个独立数据管理系统的查询。通过 Hive 统一访问这些系统的好处是多方面的。 应用程序开发人员可以选择混合多个系统来实现所需的性能和功能，但他们只需要针对单个接口进行编码。因此，应用程序变得独立于底层数据系统，这为以后更改系统提供了更大的灵活性。 Hive 本身可用于实现不同系统之间的数据移动和转换，从而减轻对第三方工具的需求。 此外，作为中介的 Hive 可以通过 Ranger [12] 或 Sentry [13] 在进群范围内实施访问控制和捕获审计跟踪，还可以通过 Atlas [2] 帮助满足合规性要求。 6.1 存储处理程序 为了以模块化和可扩展的方式与其他引擎交互，Hive 包含一个需要为每个引擎实现的存储处理程序接口。 存储处理程序包括：(i) 输入格式，描述如何从外部引擎读取数据，包括如何拆分工作以增加并行度，(ii) 输出格式，描述如何将数据写入外部 引擎，(iii) 一个 SerDe（序列化器和反序列化器），它描述了如何将数据从 Hive 内部表示转换为外部引擎表示，反之亦然 (iv) Metastore 钩子，它定义了作为针对 HMS 的事务的一部分调用的通知方法，例如，当创建由外部系统支持的新表时或将新行插入该表时触发。 从外部系统读取数据的可用存储处理程序的最小实现至少包含输入格式和解码器。 一旦实现了存储处理程序接口，从 Hive 查询外部系统就很简单了，所有的复杂性都对用户隐藏在存储处理程序实现后面。 例如，Apache Druid [4, 58] 是一个开源数据存储，专为事件数据的商业智能 (OLAP) 查询而设计，广泛用于支持面向用户的分析应用程序；Hive 提供了一个 Druid 存储处理程序，因此它可以利用其效率来执行交互式查询。 要开始从 Hive 查询 Druid，唯一需要的操作是从 Hive 注册或创建 Druid 数据源。首先，如果 Druid 中已经存在数据源，我们可以通过简单的语句将 Hive 外部表映射到它： 123CREATE EXTERNAL TABLE druid_table_1STORED BY &#x27;org.apache.hadoop.hive.druid.DruidStorageHandler&#x27;TBLPROPERTIES (&#x27;druid.datasource&#x27; = &#x27;my_druid_source&#x27;); 请注意，我们不需要为数据源指定列名或类型，因为它们是从 Druid 元数据中自动推断出来的。 反过来，我们可以使用如下简单语句在 Hive 中创建 Druid 中的数据源： 123CREATE EXTERNAL TABLE druid_table_2 ( __time TIMESTAMP, dim1 VARCHAR(20), m1 FLOAT)STORED BY &#x27;org.apache.hadoop.hive.druid.DruidStorageHandler&#x27; 一旦 Druid 源在 Hive 中作为外部表可用，我们就可以对它们执行任何允许的表操作。 6.2 使用 Calcite 推动计算 Hive 最强大的功能之一是可以利用 Calcite 适配器 [19] 将复杂的计算推送到支持的系统并以这些系统支持的语言生成查询。 继续 Druid 示例，查询 Druid 的最常见方式是通过 HTTP 上的 REST API 使用 JSON 表示的查询。 一旦用户声明了存储在 Druid 中的表，Hive 就可以从输入 SQL 查询透明地生成 Druid JSON 查询。 特别是，优化器应用匹配计划中的一系列操作符的规则，并生成一个新的等效序列，在 Druid 中执行更多操作。 一旦我们完成了优化阶段，需要由 Druid 执行的运算符子集由 Calcite 转换为有效的 JSON 查询，该查询附加到将从 Druid 读取的扫描运算符。 请注意，对于支持 Calcite 自动生成的查询的存储处理程序，其输入格式需要包含将查询发送到外部系统(可能将查询拆分为多个可以并行执行的子查询)并读回查询的结果。 截至今天，Hive 可以使用 Calcite 将操作推送到 Druid 和具有 JDBC 支持的多个引擎。 图 6 描述了对存储在 Druid 中的表执行的查询，以及 Calcite 生成的相应计划和 JSON 查询。 7 绩效评估 为了研究我们工作的影响，我们使用不同的标准基准随着时间的推移评估 Hive。在本节中，我们对结果进行了总结，强调了整篇论文中提出的改进的影响。 实验装置 下面介绍的实验在由 10 个节点组成的集群上运行，这些节点由 10 Gigabit 以太网连接。 每个节点都有一个 8 核 2.40GHz Intel Xeon CPU E5-2630 v3 和 256GB RAM，并有两个 6TB 磁盘用于 HDFS 和 YARN 存储。 7.1 与以前版本的比较 我们在 10TB 规模的数据集上使用 TPC-DS 查询进行了实验。数据使用 ORC 文件格式存储在 HDFS 中的 ACID 表中，事实表按天分区。 这些结果的可重复性所需的所有信息都是公开的。我们比较了 Hive [7] 的两个不同版本：(i) Hive v1.2，于 2015 年 9 月发布，运行在 Tez v0.5 之上，(ii) 最新的 Hive v3.1，于 11 月发布 2018 年，使用启用了 LLAP 的 Tez v0.9。 图 7 显示了两个 Hive 版本的响应时间(由用户感知)；注意对数 y 轴。对于每个查询，我们报告了三个命中缓存的平均值。首先，观察在 Hive v1.2 中只能执行 50 个查询；图中省略了无法执行的查询的响应时间值。 原因是 Hive v1.2 缺乏对集合操作的支持，例如 EXCEPT 或 INTERSECT、具有非等连接条件的相关标量子查询、区间表示法和未选择列的排序以及其他 SQL 功能。 对于这 50 个查询，Hive v3.1 的性能明显优于之前的版本。特别是，Hive v3.1 平均快了 4.6 倍，最高快了 45.5 倍(参见 q58)。 图中强调了改进超过 15 倍的查询。更重要的是，由于对 SQL 支持的改进，Hive v3.1 可以执行完整的 99 个 TPC-DS 查询。 两个版本之间的性能差异如此之大，以至于 Hive v3.1 执行的所有查询的聚合响应时间仍然比 Hive v1.2 中 50 个查询的时间低 15%。 新的优化功能，例如共享工作优化器，本身就产生了很大的不同；例如，启用 q88 后，它的速度提高了 2.7 倍。 7.2 LLAP 加速 执行模式 总响应时长(s) 容器化(未开启 LLAP) 41576 LLAP 15540 表 1：使用 LLAP 之后的相应时间性能提升。 为了说明 LLAP 对使用 Tez 容器执行查询的优势，我们使用相同的配置在 Hive v3.1 中运行所有 99 个 TPC-DS 查询，但启用/禁用了 LLAP。 表 1 显示了实验中所有查询的聚合时间；我们可以观察到，LLAP 本身可以将工作负载响应时间显着降低 2.7 倍。 7.3 Druid 上的联邦查询 在下文中，我们说明了可以从使用 Hive 的物化视图和联邦功能的组合中获得的显着性能优势。对于这个实验，我们在 1TB 规模的数据集上使用 Star-Schema Benchmark (SSB) [47]。 SSB 基准测试基于 TPC-H，旨在模拟迭代和交互式查询数据仓库的过程，以播放假设场景、深入挖掘并更好地了解趋势。 它由一个事实表和 4 个维度表组成，工作负载包含 13 个查询，这些查询在不同的表集上连接、聚合和放置相当严格的维度过滤器。 对此实验，我们创建了一个非规范化数据库模式的物化视图。物化存储在 Hive 中。然后我们在基准中运行查询，这些查询由优化器自动重写，以从物化视图中得到响应。 随后，我们将物化视图存储在 Druid v0.12 中并重复相同的步骤。图 8 描述了每个变体的响应时间。 观察到 Hive/Druid 比 Hive 中本地存储的物化视图的执行速度快 1.6 倍。原因是 Hive 使用 Calcite 将大部分查询计算推送到 Druid，因此它可以受益于 Druid 为这些查询提供更低的延迟。 8 讨论 在本节中，我们将讨论这项工作中提出的改进的影响，以及在实施这些新功能时面临的多重挑战和经验教训。 随着时间的推移，应该添加到项目中的功能面向于用户面临的缺陷，Hive 社区中的报告，或将项目商业化的公司决定。 例如，企业用户要求实施 ACID 保证，以便卸载在 Hadoop 产生之前很久就存在的数据仓库。此外，最近生效的新法规(赋予个人要求删除个人数据的权利的欧洲 GDPR)也强调了这一新功能的重要性。 毫不奇怪，ACID 支持已迅速成为在 Hadoop 之上选择 Hive 而不是其他 SQL 引擎的关键区别。 但是，将 ACID 功能引入 Hive 并不简单，初始实现必须进行重大更改，因为与非 ACID 表相比，它引入了读取延迟损失，这是用户无法接受的。 这是由于原始设计中的一个细节，我们没有预见到它会对生产性能产生如此大的影响：使用单一 delta type 的文件来存储插入、更新和删除的记录。 如果工作负载包含大量写入操作或压缩运行不够频繁，则文件读取器必须对大量基本文件和增量文件执行排序合并以整合数据，这可能会造成内存压力。 此外，过滤器下推以跳过读取整个行组不能应用于这些增量文件。本文中描述的 ACID 的第二个实现是随 Hive v3.1 推出的。它解决了上述问题，性能与非 ACID 表相当。 随着时间的推移，Hive 用户的另一个常见且频繁的请求是改善其运行时延迟。LLAP 开发始于 4 年前，旨在解决系统架构固有的问题，例如缺乏数据缓存或长时间运行的执行程序。 来自初始部署的反馈很快被纳入系统。例如，由于用户查询之间的集群资源争用，可预测性是生产中的一个巨大问题，这导致了工作负载管理器的实施。 目前，看到公司部署 LLAP 来为多租户集群中的 TB 数据提供查询服务，实现秒级的平均延迟是非常值得的。 另一个重要的决定是与 Calcite 集成以进行查询优化。在正确的抽象级别表示查询对于在 Hive 中实现高级优化算法至关重要，这反过来又为系统带来了巨大的性能优势。 此外，利用该集成可以轻松地生成查询，以便与其他系统进行联合。由于现在组织使用过多的数据管理系统很常见，因此 Hive 用户对这一新功能感到非常兴奋。 9 路线图 对 Hive 的持续改进 社区将继续致力于本文讨论的领域，以使 Hive 成为更好的仓储解决方案。例如，基于我们已经完成的支持 ACID 功能的工作，我们计划实现多语句事务。 此外，我们将继续改进 Hive 中使用的优化技术。在运行时捕获的用于查询重新优化的统计信息已经保存在 HMS 中，这将使我们能够将该信息反馈到优化器中以获得更准确的估计，类似于其他系统 [53, 60]。 物化视图工作仍在进行中，最需要的功能之一是为 Hive 实现顾问或推荐器 [1, 61]。对 LLAP 性能和稳定性的改进以及与其他专业系统(例如 Kafka [9, 43])的新连接器的实施也在进行中。 独立元存储 HMS 已成为 Hadoop 中的关键部分，因为它被 Spark 或 Presto 等其他数据处理引擎用于为中央存储库提供有关每个系统中所有数据源的信息。 因此，从 Hive 中分离出 Metastore 并将其作为独立项目开发的兴趣和正在进行的工作越来越多。这些系统中的每一个都将在 Metastore 中拥有自己的目录，并且可以更轻松地跨不同引擎访问数据。 云中的容器化 Hive 基于云的数据仓库解决方案，如 Azure SQL DW [18]、Redshift [33, 49]、BigQuery [21, 46] 和 Snowflake [29, 51] 在过去几年中越来越受欢迎。 此外，人们对 Kubernetes [44] 等系统越来越感兴趣，以为在本地或云端部署的应用程序提供容器编排。 Hive 的模块化架构可以轻松隔离其组件(HS2、HMS、LLAP)并在容器中运行它们。正在进行的工作重点是完成使用 Kubernetes 在商业云中部署 Hive 的工作。 10 结论 Apache Hive 的早期成功源于利用众所周知的接口进行批处理操作的并行性的能力。它使数据加载和管理变得简单，优雅地处理节点、软件和硬件故障，无需昂贵的修复或恢复时间。 在本文中，我们展示了社区如何将系统的实用性从 ETL 工具扩展到成熟的企业级数据仓库。我们描述了添加一个事务系统，该系统非常适合星型数据库中所需的数据修改。 我们展示了将查询延迟和并发性带入交互式操作领域所必需的主要运行时改进。我们还描述了处理当今视图层次结构和大数据操作所必需的基于成本的优化技术。 最后，我们展示了 Hive 如何在今天用作多个存储和数据系统的关系前端。所有这一切的发生在都没有损害使其流行的系统的原始特征的基础上。 Apache Hive 架构和设计原则已被证明在当今的分析环境中非常强大。我们相信，随着新的部署和存储环境的出现，它将继续在新的部署和存储环境中蓬勃发展，正如今天在容器化和云中所展示的那样。 致谢 12We would like to thank the Apache Hive community, contributors and users, who build, maintain, use, test, write about, and continue to push the project forward. We would also like to thank Oliver Draese for his feedback on the original draft of this paper. 参考资料 [1] Sanjay Agrawal, Surajit Chaudhuri, and Vivek R. Narasayya. 2000. Automated Selection of Materialized Views and Indexes in SQL Databases. In PVLDB. [2] Apache Atlas 2018. Apache Atlas: Data Governance and Metadata framework for Hadoop. http://atlas.apache.org/. [3] Apache Calcite 2018. Apache Calcite: Dynamic data management framework. http://calcite.apache.org/. [4] Apache Druid 2018. Apache Druid: Interactive analytics at scale. http://druid.io/. [5] Apache Flink 2018. Apache Flink: Stateful Computations over Data Streams. http://flink.apache.org/. [6] Apache HBase 2018. Apache HBase. http://hbase.apache.org/. [7] Apache Hive 2018. Apache Hive. http://hive.apache.org/. [8] Apache Impala 2018. Apache Impala. http://impala.apache.org/. [9] Apache Kafka 2018. Apache Kafka: A distributed streaming platform. http://kafka.apache.org/. [10] Apache ORC 2018. Apache ORC: High-Performance Columnar Storage for Hadoop. http://orc.apache.org/. [11] Apache Parquet 2018. Apache Parquet. http://parquet.apache.org/. [12] Apache Ranger 2018. Apache Ranger: Framework to enable, monitor and manage comprehensive data security across the Hadoop platform. http://ranger.apache.org/. [13] Apache Sentry 2018. Apache Sentry: System for enforcing fine grained role based authorization to data and metadata stored on a Hadoop cluster. http://sentry.apache.org/. [14] Apache Spark 2018. Apache Spark: Unified Analytics Engine for Big Data. http://spark.apache.org/. [15] Apache Tez 2018. Apache Tez. http://tez.apache.org/. [16] Apache Thrift 2018. Apache Thrift. http://thrift.apache.org/. [17] Peter M. G. Apers, Alan R. Hevner, and S. Bing Yao. 1983. Optimization Algorithms for Distributed Queries. IEEE Trans. Software Eng. 9, 1(1983), 57–68. [18] Azure SQL DW 2018. Azure SQL Data Warehouse. mhttp://azure.microsoft.com/en-us/services/sql-data-warehouse/. [19] Edmon Begoli, Jesús Camacho-Rodríguez, Julian Hyde, Michael J. Mior, and Daniel Lemire. 2018. Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources. In SIGMOD. [20] Philip A. Bernstein, Nathan Goodman, Eugene Wong, Christopher L. Reeve, and James B. Rothnie Jr. 1981. Query Processing in a System for Distributed Databases (SDD-1). ACM Trans. Database Syst. 6, 4 (1981), 602–625. [21] BigQuery 2018. BigQuery: Analytics Data Warehouse. http://cloud.google.com/bigquery/. [22] Vinayak R. Borkar, Michael J. Carey, Raman Grover, Nicola Onose, and Rares Vernica. 2011. Hyracks: A flexible and extensible foundation for data-intensive computing. In ICDE. [23] Francesca Bugiotti, Damian Bursztyn, Alin Deutsch, Ioana Ileana, and Ioana Manolescu. 2015. Invisible Glue: Scalable Self-Tunning MultiStores. In CIDR. [24] Michael J. Cahill, Uwe Röhm, and Alan David Fekete. 2008. Serializable isolation for snapshot databases. In SIGMOD. [25] Jesús Camacho-Rodríguez, Dario Colazzo, Melanie Herschel, Ioana Manolescu, and Soudip Roy Chowdhury. 2016. Reuse-based Optimization for Pig Latin. In CIKM. [26] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, and Kostas Tzoumas. 2015. Apache Flink™: Stream and Batch Processing in a Single Engine. IEEE Data Eng. Bull. 38, 4 (2015), 28–38. [27] Michael J. Carey, Laura M. Haas, Peter M. Schwarz, Manish Arya, William F. Cody, Ronald Fagin, Myron Flickner, Allen Luniewski, Wayne Niblack, Dragutin Petkovic, Joachim Thomas, John H. Williams, and Edward L. Wimmers. 1995. Towards Heterogeneous Multimedia Information Systems: The Garlic Approach. In RIDE-DOM Workshop. [28] Surajit Chaudhuri, Ravi Krishnamurthy, Spyros Potamianos, and Kyuseok Shim. 1995. Optimizing Queries with Materialized Views. In ICDE. [29] Benoît Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. The Snowflake Elastic Data Warehouse. In SIGMOD. [30] DataNucleus 2018. DataNucleus: JDO/JPA/REST Persistence of Java Objects. http://www.datanucleus.org/. [31] Jonathan Goldstein and Per-Åke Larson. 2001. Optimizing Queries Using Materialized Views: A practical, scalable solution. In SIGMOD. [32] Timothy Griffin and Leonid Libkin. 1995. Incremental Maintenance of Views with Duplicates. In SIGMOD. [33] Anurag Gupta, Deepak Agarwal, Derek Tan, Jakub Kulesza, Rahul Pathak, Stefano Stefani, and Vidhya Srinivasan. 2015. Amazon Redshift and the Case for Simpler Data Warehouses. In SIGMOD. [34] Ashish Gupta and Inderpal Singh Mumick. 1995. Maintenance of Materialized Views: Problems, Techniques, and Applications. IEEE Data Eng. Bull. 18, 2 (1995), 3–18. [35] Himanshu Gupta. 1997. Selection of Views to Materialize in a Data Warehouse. In ICDT. [36] Himanshu Gupta, Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman. 1997. Index Selection for OLAP. In ICDE. [37] Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman. 1996. Implementing Data Cubes Efficiently. In SIGMOD. [38] Stefan Heule, Marc Nunkesser, and Alexander Hall. 2013. HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm. In EDBT. [39] Yin Huai, Ashutosh Chauhan, Alan Gates, Günther Hagleitner, Eric N. Hanson, Owen O’Malley, Jitendra Pandey, Yuan Yuan, Rubao Lee, and Xiaodong Zhang. 2014. Major technical advancements in apache hive. In SIGMOD. [40] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. 2007. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys. [41] Alekh Jindal, Shi Qiao, Hiren Patel, Zhicheng Yin, Jieming Di, Malay Bag, Marc Friedman, Yifung Lin, Konstantinos Karanasos, and Sriram Rao. 2018. Computation Reuse in Analytics Job Service at Microsoft. In SIGMOD. [42] Marcel Kornacker, Alexander Behm, Victor Bittorf, Taras Bobrovytsky, Casey Ching, Alan Choi, Justin Erickson, Martin Grund, Daniel Hecht, Matthew Jacobs, Ishaan Joshi, Lenni Kuff, Dileep Kumar, Alex Leblang, Nong Li, Ippokratis Pandis, Henry Robinson, David Rorke, Silvius Rus, John Russell, Dimitris Tsirogiannis, Skye Wanderman-Milne, and Michael Yoder. 2015. Impala: A Modern, Open-Source SQL Engine for Hadoop. In CIDR. [43] Jay Kreps, Neha Narkhede, and Jun Rao. 2011. Kafka : a Distributed Messaging System for Log Processing. In NetDB. [44] Kubernetes 2018. Kubernetes: Production-Grade Container Orchestration. http://kubernetes.io/. [45] Per-Åke Larson, Cipri Clinciu, Campbell Fraser, Eric N. Hanson, Mostafa Mokhtar, Michal Nowakiewicz, Vassilis Papadimos, Susan L. Price, Srikumar Rangarajan, Remus Rusanu, and Mayukh Saubhasik.2013. Enhancements to SQL server column stores. In SIGMOD. [46] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. 2010. Dremel: Interactive Analysis of Web-Scale Datasets. In PVLDB. [47] Patrick E. O’Neil, Elizabeth J. O’Neil, Xuedong Chen, and Stephen Revilak. 2009. The Star Schema Benchmark and Augmented Fact Table Indexing. In TPCTC. [48] Presto 2018. Presto: Distributed SQL query engine for big data.http://prestodb.io/. [49] Redshift 2018. Amazon Redshift: Amazon Web Services. http://aws.amazon.com/redshift/. [50] Bikas Saha, Hitesh Shah, Siddharth Seth, Gopal Vijayaraghavan, Arun C. Murthy, and Carlo Curino. 2015. Apache Tez: A Unifying Framework for Modeling and Building Data Processing Applications. In SIGMOD. [51] Snowflake 2018. Snowflake: The Enterprise Data Warehouse Built in the Cloud. http://www.snowflake.com/. [52] Mohamed A. Soliman, Lyublena Antova, Venkatesh Raghavan, Amr El-Helw, Zhongxian Gu, Entong Shen, George C. Caragea, Carlos Garcia-Alvarado, Foyzur Rahman, Michalis Petropoulos, Florian Waas, Sivaramakrishnan Narayanan, Konstantinos Krikellas, and Rhonda Baldwin. 2014. Orca: a modular query optimizer architecture for big data. In SIGMOD. [53] Michael Stillger, Guy M. Lohman, Volker Markl, and Mokhtar Kandil. 2014. LEO - DB2’s LEarning Optimizer. In PVLDB. [54] Michael Stonebraker and Ugur Çetintemel. 2005. “One Size Fits All”:An Idea Whose Time Has Come and Gone. In ICDE. [55] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Ning Zhang, Suresh Anthony, Hao Liu, and Raghotham Murthy. 2010. Hive - a petabyte scale data warehouse using Hadoop. In ICDE. [56] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans, Thomas Graves, Jason Lowe, Hitesh Shah, Siddharth Seth, Bikas Saha, Carlo Curino, Owen O’Malley, Sanjay Radia, Benjamin Reed, and Eric Baldeschwieler. 2013. Apache Hadoop YARN: yet another resource negotiator. In SOCC. [57] Gio Wiederhold. 1992. Mediators in the Architecture of Future Information Systems. IEEE Computer 25, 3 (1992), 38–49. [58] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, and Deep Ganguli. 2014. Druid: a real-time analytical data store. In SIGMOD. [59] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2010. Spark: Cluster Computing with Working Sets. In USENIX HotCloud. [60] Mohamed Zaït, Sunil Chakkappen, Suratna Budalakoti, Satyanarayana R. Valluri, Ramarajan Krishnamachari, and Alan Wood. 2017. Adaptive Statistics in Oracle 12c. In PVLDB. [61] Daniel C. Zilio, Jun Rao, Sam Lightstone, Guy M. Lohman, Adam J. Storm, Christian Garcia-Arellano, and Scott Fadden. 2004. DB2 Design Advisor: Integrated Automatic Physical Database Design. In PVLDB.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://wangqian0306.github.io/tags/Hive/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"regex","slug":"tmp/regex","date":"2022-01-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2022/regex/","permalink":"https://wangqian0306.github.io/2022/regex/","excerpt":"","text":"常用正则表达式整理 校验数字 纯数字：^[0-9]*$ n 位数字：^\\d&#123;n&#125;$ 至少 n 位的数字：^\\d&#123;n,&#125;$ m 到 n 位数字：^\\d&#123;m,n&#125;$ 非零正整数：^[1-9]\\d*$ 非零负整数：^\\-[1-9][]0-9&quot;*$ 浮点数：^(-?\\d+)(\\.\\d+)?$ 校验字符 汉字：^[\\u4e00-\\u9fa5]&#123;0,&#125;$ 英文和数字：^[A-Za-z0-9]+$ 由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$ 可以输入含有^%&amp;',;=?$\\&quot;等字符：[^%&amp;',;=?$\\x22]+ 禁止输入含有~的字符：[^~\\x22]+ 特殊功能 分词：\\\\W+ 邮箱验证：^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*$ URL 验证：[a-zA-z]+://[^\\s]* 手机号码验证(国内)：^(13[0-9]|14[5|7]|15[0|1|2|3|4|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d&#123;8&#125;$ 手机号码验证(国外)：^(\\+\\d&#123;1,2&#125;\\s)?\\(?\\d&#123;3&#125;\\)?[\\s.-]\\d&#123;3&#125;[\\s.-]\\d&#123;4&#125;$ 电话号码验证(国内): \\d&#123;3&#125;-\\d&#123;8&#125;|\\d&#123;4&#125;-\\d&#123;7&#125; 身份证验证：(^\\d&#123;15&#125;$)|(^\\d&#123;18&#125;$)|(^\\d&#123;17&#125;(\\d|X|x)$) 强密码验证(必须包含大小写字母和数字的组合，可以使用特殊字符，长度在8-10之间)：^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$ 中国邮政编码验证：[1-9]\\d&#123;5&#125;(?!\\d) IPV4 验证：((2(5[0-5]|[0-4]\\d))|[0-1]?\\d&#123;1,2&#125;)(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d&#123;1,2&#125;))&#123;3&#125; 合法上传文件类型验证：\\.(doc|docx|xml|xmls|ppt|pptx|rar|zip)$","categories":[],"tags":[{"name":"regex","slug":"regex","permalink":"https://wangqian0306.github.io/tags/regex/"}]},{"title":"Apache Hadoop YARN: Yet Another Resource Negotiator 中文翻译","slug":"treatise/apache_hadoop_yarn_yet_another_resource_negotiator","date":"2022-01-04T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2022/apache_hadoop_yarn_yet_another_resource_negotiator/","permalink":"https://wangqian0306.github.io/2022/apache_hadoop_yarn_yet_another_resource_negotiator/","excerpt":"","text":"Apache Hadoop YARN: Yet Another Resource Negotiator 中文翻译 作者：Vinod Kumar Vavilapalli, Arun C Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans, Thomas Graves, Jason Lowe, Hitesh Shah Siddharth Seth, Bikas Saha, Carlo Curino, Owen O’Malley, Sanjay Radia, Benjamin Reed, Eric Baldeschwieler 英文原文 版权说明 123456789Copyright c 2013 by the Association for Computing Machinery, Inc.(ACM). Permission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without fee provided thatcopies are not made or distributed for profit or commercial advantageand that copies bear this notice and the full citation on the first page.Copyrights for components of this work owned by others than the author(s) must be honored.Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee. Request permissions from permissions@acm.org. 摘要 Apache Hadoop [1] 的最初设计主要集中在运行大量的 MapReduce 作业来处理网络爬虫。 对于日益多样化的公司而言，Hadoop 已成为数据和计算集市——实际上是共享和访问数据和计算资源的地方。 这种广泛采用和无处不在的使用使初始设计远远超出其预期目标，暴露出两个主要缺点：1) 特定编程模型与资源管理基础设施的紧密耦合，迫使开发人员滥用 MapReduce 编程模型，以及 2) 作业控制流的集中处理，这导致调度器对可扩展性方面的有很高的可能性出现问题。 在本文中，我们总结了下一代 Hadoop 计算平台 YARN 的设计、开发和当前部署状态。 我们引入的新架构将编程模型与资源管理基础设施分离，并将许多调度功能(例如，任务容错)委托给每个应用程序组件。 我们提供实验证据来证明我们所做的改进，通过报告在生产环境(包括 100% 的雅虎集群)上运行 YARN 的经验来确认提高的效率，并通过讨论将几个编程框架移植到 YARN 上来确认灵活性声明。 Dryad、Giraph、Hoya、Hadoop MapReduce、REEF、Spark、Storm、Tez。 1 引言 Apache Hadoop 最初是 MapReduce [12] 的众多开源实现之一，专注于解决索引网络爬行所需的前所未有的规模。 其执行架构针对此用例进行了调整，专注于为大规模数据密集型计算提供强大的容错能力。 在许多大型网络公司和初创公司中，Hadoop 集群是存储和处理运营数据的常见场所。 更重要的是，它成为组织内工程师和研究人员可以即时且几乎不受限制地访问大量计算资源和公司的数据宝库的。 这既是 Hadoop 成功的原因，也是其最大的诅咒，因为大多数的开发人员将 MapReduce 编程模型扩展到了集群管理基板的能力之外。 一种常见的模式提交“only-map”作业以在集群中生成任意进程。 常见的例子为复制 web 服务器和分组迭代完成的计划任务负载。 开发人员为了利用物理资源，经常采取巧妙的解决方法来避开 MapReduce API 的限制。 这些限制和误用使得很多的无关的论文使用了 Hadoop 环境作为基础。 虽然许多论文都暴露了 Hadoop 架构或实现的实质性问题，但有些论文只是简单地(或多或少巧妙地)谴责了这些滥用的一些副作用。 现在，学术界和开源社区都很好地理解了原始 Hadoop 架构的局限性。 在本文中，我们展示了一项社区驱动的努力，旨在让 Hadoop 超越其最初的设定。 我们展示了称为 YARN 的下一代 Hadoop 计算平台，它不同于其熟悉的单体架构。 通过将资源管理功能与编程模型分离，YARN 将许多与调度相关的功能委托给每个作业的组件。 在这个新环境中，MapReduce 只是运行在 YARN 之上的应用程序之一。这种分离为编程框架的选择提供了很大的灵活性。 YARN 上可用的替代编程模型的示例包括：Dryad [18]、Giraph、Hoya、REEF [10]、Spark [32]、Storm [4] 和 Tez [2]。 在 YARN 上运行的编程框架会根据他们认为合适的方式协调应用程序内通信、执行流程和动态优化，从而实现显着的性能改进。 我们从早期架构和实施者的角度来描述 YARN 的起始、设计、开源开发和部署阶段。 2 历史和理由 在本节中，我们提供了 YARN 从实际需求中产生的历史记录。 对于起因不感兴趣的读者可以跳过本节(本节高亮了需求以便阅读)，在后续的第 3 节我们提供了 YARN 架构的简述。 雅虎在 2006 年采用 Apache Hadoop 来替代其原有的平台并作为其 WebMap 应用程序的基础设施 [11]，并使用该技术构建已知网络的图谱以支持其搜索引擎。 当时此网络图谱包含超过 1000 亿个节点和 1 万亿条边。 之前名为“Dreadnaught”的基础设施 [25] 已达到其在 800 台机器上的可扩展性极限，并且需要对其架构进行重大转变以适应外部网络的发展速度。 Dreadnought 已经执行了类似于 MapReduce [12] 程序的分布式应用程序，因此通过采用更具可扩展性的 MapReduce 框架，可以轻松迁移搜索管道中的重要部分。 这突出了在 Hadoop 的早期版本中一直存在的第一个要求，一直到 YARN—— [R1:] 可扩展性。 除了用于雅虎搜索的超大规模管道外，优化广告分析、垃圾邮件过滤和科学的内容优化推动了许多早期需求。 随着 Apache Hadoop 社区为越来越大的 MapReduce 作业扩展平台，围绕 [R2:] 多租户 的需求开始形成。 在这种情况下可以很好的理解工程优先级和计算平台的中间阶段。 YARN 的架构基于 MapReduce 平台发展的经验，满足了许多长期存在的需求。 在本文的其余部分，我们将假设对经典 Hadoop 架构有一般的了解，附录 A 中提供了其简要总结。 2.1 临时集群时代 一些 Hadoop 最早的用户会在少数节点上建立一个集群，将他们的数据加载到 Hadoop 分布式文件系统(HDFS [27])，通过编写 MapReduce 作业获得他们感兴趣的结果，然后将其拆除 [15]。 随着 Hadoop 容错能力的提高，持久性 HDFS 集群成为常态。 在雅虎，运营商会将“有趣”的数据集加载到共享集群中，吸引有兴趣从中获取见解的科学家。 虽然大规模计算仍然是开发的主要驱动力，但 HDFS 还获得了权限模型、配额和其他功能以改进其多租户操作。 为了解决它的一些多租户问题，雅虎开发和部署 Hadoop on Demand (HoD)，它使用 Torque [7] 和 Maui [20] 在共享硬件池上分配 Hadoop 集群。 用户将他们的作业连同适当大小的计算集群的描述提交给 Torque，Torque 会将作业排入队列，直到有足够的节点可用。 一旦节点可用，Torque 将在头节点上启动 HoD 的“领导者”进程，然后该进程将与 Torque/Maui 交互以启动 HoD 的从属进程，这些进程随后为该用户生成 JobTracker 和 TaskTracker，然后接受一系列作业。 当用户释放集群时，系统会自动收集用户的日志并将节点返回到共享池。 由于 HoD 为每个作业设置了一个新集群，用户可以运行(稍微)旧版本的 Hadoop，而开发人员可以轻松测试新功能。Hadoop 每三个月发布一次重大修订。 HoD 的灵活性对于保持这种节奏至关重要——我们将这种升级依赖关系的解耦称为 [R3:] 可服务性。 随着 HDFS 的扩展，可以在其上分配更多的计算集群，从而在更多数据集上创建用户密度增加的良性循环，从而产生新的见解。 虽然 HoD 也可以部署 HDFS 集群，但大多数用户跨共享 HDFS 实例部署计算节点。 随着 HDFS 的扩展，可以在其上分配更多的计算集群，从而在更多数据集上创建用户密度增加的良性循环，从而产生新的领悟。 由于大多数 Hadoop 集群都小于雅虎最大的 HoD 作业，因此 JobTracker 很少成为瓶颈。 HoD 证明了自己是一个多功能平台，预见了 Mesos [17] 的一些品质，它将扩展框架主模型以支持并发、不同编程模型之间的动态资源分配。 HoD 也可以被视为 EC2 Elastic MapReduce 和 Azure HDInsight 产品的私有云前身——没有任何隔离和安全方面的问题。 2.2 Hadoop on Demand 的缺点 雅虎由于其中等资源利用率，最终淘汰了 HoD，转而使用共享 MapReduce 集群。 在映射阶段，JobTracker 尽一切努力将任务放置在 HDFS 中靠近其输入数据的位置，理想情况下位于存储该数据副本的节点上。 由于 Torque 在不考虑位置的情况下分配节点，授予用户 JobTracker 的节点子集可能只包含少数相关副本。鉴于大量小作业，大多数读取来自远程主机。 打击这些作业的努力取得了好坏参半的结果；虽然将 TaskTracker 分布在机架上使得共享数据集的机架内读取更有可能，但 map 和 reduce 任务之间的记录洗牌必然会跨机架，并且 DAG 中的后续作业将有更少的机会来解释其祖先中的偏差。 [R4:] 位置意识 的这一方面是 YARN 的关键要求。 Pig [24] 和 Hive [30] 等高级框架通常在 DAG 中组成 MapReduce 作业的工作流，每个工作流在计算的每个阶段过滤、聚合和投影数据。 由于在使用 HoD 时没有在作业之间调整集群的大小，集群中的大部分容量都处于闲置状态，而随后的更精简的阶段完成。 在极端但非常常见的情况下，在一个节点上运行的单个 reduce 任务可能会阻止集群被回收。在此状态下，某些作业使数百个节点处于空闲状态。 最后，作业延迟主要由分配集群所花费的时间决定。 用户在估计他们的工作需要多少个节点时可以依靠很少的启发式方法，并且通常会要求 10 的任何倍数与他们的直觉相匹配。 集群分配延迟如此之高，用户通常会与同事共享期待已久的集群，持有节点的时间比预期的要长，从而进一步增加了延迟。 虽然用户喜欢 HoD 中的许多功能，但集群利用的经济性迫使雅虎将其用户打包到共享集群中。[R5:] 高集群利用率是 YARN 的首要任务。 2.3 共享集群 最终结果是 HoD 获取的信息太少，无法对其分配做出明智的决策，其资源粒度太粗，其 API 迫使用户向资源层提供误导性的约束。 然而，迁移到共享集群并非易事。虽然 HDFS 多年来逐渐扩展，但 JobTracker 已被 HoD 与这些力量隔离开来。 当那个守卫被移除时，MapReduce 集群突然变得更大，作业吞吐量急剧增加，并且许多无辜添加到 JobTracker 的功能成为关键错误的来源。 更糟糕的是，JobTracker 故障并没有丢失单个工作流，而是导致中断，这将丢失集群中所有正在运行的作业，并要求用户手动恢复他们的工作流。 停机会导致处理管道积压，当重新启动时，会给 JobTracker 带来巨大压力。重启通常涉及手动杀死用户的作业，直到集群恢复。 由于为每个作业存储了复杂的状态，因此在重新启动期间保留作业的实现从未完全调试过。 运行一个大型的、多租户的 Hadoop 集群是很难的。虽然容错是一个核心设计原则，但暴露给用户应用程序的表面是巨大的。 鉴于单点故障暴露的各种可用性问题，持续监控集群中的工作负载是否存在功能失调的作业至关重要。 更微妙的是，由于 JobTracker 需要为它初始化的每个作业分配跟踪结构，它的准入控制逻辑包括保护自身可用性的保障措施；它可能会延迟将闲置集群资源分配给作业，因为跟踪它们的开销可能会使 JobTracker 进程不堪重负。 所有这些问题都可以归为对 [R6] 可靠性/可用性的需求。 随着 Hadoop 管理更多租户、多样化用例和原始数据，其对隔离的要求变得更加严格，但授权模型缺乏强大的、可扩展的身份验证——这是多租户集群的关键特性。 这被添加并反向移植到多个版本。 [R7:] 安全和可审计的操作必须保留在 YARN 中。 开发者逐渐强化系统以适应对资源的多样化需求，这与面向槽的资源观点不符。 虽然 MapReduce 支持广泛的用例，但它并不是所有大规模计算的理想模型。 例如，许多机器学习程序需要对数据集进行多次迭代才能收敛到结果。 如果将此流程组合为一系列 MapReduce 作业，则调度开销将显着延迟结果 [32]。 类似地，使用批量同步并行模型(BSP)可以更好地表达许多图算法，使用消息传递在顶点之间进行通信，而不是在容错、大规模 MapReduce 作业中使用繁重的全对全通信障碍 [22]。 这种不匹配成为了用户生产力的障碍，但 Hadoop 中以 MapReduce 为中心的资源模型承认没有竞争的应用程序模型。 Hadoop 在雅虎内部的广泛部署及其数据管道的严重性使这些紧张局势无法调和。用户会编写“MapReduce”程序，生成替代框架，但不会因此而气馁。 对于调度程序，它们表现为具有完全不同的资源曲线的 map-only 作业，阻碍了平台内置的假设并导致利用率低下、潜在的死锁和不稳定。 纱线必须与其用户宣布休战，并为 [R8:]编程模型多样性 提供明确的支持。 除了与新兴框架要求不匹配之外，类型化槽还会损害利用率。 虽然 map 和reduce 容量之间的分离可以防止死锁，但它也可能成为资源瓶颈。 在 Hadoop 中，两个阶段之间的重叠由用户为每个提交的作业配置；稍后启动 reduce 任务会增加集群吞吐量，而在作业执行的早期启动它们会减少其延迟。 map 和 reduce 槽的数量由集群操作员固定，因此休闲的 map 容量不能用于产生 reduce 任务，反之亦然。 因为两种任务类型以不同的速度完成，所以没有配置会完美平衡；当任一插槽类型变得饱和时，JobTracker 可能需要对作业初始化应用背压，从而产生典型的管道气泡。 可替代的资源使调度复杂化，但它们也使分配器能够更紧密地打包集群。这突出了对 [R9:] 灵活资源模型的需求。 虽然与 HoD 相比，迁移到共享集群提高了利用率和局部性，但它也显着缓解了对可服务性和可用性的担忧。 在共享集群中部署新版本的 Apache Hadoop 是一个精心设计的、令人遗憾的常见事件。 为了修复 MapReduce 实现中的错误，操作员必须安排停机时间、关闭集群、部署新位、验证升级，然后接受新工作。 通过将负责仲裁资源使用的平台与表达该程序的框架相结合，人们被迫同时发展它们；当运营商提高平台用户的分配效率时，必然会纳入框架的变化。 因此，升级集群需要用户停止、验证和恢复他们的管道以进行正交更改。 虽然更新通常只需要重新编译，但用户对内部框架细节的假设——或开发人员对用户程序的假设——偶尔会在集群上运行的管道上造成阻塞不兼容。 基于 Apache Hadoop MapReduce 的发展经验教训，YARN 旨在满足需求(R1-R9)。 然而，MapReduce 应用程序的庞大安装基础、相关项目的生态系统、陈旧的部署实践和紧迫的时间表无法容忍彻底的重新设计。 为了避免“第二系统综合症”[6] 的陷阱，新架构尽可能多地重用现有框架中的代码，以熟悉的模式运行，并在绘图板上留下了许多推测性的功能。 这导致了对 YARN 重新设计的最终要求：[R10:] 向后兼容性。 在本文的其余部分，我们提供了 YARN 架构的描述(第 3 节)，我们报告了 YARN 在现实世界中的采用(第 4 节)，提供了验证一些关键架构选择的实验证据(第 5 节)并通过将 YARN 与一些相关工作(第 6 节)进行比较来得出结论。 3 架构 为了满足我们在第 2 节中讨论的要求，YARN 将一些功能提升到负责资源管理的平台层，将逻辑执行计划的协调留给了许多框架实现。 具体来说，每个集群的 ResourceManager (RM) 跟踪资源使用情况和节点活跃度，强制分配不变量，并仲裁租户之间的争用。 通过在 JobTracker 的章程中分离这些职责，中央分配器可以使用租户需求的抽象描述，但仍然不知道每个分配的语义。 该职责被委托给 ApplicationMaster (AM)，它通过从 RM 请求资源、根据收到的资源生成物理计划以及围绕故障协调该计划的执行来协调单个作业的逻辑计划。 3.1 概览 RM 在专用机器上作为守护进程运行，并充当集群中各种竞争应用程序之间的中央权威仲裁资源。 鉴于集群资源的这种中央和全局视图，它可以在租户之间强制执行丰富的、熟悉的属性，例如公平性 [R10]、容量 [R10] 和位置 [R4]。 根据应用程序需求、调度优先级和资源可用性，RM 动态地将租用(称为容器)分配给在特定节点上运行的应用程序。 容器是绑定到特定节点 [R4,R9] 的逻辑资源包(例如，2GB RAM、1 CPU)。 为了强制执行和跟踪此类分配，RM 与在每个节点上运行的称为 NodeManager(NM) 的特殊系统守护程序交互。 RM 和 NM 之间的通信是基于心跳的可扩展性。NM 负责监控资源可用性、报告故障和容器生命周期管理(例如，启动、终止)。RM 从这些 NM 状态的快照中组合其全局视图。 作业通过公共提交协议提交给 RM，并通过准入控制阶段，在此期间验证安全凭证并执行各种操作和管理检查 [R7]。接受的作业将传递给调度程序以运行。 一旦调度程序有足够的资源，应用程序就会从接受状态转移到运行状态。除了内部记录之外，这还涉及为 AM 分配一个容器并将其生成在集群中的一个节点上。 接受的应用程序的记录被写入持久存储RM 中并在重新启动或失败的情况下恢复。 ApplicationMaster 是作业的“头”，管理所有生命周期方面，包括动态增加和减少资源消耗，管理执行流程(例如，针对映射的输出运行减速器)，处理故障和计算偏差，以及执行其他本地优化。 事实上，AM 可以运行任意用户代码，并且可以用任何编程语言编写，因为与 RM 和 NM 的所有通信都是使用可扩展的通信协议进行编码的——例如考虑我们在第 4.2 节中讨论的 Dryad 端口。 YARN 对 AM 的假设很少，尽管在实践中我们预计大多数作业将使用更高级别的编程框架(例如 MapReduce、Dryad、Tez、REEF 等)。 通过将所有这些功能委托给 AM，YARN 的架构获得了极大的可扩展性 [R1]、编程模型灵活性 [R8] 和改进的升级/测试 [R3](因为同一框架的多个版本可以共存)。 通常，AM 需要利用多个节点上可用的资源(CPU、RAM、磁盘等)来完成一项工作。 为了获取容器，AM 向 RM 发出资源请求。这些请求的形式包括容器的位置偏好和属性的规范。RM 将根据可用性和调度策略尝试满足来自每个应用程序的资源请求。 当代表 AM 分配资源时，RM 会为该资源生成一个租约，该租约由随后的 AM 心跳拉取。 当 AM 将容器租约提交给 NM [R4] 时，基于令牌的安全机制可保证其真实性。 一旦 ApplicationMaster 发现一个容器可供其使用，它就会使用租约对特定于应用程序的启动请求进行编码。 在 MapReduce 中，容器中运行的代码要么是 map 任务，要么是 reduce 任务 7。 如果需要，正在运行的容器可以通过特定于应用程序的协议直接与 AM 通信，以报告状态和活跃度并接收特定于框架的命令——YARN 既不促进也不强制这种通信。 总的来说，YARN 部署为容器的生命周期管理和监控提供了一个基本但健壮的基础设施，而特定于应用程序的语义由每个框架管理 [R3,R8]。 注 7：事实上，Hadoop 1 中的 TaskTracker 生成了相同的代码 [R10]。一旦启动，进程将通过网络从 AM 而不是从本地守护进程获得有效负载。 YARN 的架构概述到此结束。在本节的其余部分，我们提供每个主要组件的详细信息。 3.2 Resource Manager (RM) ResourceManager 公开了两个公共接口：1) 客户端提交应用程序，2) ApplicationMaster(s) 动态协商对资源的访问，以及一个面向 NodeManager 的内部接口，用于集群监控和资源访问管理。 在下文中，我们关注 RM 和 AM 之间的公共协议，因为这最能代表 YARN 平台与其上运行的各种应用程序/框架之间的重要边界。 RM 会将集群状态的全局模型与正在运行的应用程序报告的资源需求摘要进行匹配。 这使得严格执行全局调度属性成为可能(YARN 中的不同调度器关注不同的全局属性，例如容量或公平性)，但它需要调度器准确了解应用程序的资源需求。 通信消息和调度程序状态必须紧凑且高效，以便 RM 能够根据应用程序需求和集群大小进行扩展 [R1]。 捕获资源请求的方式在捕获资源需求的准确性和紧凑性之间取得了平衡。幸运的是，调度程序只处理每个应用程序的整体资源配置文件，而忽略了本地优化和内部应用程序流。 幸运的是，调度程序只处理每个应用程序的整体资源配置文件，而忽略了本地优化和内部应用程序流。 YARN 完全脱离了 map 和 reduce 的静态资源划分；它将集群资源视为(离散化的)连续体 [R9]——正如我们将在第 4.1 节中看到的，这显着提高了集群利用率。 ApplicationMaster 将他们对资源的需求编码为一个或多个 ResourceRequests，每个资源请求跟踪如下内容： 容器数量(例如 200个) 每个容器所需的资源(2 GB RAM, 1 CPU，可扩展) 地点偏好 应用程序中请求的优先级 ResourceRequests 旨在允许用户捕获他们需求的完整细节和/或它的汇总版本（例如，可以指定节点级别、机架级别和全局位置首选项 [R4])。这允许将更统一的请求紧凑地表示为聚合。 此外，这种设计将允许我们对 ResourceRequests 施加大小限制，从而利用 ResourceRequests 的汇总特性对应用程序首选项进行有损压缩。 这使得调度程序可以有效地通信和存储此类请求，并允许应用程序清楚地表达他们的需求 [R1,R5,R9]。 如果无法实现完美的局部性，这些请求的汇总性质还可以指导调度程序朝着好的替代方案(例如，如果所需节点繁忙，则在该机架本地分配)。 此外，这种设计将允许我们对 ResourceRequests 施加大小限制，从而利用 ResourceRequests 的汇总特性对应用程序首选项进行有损压缩。 这种资源模型在同类环境中很好地服务于当前的应用程序，但我们预计它会随着生态系统的成熟和新需求的出现而随着时间的推移而发展。 最近和正在开发的扩展包括：显式跟踪帮派调度需求，以及软/硬约束来表达任意的协同定位或不相交的放置。 调度程序使用可用资源跟踪、更新和满足这些请求，如 NM 心跳所通告的那样。为响应 AM 请求，RM 生成容器以及授予对资源访问权限的令牌。 RM 将 NM 报告的成品容器的退出状态转发给负责的 AM。当新的 NM 加入集群时，AM 也会收到通知，以便它们可以开始请求新节点上的资源。 该协议的最新扩展允许 RM 对称地从应用程序请求返回资源。 这通常发生在集群资源变得稀缺并且调度程序决定撤销分配给应用程序以维护调度不变量的一些资源时。 我们使用类似于 ResourceRequests 的结构来捕获位置偏好(可以是严格的或可协商的)。 AM 在满足此类“抢占”请求时具有一定的灵活性，例如，通过生成对其工作不太重要的容器(例如，到目前为止仅取得很小进展的任务)，通过检查任务的状态，或通过迁移计算到其他正在运行的容器。 总体而言，这允许应用程序保留工作，与强制终止容器以满足资源限制的平台形成对比。 如果应用是非协作的，RM 可以在等待一定时间后，通过通知 NM 强制终止容器来获取所需的资源。 考虑到第 2 节中的预设要求，指出 ResourceManager 不负责的内容很重要。 如前所述，它不负责协调应用程序执行或任务容错，但也不负责 1) 提供运行应用程序的状态或指标(现在是 ApplicationMaster 的一部分)，也不负责 2) 提供已完成作业的框架特定报告(现在委托给每个框架的守护进程)。 这与 ResourceManager 应该只处理实时资源调度的观点一致，并帮助 YARN 中的中心组件扩展到 Hadoop 1.0 JobTracker 之外。 3.3 Application Master (AM) 应用程序可能是一组静态进程、工作的逻辑描述，甚至是长期运行的服务。 ApplicationMaster 是协调应用程序在集群中执行的进程，但它本身就像任何其他容器一样在集群中运行。 RM 的一个组件为容器协商以产生这个引导过程。 AM 定期向 RM 发出心跳以确认其活跃度并更新其需求记录。在构建其需求模型后，AM 在发送给 RM 的心跳消息中对其偏好和约束进行编码。 作为对后续心跳的响应，AM 将收到绑定到集群中特定节点的资源包的容器租用。根据从 RM 接收到的容器，AM 可以更新其执行计划以适应感知的丰富性或稀缺性。 与某些资源模型相反，对应用程序的分配是后期绑定的：产生的进程不绑定到请求，而是绑定到租约。 导致 AM 发出请求的条件在收到其资源时可能不会保持正确，但容器的语义是可替代的且特定于框架的 [R3,R8,R10]。 AM 还将向 RM 更新其资源请求，因为它收到的容器会影响其当前和未来的需求。 举例来说，MapReduce AM 优化了具有相同资源需求的 map 任务之间的局部性。在 HDFS 上运行时，每个输入数据块都在 k 台机器上复制。 当 AM 接收到一个容器时，它将它与一组挂起的 map 任务进行匹配，选择一个输入数据接近容器的任务。 如果 AM 决定在容器中运行 map 任务 mi，那么存储 mi 输入数据副本的主机就不太理想了。AM 将更新其请求以减少其他 k-1 台主机的权重。 主机之间的这种关系对 RM 来说仍然是不透明的；同样，如果 mi 失败，AM 负责更新其需求以进行补偿。 在 MapReduce 的情况下，请注意 Hadoop JobTracker 提供的一些服务——例如通过 RPC 的作业进度、状态的 Web 界面、对 MapReduce 特定历史数据的访问——不再是 YARN 架构的一部分。 这些服务要么由 ApplicationMaster 提供，要么由框架守护进程提供。 由于 RM 不解释容器状态，AM 通过 RM 确定 NM 报告的容器退出状态成功或失败的语义。由于 AM 本身就是一个运行在不可靠硬件集群中的容器，因此它应该能够抵御故障。 YARN 为恢复提供了一些支持，但由于容错和应用程序语义是如此紧密地交织在一起，所以大部分负担落在了 AM 上。我们在 3.6 节讨论容错模型。 3.4 Node Manager (NM) NodeManager 是 YARN 中的“worker”守护进程。它验证容器租约，管理容器的依赖关系，监控它们的执行，并为容器提供一组服务。 操作员将其配置为报告此节点上可用并分配给 YARN 的内存、CPU 和其他资源。向 RM 注册后，NM 其状态放入心跳数据中并接收指令。 YARN 中的所有容器(包括 AM)都由容器启动上下文 (CLC) 描述。 该记录包括环境变量映射、存储在可远程访问的存储中的依赖项、安全令牌、NM 服务的有效负载以及创建进程所需的命令。 在验证租约的真实性后 [R7]，NM 为容器配置环境，包括使用租约中指定的资源约束初始化其监控子系统。 为了启动容器，NM 将所有必要的依赖项——数据文件、可执行文件、tarball——复制到本地存储。如果需要，CLC 还包括验证下载的凭据。 依赖关系可以在应用程序中的容器之间、由同一租户启动的容器之间、甚至在租户之间共享，如 CLC 中所指定。 NM 最终会运行垃圾收集机制检查容器未使用的依赖项。 NM 还将按照 RM 或 AM 的指示杀死容器。 当 RM 报告其拥有的应用程序已完成时，当调度程序决定为另一个租户驱逐它时，或者当 NM 检测到容器超出其租用限制 [R2,R3,R7] 时，容器可能会被终止。 当不再需要相应的工作时，AM 可能会要求杀死容器。每当容器退出时，NM 将清理其在本地存储中的工作目录。 当应用程序完成时，其容器拥有的所有资源在所有节点上都将被丢弃，包括仍在集群中运行的任何进程。 NM 还定期监视物理节点的健康状况。它监视本地磁盘的任何问题，并经常运行管理员配置的脚本，该脚本反过来可以指向任何硬件/软件问题。 当发现这样的问题时，NM 将其状态更改为不健康，并报告 RM 大致相同的状态，然后做出调度程序特定的决定，在此节点上终止容器和/或停止未来分配，直到健康问题得到解决。 除了上述之外，NM 还为在该节点上运行的容器提供本地服务。 例如，当前的实现包括一个日志聚合服务，一旦应用程序完成，它将把应用程序写入的 stdout 和 stderr 数据上传到 HDFS。 最后，管理员可以使用一组可插入的辅助服务来配置 NM。虽然容器在退出后的本地存储将被清理，但允许提升一些输出以保留直到应用程序退出。 通过这种方式，进程可能会产生在容器生命周期之外持续存在的数据，由节点管理。 这些服务的一个重要用例是 Hadoop MapReduce 应用程序，其中使用辅助服务在 map 和 reduce 任务之间传输中间数据。 如前所述，CLC 允许 AM 将有效载荷寻址到辅助服务； MapReduce 应用程序使用此通道将验证 reduce 任务的令牌传递给 shuffle 服务。 3.5 YARN 框架/面向程序编写者 从前面对核心架构的描述中，我们提取出一个 YARN 应用程序作者的职责： 通过将 ApplicationMaster 的 CLC 传递给 RM 来提交应用程序。 当 RM 启动 AM 时，它应该向 RM 注册并定期通过心跳协议通告其活跃度和要求。 一旦 RM 分配了一个容器，AM 就可以构建一个 CLC 在相应的 NM 上启动容器。 它还可以监视正在运行的容器的状态，并在需要回收资源时停止它。 监控容器内完成的工作进度是 AM 的职责。 一旦 AM 完成其工作，它应该从 RM 注销并干净地退出。 (可选)框架作者可以在他们自己的客户端之间添加控制流以报告作业状态并公开控制平面。 即使是简单的 AM 也可能相当复杂。一个具有少量特性的分布式 shell 示例是 450 多行 Java。存在简化 YARN 应用程序开发的框架。我们将在 4.2 节中探讨其中的一些。 客户端库 - YarnClient、NMClient、AMRMClient - 与 YARN 一起提供并公开更高级别的 API，以避免针对低级别协议进行编码。增强抗故障能力的 AM 是非常重要的——包括它自己的故障。 如果应用程序公开服务或连接通信图，它也负责其安全操作的所有方面；YARN 仅保护其部署。 3.6 可靠性和容错 从一开始，Hadoop 就被设计为在商品硬件上运行。通过在其堆栈的每一层中构建容错，它向用户隐藏了从硬件故障中检测和恢复的复杂性。 YARN 继承了这一理念，尽管现在职责分布在集群中运行的 ResourceManager 和 ApplicationMaster 之间。 在撰写本文时，RM 仍然是 YARN 架构中的单点故障。RM 通过在初始化时从持久存储恢复其状态来从其自身的故障中恢复。 恢复过程完成后，它会杀死集群中运行的所有容器，包括实时 ApplicationMaster。然后它启动每个 AM 的新实例。 如果框架支持恢复——并且大多数会，为了常规容错——平台将自动恢复用户的管道 [R6]。 正在为 AM 添加足够的协议支持以在 RM 重启后继续工作。这样，当 RM 关闭时，AM 可以继续使用现有容器进行处理，并在 RM 恢复时与 RM 重新同步。 还在努力通过将 RM 被动/主动故障转移到备用节点来解决 YARN 集群的高可用性。 当 NM 发生故障时，RM 通过超时其心跳响应来检测它，将在该节点上运行的所有容器标记为已终止，并向所有正在运行的 AM 报告故障。 如果故障是暂时性的，NM将与RM重新同步，清理其本地状态，然后继续。 在这两种情况下，AM 负责对节点故障做出反应，可能会在故障期间重做由在该节点上运行的任何容器完成的工作。 由于 AM 在集群中运行，它的故障不会影响集群的可用性 [R6,R8]，但是由于 AM 故障导致应用程序暂停的概率比 Hadoop 1.x 中要高。 RM 可能会在 AM 失败时重新启动 AM，但该平台不提供恢复 AM 状态的支持。重新启动的 AM 与其自己运行的容器同步也不是平台关注的问题。 例如，Hadoop MapReduce AM 将恢复其已完成的任务，但在撰写本文时，正在运行的任务——以及在 AM 恢复期间完成的任务——将被终止并重新运行。 最后，容器本身的故障处理完全留给框架。RM 从 NM 收集所有容器退出事件，并在心跳响应中将这些事件传播到相应的 AM。 MapReduce ApplicationMaster 已经侦听了这些通知，并通过从 RM 请求新容器来重试 map 或 reduce 任务。 至此，我们结束了对架构的描述，并深入探讨了 YARN 的实际执行步骤。 4 在现实中的 YARN 我们知道在几家大公司都在积极使用 YARN。以下报告我们在雅虎运行 YARN 的经验。然后我们接着讨论一些已经移植到 YARN 的流行框架。 4.1 雅虎使用 YARN 的方式 雅虎将其生产集群从经典 Hadoop 的稳定分支之一升级到 YARN。 我们在下面报告的统计数据与经典Hadoop升级前的最后30天有关，以及从升级时间(每个集群不同)到2013年6月13日的平均统计数据。 解释以下统计数据的一个重要警告：我们即将展示的结果来自大型集群的真实升级体验，其中重点是保持关键共享资源的可用性，而不是科学的可重复性/一致性，就像在典型的合成实验中一样。 为了帮助读者轻松解释结果，我们将报告一些全局统计数据，然后重点关注升级前后硬件(大部分)没有改变的大型集群，最后表征该特定集群上的工作负载转移。 4.1.1 YARN 跨集群 总之，升级后，在所有集群中，YARN 每天处理约 500,000 个作业，每天处理总计超过 230 个计算年(compute-year)。底层存储超过 350 PB。 虽然 YARN 的最初目标之一是提高可扩展性，但 Yahoo 报告说他们没有运行任何超过 4000 个节点的集群，这在 YARN 之前曾经是最大的集群规模。 YARN 提供的资源利用率的显着提高增加了每个集群可以维持的工作数量。这暂时消除了进一步扩展的需要，甚至允许运营团队推迟重新配置已退役的 7000 多个节点。 另一方面，似乎 YARN 的日志聚合组件增加了 HDFS NameNode 的压力，尤其是对于大型作业。NameNode 现在估计是雅虎集群的可扩展性瓶颈。 幸运的是，社区正在进行大量工作，以通过优化 YARN 日志聚合来提高 NameNode 吞吐量并限制 NameNode 的压力。 在一些具有大量小型应用程序的大型集群上观察到 YARN 中的可扩展性问题，但最近在心跳处理方面的改进已经缓解了其中的一些问题。 4.1.2 特定集群的统计信息 我们现在将关注雅虎集群运营团队在一个特定的 2500 个节点集群上运行 YARN 的经验和统计数据。 图 2 显示了运维团队在 2500 台机器集群上运行时的负载前后情况。这是雅虎最繁忙的集群，它一直在接近其极限。 在图 2a 中，我们显示运行的作业数量显着增加：从 1.0 版 Hadoop 上的约 77k 增加到 YARN 上的约 100k 定期作业。 此外，该集群实现了每天 12.5 万个作业的持续吞吐量，峰值约为每天 15 万个作业(或几乎是该集群之前舒适区的作业数量的两倍)。 平均作业大小从 58 个 map、20 个 reduce 增加到 85 个 map、16 个 reduce - 请注意，这些作业包括用户作业以及由系统应用程序产生的其他小作业，例如通过 distcp、Oozie [19] 和其他的数据复制/传输数据管理框架。 同样，所有作业的任务数量从 Hadoop 1.0 上的 4M 增加到 YARN 上的平均约 10M，持续负载约为 12M，峰值约为 15M——见图 2。 评估集群管理系统效率的另一个关键指标是平均资源利用率。同样，工作负载的变化使我们将要讨论的统计数据对直接比较的用处不大，但我们观察到 CPU 利用率显着增加。 在 Hadoop 1.0 上，估计的 CPU 利用率 11 徘徊在 320% 左右或 3.2/16 个内核，与 2500 台机器中的每台机器挂钩。 从本质上讲，转向 YARN，CPU 利用率几乎翻了一番，相当于每盒 6 个连续挂钩的核心，峰值高达 10 个充分利用的核心。总的来说，这表明 YARN 能够保持大约 2.8*2500 = 7000 个以上的内核完全忙于运行用户代码。 这与我们上面讨论的集群上运行的作业和任务数量的增加是一致的。部分解释这些改进的最重要的架构差异之一是删除了 map 和 reduce 插槽之间的静态拆分。 在图 3 中，我们绘制了随时间变化的几个作业统计信息：并发运行和挂起的容器、已提交、已完成、正在运行和挂起的作业。这显示了资源管理器处理大量应用程序、容器请求和执行的能力。 此集群中使用的 CapacityScheduler 版本尚未使用抢占，因为这是最近添加的功能。 虽然抢占尚未进行大规模测试，但我们相信谨慎使用抢占将显着提高集群利用率，我们在第 5.3 节中展示了一个简单的微基准测试。 为了进一步表征在该集群上运行的工作负载的性质，我们在图 4 中展示了不同规模应用程序的直方图，描述了：1) 每个桶中的应用程序总数，2) 该桶中应用程序使用的 CPU 总量，和 3) 该存储桶中的应用程序使用的容器总数。 正如过去所观察到的，虽然大量应用程序非常小，但它们只占集群容量的一小部分(在这个 2500 台机器集群中，大约有 3.5 台机器的 CPU)。 有趣的是，如果我们比较插槽利用率与 CPU 利用率，我们会发现大型作业似乎为每个容器使用了更多的 CPU。 这与更好地调整大型作业(例如，一致使用压缩)以及可能运行更长时间的任务一致，从而分摊启动开销。 总体而言，雅虎报告称，为了将 YARN 强化为生产就绪、可扩展的系统而进行的工程工作非常值得。计划升级到最新版本 2.1-beta，继续快速升级。 经过超过 36,000 年的汇总计算时间和几个月的生产压力测试，雅虎的运营团队确认 YARN 的架构转变对其工作负载非常有益。 这在下面的引用中得到了很好的总结：“升级到 YARN 相当于添加 1000 台机器(到这个 2500 台机器集群)”。 4.2 应用程序和框架 YARN 的一个关键要求是实现更大的编程模型灵活性。尽管在撰写本文时 YARN 处于新的测试状态，但 YARN 已经吸引的许多编程框架已经验证了这一点。我们简要总结了一些 YARN 原生或移植到平台的项目，以说明其架构的通用性。 Apache Hadoop MapReduce 已经在具有几乎相同功能集的 YARN 之上运行。 它经过大规模测试，其余生态系统项目(如 Pig、Hive、Oozie 等)经过修改以在 YARN 上的 MR 之上工作，以及与经典 Hadoop 相比性能相当或更好的标准基准测试。 MapReduce 社区已确保针对 1.x 编写的应用程序可以以完全二进制兼容的方式(mapred API)或仅通过重新编译(mapreduce API 的源代码兼容性)在 YARN 之上运行。 Apache Tez 是一个 Apache 项目(在撰写本文时为孵化器)，旨在提供一个通用的有向循环图(DAG) 执行框架。 它的目标之一是提供一组可以组合成任意 DAG 的构建块(包括一个简单的 2 阶段(Map 和 Reduce) DAG，以保持与 MapReduce 的兼容性)。 Tez 为 Hive 和 Pig 等查询执行系统提供了更自然的执行计划模型，而不是强制将这些计划转换为 MapReduce。 当前的重点是加速复杂的 Hive 和 Pig 查询，这些查询通常需要多个 MapReduce 作业，允许作为单个 Tez 作业运行。 未来将考虑丰富的功能，例如对交互式查询的通用支持和通用 DAG。 Spark 是加州大学伯克利分校 [32] 的一个开源研究项目，其目标是机器学习和交互式查询工作负载。 利用弹性分布式数据集 (RDD) 的中心思想来实现对此类应用程序的经典 MapReduce 的显着性能改进。 spark 最近已被移植到 YARN [?]。 Dryad [18] 提供了 DAG 作为执行流的抽象，并且已经与 LINQ [31] 集成。 移植到 YARN 的版本是 100% 本地 C++ 和 C# 用于工作节点，而 ApplicationMaster 利用 Java 的薄层与本地 Dryad 图管理器周围的 ResourceManager 接口。 最终，Java 层将被与协议缓冲区接口的直接交互所取代。 Dryad-on-YARN 与其非 YARN 版本完全兼容。 Giraph 是一个高度可扩展、以顶点为中心的图计算框架。它最初设计为在 Hadoop 1.0 之上作为仅 map 作业运行，其中一个 map 是特殊的并且充当协调器。 Giraph 到 YARN 的接口很自然，执行协调器的角色由 ApplicationMaster 承担，资源是动态请求的。 Storm 是一种开源分布式实时处理引擎，旨在跨机器集群进行扩展并提供并行流处理。一个常见的用例结合了用于在线计算的 Storm 和作为批处理器的 MapReduce。 通过在 YARN 上移植 Storm，可以解除资源分配的极大灵活性。此外，对底层 HDFS 存储层的共享访问简化了多框架工作负载的设计。 REEF 元框架：YARN 的灵活性为应用程序实现者带来了潜在的巨大努力。编写 ApplicationMaster 并处理容错、执行流程、协调等各个方面的工作是一项不平凡的工作。 REEF 项目 [10] 认识到了这一点，并排除了许多应用程序通用的几个难以构建的组件。这包括存储管理、缓存、故障检测、检查点、基于推送的控制流(稍后通过实验展示)和容器重用。 框架设计者可以在 REEF 之上构建，比直接在 YARN 上构建更容易，并且可以重用 REEF 提供的许多公共服务/库。REEF 设计使其适用于 MapReduce 和 DAG 之类的执行以及迭代和交互式计算。 Hoya 是一个 Java 工具，旨在利用 YARN 按需启动动态 HBase 集群 [21]。 在 YARN 上运行的 HBase 集群也可以动态增长和收缩(在我们的测试集群中，可以在不到 20 秒的时间内添加/删除 RegionServers)。 虽然仍在探索在 YARN 中混合服务和批处理工作负载的影响，但该项目的早期结果令人鼓舞。 5 实验 在上一节中，我们通过报告大型生产部署和蓬勃发展的框架生态系统，确立了 YARN 在现实世界中的成功。在本节中，我们将展示更具体的实验结果来展示 YARN 的一些胜利。 5.1 打破排序记录 在撰写本文时，YARN 上的 MapReduce 实现正式 12 持有 Daytona 和 Indy GraySort 基准记录，排序 1.42TB/分钟。 同一系统还报告(在比赛之外)MinuteSort 结果在一分钟内排序 1.61TB 和 1.50TB，优于当前记录。 实验在 2100 个节点上运行，每个节点配备两个 2.3Ghz hexcore Xeon E5-2630、64 GB 内存和 12x3TB 磁盘。结果总结如下表所示： 测试方式 数据类型 数据大小 耗费时间 速率 Daytona GraySort no-skew 102.5 TB 72min 1.42TB/min Daytona GraySort skew 102.5 TB 117min 0.87TB/min Daytona MinuteSort no-skew 11497.86 GB 87.242 sec - Daytona MinuteSort skew 1497.86 GB 59.223 sec - Indt MinuteSort no-skew 1612.22 GB 58.027 sec - 完整的报告 [16] 详细描述了此实验。 5.2 MapReduce 基准测试 MapReduce 仍然是 YARN 之上最重要和最常用的应用程序。 我们很高兴地报告，与当前稳定的 Hadoop-1 版本 1.2.1 相比，大多数标准 Hadoop 基准测试在 Hadoop 2.1.0 中的 YARN 上表现更好。 下面提供了比较 1.2.1 和 2.1.0 的 260 节点集群上的 MapReduce 基准测试摘要。 每个从节点都运行 2.27GHz Intel® Xeon® CPU，共 16 个内核，具有 38GB 物理内存，每个 6x1TB 7200 RPM 磁盘，格式化为 ext3 文件系统。 每个节点的网络带宽为 1Gb/秒。 每个节点运行一个 DataNode 和一个 NodeManager，为容器分配 24GB RAM。 我们在 1.2.1 中运行 6 个 map 和 3 个 reduce，在 2.1.0 中运行 9 个容器。每个 map 占用 1.5GB JVM heap 和 2GB 总内存，而每个 reduce 占用 3GB heap 总共 4GB。 JobTracker/ResourceManager 在专用机器上运行，HDFS NameNode 也是如此。 我们将这些基准测试的结果解释如下。排序基准测试使用默认设置测量 HDFS 中 1.52 TB(每个节点 6GB)排序的端到端时间。 shuffle 基准校准仅使用合成数据将 m 个 map 的中间输出混洗到 n 个 reduce 的速度；记录既不从 HDFS 读取也不写入到 HDFS 的时间。 虽然排序基准通常会从 HDFS 数据路径的改进中受益，但这两个基准在 YARN 上的表现更好，主要是由于 MapReduce 运行时本身的显着改进：映射端排序改进，减少了对映射输出进行管道和批量传输的客户端，以及基于 Netty [3] 的服务器端 shuffle。 扫描和 DFSIO 作业是用于评估在 Hadoop MapReduce 下运行的 HDFS 和其他分布式文件系统的规范基准；表 1 中的结果是对我们实验中 HDFS 影响的粗略测量。我们对集群的访问时间太短，无法调试和表征 2.1.0 文件系统的中等性能。 尽管有这种影响，尽管 YARN 的设计针对多租户吞吐量进行了优化，但它的单个作业的性能与中央协调器相比具有竞争力。 测试方式 1.2.1 平均运行时长(s) 2.1.0 平均运行时长(s) 1.2.1 吞吐量(GB/s) 2.1.0 吞吐量(GB/s) RandomWriter 222 228 7.03 6.84 Sort 475 398 3.28 3.92 Shuffle 951 648 - - AM Scalability 1020 353/303 - - Terasort 175.7 215.7 5.69 4.64 Scan 59 65 - - Read DFSIO 50.8 58.6 - - Write DFSIO 50.82 57.74 - - 表 1：来自规范 Hadoop 基准测试的结果 AM 可扩展性基准测试通过使 AM 充满容器簿记职责来衡量单个作业的稳健性。表 1 包括 MapReduce AM 的两个测量值。 第一个实验限制可用资源以匹配 1.x 部署可用的插槽。当我们移除这个人为限制并允许 YARN 使用完整节点时，其性能会显着提高。 这两个实验估计了类型插槽的开销。我们还将改进的性能归因于更频繁的节点心跳和更快的调度周期，我们将在下面更详细地讨论。 由于 YARN 主要负责分发和启动应用程序，我们认为可扩展性基准是一个关键指标。 YARN 中的一些架构选择针对我们在生产集群中观察到的瓶颈。 如第 2.3 节所述，类型槽会在节点上的可替代资源供应与执行 Hadoop MapReduce 任务的语义之间造成人为的不匹配，从而损害吞吐量。 虽然第 4.1 节涵盖了聚合工作负载的收益，但我们看到了调度甚至单个作业的好处。 我们将大部分收益归因于改进的心跳处理。在 Hadoop-1 中，由于 JobTracker 中的粗粒度锁定，每个节点在大型集群中只能每 30-40 秒心跳一次。 尽管有降低延迟的巧妙解决方法，例如用于处理轻量级更新的短路路径和用于加速重要通知的自适应延迟，但 JobTracker 中的协调状态仍然是大型集群中延迟的主要原因。 相比之下，NodeManager 每 1-3 秒发送一次心跳。RM 代码更具可扩展性，但它也解决了每个请求处理一组更简单的约束。 5.3 抢占的好处 在图 5 中，我们展示了 YARN 中最近添加的一项功能：使用工作保留抢占来强制执行全局属性的能力。我们在一个小型（10 台机器）集群上进行了实验，以突出工作保留抢占的潜在影响。 集群运行 CapacityScheduler，配置了两个队列 A 和 B，分别享有 80% 和 20% 的容量。一个 MapReduce 作业在较小的队列 B 中提交，几分钟后另一个 MapReduce 作业在较大的队列 A 中提交。 在图中，我们显示了在三种配置下分配给每个队列的容量：1) 不向队列提供超出其保证的容量(固定容量) 2) 队列可能会消耗 100% 的集群容量，但不执行抢占，3) 队列可能会消耗 100% 的集群容量，但容器可能会被抢占。 工作保护抢占允许调度程序为队列 B 过度使用资源，而不必担心队列 A 中的应用程序会缺失资源而运行失败。 当队列 A 中的应用程序请求资源时，调度程序发出抢占请求，由 ApplicationMaster 通过检查点其任务和让出容器来提供服务。 这允许队列 A 在几秒钟内获得其所有保证容量(集群的 80%)，而不是在情况 (2) 中，容量重新平衡需要大约 20 分钟。 最后，由于我们使用的抢占是基于检查点的并且不会浪费工作，因此在 B 中运行的作业可以从它们停止的地方重新启动任务，并且这样做很有效。 5.4 Apache Tez 相关的改进 在针对 Apache Tez 运行的 Apache Hive 上运行决策支持查询时，我们提出了一些基本改进(在撰写本文时处于早期阶段的集成)。 来自 TPC-DS 基准测试 [23] 的查询 12，涉及很少的连接、过滤器和按聚合分组。即使在积极的计划级别优化之后，Hive 在使用 MapReduce 时也会生成由多个作业组成的执行计划。 在针对 Tez 执行时，相同的查询会产生线性 DAG，单个 Map 阶段后跟多个 Reduce 阶段。 使用 MapReduce 在 20 节点集群上针对 200 个比例因子数据时，查询执行时间为 54 秒，而使用 Tez 时，查询执行时间提高到 31 秒。 大部分节省可归因于调度和启动多个 MapReduce 作业的开销，以及避免将中间 MapReduce 作业的输出持久化到 HDFS 的不必要步骤。 5.5 REEF：使用 session 降低延迟 YARN 的关键方面之一是它使构建在它之上的框架能够按照他们认为合适的方式管理容器和通信。 我们通过利用 REEF 提供的容器重用和基于推送的通信的概念来展示这一点。该实验基于一个构建在 REEF 之上的简单分布式 shell 应用程序。 我们在提交一系列 UNIX 命令(例如日期)时测量完全空闲集群上的客户端延迟。 发出的第一个命令会产生调度应用程序和获取容器的全部成本，而后续命令会通过 Client 和 ApplicationMaster 快速转发到已经运行的容器以供执行。基于推送的消息传递进一步减少了延迟。 我们实验中的加速非常显着，接近三个数量级—— 从平均超过 12 秒到 31 毫秒。 6 相关工作 其他人已经认识到经典 Hadoop 架构中的相同局限性，并同时开发了替代解决方案，可以与 YARN 进行密切比较。 在众多与 YARN 最相似的相关工作中，有：Mesos [17]、Omega [26]、Corona [14] 和 Cosmos [8]，分别由 Twitter、Google、Facebook 和 Microsoft 维护和使用。 这些系统共享一个共同的灵感，以及提高可扩展性、延迟和编程模型灵活性的高级目标。许多架构差异反映了不同的设计优先级，有时只是不同历史背景的影响。 虽然无法提供真正的定量比较，但我们将尝试强调一些架构差异和我们对它们的基本原理的理解。 Omega 的设计更倾向于分布式、多级调度。 这反映了对可扩展性的更多关注，但使得执行全局属性(例如容量/公平性/截止时间)变得更加困难。 为了这个目标，作者似乎依赖于在运行时相互尊重的各种框架的协调开发。 这对于像谷歌这样的封闭世界来说是明智的，但不适用于像 Hadoop 这样的开放平台，在那里来自不同独立来源的任意框架共享同一个集群。 Corona 使用基于推送的通信，而不是 YARN 和其他框架中基于心跳的控制平面框架方法。延迟/可扩展性权衡非常重要，值得进行详细比较。 虽然 Mesos 和 YARN 都有两个级别的调度程序，但有两个非常显着的差异。首先，Mesos 是一个基于提案的资源管理器，而 YARN 有一个基于请求的实现方式。 YARN 允许 AM 根据包括位置在内的各种标准请求资源，允许请求者根据给定的内容和当前使用情况修改未来的请求。我们的方法对于支持基于位置的分配是必要的。 其次，Mesos 利用中央调度器池(例如，经典 Hadoop 或 MPI)，而不是每个作业的框架内调度器。 YARN 支持将容器后期绑定到任务，其中每个单独的作业可以执行本地优化，并且似乎更适合滚动升级(因为每个作业可以在不同版本的框架上运行)。 另一方面，每个作业的 ApplicationMaster 可能会导致比 Mesos 方法更大的开销。 Cosmos 在存储和计算层的架构上与 Hadoop 2.0 非常相似，主要区别在于没有中央资源管理器。然而，它似乎只用于单一的应用程序类型：Scope [8]。 凭借更窄的目标，Cosmos 可以利用许多优化，例如本机压缩、索引文件、数据集分区的协同定位来加速 Scope。多个应用程序框架的行为不能清楚的定义。 在最近的这些努力之前，资源管理和调度方面的工作有着悠久的历史 - Condor [29]、Torque [7]、Moab [13] 和 Maui [20]。 我们早期的 Hadoop 集群使用了其中一些系统，但我们发现它们无法以一流的方式支持 MapReduce 模型。 具体来说，map 和 reduce 阶段的数据局部性和弹性调度需求都无法表达，因此被迫分配“虚拟”Hadoop，伴随着第 2.1 节中讨论的使用成本。 也许其中一些问题是由于许多这些分布式调度程序最初是为了支持 MPI 风格和 HPC 应用程序模型以及运行粗粒度的非弹性工作负载而创建的。 这些集群调度器确实允许客户端指定处理环境的类型，但不幸的是不能指定 Hadoop 的一个关键问题的位置约束。 另一类相关技术来自云基础架构领域，例如 EC2、Azure、Eucalyptus 和 VMWare 产品。这些主要针对基于 VM 的集群共享，并且通常设计用于长时间运行的进程(因为 VM 启动时间开销过高)。 7 结论 在本文中，我们总结了对 Hadoop 历史的回忆，并讨论了广泛采用和新型应用程序如何推动初始架构远远超出其设计目标。 然后，我们描述了导致 YARN 的进化但深刻的架构转型。 由于资源管理和编程框架的解耦，YARN 提供：1) 更大的可扩展性，2) 更高的效率，以及 3) 使大量不同的框架能够高效地共享一个集群。 这些说法通过实验（通过基准测试）和展示雅虎的大规模生产经验(现在 100% 在 YARN 上运行)得到证实。 最后，我们试图通过提供社区活动的快照并简要报告已移植到 YARN 的许多框架来捕捉围绕该平台的极大兴奋。 我们相信 YARN 既可以作为一个坚实的生产框架，也可以作为研究社区的宝贵游乐场。 8 致谢 123456789101112We began our exposition of YARN by acknowledging the pedigree of its architecture.Its debt to all the individuals who developed, operated, tested, supported, documented, evangelized, funded,and most of all, used Apache Hadoop over the years is incalculable.In closing, we recognize a handful. Ram Marti and Jitendranath Panday influenced its security design.Dick King, Krishna Ramachandran, Luke Lu, Alejandro Abdelnur, Zhijie Shen, Omkar Vinit Joshi,Jian He have made or continue to make significant contributions to the implementation.Karam Singh, Ramya Sunil, Rajesh Balamohan and Srigurunath Chakravarthi have been ever helpful in testing and performance benchmarking.Rajive Chittajallu and Koji Noguchi helped in channeling requirements and insight from operations and user support points of view respectively.We thank Yahoo! for both supporting YARN via massive contributions and heavy usage and also for sharing the statistics on some of its clusters.We thank Dennis Fetterly and Sergiy Matusevych for providing some of the experimental results on Dryad and REEF, and Mayank Bansal for helping with 2.1.0 MapReduce benchmarks.Last but not the least, Apache Hadoop YARN continues to be a community driven open source project and owes much of its success to theApache Hadoop YARN and MapReduce communities—a big thanks to all the contributors and committers who have helped YARN in every way possible. 典型的 Hadoop 调度方式 在 YARN 之前，Hadoop MapReduce 集群由称为 JobTracker (JT) 的主节点和运行 TaskTracker (TT) 的工作节点组成。 用户将 MapReduce 作业提交给 JT，后者在 TT 之间协调其执行。TT 由运营商配置，具有固定数量的映射时隙和减少时隙。 TT 定期向 JT 发送心跳，以报告该节点上正在运行的任务的状态并确认其活跃度。 在心跳中，JT 更新其与该节点上运行的任务相对应的状态，代表该作业采取行动(例如，调度失败的任务以重新执行)，将该节点上的空闲槽与调度程序不变量匹配，匹配符合条件的针对可用资源的作业，有利于具有该节点本地数据的任务。 作为计算集群的中央仲裁者，JT 还负责准入控制、跟踪 TT 的活跃度(重新执行正在运行的任务或输出变得不可用的任务)、 推测性地启动任务以绕过慢节点、报告作业状态 通过网络服务器向用户提供，记录审计日志和汇总统计数据，对用户进行身份验证等多项功能；每一个都限制了它的可扩展性。 参考资料 [1] Apache hadoop. http://hadoop.apache.org. [2] Apache tez. http://incubator.apache.org/projects/tez.html. [3] Netty project. http://netty.io. [4] Storm. http://storm-project.net/. [5] H. Ballani, P. Costa, T. Karagiannis, and A. I. Rowstron. Towards predictable datacenter networks. In SIGCOMM, volume 11, pages 242–253, 2011. [6] F. P. Brooks, Jr. The mythical man-month (anniversary ed.). Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1995. [7] N. Capit, G. Da Costa, Y. Georgiou, G. Huard, C. Martin, G. Mounie, P. Neyron, and O. Richard. A batch scheduler with high level components. In Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on, volume 2, pages 776–783 Vol. 2, 2005. [8] R. Chaiken, B. Jenkins, P.-A. Larson, B. Ramsey, D. Shakib, S. Weaver, and J. Zhou. Scope: easy and efficient parallel processing of massive data sets. Proc. VLDB Endow., 1(2):1265–1276, Aug. 2008. [9] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica. Managing data transfers in computer clusters with orchestra. SIGCOMMComputer Communication Review, 41(4):98, 2011. [10] B.-G. Chun, T. Condie, C. Curino, R. Ramakrishnan, R. Sears, and M. Weimer. Reef: Retainable evaluator execution framework. In VLDB 2013, Demo, 2013. [11] B. F. Cooper, E. Baldeschwieler, R. Fonseca, J. J. Kistler, P. Narayan, C. Neerdaels, T. Negrin, R. Ramakrishnan, A. Silberstein, U. Srivastava, et al. Building a cloud for Yahoo! IEEE Data Eng. Bull., 32(1):36–43, 2009. [12] J. Dean and S. Ghemawat. MapReduce: simplified data processing on large clusters. Commun. ACM, 51(1):107–113, Jan. 2008. [13] W. Emeneker, D. Jackson, J. Butikofer, and D. Stanzione. Dynamic virtual clustering with xen and moab. In G. Min, B. Martino, L. Yang, M. Guo, and G. Rnger, editors, Frontiers of High Performance Computing and Networking, ISPA 2006 Workshops, volume 4331 of Lecture Notes in Computer Science, pages 440–451. Springer Berlin Heidelberg, 2006. [14] Facebook Engineering Team. Under the Hood:Scheduling MapReduce jobs more efficiently with Corona. http://on.fb.me/TxUsYN, 2012. [15] D. Gottfrid. Self-service prorated super-computing fun. http://open.blogs.nytimes.com/2007/11/01/self-service-prorated-super-computing-fun,2007. [16] T. Graves. GraySort and MinuteSort at Yahoo on Hadoop 0.23. http://sortbenchmark.org/Yahoo2013Sort.pdf, 2013. [17] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. Katz, S. Shenker, and I. Stoica. Mesos: a platform for fine-grained resource sharing in the data center. In Proceedings of the 8th USENIX conference on Networked systems design and implementation, NSDI’11, pages 22–22, Berkeley, CA, USA, 2011. USENIX Association. [18] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, EuroSys ’07, pages 59–72, New York, NY, USA, 2007. ACM. [19] M. Islam, A. K. Huang, M. Battisha, M. Chiang, S. Srinivasan, C. Peters, A. Neumann, and A. Abdelnur. Oozie: towards a scalable workflow management system for hadoop. In Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies, page 4. ACM, 2012. [20] D. B. Jackson, Q. Snell, and M. J. Clement. Core algorithms of the maui scheduler. In Revised Papers from the 7th International Workshop on Job Scheduling Strategies for Parallel Processing, JSSPP ’01, pages 87–102, London, UK, UK, 2001. Springer-Verlag. [21] S. Loughran, D. Das, and E. Baldeschwieler. Introducing Hoya – HBase on YARN. http://hortonworks.com/blog/introducing-hoya-hbase-on-yarn/, 2013. [22] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, SIGMOD ’10, pages 135–146, New York, NY, USA,2010. ACM. [23] R. O. Nambiar and M. Poess. The making of tpcds. In Proceedings of the 32nd international conference on Very large data bases, VLDB ’06, pages 1049–1058. VLDB Endowment, 2006. [24] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig Latin: a not-so-foreign language for data processing. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD ’08, pages 1099–1110, New York, NY, USA, 2008. ACM. [25] O. O’Malley. Hadoop: The Definitive Guide, chapter Hadoop at Yahoo!, pages 11–12. O’Reilly Media, 2012. [26] M. Schwarzkopf, A. Konwinski, M. Abd-ElMalek, and J. Wilkes. Omega: flexible, scalable schedulers for large compute clusters. In Proceedings of the 8th ACM European Conference on Computer Systems, EuroSys ’13, pages 351–364, New York, NY, USA, 2013. ACM. [27] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop Distributed File System. In Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), MSST ’10, pages 1–10, Washington, DC, USA, 2010. IEEE Computer Society. [28] T.-W. N. Sze. The two quadrillionth bit of π is 0! http://developer.yahoo.com/blogs/hadoop/twoquadrillionth-bit-0-467.html. [29] D. Thain, T. Tannenbaum, and M. Livny. Distributed computing in practice: the Condor experience. Concurrency and Computation: Practice and Experience, 17(2-4):323–356, 2005. [30] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, N. Z. 0002, S. Anthony, H. Liu, and R. Murthy. Hive - a petabyte scale data warehouse using Hadoop. In F. Li, M. M. Moro, S. Ghandeharizadeh, J. R. Haritsa, G. Weikum, M. J. Carey, F. Casati, E. Y. Chang, I. Manolescu, S. Mehrotra, U. Dayal, and V. J. Tsotras, editors, Proceedings of the 26th International Conference on Data Engineering, ICDE 2010, March 1-6, 2010, Long Beach, California, USA, pages 996–1005. IEEE,2010. [31] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. K. Gunda, and J. Currey. DryadLINQ: a system for general-purpose distributed data-parallel computing using a high-level language. In Proceedings of the 8th USENIX conference on Operating systems design and implementation, OSDI’08, pages 1–14, Berkeley, CA, USA, 2008. USENIX Association. [32] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: cluster computing with working sets. In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing, HotCloud’10, pages 10–10, Berkeley, CA, USA, 2010. USENIX Association.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"YARN","slug":"YARN","permalink":"https://wangqian0306.github.io/tags/YARN/"}]},{"title":"Large-scale cluster management at Google with Borg 中文翻译版","slug":"treatise/large-scale_cluster_management_at_google_with_borg","date":"2021-12-28T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/large-scale_cluster_management_at_google_with_borg/","permalink":"https://wangqian0306.github.io/2021/large-scale_cluster_management_at_google_with_borg/","excerpt":"","text":"Large-scale cluster management at Google with Borg 中文翻译版 作者： Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Oppenheimer, Eric Tune, John Wilkes 原版的版权说明 12345Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).EuroSys’15, April 21–24, 2015, Bordeaux, France.Copyright is held by the owner/author(s). ACM 978-1-4503-3238-5/15/04.http://dx.doi.org/10.1145/2741948.2741964 摘要 Google 的 Borg 系统是一个集群管理器，它在多个集群中运行数十万个作业，来自数千个不同的应用程序，每个集群都有多达数万台机器。 它通过将准入控制、高效任务打包、过度承诺(over-commit)和机器共享与进程级性能隔离相结合来实现高效利用资源。 它支持具有运行时功能的高可用性应用程序，这些功能可以最大限度地缩短故障恢复时间，以及减少相关故障概率的调度策略。 Borg 通过提供声明性的工作规范语言、名称服务集成、实时工作监控以及分析和模拟系统行为的工具来简化其用户的生活。 我们总结了 Borg 系统架构和特征、重要的设计决策、对其某些政策决策的定量分析，以及对从十年的操作经验中吸取的经验教训的定性检验。 1 引言 我们内部称为 Borg 的集群管理系统负责接纳、调度、启动、重启和监控 Google 运行的所有应用程序。这篇论文描述了它的工作方式。 Borg 提供了三个主要好处：(1) 隐藏了资源管理和故障处理的细节，因此它的用户可以专注于应用程序开发；(2) 以非常高的可靠性和可用性运行，并支持具有相同功能的应用程序；(3) 让我们有效地在数万台机器上运行工作负载。 Borg 不是第一个解决这些问题的系统，但它是为数不多的以这种规模运作的系统之一，具有这种程度的弹性和完整性。 本文围绕这些主题展开，最后总结了我们在生产环境中运行 Borg 十多年来所做的一系列定性观察。 2 用户视角 Borg 的用户是运行 Google 应用程序和服务的 Google 开发人员和系统管理员(站点可靠性工程师或 SRE)。 用户以作业的形式向 Borg 提交他们的工作，每个作业由一个或多个运行相同(二进制)程序的任务组成。 每个作业都在一个 Borg 单元中运行，这是一组作为一个单元进行管理的机器。 本节的其余部分描述了 Borg 用户视图中公开的主要功能。 2.1 工作负载 Borg 单元运行具有两个主要部分的异构工作负载。 第一个是长期运行的服务，它们应该“永远”不停机，并处理短期的延迟敏感请求(几微秒到几百毫秒)。 此类服务用于面向最终用户的产品，例如 Gmail、Google Docs 和网络搜索，以及内部基础设施服务(例如 BigTable)。 第二种是需要几秒钟到几天才能完成的批处理作业；这些对短期业务波动的敏感性要低得多。 工作负载组合因单元而异，这些单元根据主要租户运行不同的应用程序组合(例如，某些单元是批量密集型的)，并且还会随时间变化： 批处理作业大量运行之后我们发现许多面向最终用户的服务中出现了日间使用的模式。 Borg 需要同样好地处理所有这些情况。 可以在 2011 年 5 月的公开可用的长达一个月的跟踪中找到具有代表性的 Borg 工作负载 [80]，该跟踪已被广泛分析(例如，[68] 和 [1, 26, 27, 57]) 在过去几年中，许多应用程序框架都建立在 Borg 之上，包括我们内部的 MapReduce 系统 [23]、FlumeJava [18]、Millwheel [3] 和 Pregel [59]。 其中大多数都有一个控制器，用于提交一个主作业和一个或多个工作作业； 前两个扮演与 YARN 的应用程序管理器类似的角色 [76]。 我们的分布式存储系统，例如 GFS [34] 及其后继 CFS、Bigtable [19] 和 Megastore [8]，都在 Borg 上运行。 对于本文，我们将优先级较高的 Borg 工作归类为“生产”(prod)工作，其余归为 “非生产”(non-prod)工作。 大多数长时间运行的服务器作业都是生产工作；大多数批处理作业是非生产的。 在一个有代表性的运行单元中，生产作业分配了大约 70% 的总 CPU 资源和大约 60% 的总 CPU 使用率；它们被分配了大约 55% 的总内存并代表了大约 85% 的总内存使用量。 分配和使用之间的差异将在第 5.5 节中证明他们的重要性。 2.2 集群和运行单元 运行单元中的机器属于单个集群，由连接它们的高性能数据中心规模的网络结构定义。 集群运行在一个数据中心大楼中，一组这样的基础设施构成了一个站点1。 一个集群通常承载一个大型单元，可能有一些较小规模的测试或专用单元。 我们努力避免任何单点故障。 注 1：每种关系都有一些例外。 排除测试用的运行单元后，我们的通常的运行单元大小约为 10k 台机器；有些要大得多。 运行单元中的机器在许多方面都是异构的：大小(CPU、RAM、磁盘、网络)、处理器类型、性能和功能，例如外部 IP 地址或闪存。 Borg 通过确定在单元中运行任务的位置、分配他们的资源、安装他们的程序和其他依赖项、监控他们的健康状况以及在失败时重新启动他们，将用户与大多数这些差异隔离开来。 2.3 工作和任务 Borg 作业的属性包括它的名称、所有者和它拥有的任务数量。 作业可以有约束来强制其任务在具有特定属性的机器上运行，例如处理器架构、操作系统版本或外部 IP 地址。 约束可以是硬的，也可以是软的； 后者表现为偏好而不是要求。 一项工作的开始的方式可以是到前一项工作完成。一项作业仅在一个运行单元中运行。 每个任务都映射到在机器上的容器中运行的一组 Linux 进程 [62]。 绝大多数 Borg 工作负载不在虚拟机(VM)内运行，因为我们不想支付虚拟化成本。 此外，该系统是在我们对处理器进行大量投资的时候设计的，而硬件没有虚拟化支持。 任务也有属性，例如它的资源需求和任务在作业中的索引。 大多数任务属性在作业中的所有任务中都是相同的，但可以被覆盖——例如，提供特定于任务的命令行标志。 每个资源维度(CPU 内核、RAM、磁盘空间、磁盘访问速率、TCP 端口等)都是以细粒度独立指定的； 我们不强加固定大小的桶或槽(参见第 5.4 节)。 Borg 程序是静态链接的，以减少对其运行时环境的依赖，并将其结构化为二进制文件和数据文件的包，其安装由 Borg 编排。 用户通过向 Borg 发出远程过程调用(RPC)来操作作业，最常见的是来自命令行工具、其他 Borg 作业或我们的监控系统(参见第 2.6 节)。 大多数工作描述都是用声明性配置语言 BCL 编写的。 这是 GCL [12] 的变体，它生成 protobuf 文件 [67]，并扩展了一些 Borg 特定的关键字。 GCL 提供了 lambda 函数来允许计算，应用程序使用这些函数来调整它们的配置以适应它们的环境；成千上万的 BCL 文件都超过 1k 行，我们已经积累了数千万行的 BCL。 Borg 作业配置与 Aurora 配置文件有相似之处 [6]。 图 2 说明了作业和任务在其生命周期中经历的状态。 用户可以通过将新的作业配置推送到 Borg，然后指示 Borg 将任务更新配置来更改正在运行的作业中部分或全部任务的属性。 这是一个轻量级的非原子事务，在直到它关闭(提交)前都可以很容易地撤消。 更新通常以滚动方式完成，并且可以对更新导致的任务中断(重新安排或抢占)的数量施加限制；任何会导致更多中断的更改都会被跳过。 某些任务更新(例如，推送新的二进制文件)将始终需要重新启动任务；一些(例如，增加资源需求或改变约束)可能会使任务不再适合机器，并导致它被停止和重新安排；有些(例如，更改优先级)总是可以在不重新启动或移动任务的情况下完成。 任务可以在被 SIGKILL 抢占之前通过 Unix SIGTERM 信号请求通知，因此它们有时间清理、保存状态、完成任何当前正在执行的请求，并拒绝新的请求。 如果抢占者设置了延迟界限，则实际通知可能会更少。实际上，大约 80% 的时间都会发送通知。 2.4 分配(alloc) Borg alloc(分配的缩写)是机器上的一组保留资源，可以在其中运行一个或多个任务；无论是否使用，资源都保持分配状态。 Allocs 可用于为未来的任务留出资源，在停止任务和重新启动任务之间保留资源，以及将来自不同作业的任务收集到同一台机器上——例如，一个 web 服务器实例和一个复制保存其 URL 日志从到本地磁盘到分布式文件系统的任务。 alloc 的资源的处理方式与机器的资源类似；在一个内部运行的多个任务共享其资源。如果必须将 alloc 重新定位到另一台机器，则其任务将与它一起重新安排。 一个 alloc 集合就像一个作业：它是一组在多台机器上保留资源的 alloc。 创建 alloc 集合后，可以提交一个或多个作业以在其中运行。为简洁起见，我们通常使用“task”来指代一个 alloc 或顶层任务(一个在 alloc 之外的任务)，使用 “job” 来指代一个作业或 alloc 集合。 2.5 优先级、配额和准入控制 当出现的工作量超过可容纳的量时会发生什么？我们对此的解决方案是优先级和配额。 每个工作都有一个优先级，一个小的正整数。高优先级任务可以以牺牲低优先级任务为代价获得资源，即使这涉及抢占(杀死)后者。 Borg 为不同的用途定义了不重叠的优先级带，包括(按优先级递减的顺序)：监控、生产、批处理和最佳努力(也称为测试或免费)。 对于本文，生产作业是监控和生产环境中的作业。 虽然被抢占的任务通常会被重新安排在运行单元的其他地方，但如果一个高优先级的任务与一个稍低优先级的任务相撞，又与另一个稍低优先级的任务相撞，依此类推，就会发生抢占级联。 为了消除大部分这种情况，我们不允许生产优先级范围内的任务相互抢占。 细粒度的优先级在其他情况下仍然有用——例如，MapReduce 主任务的运行优先级略高于他们控制的工作节点，以提高其可靠性。 优先级表示单元格中正在运行或等待运行的作业的相对重要性。配额用于决定允许调度哪些作业。 配额表示为一段时间(通常为几个月)内给定优先级的资源数量(CPU、RAM、磁盘等)向量。 这些数量指定了用户的作业请求一次可以请求的最大资源量(例如，“从现在到 7 月底，运行单元 xx 中的生产优先级为 20 TiB 的 RAM”)。 配额检查是准入控制的一部分，而不是调度：配额不足的作业在提交后会立即被拒绝。 高优先级配额的成本高于低优先级配额。生产优先配额仅限于单元中可用的实际资源，因此提交符合其配额的生产优先作业的用户可以期望它运行，模碎片和约束。 尽管我们鼓励用户购买的配额不要超过他们需要的数量，但许多用户还是会过度购买，因为当他们的应用程序的用户群增长时，这可以使他们免受未来短缺的影响。 我们通过在较低优先级的超额销售配额来对此做出回应：每个用户在优先级为零的情况下都有一个无限的配额，尽管这通常很难行使，因为资源被超额订阅。 低优先级的作业可能会被接纳但由于资源不足而保持挂起(未安排)。 配额分配在 Borg 之外处理，与我们的物理容量规划密切相关，其结果反映在不同数据中心的配额价格和可用性上。用户作业仅在具有所需优先级的足够配额时才被允许。 配额的使用减少了对显性资源公平(DRF)等政策的需求 [29, 35, 36, 66]。 Borg 有一个能力系统，赋予一些用户特殊的权限；例如，允许管理员删除或修改运行单元中的任何作业，或允许用户访问受限内核功能或 Borg 行为，例如禁用其作业的资源估计(参见第 5.5 节)。 2.6 名称服务和监控 创建和放置任务是不够的：服务的客户端和其他系统需要能够找到它们，即使它们被重新定位到新机器上。 为了实现这一点，Borg 为每个包含单元名称、作业名称和任务编号的任务创建了一个稳定的“Borg 名称服务”(BNS)名称。 Borg 使用此名称将任务的主机名和端口写入 Chubby [14] 中一致的、高度可用的文件中，我们的 RPC 系统使用它来查找任务端点。 BNS 名称也构成了任务 DNS 名称的基础，因此运行单元 cc 中用户 ubar 所拥有的作业 jfoo 中的第 50 个任务将可以通过 50.jfoo.ubar.cc.borg.google.com 访问。 Borg 还会在发生变化时将作业大小和任务健康信息写入 Chubby，因此负载均衡器可以看到将请求路由到哪里。 几乎在 Borg 下运行的每个任务都包含一个内置的 HTTP 服务器，该服务器发布有关任务健康状况和数千个性能指标(例如，RPC 延迟)的信息。 Borg 监控健康检查 URL 并重新启动没有及时响应或返回 HTTP 错误代码的任务。其他数据由仪表板和服务级别目标 (SLO) 违规警报的监控工具跟踪。 名为 Sigma 的服务提供基于 Web 的用户界面 (UI)，用户可以通过该界面检查所有作业、特定运行单元的状态，或深入查看单个作业和任务以检查其资源行为、详细日志、执行历史记录，以及最终的状态。 我们的应用程序会生成大量日志；这些应用会自动轮换以避免磁盘空间不足，并在任务退出后保留一段时间以协助调试。 如果应用程序没有在运行，Borg 提供了一个”为什么等待”的注解，与如何修改作业的资源请求以更好地适应运行单元的指南。 我们发布了可能容易调度的“符合”资源形状的指南。 Borg 在 Infrastore 中记录了所有作业提交和任务事件，以及详细的每个任务资源使用信息，Infrastore 是一个可扩展的只读数据存储，通过 Dremel [61] 提供类似 SQL 的交互式界面。 此数据用于基于使用情况的计费、调试作业和系统故障以及长期容量规划。它还提供了 Google 集群工作负载跟踪的数据 [80]。 所有这些功能都可以帮助用户了解和调试 Borg 及其作业的行为，并帮助我们的 SRE 管理每人数万台机器。 3 Borg 的架构设计 Borg 单元由一组机器、一个称为 Borgmaster 的逻辑集中控制器和一个称为 Borglet 的代理进程组成，该代理进程在单元中的每台机器上运行(参见图 1)。Borg 的所有组件都是用 C++ 编写的。 3.1 Borgmaster 每个运行单元的 Borgmaster 由两个进程组成：主 Borgmaster 进程和一个单独的调度程序(参见第 3.2 节)。 主 Borgmaster 进程处理客户端 RPC，这些 RPC 要么改变状态(例如，创建作业)，要么提供对数据的只读访问(例如，查找作业)。 它还管理系统中所有对象(机器、任务、alloc 等)的状态机，与 Borglet 通信，并提供 Web UI 作为 Sigma 的备份。 Borgmaster 在逻辑上是一个单一的进程，但实际上被复制了五次。 每个副本都维护了大部分单元状态的内存副本，并且该状态也记录在副本本地磁盘上的高可用、分布式、基于 Paxos [55] 的存储中。 每个运行单元选出的单个 master 既充当 Paxos 的领导节点又充当状态修改器，处理所有改变运行单元状态的操作，例如提交作业或终止机器上的任务。 当运行单元被启动时以及当被选举的 master 失效时，一个 master 会被选举(使用 Paxos)；它会获取一个 Chubby 锁，以便其他系统可以找到它。 选举一个 master 并故障转移到新的 master 通常需要大约 10 秒，但在大的运行单元中可能需要长达一分钟的时间，因为必须重建一些内存中的状态。 当一个副本从中断中恢复时，它会从其他最新的 Paxos 副本动态地重新同步它的状态。 Borgmaster 在某个时间点的状态称为检查点，它采用定期快照和保存在 Paxos 存储中的更改日志的形式。 检查点有很多用途，包括将 Borgmaster 的状态恢复到过去的任意点(例如，就在接受触发 Borg 中软件缺陷的请求之前，以便对其进行调试)；在极端情况下手动修复； 为未来的查询建立一个持久的事件日志；和模拟离线情况。 一个名为 Fauxmaster 的高保真 Borgmaster 模拟器可用于读取检查点文件，并包含生产 Borgmaster 代码的完整副本，以及 Borglet 的存根接口。 它接受 RPC 以进行状态机更改并执行操作，例如“调度所有挂起的任务”，我们使用它来调试故障，方法是与它进行交互，就好像它是一个实时的 Borgmaster，模拟 Borglet 重放来自检查点文件的真实交互。 用户可以单步执行并观察过去实际发生的系统状态的变化。 Fauxmaster 还可用于容量规划(“这种类型的新工作有多少适合？”)，以及在对单元配置进行更改之前的健全性检查(“此更改会驱逐任何重要工作吗？”) 3.2 定时任务 当一个作业被提交时，Borgmaster 将它持久地记录在 Paxos 存储中，并将作业的任务添加到待处理队列中。 调度程序会异步扫描它们，如果有足够的可用资源满足作业的约束，调度程序就会将任务分配给机器。(调度程序主要对任务(task)进行操作，而不是作业(job)。) 扫描从高优先级到低优先级进行，由优先级内的循环方案调制，以确保用户之间的公平性并避免大型作业背后的队头阻塞。 调度算法有两部分：可行性检查，寻找可以运行任务的机器，以及评分，选择一台可行的机器。 在可行性检查中，调度程序会找到一组满足任务约束条件并具有足够“可用”资源的机器——包括分配给可以被驱逐的低优先级任务的资源。 在评分时，调度器确定每个可行机器的“优点”。 该分数考虑了用户指定的偏好，但主要由内置标准驱动，例如最小化被抢占任务的数量和优先级、挑选已经拥有任务包副本的机器、跨电源域和故障域传播任务，以及打包质量(将高优先级和低优先级任务混合到一台机器上，以允许高优先级任务在负载高峰时扩展)。 Borg 最初使用 E-PVM [4] 的变体进行评分，它在异构资源中生成单一成本值，并在放置任务时最小化成本的变化。 在实践中，E-PVM 最终将负载分散到所有机器上，为负载峰值留出空间——但以增加碎片为代价，特别是对于需要大部分机器的大型任务；我们有时称其为“最糟糕的选择”。 频谱的另一端是“最佳拟合”，它试图尽可能紧密地填充机器。 这使得一些机器没有用户作业(它们仍然运行存储服务器)，因此放置大型任务很简单，但紧密包装会惩罚用户或 Borg 对资源需求的任何错误估计。 这会损害具有突发负载的应用程序，对于指定低 CPU 需求的批处理作业尤其不利，因此它们可以轻松调度并尝试在未使用的资源中机会性地运行：20% 的非生产任务请求少于 0.1 个 CPU 内核。 我们当前的评分模型是一种混合模型，它试图减少闲置资源的数量——因为机器上的另一个资源已完全分配而无法使用。 与最适合我们的工作负载相比，它提供了大约 3-5% 的打包效率(在 [78] 中定义)。 如果评分阶段选择的机器没有足够的可用资源来适应新任务，Borg 会抢占(杀死)较低优先级的任务，从最低到最高优先级，直到它完成。 我们将被抢占的任务添加到调度程序的支出队列中，而不是迁移或休眠它们。 任务启动延迟(从作业提交到任务运行的时间)是一个已经受到并将继续受到显着关注的区域。它是高度可变的，中位数通常约为 25 秒。 软件包安装约占总数的 80%：已知瓶颈之一是对写入软件包的本地磁盘的争用。 为了减少任务启动时间，调度程序更喜欢将任务分配给已经安装了必要包(程序和数据)的机器：大多数包是不可变的，因此可以共享和缓存。 (这是 Borg 调度器支持的唯一数据本地形式。) 此外，Borg 使用类似树和 torrent 的协议将包并行分发到机器。 此外，调度程序使用多种技术让它扩展到拥有数万台机器的单元(第 3.4 节)。 3.4 Borglet Borglet 是一个本地 Borg 代理，存在于运行单元中的每台机器上。 它负责启动和停止任务；如果发生故障，则重新启动它们；通过对操作系统内核的设置来管理本地资源；滚动调试日志；并将机器状态报告给 Borgmaster 和其他监控系统。 Borgmaster 每隔几秒钟轮询每个 Borglet 以检索机器的当前状态并向其发送任何未完成的请求。 这使 Borgmaster 可以控制通信速率，避免需要明确的流量控制机制，并防止恢复风暴 [9]。 选定的主节点负责准备要发送给 Borglet 的消息，并用它们的响应更新运行单元的状态。 为了性能的可扩展性，每个 Borgmaster 副本运行一个无状态链接分片来处理与一些 Borglet 的通信；每当 Borgmaster 选举发生时，就会重新计算分区。 为了弹性，Borglet 始终报告其完整状态，但链接分片通过仅向状态机报告差异来聚合和压缩此信息，以减少所选主节点的更新负载。 如果 Borglet 没有响应多个轮询消息，它的机器将被标记为停机，并且它正在运行的任何任务都将重新安排在其他机器上。 如果通信恢复，Borgmaster 会告诉 Borglet 终止那些已重新安排的任务，以避免重复。 即使 Borglet 与 Borgmaster 失去联系，它也会继续正常运行，因此即使所有 Borgmaster 副本都失败，当前正在运行的任务和服务也会保持正常运行。 3.4 可伸缩性 我们不确定 Borg 中心化架构的最终可伸缩性的限制从何而来；到目前为止，每次我们接近极限时，我们都设法消除了它。 一个 Borgmaster 可以在一个运行单元中管理数千台机器，并且几个单元的到达率超过每分钟 10000 个任务。 繁忙的 Borgmaster 使用 10-14 个 CPU 内核和高达 50 GiB 的 RAM。我们使用了几种技术来达到这个规模。 Borgmaster 的早期版本有一个简单的同步循环，用于接受请求、计划任务并与 Borglet 通信。 为了处理更大的单元，我们将调度程序拆分为一个单独的进程，以便它可以与其他 Borgmaster 功能并行运行，这些功能被复制以实现容错。 调度程序副本对运行单元状态的缓存副本进行操作。 它会重复以下事件：从选定的主节点中检索状态更改(包括已分配的和未决的工作)；更新其本地副本；进行调度传递以分配任务；并将这些作业通知选定的主人。 Borgmaster 将接受并应用这些分配，除非它们不合适(例如，基于过时的状态)，这将导致它们在调度程序的下一次通过时被重新考虑。 这在本质上与 Omega [69] 中使用的乐观并发控制非常相似，实际上我们最近为 Borg 添加了针对不同工作负载类型使用不同调度程序的能力。 为了缩短响应时间，我们添加了单独的线程来与 Borglet 对话并响应只读 RPC。 为了获得更高的性能，我们将这些功能分片(分区)到 5 个 Borgmaster 副本(参见第 3.3 节)。 以上因素将 UI 的 99%ile (正态分布中的平均值的 99%) 响应时间保持在 1 以下，并将 Borglet 轮询间隔的 95%ile (正态分布中的平均值的 95%) 保持在 10 秒以下。 有几件事使 Borg 调度器更具可扩展性: 分数缓存：评估可行性并对机器进行评分是昂贵的，因此 Borg 会缓存分数，直到机器的属性或任务发生变化——例如，机器上的任务终止、属性改变或任务的要求发生变化。忽略资源数量的微小变化会减少缓存失效。 等价类：Borg 作业中的任务通常具有相同的要求和约束，因此与其为每台机器上的每个待处理任务确定可行性，并对所有可行机器进行评分，Borg 只对每个等价类的一个任务进行可行性和评分-一组具有相同要求的任务。 宽松的随机化：为一个运行单元中的所有机器计算可行性和分数是一种浪费，因此调度程序以随机顺序检查机器，直到找到“足够”的可行机器来评分，然后在该集合中选择最好的。 这减少了任务进入和离开系统时所需的评分和缓存失效量，并加快了将任务分配给机器的速度。 宽松随机化有点类似于 Sparrow [65] 的批量抽样，同时也处理优先级、抢占、异质性和包安装成本。 在我们的实验中(第 5 节)，从头开始调度一个单元的整个工作负载通常需要几百秒，但当上述技术被禁用时，超过 3 天后仍未完成。 但是，通常情况下，挂起队列上的在线调度会在不到半秒的时间内完成。 4 能力 故障是大规模系统中的常态 [10, 11, 22]。图 3 提供了 15 个样本单元中任务驱逐原因的细分。 运行在 Borg 上的应用程序应该使用诸如复制、在分布式文件系统中存储持久状态和(如果合适)偶尔保存检查点等技术来处理此类事件。 即便如此，我们仍会努力减轻这些事件的影响。例如，Borg 的处理方式如下： 如有必要，在新机器上自动重新安排被驱逐的任务 通过跨故障域(例如机器、机架和电源域)分散作业的任务来减少相关故障； 限制允许的任务中断率以及在维护活动(例如操作系统或机器升级)期间可以同时关闭的作业中的任务数量； 使用声明性的期望状态表示和幂等的变异操作，以便失败的客户端可以无害地重新提交任何被遗忘的请求； 限制提交速率，将无法传输到设备的任务寻找新的位置。因为它因为它无法区分大规模机器故障和网络分区； 避免重复 task::machine 配对导致任务或机器崩溃 通过反复重新运行日志保护程序任务(第 2.4 节)来恢复写入本地磁盘的关键中间数据，即使它所附加的 alloc 已终止或移动到另一台机器。用户可以设置系统重试时间(通常是几天)。 Borg 的一个关键设计特性是，即使 Borgmaster 或任务的 Borglet 出现故障，已经运行的任务也会继续运行。 但是保持 master 运行仍然很重要，因为当它宕机时，无法提交新作业或更新现有作业，并且无法重新安排来自故障机器的任务。 Borgmaster 使用了多种技术组合，使其能够在实践中实现 99.99% 的可用性：机器故障复制；准入控制以避免过载；并使用简单的低级工具部署实例以最大限度地减少外部依赖性。 每个运行单元都独立于其他运行单元，以最大限度地减少相关操作员错误和故障传播的机会。这些目标，而不是可扩展性限制，是反对更大运行单元的主要论据。 5 利用率 Borg 的主要目标之一是有效利用 Google 的机器群，这是一项重大的财务投资：将利用率提高几个百分点可以节省数百万美元。本节讨论和评估 Borg 用于这样做的一些策略和技术。 5.1 评估方法 我们的作业有放置约束，需要处理罕见的工作负载峰值，我们的机器是异构的，我们在从服务作业回收的资源中运行批处理作业。 因此，为了评估我们的政策选择，我们需要一个比“平均利用率”更复杂的指标。 经过大量实验，我们选择了运行单元压缩的方式：给定一个工作负载，我们通过移除机器直到工作负载不再适合它来发现它可以安装到多小的运行单元中，从头开始反复重新打包工作负载，以确保我们不会被一个不走运的配置所困扰。 这提供了干净的终止条件，并促进了自动比较，而不存在合成额外产生工作负载和建模的陷阱 [31]。 评估技术的定量比较可以在 [78] 中找到：细节出奇地微妙。 不可能在实时生产运行单元上进行实验，但我们使用 Fauxmaster 获得高保真模拟结果，使用来自真实生产单元和工作负载的数据，包括它们的所有约束、实际限制、预留和使用数据(参见第 5.5 节)。 该数据来自于名美国太平洋夏令时间 2014-10-01 14:00 星期三拍摄的 Borg 检查点。 我们选择了 15 个 Borg 运行单元进行报告，首先消除了特殊用途的、测试的和小型(&lt; 5000 台机器)运行单元，然后对剩余的群体进行采样，以在大小范围内实现大致均匀的分布。 为了保持压缩单元中的机器异质性，我们随机选择要移除的机器。 为了保持工作负载的异构性，我们保留了所有内容，除了与特定机器(例如 Borglet)相关的服务器和存储任务。 对于大于原始单元大小一半的作业，我们将硬约束更改为软约束，并允许多达 0.2% 的任务挂起，如果它们非常“挑剔”并且只能放置在少数机器上；大量实验表明，这产生了具有低方差的可重复结果。 如果我们需要一个比原始运行单元更大的运行单元，我们会在压缩前克隆原始运行单元几次；如果我们需要更多运行单元，我们就克隆原始运行单元。 对于具有不同随机数种子的每个运行单元，每个实验重复 11 次。 在图中，我们使用误差条来显示所需机器数量的最小值和最大值，并选择 90% 的值作为“结果”——平均值或中位数不会反映系统管理员会做什么如果他们想合理地确定工作量是否合适。 我们相信单元压缩提供了一种公平、一致的方式来比较调度策略，它直接转化为成本/收益结果：更好的策略需要更少的机器来运行相同的工作负载。 我们的实验侧重于从某个时间点调度(打包)工作负载，而不是重放长期工作负载跟踪。 这部分是为了避免处理开放和封闭队列模型的困难 [71, 79]，部分是因为传统的完成时间指标不适用于我们的长期运行服务的环境，部分是为了提供清晰的信号进行比较，部分是因为我们不相信结果会有显着不同，部分是因为实际问题：我们发现自己曾在某个时间点消耗 200 000 个 Borg CPU 内核用于我们的实验——即使在 Google 的规模下，这也是非常重要的投资。 在生产中，我们故意为工作负载增长、偶尔的“黑天鹅”事件、负载尖峰、机器故障、硬件升级和大规模局部故障(例如，电源总线故障)留出很大的空间。 图 4 显示了如果我们对它们应用运行单元压缩，我们的现实世界的运行单元会小多少。下图中的基线使用这些压缩尺寸。 5.2 运行单元共享 我们几乎所有的机器都同时运行生产和非生产任务：98% 的机器在共享的 Borg 单元中，83% 的机器在 Borg 管理的整个机器集中。(我们有一些特殊用途的专用运行单元。) 注：两个图表都显示了如果生产和非生产工作负载被发送到不同的单元需要多少额外的机器，表示为在单个运行单元中运行工作负载所需的最少机器数量的百分比。在这个和随后的 CDF 图中，每个运行单元显示的值来自我们的实验试验产生的不同运行单元大小的 90%；误差条显示了试验值的完整范围。 由于许多其他组织在不同的集群中运行面向用户的批处理作业，因此我们研究了如果我们这样做会发生什么。 图 5 显示隔离生产和非生产工作需要在中间单元中多 20-30% 的机器来运行我们的工作负载。 这是因为生产作业通常会预留资源来处理罕见的工作负载高峰，但大多数时候不会使用这些资源。 Borg 回收未使用的资源(参见第 5.5 节)来运行大部分非生产工作，因此我们总体上需要更少的机器。 大多数 Borg 运行单元由数千个用户共享。图 6 显示了原因。 对于此测试，如果用户消耗至少 10 TiB 的内存(或 100 TiB)，我们会将用户的工作负载拆分到一个新单元中。 我们现有的策略看起来不错：即使使用更大的阈值，我们也需要 2-16 倍的单元，以及 20-150% 的额外机器。再一次证明了，集中资源显着降低了成本。 但也许将不相关的用户和作业类型打包到同一台机器上会导致 CPU 干扰，因此我们需要更多的机器来补偿？ 为了评估这一点，我们研究了 CPI (每条指令的周期数)如何在不同环境中以相同的时钟速度运行在相同机器类型上的任务中发生变化。 在这些条件下，CPI 值是可比较的，并且可以用作性能干扰的代理，因为 CPI 加倍会使 CPU 密集型程序的运行时间加倍。 数据是从一周内随机选择的 12000 个生产任务中收集的，使用 [83] 中描述的硬件分析基础设施在 5 分钟的时间间隔内计算周期和指令，并对样本进行加权，以便 CPU 时间的每一秒都被相等地计数。结果并不明确。 (1) 我们发现 CPI 与同一时间间隔内的两个测量值呈正相关：机器上的总体 CPU 使用率和(很大程度上独立的)机器上的任务数量；向机器添加任务会使其他任务的 CPI 增加 0.3%(使用拟合数据的线性模型）向机器添加任务会使其他任务的 CPI 增加 0.3%（使用拟合数据的线性模型)；将机器 CPU 使用率提高 10% 会使 CPI 增加不到 2%。 但即使相关性在统计上显着，它们也只能解释我们在 CPI 测量中看到的方差的 5%； 其他因素占主导地位，例如应用和特定干扰模式的固有差异 [24, 83]。 (2) 将我们从共享单元中采样的 CPI 与应用程序不太多样化的几个专用单元中的 CPI 进行比较，我们看到共享单元中的平均 CPI 为 1.58 (σ = 0.35)，而专用单元中的平均值为 1.53 (σ = 0.32) – 即 , CPU 性能在共享单元中大约差 3%。 (3) 为了解决不同单元中的应用程序可能具有不同的工作负载，甚至遭受选择偏差(可能对干扰更敏感的程序已移至专用单元)的担忧，我们查看了 Borglet 的 CPI，它们运行在所有设备中两种不同类型的运行单元上。 这些实验证实了仓库规模的性能比较是棘手的，又一次证明了 [51] 中的观察结果，并且还表明共享不会显着增加运行程序的成本。 但即使假设我们的结果最不利，共享仍然是一个胜利：CPU 速度下降被几种不同分区方案所需的机器减少所抵消，并且共享优势适用于包括内存和磁盘在内的所有资源，而不仅仅是 CPU。 5.3 大型运行单元 Google 构建大型单元，既可以运行大型计算，又可以减少资源碎片。 我们通过将一个单元的工作负载划分到多个较小的单元来测试后者的效果——首先随机排列作业，然后在分区之间以循环方式分配它们。 图 7 证实，使用更小的单元将需要更多的机器。 5.4 细粒度的资源请求 Borg 用户以毫核(milli-core)为单位请求 CPU，以字节为单位请求内存和磁盘空间。 (核心是处理器超线程，针对跨机器类型的性能进行了标准化。) 图 8 显示他们利用了这种粒度：在请求的内存或 CPU 内核量中几乎没有明显的“最佳点”，并且这些资源之间几乎没有明显的相关性。 这些分布与 [68] 中呈现的分布非常相似，除了我们在 90% 和以上看到稍大的内存请求。 提供一组固定大小的容器或虚拟机，虽然在 IaaS (基础设施即服务)提供商 [7, 33] 中很常见，但并不能很好地满足我们的需求。 为了证明这一点，我们通过将每个资源维度中的下一个最接近的 2 次幂四舍五入来“分桶”生产作业和分配(第 2.4 节)的 CPU 内核和内存资源限制，从 0.5 个 CPU 内核和 1 GiB RAM 开始。 图 9 显示，在中等情况下，这样做需要多 30-50% 的资源。 上限来自将整台机器分配给大型任务，这些任务在压缩开始之前将原始运行单元翻了四倍；允许这些任务挂起的下限。 (这低于 [37] 中报告的大约 100% 的开销，因为我们支持 4 个以上的存储桶并允许 CPU 和 RAM 容量独立扩展。) 5.5 资源回收 一个作业可以指定一个资源限制——每个任务应该被授予的资源的上限。 Borg 使用该限制来确定用户是否有足够的配额来接受作业，以及确定特定机器是否有足够的空闲资源来安排任务。 就像有些用户购买的配额比他们需要的多一样，有些用户请求的资源比他们的任务使用的资源多，因为 Borg 通常会杀死一个尝试使用比请求更多的 RAM 或磁盘空间的任务，或者节流 CPU 以它要求什么。 此外，某些任务偶尔需要使用其所有资源(例如，在一天的高峰时间或在应对拒绝服务攻击时)，但大多数时间不需要。 我们不会浪费当前未消耗的已分配资源，而是估计任务将使用多少资源，并将剩余的资源用于可以容忍较低质量资源的工作，例如批处理作业。这整个过程称为资源回收。 该估计值称为任务预留，由 Borgmaster 每隔几秒计算一次，使用 Borglet 捕获的细粒度使用(资源消耗)信息。 初始预留设置为等于资源请求(限制)；300 秒后，为了允许启动瞬变，它会缓慢衰减到实际使用情况加上安全裕度。如果使用量超过它，则预订会迅速增加。 Borg 调度器使用限制来计算生产任务的可行性（第 3.2 节），4 因此它们从不依赖回收资源，也不会暴露于资源超额订阅；对于非生产任务，它使用现有任务的预留，以便可以将新任务安排到回收资源中。 如果预留(预测)错误，机器可能会在运行时耗尽资源——即使所有任务使用的资源都少于其限制。如果发生这种情况，我们会终止或限制非生产任务，而不是生产任务。 图 10 显示，如果不进行资源回收，将需要更多的机器。大约 20% 的工作负载（第 6.2 节）在中间单元中的回收资源中运行。 注：资源估计在识别未使用的资源方面是成功的。虚线显示了 CPU 和内存使用率与 15 个单元中任务的请求（限制）比率的 CDF。大多数任务使用的 CPU 比其限制少得多，但也有少数任务使用的 CPU 比请求的多。 实线表示 CPU 和内存预留与限制的比率的 CDF； 这些都接近 100%。直线是资源估算过程的产物。 我们可以在图 11 中看到更多详细信息，其中显示了预留和使用与限制的比率。 如果需要资源，超过其内存限制的任务将首先被抢占，无论其优先级如何，因此任务很少会超过其内存限制。 另一方面，CPU 很容易受到限制，因此短期峰值可以相当无害地将使用率推高至预留值之上。 注：一个时间线（从 2013 年 11 月 11 日开始）用于一个生产单元的使用、预留和限制平均超过 5 分钟的窗口和累积的内存不足事件；后者的斜率是 OOM 的总比率。竖线将具有不同资源估计设置的周分开。 图 11 表明资源回收可能过于保守：预留线和使用线之间有很大的区域。 为了测试这一点，我们选择了一个实时生产单元，并通过降低安全裕度，将其资源估算算法的参数调整为一周内的激进设置，然后调整为下周介于基线和激进设置之间的中等设置，然后恢复到基线。 图 12 显示了发生的情况。 预订量显然更接近第二周的使用量，而在第三周的情况略少，最大的差距出现在基准周(第 1 周和第 4 周)。 正如预期的那样，内存不足 (OOM) 事件的发生率在第 2 周和第 3.5 周略有增加在审查这些结果后，我们决定净收益超过不利因素，并将中等资源回收参数部署到其他单元。 6 隔离策略 我们 50% 的机器运行 9 个或更多任务；一台 90%ile 的机器有大约 25 个任务，将运行大约 4500 个线程 [83]。 尽管在应用程序之间共享机器提高了利用率，但它也需要良好的机制来防止任务相互干扰。这适用于安全性和性能。 6.1 安全隔离 我们使用 Linux chroot jail 作为同一台机器上多个任务之间的主要安全隔离机制。 为了允许远程调试，我们曾经自动分发(和撤销)ssh 密钥，以便用户仅在机器为用户运行任务时才能访问它。 对于大多数用户来说，这已被 borgssh 命令所取代，该命令与 Borglet 合作构建到与任务在相同 chroot 和 cgroup 中运行的 shell 的 ssh 连接，从而更紧密地锁定访问。 谷歌的 AppEngine (GAE) [38] 和谷歌计算引擎 (GCE) 使用虚拟机和安全沙箱技术来运行外部软件。 我们在作为 Borg 任务运行的 KVM 进程 [54] 中运行每个托管 VM。 6.2 性能隔离 Borglet 的早期版本具有相对原始的资源隔离强制：对内存、磁盘空间和 CPU 周期进行事后使用检查，同时终止使用过多内存或磁盘的任务，并积极应用 Linux 的 CPU 优先级来控制使用过多 CPU 的任务。 但是流氓任务仍然太容易影响机器上其他任务的性能，因此一些用户夸大了他们的资源请求，以减少 Borg 可以与他们共同调度的任务数量，从而降低利用率。 由于涉及安全边际，资源回收可以收回部分盈余，但不是全部。在最极端的情况下，用户请求使用专用机器或单元。 现在，所有 Borg 任务都在基于 Linux cgroup 的资源容器内运行 [17, 58, 62] 并且 Borglet 操作容器设置，由于操作系统内核处于循环中，因此提供了大大改进的控制。 即便如此，偶尔的低级资源干扰(例如，内存带宽或 L3 缓存污染)仍然会发生，如 [60, 83]。 为了帮助处理过载和过度使用，Borg 任务有一个应用程序类或应用程序类。 最重要的区别在于延迟敏感 (LS) 应用程序类和其他应用程序类之间的区别，我们在本文中将其称为批处理。 LS 任务用于需要快速响应请求的面向用户的应用程序和共享基础设施服务。 高优先级的 LS 任务得到最好的处理，并且能够一次使批处理任务暂时饥饿(缺少资源)几秒钟。 第二次分裂是在基于速率的可压缩资源(例如，CPU 周期、磁盘 I/O 带宽)之间进行划分，并且可以通过降低服务质量而不杀死它来从任务中回收；和不可压缩的资源(例如，内存、磁盘空间)，这些资源通常无法在不终止任务的情况下回收。 如果机器用完不可压缩的资源，Borglet 会立即终止任务，从最低到最高优先级，直到可以满足剩余的预留。 如果机器用完可压缩资源，Borglet 会限制使用(支持 LS 任务)，以便可以在不终止任何任务的情况下处理短暂的负载峰值。 如果情况没有改善，Borgmaster 将从机器中删除一项或多项任务。 Borglet 中的用户空间控制循环根据预测的未来使用情况(对于生产任务)或内存压力(对于非生产任务)为容器分配内存；处理来自内核的内存不足(OOM)事件；并在任务尝试分配超出其内存限制时，或者当过度使用的机器实际耗尽内存时终止任务。 由于需要准确的内存计算，Linux 的急切文件缓存使实现变得非常复杂。 为了提高性能隔离，LS 任务可以保留整个物理 CPU 内核，从而阻止其他 LS 任务使用它们。 批处理任务可以在任何内核上运行，但相对于 LS 任务，它们的调度程序份额很小。 Borglet 动态调整贪婪 LS 任务的资源上限，以确保它们不会让批处理任务饿死多分钟，在需要时选择性地应用 CFS 带宽控制 [75]；份额不足，因为我们有多个优先级。 与 Leverich [56] 一样，我们发现标准 Linux CPU 调度程序 (CFS) 需要大量调整以支持低延迟和高利用率。 为了减少调度延迟，我们的 CFS 版本使用扩展的每个 cgroup 加载历史记录 [16]，允许 LS 任务抢占批处理任务，并在多个 LS 任务可在 CPU 上运行时减少调度时间。 幸运的是，我们的许多应用程序都使用线程每请求模型，这减轻了持续负载不平衡的影响。我们谨慎地使用 cpuset 将 CPU 内核分配给具有特别严格的延迟要求的应用程序。 这些努力的一些结果如图 13 所示。该领域的工作仍在继续，添加线程放置和 CPU 管理，即 NUMA、超线程和功耗感知(例如，[81])，并提高 Borglet 的控制保真度。 允许任务消耗资源达到其限制。他们中的大多数被允许超越 CPU 等可压缩资源的限制，以利用未使用的(松弛)资源。 只有 5% 的 LS 任务禁用此功能，大概是为了获得更好的可预测性；不到 1% 的批处理任务会这样做。 默认情况下禁用使用 slack 内存，因为它增加了任务被杀死的机会，但即便如此，10% 的 LS 任务会覆盖这一点，而 79% 的批处理任务会这样做，因为这是 MapReduce 框架的默认设置。 这补充了回收资源的结果(第 5.5 节)。 批处理任务愿意机会性地利用未使用和回收的内存：大多数情况下这是有效的，尽管当 LS 任务急需资源时，偶尔会牺牲批处理任务。 7 相关工作 资源调度已被研究了数十年，其环境多种多样，如广域 HPC 超级计算网格、工作站网络和大型服务器集群。 我们在这里只关注大规模服务器集群环境中最相关的工作。 最近的几项研究分析了来自 Yahoo!、Google 和 Facebook [20, 52, 63, 68, 70, 80, 82] 的集群痕迹，并说明了这些现代数据中心和工作负载中固有的规模和异构性挑战。 [69] 包含集群管理器架构的分类。 Apache Mesos [45] 使用基于报价的机制在中央资源管理器(有点像 Borgmaster 减去其调度程序)和多个“框架”(例如 Hadoop [41] 和 Spark [73])之间拆分资源管理和放置功能。 Borg 主要使用可扩展的基于请求的机制来集中这些功能。DRF [29, 35, 36, 66] 最初是为 Mesos 开发的； Borg 使用优先级和准入配额。 Mesos 开发人员已宣布将扩展 Mesos 以包括推测性资源分配和回收，并解决 [69] 中确定的一些问题。 YARN [76] 是一个以 Hadoop 为中心的集群管理器。 每个应用程序都有一个管理器，它与一个中央资源管理器协商它需要的资源； 这与 Google MapReduce 作业从 2008 年左右以来从 Borg 获取资源所使用的方案大致相同。 YARN 的资源管理器最近才变得容错。 一个相关的开源工作是 Hadoop 容量调度器 [42]，它通过容量保证、分层队列、弹性共享和公平性提供多租户支持。 YARN 最近已经扩展到支持多种资源类型、优先级、抢占和高级准入控制 [21]。 俄罗斯方块研究原型 [40] 支持制作跨度感知的作业打包。 Facebook 的 Tupperware [64]，是一个类似于 Borg 的系统，用于在集群上调度 cgroup 容器；虽然它似乎提供了一种资源回收的形式，但只披露了一些细节。 Twitter 开源了 Aurora [5]，这是一个类似于 Borg 的调度程序，用于在 Mesos 之上运行的长时间运行的服务，具有类似于 Borg 的配置语言和状态机。 Microsoft [48] 的 Autopilot 系统提供了“自动化软件供应和部署；系统监控；并针对 Microsoft 群集执行修复操作以处理错误的软件和硬件”。 Borg 生态系统提供了类似的功能，但篇幅限制了这里的讨论； Isaard [48] 概述了我们也遵循的许多最佳实践。 Quincy [49] 使用网络流模型为数百个节点的集群上的数据处理 DAG 提供公平性和数据位置感知调度。 Borg 使用配额和优先级在用户之间共享资源并扩展到数万台机器。Quincy 直接处理执行图，而它是在 Borg 之上单独构建的。 Cosmos [44] 专注于批处理，强调确保其用户能够公平地访问他们捐赠给集群的资源。它使用 per-job manager 来获取资源；但很少公开实现细节。 微软的 Apollo 系统 [13] 使用 per-job 调度程序进行短期批处理作业，以在似乎与 Borg 单元相当大小的集群上实现高吞吐量。 Apollo 使用机会执行较低优先级的后台工作，以(有时)多天排队延迟为代价将利用率提高到高水平。 Apollo 节点提供任务开始时间的预测矩阵，作为两个资源维度大小的函数，调度程序将其与启动成本和远程数据访问的估计相结合，以做出放置决策，并通过随机延迟进行调制以减少冲突。 Borg 使用中央调度器根据先前分配的状态进行放置决策，可以处理更多资源维度，并专注于高可用性、长时间运行的应用程序的需求；Apollo 可能可以处理更高的任务到达率。 阿里巴巴的 Fuxi [84] 支持数据分析工作负载；它从 2009 年开始运行。与 Borgmaster 一样，中央 FuxiMaster(为了容错而复制)从节点收集资源可用性信息，接受来自应用程序的请求，并相互匹配。 Fuxi 增量调度策略与 Borg 的等价类相反：Fuxi 将新可用资源与待处理工作的积压相匹配，而不是将每个任务与一组合适的机器匹配。 与 Mesos 一样，Fuxi 允许定义“虚拟资源”类型。只有合成工作负载的结果是公开的。 Omega [69] 支持多个并行的、专门的“vertical&quot;，它大致相当于一个 Borgmaster 减去它的持久存储和链接分片。 Omega 调度器使用乐观并发控制来操纵存储在中央持久存储中的所需和观察到的单元状态的共享表示，该状态通过单独的链接组件与 Borglets 同步。 Omega 架构旨在支持多个不同的工作负载，这些工作负载具有自己的特定于应用程序的 RPC 接口、状态机和调度策略(例如，长时间运行的服务器、来自各种框架的批处理作业、集群存储系统等基础设施服务、来自谷歌云平台)。 另一方面，Borg 提供了“一刀切”的 RPC 接口、状态机语义和调度程序策略，由于需要支持许多不同的工作负载，它们的大小和复杂性随着时间的推移而增长，而可扩展性尚未是一个问题(第 3.4 节)。 Google 的开源 Kubernetes 系统 [53] 将 Docker 容器 [28] 中的应用程序放置在多个主机节点上。 它既可以在裸机(如 Borg)上运行，也可以在各种云托管服务提供商(如 Google Compute Engine)上运行。 许多建造 Borg 的工程师正在积极开发它。Google 提供了一个托管版本，称为 Google Container Engine [39]。 我们将在下一节讨论如何将 Borg 的经验教训应用于 Kubernetes。 高性能计算社区在该领域有着悠久的工作传统(例如 Maui、Moab、Platform LSF [2, 47, 50])；然而，规模、工作负载和容错的要求与谷歌的单元不同。 通常，此类系统通过大量待办工作的积压(队列)来实现高利用率。 VMware [77] 等虚拟化提供商和 HP 和 IBM [46] 等数据中心解决方案提供商提供的集群管理解决方案通常可扩展到 O(1000) 台机器。 此外，几个研究小组已经设计出以某些方式提高调度决策质量的系统原型(例如，[25、40、72、74])。 最后，正如我们所指出的，管理大规模集群的另一个重要部分是自动化和“运营商横向扩展”。 [43] 描述了如何规划故障、多租户、健康检查、准入控制和可重启性，以实现每个操作者管理大量机器。 Borg 的设计理念是相似的，它使我们能够支持每个操作员(SRE)管理数以万计的机器。 8 经验教训和未来的工作 在本节中，我们将回顾我们从十多年来在生产中运行 Borg 中学到的一些定性经验教训，并描述如何利用这些观察结果来设计 Kubernetes [53]。 8.1 经验教训：坏的方面 我们从 Borg 的一些作为警示故事的功能开始，并为 Kubernetes 中的替代设计提供了参考。 作业作为任务的唯一分组机制是有局限性的。 Borg 没有一流的方法来将整个多作业服务作为单个实体进行管理，或者引用服务的相关实例(例如，金丝雀(Canary)和生产轨道)。 作为一个黑客，用户在作业名称中编码他们的服务拓扑，并构建更高级别的管理工具来解析这些名称。 另一方面，不可能引用作业的任意子集，这会导致诸如滚动更新和作业调整大小不灵活的语义等问题。 为了避免这种困难，Kubernetes 拒绝了作业概念，而是使用标签来组织其调度单元(pod)——用户可以附加到系统中任何对象的任意键/值对。 可以通过将 job:jobname 标签附加到一组 pod 来实现 Borg 作业的等价物，但也可以表示任何其他有用的分组，例如服务、层或发布类型(例如，生产、暂存、测试)。 Kubernetes 中的操作通过标签查询来识别它们的目标，该标签查询选择操作应该应用到的对象。 这种方法比单一的固定作业分组具有更大的灵活性。 每台机器一个 IP 地址会使事情复杂化。 在 Borg 中，机器上的所有任务都使用其主机的单个 IP 地址，因此共享主机的端口空间。 这导致了许多困难：Borg 必须将端口作为资源进行调度； 任务必须预先声明它们需要多少端口，并愿意在启动时被告知使用哪些端口；Borglet 必须强制执行端口隔离；命名和 RPC 系统必须处理端口和 IP 地址。 由于 Linux 命名空间、VM、IPv6 和软件定义网络的出现，Kubernetes 可以采用更加用户友好的方法来消除这些复杂性：每个 pod 和服务都有自己的 IP 地址，允许开发人员选择端口而不是要求他们的软件适应基础设施选择的软件，并消除了管理端口的基础设施复杂性。 以牺牲临时用户为代价为高级用户进行优化。 Borg 提供了大量针对“高级用户”的功能，因此他们可以微调其程序的运行方式(BCL 规范列出了大约 230 个参数)：最初的重点是支持 Google 最大的资源消费者，为他们提高效率 收益是最重要的。 不幸的是，这个 API 的丰富性使“临时”用户的工作变得更加困难，并限制了它的发展。 我们的解决方案是构建在 Borg 之上运行的自动化工具和服务，并通过实验确定适当的设置。 这些受益于容错应用程序提供的试验自由：如果自动化出错，那只是麻烦，而不是灾难。 8.2 经验教训：好的方面 另一方面，Borg 的许多设计特点非常有益，并且经受住了时间的考验。 Alloc 很有用。 Borg alloc 抽象产生了广泛使用的日志保存模式(第 2.4 节)和另一种流行的模式，其中一个简单的数据加载器任务定期更新 Web 服务器使用的数据。 Alloc 和包允许由不同的团队开发此类帮助服务。Kubernetes 相当于 alloc 是 pod，它是一个或多个容器的资源信封，这些容器总是被调度到同一台机器上并且可以共享资源。 Kubernetes 在同一个 pod 中使用辅助容器，而不是在 alloc 中使用任务，但想法是一样的。 集群管理不仅仅是任务管理。 尽管 Borg 的主要角色是管理任务和机器的生命周期，但在 Borg 上运行的应用程序受益于许多其他集群服务，包括命名和负载平衡。 Kubernetes 使用服务抽象支持命名和负载平衡：服务具有名称和由标签选择器定义的动态 pod 集。集群中的任何容器都可以使用服务名称连接到服务。 在幕后，Kubernetes 会自动在与标签选择器匹配的 pod 之间平衡与服务的连接，并在 pod 因故障而重新调度时跟踪它们的运行位置。 内省至关重要。 尽管 Borg 几乎总是“正常工作”，但当出现问题时，找到根本原因可能具有挑战性。 Borg 的一个重要设计决策是向所有用户显示调试信息而不是隐藏它：Borg 有成千上万的用户，因此“自助”必须是调试的第一步。 尽管这让我们更难弃用功能并更改用户所依赖的内部策略，但这仍然是一个胜利，我们没有找到现实的替代方案。 为了处理海量数据，我们提供了多个级别的 UI 和调试工具，因此用户可以快速识别与其作业相关的异常事件，然后从其应用程序和基础架构本身深入了解详细的事件和错误日志。 Kubernetes 旨在复制 Borg 的许多内省技术。例如，它附带了用于资源监控的 cAdvisor [15] 等工具，以及基于 Elasticsearch/Kibana [30] 和 Fluentd [32] 的日志聚合。 可以查询 master 以获取其对象状态的快照。Kubernetes 有一个统一的机制，所有组件都可以使用该机制来记录可供客户端使用的事件(例如，一个 pod 被调度，一个容器失败)。 Master是分布式系统的内核。 Borgmaster 最初被设计为一个整体系统，但随着时间的推移，它更像是一个内核，位于合作管理用户作业的服务生态系统的核心。 例如，我们将调度程序和主 UI(Sigma)拆分为单独的进程，并添加了用于准入控制、垂直和水平自动缩放、重新打包任务、定期作业提交(corn)、工作流管理和归档系统操作的服务用于离线查询。 总之，这些使我们能够在不牺牲性能或可维护性的情况下扩展工作负载和功能集。 Kubernetes 架构更进一步：它的核心有一个 API 服务器，它只负责处理请求和操作底层状态对象。 群集管理逻辑构建为小型、可组合的微服务，这些服务是此 API 服务器的客户端，例如 replication controller (在出现故障时维护 pod 的所需副本数量)和 node controller(管理机器生命周期)。 8.3 结论 在过去十年中，几乎所有 Google 的集群工作负载都转而使用 Borg。我们将继续改进它，并将从中学到的经验应用到 Kubernetes 中。 致谢 123456789101112131415161718192021222324252627282930313233343536373839The authors of this paper performed the evaluations andwrote the paper, but the dozens of engineers who designed, implemented, and maintained Borg’s componentsand ecosystem are the key to its success. We list here justthose who participated most directly in the design, implementation, and operation of the Borgmaster and Borglets.Our apologies if we missed anybody.The initial Borgmaster was primarily designed and implemented by Jeremy Dion and Mark Vandevoorde, withBen Smith, Ken Ashcraft, Maricia Scott, Ming-Yee Iu, andMonika Henzinger. The initial Borglet was primarily designed and implemented by Paul Menage.Subsequent contributors include Abhishek Rai, AbhishekVerma, Andy Zheng, Ashwin Kumar, Beng-Hong Lim,Bin Zhang, Bolu Szewczyk, Brian Budge, Brian Grant,Brian Wickman, Chengdu Huang, Cynthia Wong, DanielSmith, Dave Bort, David Oppenheimer, David Wall, DawnChen, Eric Haugen, Eric Tune, Ethan Solomita, Gaurav Dhiman, Geeta Chaudhry, Greg Roelofs, Grzegorz Czajkowski,James Eady, Jarek Kusmierek, Jaroslaw Przybylowicz, Jason Hickey, Javier Kohen, Jeremy Lau, Jerzy Szczepkowski,John Wilkes, Jonathan Wilson, Joso Eterovic, Jutta Degener, Kai Backman, Kamil Yurtsever, Kenji Kaneda, Kevan Miller, Kurt Steinkraus, Leo Landa, Liza Fireman,Madhukar Korupolu, Mark Logan, Markus Gutschke, MattSparks, Maya Haridasan, Michael Abd-El-Malek, MichaelKenniston, Mukesh Kumar, Nate Calvin, Onufry Wojtaszczyk,Patrick Johnson, Pedro Valenzuela, Piotr Witusowski, PraveenKallakuri, Rafal Sokolowski, Richard Gooch, Rishi Gosalia, Rob Radez, Robert Hagmann, Robert Jardine, RobertKennedy, Rohit Jnagal, Roy Bryant, Rune Dahl, Scott Garriss, Scott Johnson, Sean Howarth, Sheena Madan, SmeetaJalan, Stan Chesnutt, Temo Arobelidze, Tim Hockin, ToddWang, Tomasz Blaszczyk, Tomasz Wozniak, Tomek Zielonka,Victor Marmol, Vish Kannan, Vrigo Gokhale, WalfredoCirne, Walt Drummond, Weiran Liu, Xiaopan Zhang, XiaoZhang, Ye Zhao, and Zohaib Maya.The Borg SRE team has also been crucial, and has included Adam Rogoyski, Alex Milivojevic, Anil Das, CodySmith, Cooper Bethea, Folke Behrens, Matt Liggett, JamesSanford, John Millikin, Matt Brown, Miki Habryn, Peter Dahl, Robert van Gent, Seppi Wilhelmi, Seth Hettich,Torsten Marek, and Viraj Alankar. The Borg configurationlanguage (BCL) and borgcfg tool were originally developedby Marcel van Lohuizen and Robert Griesemer.We thank our reviewers (especially Eric Brewer, MalteSchwarzkopf and Tom Rodeheffer), and our shepherd, Christos Kozyrakis, for their feedback on this paper. 参考资料 [1] O. A. Abdul-Rahman and K. Aida. Towards understanding the usage behavior of Google cloud users: the mice and elephants phenomenon. In Proc. IEEE Int’l Conf. on Cloud Computing Technology and Science (CloudCom), pages 272–277, Singapore, Dec. 2014. [2] Adaptive Computing Enterprises Inc., Provo, UT. Maui Scheduler Administrator’s Guide, 3.2 edition, 2011. [3] T. Akidau, A. Balikov, K. Bekiroglu, S. Chernyak, J. Haberman, R. Lax, S. McVeety, D. Mills, P. Nordstrom, and S. Whittle. MillWheel: fault-tolerant stream processing at internet scale. In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 734–746, Riva del Garda, Italy, Aug.2013 [4] Y. Amir, B. Awerbuch, A. Barak, R. S. Borgstrom, and A. Keren. An opportunity cost approach for job assignment in a scalable computing cluster. IEEE Trans. Parallel Distrib. Syst., 11(7):760–768, July 2000. [5] Apache Aurora. http://aurora.incubator.apache.org/, 2014. [6] Aurora Configuration Tutorial. https://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/, 2014 [7] AWS. Amazon Web Services VM Instances.http://aws.amazon.com/ec2/instance-types/, 2014. [8] J. Baker, C. Bond, J. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh. Megastore: Providing scalable, highly available storage for interactive services. In Proc. Conference on Innovative Data Systems Research (CIDR), pages 223–234, Asilomar, CA, USA, Jan. 2011. [9] M. Baker and J. Ousterhout. Availability in the Sprite distributed file system. Operating Systems Review, 25(2):95–98, Apr. 1991. [10] L. A. Barroso, J. Clidaras, and U. Holzle. The datacenter as a computer: an introduction to the design of warehouse-scale machines. Morgan Claypool Publishers, 2nd edition, 2013. [11] L. A. Barroso, J. Dean, and U. Holzle. Web search for a planet: the Google cluster architecture. In IEEE Micro, pages 22–28, 2003. [12] I. Bokharouss. GCL Viewer: a study in improving the understanding of GCL programs. Technical report, Eindhoven Univ. of Technology, 2008. MS thesis. [13] E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou, Z. Qian, M. Wu, and L. Zhou. Apollo: scalable and coordinated scheduling for cloud-scale computing. In Proc. USENIX Symp. on Operating Systems Design and Implementation(OSDI), Oct. 2014. [14] M. Burrows. The Chubby lock service for loosely-coupled distributed systems. In Proc. USENIX Symp. on Operating Systems Design and Implementation (OSDI), pages 335–350, Seattle, WA, USA, 2006. [15] cAdvisor. https://github.com/google/cadvisor,2014. [16] CFS per-entity load patches. http://lwn.net/Articles/531853, 2013. [17] cgroups. http://en.wikipedia.org/wiki/Cgroups,2014. [18] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient data-parallel pipelines. In Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation (PLDI), pages 363–375, Toronto, Ontario, Canada, 2010. [19] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: a distributed storage system for structured data. ACM Trans. on Computer Systems, 26(2):4:1–4:26, June 2008. [20] Y. Chen, S. Alspaugh, and R. H. Katz. Design insights for MapReduce from diverse production workloads. Technical Report UCB/EECS–2012–17, UC Berkeley, Jan. 2012. [21] C. Curino, D. E. Difallah, C. Douglas, S. Krishnan, R. Ramakrishnan, and S. Rao. Reservation-based scheduling: if you’re late don’t blame us! In Proc. ACM Symp. on Cloud Computing (SoCC), pages 2:1–2:14, Seattle, WA, USA,2014. [22] J. Dean and L. A. Barroso. The tail at scale. Communications of the ACM, 56(2):74–80, Feb. 2012. [23] J. Dean and S. Ghemawat. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107–113, 2008. [24] C. Delimitrou and C. Kozyrakis. Paragon: QoS-aware scheduling for heterogeneous datacenters. In Proc. Int’l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Mar. 201. [25] C. Delimitrou and C. Kozyrakis. Quasar: resource-efficient and QoS-aware cluster management. In Proc. Int’l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 127–144, Salt Lake City, UT, USA, 2014. [26] S. Di, D. Kondo, and W. Cirne. Characterization and comparison of cloud versus Grid workloads. In International Conference on Cluster Computing (IEEE CLUSTER), pages 230–238, Beijing, China, Sept. 2012. [27] S. Di, D. Kondo, and C. Franck. Characterizing cloud applications on a Google data center. In Proc. Int’l Conf. on Parallel Processing (ICPP), Lyon, France, Oct. 2013. [28] Docker Project. https://www.docker.io/, 2014. [29] D. Dolev, D. G. Feitelson, J. Y. Halpern, R. Kupferman, and N. Linial. No justified complaints: on fair sharing of multiple resources. In Proc. Innovations in Theoretical Computer Science (ITCS), pages 68–75, Cambridge, MA, USA, 2012. [30] ElasticSearch. http://www.elasticsearch.org, 2014. [31] D. G. Feitelson. Workload Modeling for Computer Systems Performance Evaluation. Cambridge University Press, 2014. [32] Fluentd. http://www.fluentd.org/, 2014. [33] GCE. Google Compute Engine. http://cloud.google.com/products/compute-engine/ [34] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google File System. In Proc. ACM Symp. on Operating Systems Principles (SOSP), pages 29–43, Bolton Landing, NY, USA, 2003. ACM. [35] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker, and I. Stoica. Dominant Resource Fairness: fair allocation of multiple resource types. In Proc. USENIX Symp. on Networked Systems Design and Implementation (NSDI), pages 323–326, 2011. [36] A. Ghodsi, M. Zaharia, S. Shenker, and I. Stoica. Choosy:max-min fair sharing for datacenter jobs with constraints. In Proc. European Conf. on Computer Systems (EuroSys), pages 365–378, Prague, Czech Republic, 2013. [37] D. Gmach, J. Rolia, and L. Cherkasova. Selling T-shirts and time shares in the cloud. In Proc. IEEE/ACM Int’l Symp. on Cluster, Cloud and Grid Computing (CCGrid), pages 539–546, Ottawa, Canada, 2012. [38] Google App Engine.http://cloud.google.com/AppEngine, 2014. [39] Google Container Engine (GKE). https://cloud.google.com/container-engine/,2015. [40] R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao, and A. Akella. Multi-resource packing for cluster schedulers. In Proc. ACM SIGCOMM, Aug. 2014. [41] Apache Hadoop Project. http://hadoop.apache.org/,2009. [42] Hadoop MapReduce Next Generation – Capacity Scheduler. http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html, 2013. [43] J. Hamilton. On designing and deploying internet-scale services. In Proc. Large Installation System Administration Conf. (LISA), pages 231–242, Dallas, TX, USA, Nov. 2007. [44] P. Helland. Cosmos: big data and big challenges. http://research.microsoft.com/en-us/events/fs2011/helland\\_cosmos\\_big\\_data\\_and\\_big\\_challenges.pdf, 2011. [45] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. Joseph, R. Katz, S. Shenker, and I. Stoica. Mesos: a platform for fine-grained resource sharing in the data center. In Proc. USENIX Symp. on Networked Systems Design and Implementation (NSDI), 2011. [46] IBM Platform Computing. http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/ products/clustermanager/index.html. [47] S. Iqbal, R. Gupta, and Y.-C. Fang. Planning considerations for job scheduling in HPC clusters. Dell Power Solutions, Feb. 2005. [48] M. Isaard. Autopilot: Automatic data center management. ACM SIGOPS Operating Systems Review, 41(2), 2007. [49] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. Quincy: fair scheduling for distributed computing clusters. In Proc. ACM Symp. on Operating Systems Principles (SOSP), 2009. [50] D. B. Jackson, Q. Snell, and M. J. Clement. Core algorithms of the Maui scheduler. In Proc. Int’l Workshop on Job Scheduling Strategies for Parallel Processing, pages 87–102. Springer-Verlag, 2001. [51] M. Kambadur, T. Moseley, R. Hank, and M. A. Kim. Measuring interference between live datacenter applications. In Proc. Int’l Conf. for High Performance Computing, Networking, Storage and Analysis (SC), Salt Lake City, UT, Nov. 2012. [52] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan. An analysis of traces from a production MapReduce cluster. In Proc. IEEE/ACM Int’l Symp. on Cluster, Cloud and Grid Computing (CCGrid), pages 94–103, 2010. [53] Kubernetes. http://kubernetes.io, Aug. 2014. [54] Kernel Based Virtual Machine. http://www.linux-kvm.org. [55] L. Lamport. The part-time parliament. ACM Trans. on Computer Systems, 16(2):133–169, May 1998. [56] J. Leverich and C. Kozyrakis. Reconciling high server utilization and sub-millisecond quality-of-service. In Proc. European Conf. on Computer Systems (EuroSys), page 4, 2014. [57] Z. Liu and S. Cho. Characterizing machines and workloads on a Google cluster. In Proc. Int’l Workshop on Scheduling and Resource Management for Parallel and Distributed Systems (SRMPDS), Pittsburgh, PA, USA, Sept. 2012. [58] Google LMCTFY project (let me contain that for you). http://github.com/google/lmctfy, 2014. [59] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In Proc. ACM SIGMOD Conference, pages 135–146, Indianapolis, IA, USA, 2010. [60] J. Mars, L. Tang, R. Hundt, K. Skadron, and M. L. Soffa. Bubble-Up: increasing utilization in modern warehouse scale computers via sensible co-locations. In Proc. Int’l Symp. on Microarchitecture (Micro), Porto Alegre, Brazil, 2011. [61] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton, and T. Vassilakis. Dremel: interactive analysis of web-scale datasets. In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 330–339, Singapore, Sept. 2010. [62] P. Menage. Linux control groups. http://www.kernel.org/doc/Documentation/cgroups/cgroups.txt, 2007–2014. [63] A. K. Mishra, J. L. Hellerstein, W. Cirne, and C. R. Das. Towards characterizing cloud backend workloads: insights from Google compute clusters. ACM SIGMETRICS Performance Evaluation Review, 37:34–41, Mar. 2010. [64] A. Narayanan. Tupperware: containerized deployment at Facebook. http://www.slideshare.net/dotCloud/tupperware-containerized-deployment-at-facebook, June 2014. [65] K. Ousterhout, P. Wendell, M. Zaharia, and I. Stoica. Sparrow: distributed, low latency scheduling. In Proc. ACM Symp. on Operating Systems Principles (SOSP), pages 69–84, Farminton, PA, USA, 2013. [66] D. C. Parkes, A. D. Procaccia, and N. Shah. Beyond Dominant Resource Fairness: extensions, limitations, and indivisibilities. In Proc. Electronic Commerce, pages 808–825, Valencia, Spain, 2012. [67] Protocol buffers. https://developers.google.com/protocol-buffers/, and https://github.com/google/protobuf/., 2014. [68] C. Reiss, A. Tumanov, G. Ganger, R. Katz, and M. Kozuch. Heterogeneity and dynamicity of clouds at scale: Google trace analysis. In Proc. ACM Symp. on Cloud Computing(SoCC), San Jose, CA, USA, Oct. 2012. [69] M. Schwarzkopf, A. Konwinski, M. Abd-El-Malek, and J. Wilkes. Omega: flexible, scalable schedulers for large compute clusters. In Proc. European Conf. on Computer Systems (EuroSys), Prague, Czech Republic, 2013. [70] B. Sharma, V. Chudnovsky, J. L. Hellerstein, R. Rifaat, and C. R. Das. Modeling and synthesizing task placement constraints in Google compute clusters. In Proc. ACM Symp. on Cloud Computing (SoCC), pages 3:1–3:14, Cascais, Portugal, Oct. 2011. [71] E. Shmueli and D. G. Feitelson. On simulation and design of parallel-systems schedulers: are we doing the right thing? IEEE Trans. on Parallel and Distributed Systems, 20(7):983–996, July 2009. [72] A. Singh, M. Korupolu, and D. Mohapatra. Server-storage virtualization: integration and load balancing in data centers. In Proc. Int’l Conf. for High Performance Computing, Networking, Storage and Analysis (SC), pages 53:1–53:12, Austin, TX, USA, 2008. [73] Apache Spark Project. http://spark.apache.org/, 2014. [74] A. Tumanov, J. Cipar, M. A. Kozuch, and G. R. Ganger. Alsched: algebraic scheduling of mixed workloads in heterogeneous clouds. In Proc. ACM Symp. on Cloud Computing (SoCC), San Jose, CA, USA, Oct. 2012. [75] P. Turner, B. Rao, and N. Rao. CPU bandwidth control for CFS. In Proc. Linux Symposium, pages 245–254, July 2010. [76] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, B. Saha, C. Curino, O. O’Malley, S. Radia, B. Reed, and E. Baldeschwieler. Apache Hadoop YARN: Yet Another Resource Negotiator. In Proc. ACM Symp. on Cloud Computing (SoCC), Santa Clara, CA, USA, 2013. [77] VMware VCloud Suite. http://www.vmware.com/products/vcloud-suite/. [78] A. Verma, M. Korupolu, and J. Wilkes. Evaluating job packing in warehouse-scale computing. In IEEE Cluster, pages 48–56, Madrid, Spain, Sept. 2014. [79] W. Whitt. Open and closed models for networks of queues. AT&amp;T Bell Labs Technical Journal, 63(9), Nov. 1984. [80] J. Wilkes. More Google cluster data. http://googleresearch.blogspot.com/2011/11/more-google-cluster-data.html, Nov. 2011. [81] Y. Zhai, X. Zhang, S. Eranian, L. Tang, and J. Mars. HaPPy: Hyperthread-aware power profiling dynamically. In Proc. USENIX Annual Technical Conf. (USENIX ATC), pages 211–217, Philadelphia, PA, USA, June 2014. USENIX Association. [82] Q. Zhang, J. Hellerstein, and R. Boutaba. Characterizing task usage shapes in Google’s compute clusters. In Proc. Int’l Workshop on Large-Scale Distributed Systems and Middleware (LADIS), 2011. [83] X. Zhang, E. Tune, R. Hagmann, R. Jnagal, V. Gokhale, and J. Wilkes. CPI2: CPU performance isolation for shared compute clusters. In Proc. European Conf. on Computer Systems (EuroSys), Prague, Czech Republic, 2013. [84] Z. Zhang, C. Li, Y. Tao, R. Yang, H. Tang, and J. Xu. Fuxi: a fault-tolerant resource management and job scheduling system at internet scale. In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 1393–1404. VLDB Endowment Inc., Sept. 2014.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Borg","slug":"Borg","permalink":"https://wangqian0306.github.io/tags/Borg/"}]},{"title":"LaTeX","slug":"tmp/latex","date":"2021-12-20T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/latex/","permalink":"https://wangqian0306.github.io/2021/latex/","excerpt":"","text":"LaTeX 简介 LaTeX 是一种基于 Τ Ε Χ 的排版系统。经常用于生成复杂表格个数学公式。 在 Hexo 中使用 LaTeX 公式 进入 Hexo 项目内的根目录输入如下命令 1npm i hexo-math --save 在配置文件中新增如下配置项目： 123456789101112math: katex: css: &#x27;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css&#x27; options: throwOnError: false mathjax: css: &#x27;https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css&#x27; options: conversion: display: false tex: svg: 在目标文章中使用如下格式插入所需内容 123&#123;% mathjax %&#125;\\frac&#123;1&#125;&#123;x^2-1&#125;&#123;% endmathjax %&#125; 参考资料 Overleaf hexo-math","categories":[{"name":"LaTeX","slug":"LaTeX","permalink":"https://wangqian0306.github.io/categories/LaTeX/"}],"tags":[{"name":"LaTeX","slug":"LaTeX","permalink":"https://wangqian0306.github.io/tags/LaTeX/"}]},{"title":"Spanner: Google’s Globally Distributed Database 中文翻译版","slug":"treatise/spanner_googles_globally_distributed_database","date":"2021-12-15T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/spanner_googles_globally_distributed_database/","permalink":"https://wangqian0306.github.io/2021/spanner_googles_globally_distributed_database/","excerpt":"","text":"Spanner: Google’s Globally Distributed Database 中文翻译版 作者： JAMES C. CORBETT, JEFFREY DEAN, MICHAEL EPSTEIN, ANDREW FIKES,CHRISTOPHER FROST, J. J. FURMAN, SANJAY GHEMAWAT, ANDREY GUBAREV,CHRISTOPHER HEISER, PETER HOCHSCHILD, WILSON HSIEH,SEBASTIAN KANTHAK, EUGENE KOGAN, HONGYI LI, ALEXANDER LLOYD,SERGEY MELNIK, DAVID MWAURA, DAVID NAGLE, SEAN QUINLAN, RAJESH RAO,LINDSAY ROLIG, YASUSHI SAITO, MICHAL SZYMANIAK, CHRISTOPHER TAYLOR,RUTH WANG, and DALE WOODFORD, Google, Inc. 原版的版权说明 1234567891011121314This article is essentially the same (with minor reorganizations, corrections, and additions) as the paper ofthe same title that appeared in the Proceedings of OSDI 2012.Authors’ address: J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghemawat,A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh (corresponding author), S. Kanthak, E. Kogan, H. Li,A. Lloyd, S. Melnik, D. Mwaura, D. Nagle, S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak,C. Taylor, R. Wang, and D. Woodford, Google, Inc. 1600 Amphitheatre Parkway, Mountain View, CA 94043;email: wilsonh@google.com.Permission to make digital or hard copies of part or all of this work for personal or classroom use is grantedwithout fee provided that copies are not made or distributed for profit or commercial advantage and thatcopies bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses contact the Owner/Author.2013 Copyright is held by the author/owner(s).0734-2071/2013/08-ART8DOI:http://dx.doi.org/10.1145/2491245 摘要 Spanner 是 Google 的一种数据库，它具有：可扩展，多版本，全球分布式部署，同步复制的特性。 它是第一个在全球范围内分发数据并支持外部一致性的分布式事务的系统。 这篇文章描述了 Spanner 是如何设计的，它的特性，各种设计决策的基本原理，以及一个暴露时钟不确定性的新的时间 API。 该 API 及其实现对于支持外部一致性和各种强大功能至关重要：过去的非阻塞读取、无锁快照事务和原子模式更改，这些功能跨越了整个 Spanner。 General Terms Design, Algorithms, Performance 关键词 分布式数据库，并发控制，副本集，事务性，时间管理。 1 引言 Spanner 是一个可扩展的全球分布式数据库，由 Google 设计、实施和部署。 在最高的抽象层次上，它是一个数据库，可以在遍布世界各地的数据中心的多组 Paxos [Lamport 1998] 状态机上分割数据。 在全球的数据中心上进行复制来满足本地的使用需求，客户端可以自动在副本之间进行故障切换。 Spanner 会随着数据量或服务器数量的变化自动而跨机器重新对数据进行分片，并自动跨机器（甚至跨数据中心）迁移数据以平衡负载和响应故障。 Spanner 在设计之初的目标是扩展至数百个数据中心中的百万台设备上支持数兆(万亿)行数据的存储。 应用程序可以使用 Spanner 的高可用性，即使在面临广域自然灾害时，通过在大陆内甚至跨大陆复制其数据也是如此。 我们最初的客户是 F1 [Shute et al. 2012]，重写了谷歌的广告后端。 F1 使用分布在美国各地的五个副本。 大多数其他应用程序可能会在一个地理区域的 3 到 5 个数据中心之间复制其数据，但具有相对独立的故障模式。 也就是说，大多数应用程序将选择更低的延迟而不是更高的可用性，只要它们能够在 1 或 2 个数据中心故障中幸存下来。 Spanner 的主要重点是管理跨数据中心的复制数据，但我们也花了大量时间在分布式系统基础架构之上设计和实现重要的数据库功能。 尽管许多项目很乐意使用 Bigtable [Chang et al. 2008]，我们还不断收到用户抱怨 Bigtable 可能难以用于某些类型的应用程序：那些具有复杂的、不断发展的模式的应用程序，或者那些在存在广域复制的情况下需要强一致性的应用程序。 (其他作者也提出了类似的主张 [Stonebraker 2010b]。) Google 的许多应用程序都选择使用 Megastore [Baker et al. 2011] 因为它的半数据模型和对同步复制的支持，尽管它的写入吞吐量相对较差。 因此，Spanner 已从类似 Bigtable 的版本化键值存储演变为时态多版本数据库。 数据存储在模式化的半关系表中；数据是版本化的，每个版本都会自动加上其提交时间的时间戳；旧版本的数据受可配置的垃圾收集策略的约束；应用程序可以读取旧时间戳的数据。 Spanner 支持通用事务，并提供基于 SQL 的查询语言。 作为一个全球分布式数据库，Spanner 提供了几个有趣的特性。 首先，数据的复制配置可以由应用程序进行细粒度动态控制。 应用程序可以指定约束来控制哪些数据中心包含哪些数据、数据与其用户的距离(以控制读取延迟)、副本彼此之间的距离(以控制写入延迟)以及维护的副本数量(以控制持久性、可用性和读取性能)。 系统还可以在数据中心之间动态透明地移动数据，以平衡数据中心之间的资源使用。 其次，Spanner 有两个难以在分布式数据库中实现的特性：它提供外部一致的 [Gifford 1982] 读取和写入，以及跨数据库同一时间戳的一致性读取。 这些功能使 Spanner 能够支持一致的备份、一致的 MapReduce 执行 [Dean and Ghemawat 2010] 和原子性的更新，所有这些都在全球范围内，甚至在正在进行的事务的情况下。 这些功能是由 Spanner 为事务分配全局有意义的提交时间戳这一事实所启用的，即使事务可能是分布式的。 时间戳反映了序列化顺序。 此外，序列化顺序满足外部一致性(或等效的线性化 [Herlihy and Wing 1990])：如果事务 T1 在另一个事务 T2 开始之前提交，则 T1 的提交时间戳小于 T2 的。 Spanner 是第一个在全球范围内提供此类保证的系统。 这些属性的关键促成因素是新的 TrueTime API 及其实现。 API 直接暴露了时钟不确定性，对 Spanner 时间戳的保证取决于实现提供的边界。 如果不确定性很大，Spanner 会放慢速度以等待不确定性消失。 Google 的集群管理软件提供了 TrueTime API 的实现。 此实现通过使用多个现代时钟参考(GPS 和原子钟)将不确定性保持在很小(通常小于 10 毫秒)。 保守地报告不确定性对于正确性是必要的； 保持对不确定性的限制很小对于性能来说是必要的。 第 2 节描述了 Spanner 实现的结构、它的特性集，以及在设计过程中的方案。 第 3 节描述了我们新的 TrueTime API 并概述了它的实现。 第 4 节描述了 Spanner 如何使用 TrueTime 实现外部一致性分布式事务、无锁快照事务和原子模式更新。 第 5 节提供了有关 Spanner 性能和 TrueTime 行为的一些基准，并讨论了 F1 的经验。 第 6、7 和 8 节描述了相关和未来的工作，并总结了我们的结论。 2 实现方式 本节描述 Spanner 实现的结构和基本原理。 然后描述目录的抽象过程，将其用于管理副本和存储位置，并将目录作为数据移动的基本单元。 最后，它描述了我们的数据模型，为什么 Spanner 看起来像关系数据库而不是键值存储，以及应用程序如何控制数据的存储地点。 一个 Spanner 部署的实例被称为 Universe。 鉴于 Spanner 在全球范围内管理数据，因此只有少数几个正在运行的 Universe。 目前有一个测试环境，一个研发/生产环境，一个只用于生产的环境。 Spanner 被组织为一组区域(zone)，其中每个区域都是 Bigtable 服务器部署的粗略模拟 [Chang et al. 2008]。 区域是管理部署的单位。 区域集也是可以复制数据的位置集。 当新数据中心投入使用和旧数据中心关闭时，可以分别在正在运行的系统中添加或删除区域。 区域也是物理隔离的单位：一个数据中心内可能有一个或多个区域，例如，如果不同应用程序的数据必须在同一数据中心内的不同服务器组之间进行分区。 图 1 说明了 Spanner Universe 中的服务器组织架构。 一个区域有一个 zonemaster 和一百到几千个 spanserver。 前者将数据分配给 spanservers；后者为客户提供数据。 客户端使用每个区域的位置代理来定位分配给其数据的跨服务器。 Universe 主控(universe master)和布局驱动程序(placement driver)目前是单例的。 Universe 主控器主要是一个控制台，用于显示有关所有区域的状态信息以进行交互式调试。 布局驱动程序以分钟为单位处理跨区域的自主数据移动。 布局驱动程序定期与 spanserver 通信以查找需要移动的数据，以满足更新的复制约束或平衡负载。 由于篇幅原因，我们只会详细描述 spanserver 。 2.1 Spanserver 软件栈 本节重点介绍 spanserver 实现，以说明复制如何分布式事务已分层到我们基于 Bigtable 的实现中。 软件堆栈如图 2 所示。 在底部，每个 spanserver 负责一个称为 tablet 的数据结构的 100 到 1000 个实例。 tablet 类似于 Bigtable 的 tablet 抽象，因为它实现了以下映射的包。 1(key:string, timestamp:int64) → string 与 Bigtable 不同，Spanner 为数据分配时间戳，这使 Spanner 更像是一个多版本数据库而不是键值存储工具。 tablet 的状态存储在一组类似 B 树的文件和预写日志中，所有这些都存储在 Colossus 分布式文件系统中(Google 文件系统的继任者 [Ghemawat et al. 2003])。 为了支持复制，每个 spanserver 在每个 tablet 上实现了一个单独的 Paxos 状态机。 (早期的 Spanner 版本支持每个 tablet 多个 Paxos 状态机，这允许更灵活的复制配置。但该设计的复杂性导致我们放弃了它。) 每个状态机存储其元数据并写入其相应的 tablet。 我们的 Paxos 实现支持具有基于时间的领导者租用的长期领导者，其长度默认为 10 秒。 当前的 Spanner 实现记录每个 Paxos 写入两次：一次在平板电脑的日志中，一次在 Paxos 日志中。 这个选择是出于权宜之计，我们很可能最终会解决这个问题。 我们对 Paxos 的实现是流水线式的，以便在存在 WAN 延迟的情况下提高 Spanner 的吞吐量。 通过“流水线”，我们指的是 Lamport 的“多法令议会” [Lamport 1998]，它既分摊了跨多项法令选举领导人的成本，又允许对不同法令进行同时投票。 重要的是要注意，虽然法令可能会被无序批准，但法令是按顺序应用的(我们将在第 4 节中依赖这一事实)。 Paxos 状态机用于实现一致并且具有副本的一组键值对映射。 每个副本的键值映射状态存储在其对应的 tablet 中。 写入操作必须在领导者处发起 Paxos 协议；读取操作可以直接访问任何最够新的副本。副本的集合构成了一个 Paxos 组。 在作为领导者的每个副本上，spanserver 实现了一个锁表来实现并发控制。 锁表包含两阶段锁定的状态：它将键的范围映射到锁定状态。(请注意，拥有一个长期存在的 Paxos 领导者对于有效管理锁表至关重要。) 在 Bigtable 和 Spanner 中，我们为长期事务(例如，生成报告，可能需要几分钟的时间)，在存在冲突的少量并发控制下表现不佳。 需要同步的操作，比如事务性读，获取锁表中的锁；其他操作绕过(bypass)锁表。 锁表的状态大多是易变的(即不通过 Paxos 复制)：我们解释细节在第 4.2.1 节中。 在作为领导者的每个副本上，每个 spanserver 还实现了一个事务管理器来支持分布式事务。 事务管理器用于实现参与者领导者；组中的其他副本将被称为参与者从属。 如果一个事务只涉及一个 Paxos 组(大多数事务都是这种情况)，它可以绕过事务管理器，因为锁表和 Paxos 一起提供了事务性。 如果一个事务涉及多个 Paxos 组，这些组的领导者会协调执行两阶段提交。 选择其中一个参与者组作为协调者：该组的参与者领导者将被称为协调者领导者，而该组的从属则被称为协调者从属。 每个事务管理器的状态都存储在底层 Paxos 组中(因此被复制)。 2.2 目录和位置 在键值映射包之上，Spanner 实现支持称为目录的分桶抽象，这是一组共享公共前缀的连续键。 (目录这一术语的选择是一个历史意外；更好的术语应该是桶(bucket)。) 我们将在 2.3 节解释该前缀的来源。 支持目录允许应用程序通过仔细选择键来控制其数据的位置。 目录是数据放置的单位。 目录中的所有数据都具有相同的复制配置。 当数据在 Paxos 组之间移动时，它会逐个目录移动，如图 3 所示。 Spanner 可能会移动一个目录以减轻 Paxos 组的负载；将经常访问的目录放在同一组中；或者将目录移动到更接近其访问者的组中。 Paxos 组可能包含多个目录的事实意味着 Spanner tablet 与 Bigtable tablet不同：前者不一定是行空间的按字典顺序排列的单个分区。 相反，Spanner tablet 是一个容器，可以在行内的空间封装多个分区。我们做出这个决定是为了可以将多个经常一起访问的目录一起放置。 Movedir 是用于在 Paxos 组之间移动目录的后台任务 [Douceur and Howell 2006]。 Movedir 还可以通过将组内的所有数据移动到具有所需配置的新组，从而在 Paxos 组中添加或删除副本 [Lorch et al. 2006](因为 Spanner 尚不支持在 Paxos 中更改配置)。 Movedir 不是作为单个事务实现的，以避免在大量数据移动时阻塞正在进行的读取和写入。 相反，Movedir 记录了它开始移动数据的事件并在后台移动数据的事实。当它移动了除指定数量之外的所有数据时，它使用事务来原子地移动名义数量并更新两个 Paxos 组的元数据。 目录也是应用程序可以指定其地理复制属性(或简称放置)的最小单元。 我们的放置规范语言的设计将管理复制配置的职责分开。 管理员控制两个维度：副本的数量和类型，以及这些副本的地理位置。 他们在这两个维度中创建了一个命名选项菜单(例如，北美，用 1 个见证者(witness)复制了 5 种数据)。 应用程序通过使用这些选项的组合标记每个数据库和/或单个目录来控制数据的复制方式。 例如，应用程序可能将每个最终用户的数据存储在其自己的目录中，这将使用户 A 的数据在欧洲拥有三个副本，而用户 B 的数据在北美拥有五个副本。 为了说明的清晰，我们过度简化了实际情况。 实际上，如果目录变得太大，Spanner 会将目录分片成多个片段。 而片段可能来自不同的 Paxos 组(因此也可能来自不同的服务器)。 Movedir 实际上在组之间移动片段，而不是整个目录。 2.3 数据模型 Spanner 向应用程序公开了以下数据特性集：基于模式化半关系表的数据模型、查询语言和通用事务。 有很多因素为上述功能做了支持和驱动。 Megastore 的流行支持了支持模式化半关系表和同步复制的需求 [Baker et al. 2011]。 Google 内部至少有 300 个应用程序使用 Megastore(尽管其性能相对较低)，因为它的数据模型比 Bigtable 的更易于管理，并且支持跨数据中心的同步复制。 (Bigtable 仅支持跨数据中心的最终一致性复制。) 使用 Megastore 的著名 Google 应用程序示例包括 Gmail、Picasa、Calendar、Android Market 和 AppEngine。 考虑到 Dremel [Melnik et al.2010]作为交互式数据分析工具的流行，在 Spanner 中支持类似 SQL 的查询语言的需求也很明显。 最后，Bigtable 缺少跨行事务导致投诉频繁； Percolator [Peng and Dabek 2010] 部分是为了解决这个失败。 一些作者声称，由于它带来的性能或可用性问题，一般两阶段提交的成本太高而无法支持 [Chang et al. 2008; Cooper et al. 2008; Helland 2007]。 我们认为最好让应用程序开发者处理由于出现瓶颈时过度使用事务而导致的性能问题，而不是总是围绕缺少事务进行编码。 在 Paxos 上运行两阶段提交可以缓解可用性问题。 应用程序数据模型位于实现支持的目录(桶)键值映射之上。 应用程序在一个 Universe 中创建一个或多个数据库。每个数据库可以包含无限数量的模式化表。 表看起来像关系型数据库的表，具有行、列和版本值。 我们不会详细介绍 Spanner 的查询语言。它看起来像带有一些扩展的 SQL，以支持协议缓冲区值(protocol-buffer-valued) [Google 2008] 字段。 Spanner 的数据模型不是纯粹的关系型，因为行必须有名称。 更准确地说，每个表都需要有一组有序的一个或多个主键列。 这个要求是 Spanner 仍然看起来像一个键值存储的地方：主键形成行的名称，每个表定义从主键列到非主键列的映射。 仅当为行的键定义了某个值(即使它是 NULL)时，该行才存在。 强加这种结构很有用，因为它让应用程序可以通过它们对键的选择来控制数据局部性。 123456789CREATE TABLE Users &#123; uid INT64 NOT NULL, email STRING&#125; PRIMARY KEY (uid), DIRECTORY;CREATE TABLE Albums &#123; uid INT64 NOT NULL, aid INT64 NOT NULL, name STRING&#125; PRIMARY KEY (uid, aid), INTERLEAVE IN PARENT Users ON DELETE CASCADE; 图 4 包含一个示例 Spanner 数据模型，用于在每个用户、每个相册的基础上存储照片元数据。 建表语句类似于 Megastore 但附加要求是每个 Spanner 数据库必须由客户端划分为一个或多个表层次结构。 客户端应用程序通过 INTERLEAVE IN 声明在数据库模式中声明层次结构。 表的层次结构顶部的是目录表。 具有键 K 的目录表中的每一行，连同按字典顺序以 K 开头的后代表中的所有行，形成一个目录。 ON DELETE CASCADE 表示删除目录表中的一行会删除任何关联的子行。 该图还说明了示例数据库的交错布局：例如，Albums(2,1) 表示用户 ID 为 2、相册 ID 为 1 的相册表中的行。 这种将表交错以形成目录非常重要，因为它允许客户端描述多个表之间存在的位置关系，这对于在分片分布式数据库中获得良好性能是必要的。 没有它，Spanner 就不会知道最重要的位置关系。 3 TrueTime 方法 返回数据 TT.now() TTinterval: [ earliest, latest] TT.after(t) 如果 t 已经大于当前时间则返回真 TT.before(t) 如果 t 已经小于当前时间则返回真 表 1：TrueTime API。参数 t 的类型是 TTstamp。 本节介绍 TrueTime API 并概述其实现。 我们将大部分细节留给另一篇文章：我们的目标是展示拥有这样一个 API 的好处。 表 I 列出了 API 的方法。 TrueTime 明确地将时间表示为 TTinterval，这是一个具有有限时间不确定性的间隔(不同于标准时间接口，它给客户端没有不确定性的概念)。 TTinterval 的端点属于 TTstamp 类型。 TT.now() 方法返回一个 TTinterval，它保证包含调用 TT.now() 的绝对时间。 时间纪元类似于带有闰秒拖尾的 UNIX 时间。 定义瞬时误差界为 ϵ，它是区间宽度的一半，平均误差界为 ⋷。 TT.after() 和 TT.before() 方法是围绕 TT.now() 的便捷包装器。 用函数 tabs(e) 表示事件 e 的绝对时间。 在更正式的情况下，TrueTime 保证对于调用 tt = TT.now()，tt.earliest ≤ tabs(enow) ≤ tt.latest，其中 enow 是调用事件。 TrueTime 使用的基础时间参考是 GPS 和原子钟。 TrueTime 使用两种形式的时间参考，因为它们具有不同的故障模式。 GPS 参考源漏洞包括天线和接收器故障、本地无线电干扰、相关故障(例如，不正确的闰秒处理和欺骗等设计故障)以及 GPS 系统中断。 原子钟可能会以与 GPS 和彼此无关的方式出现故障，并且在很长一段时间内可能会由于频率误差而显着漂移。 TrueTime 由每个数据中心的一组时间主机和每个机器的时间从守护程序实现。 大多数领导节点都有带专用天线的 GPS 接收器；这些主机在物理上是分开的，以减少天线故障、无线电干扰和欺骗的影响。 其余的领导节点(我们称之为世界末日领导节点)配备了原子钟。 原子钟并没有那么贵：世界末日领导节点的成本与 GPS 领导节点的成本相同。 所有领导节点的时间参考都会定期相互比较。 每个主节点还对照自己的本地时钟交叉检查其参考提前时间的速率，如果存在实质性差异，则将自身驱逐。 在同步之间，世界末日领导节点会宣传缓慢增加的时间不确定性，该不确定性源自保守应用的最坏情况时钟漂移。 GPS 主机运行异常的不确定性接近于零。 每个守护进程都会轮询各种 master [Mills 1981]，以减少任何一个 master 出错的脆弱性。 有些是从附近的数据中心选择的 GPS 领导节点；其余的是来自更远数据中心的 GPS 领导节点，以及一些世界末日领导节点。 守护进程应用 Marzullo 算法的变体 [Marzullo 和 Owicki 1983] 来检测和拒绝说谎者，并将本地机器时钟与非说谎者同步。 为了防止本地时钟中断，出现频率偏移大于从组件规范和操作环境得出的最坏情况界限的机器将被驱逐。 正确性取决于确保执行最坏情况的界限。 在同步期间，守护进程会表现出逐渐增加的时间不确定性。 ϵ 是保守情况下最坏的本地时钟漂移。 ϵ 还取决于时间主机的不确定性和与时间主机的通信延迟。 在我们的环境当中 ϵ 通常是时间的锯齿函数，每个轮询间隔从大约 1 到 7 毫秒不等。 ⋷ 因此就是 4 毫秒。 守护进程的轮询间隔当前为 30 秒，当前应用的漂移率设置为 200 微秒/秒，这加在一起占了 0 到 6 毫秒的锯齿边界。 剩余的 1 ms 来自到时间主机的通信延迟。在出现故障的情况下，可能会偏离此锯齿形边界。 例如，偶尔的时间主机不可用会导致数据中心范围内的 ϵ 增加。 同样，设备过载的和网络链接可能会导致偶尔出现 ϵ 产生局部峰值。 正确性不受 ϵ 方差的影响，因为 Spanner 可以等待不确定性，但如果 ϵ 增加太多，性能会下降。 4 并发控制 本节介绍如何使用 TrueTime 来保证围绕并发控制的正确性，以及如何使用这些属性来实现过去的外部一致性事务、无锁快照事务和非阻塞读取等特性。 例如，这些功能可以保证在时间戳 t 读取的整个数据库审计将准确看到截至 t 提交的每个事务的影响。 展望未来，区分 Paxos 所见的写入(除非上下文明确，否则我们将其称为 Paxos 写入)与 Spanner 客户端写入将非常重要。 例如，两阶段提交为准备阶段生成一个 Paxos 写入，没有对应的 Spanner 客户端写入。 4.1 时间戳管理 操作 讨论的章节 并发控制方式 需求副本 读写事务 4.1.2 悲观锁 领导节点 快照事务 4.1.4 无锁 领导节点需要指定的时间戳，其余任何读取端参见4.1.3 快照读取，客户端选择的时间戳 - 无锁 任意节点，参见4.1.3 快照读取，客户端选择的时间区域 4.1.3 无锁 任意节点，参见4.1.3 表 2：Spanner 中读取和写入的类型，以及它们的对比。 表 2 列出了 Spanner 支持的操作类型。 Spanner 实现支持读写事务、快照事务(预先声明的快照隔离事务)和快照读取。 独立写入作为读写事务实现；快照独立读取作为快照事务实现。 两者都在内部重试(客户端不需要编写自己的重试循环)。 快照事务是一种具有快照隔离性能优势的事务 [Berenson et al. 1995]。 快照事务必须预先声明为没有任何写入；它不仅仅是一个没有任何写入的读写事务。 快照事务中的读取在系统选择的时间戳执行而不锁定，因此不会阻止传入的写入。 快照事务中读取的执行可以在任何足够最新的副本上进行(第 4.1.3 节)。 快照读取是过去的快照在没有锁定的情况下执行的读取。 客户端可以为快照读取指定时间戳，也可以提供所需时间戳过时的上限并让 Spanner 选择时间戳。 在任何一种情况下，快照读取的执行都会在任何足够最新的副本上进行。 对于快照事务和快照读取，一旦选择了时间戳，提交就不可避免，除非该时间戳的数据已被垃圾收集。 因此，客户端可以避免在重试循环内缓冲结果。 当服务器出现故障时，客户端可以通过重复时间戳和当前读取位置在内部继续在不同服务器上的查询。 4.1.1 Paxos 领导租期 Spanner 的 Paxos 实现使用定时租约来延长领导权(默认为 10 秒)。 潜在的领导者发送定时租约投票请求；在收到法定人数的租约投票后，领导节点知道它持有租约。 副本在成功写入时隐式地延长其租用投票，如果它们即将到期，领导者会请求租用投票扩展。 定义一个领导者的租用间隔，当它发现它有法定租期时开始，并在它不再有法定租期时结束(因为有些已经过期)。 Spanner 依赖于以下不相交不变量：对于每个 Paxos 组，每个 Paxos 领导节点的租期间隔与其他领导者的租用间隔不相同。 第 4.2.5 节描述了如何强制执行此不变量。 Spanner 的实现允许 Paxos 领导节点通过释放其从租约投票来移除从属节点。 为了保持不相交性不变，Spanner 限制了何时允许移除节点。 将 smax 定义为领导者使用的最大时间戳。 后续部分将描述何时推进 smax 。 在退位之前，领导节点必须等到 TT.after(smax) 的返回结果为真。 4.1.2 为 RW 事务分配时间戳 事务读取和写入使用严格的两阶段锁定。 因此，可以在获取所有锁后但在释放任何锁之前的任何时间为它们分配时间戳。 对于给定的事务，Spanner 为其分配 Paxos 分配给代表事务提交的 Paxos 写入的时间戳。 Spanner 依赖于以下单调不变量：在每个 Paxos 组中，Spanner 以单调递增的顺序为 Paxos 写入分配时间戳，甚至跨越领导者。 单个领导副本可以按单调递增的顺序轻松分配时间戳。 这个不变性通过使用不相交不变性在领导节点之间强制执行：领导节点必须只在其领导节点租用的间隔内分配时间戳。 请注意，每当分配时间戳 s 时，smax 都会前进到 s 以保持不相交性。 Spanner 还强制执行以下外部一致性不变量：如果事务 T2 的开始发生在事务 T 1 提交之后，则 T2 的提交时间戳必须大于 T 1 的提交时间戳。 我们定义事务 Ti 的起始时间为 和提交时间为 ；并且在 T i 事务的提交时间戳为 s i。、 我们可以得出如下结论 。 用于执行事务和分配时间戳的协议遵循两条规则，这两条规则共同保证了该不变量，如下所示。 协调者节点定义了到达时间 Ti 的提交请求为 。 在起始阶段中：协调者节点写入 Ti 事务时分配的提交时间 si 不得小于在 计算之后的 TT.now().latest。 请注意，是否有领导者作为参与在这里无关紧要；第 4.2.1 节描述了它们如何参与下一条规则的实现。 在提交等待阶段中：协调者节点保证了客户端无法看到任何在 Ti 时提交的数据，仅当在 TT.after(si) 返回内容为真时才能可见。 提交等待阶段保证了在 si 小于提交时间 Ti，或者 。 第 4.2.1 节描述了提交等待的实现。证明如下： (提交等待) (假设) (因果关系) (起始) (传递性) 4.1.3 在某一时间点的服务器读取 第 4.1.2 节中描述的单调不变性允许 Spanner 确定副本的状态是否足以满足读取的要求。 每个副本都追踪名为 safe time 的参数 tsafe，此参数说明了副本最大的更新时间戳。 此副本可以安全的被读取，仅需满足读取时间戳为 t ，t &lt;= tsafe。 定义如下 ，Paxos 状态机当中的安全时间为 每个事务管理器的安全时间为 。 的计算方式非常简单：它是最新一次通过的 Paxos 写入事件的时间戳。 因为时间戳是单调增加的，并且写入的法令是按顺序通过的，所以在 Paxos 中写入不会低于 。 如果不存在准备(但未提交)的事务(即两阶段提交中处于两个阶段之间的事务)，则复制副本上的 是无限大。 对于参与者从属节点， 实际上是指副本领导节点的事务管理器，从属节点可以通过传递给 Paxos 写入的元数据推断其状态。 如果存在任何此类事务，则受这些事务影响的状态是不确定的：参与者副本尚不知道此类事务是否会提交。 正如我们在第 4.2.1 节中讨论的，提交协议确保每个参与者都知道准备好的事务时间戳的下限。 对于组 g 来说每个参与到过程中的领导节点在事务 Ti 为准备的数据分配了一个准备时间戳 。 协调领导节点会确保在所有组 g 中的参与者的事务的提交时间戳 。 因此，在组 g 中的每一个副本在事务 Ti 的准备时间都满足公式 。 4.1.4 为 RO 事务分配时间戳 快照事务分为两个阶段执行：分配一个时间戳 sread [Chan and Gray 1985]，然后将事务的读取作为快照读取操作在 sread 执行。 快照读取可以在任何足够最新的副本上执行。 在事务开始后的任何时间可以简单分配 sread = TT.now().latest，通过类似于第 4.1.2 节中为写入提供的参数来保持外部一致性。 但是，这样的时间戳可能在 sread 执行数据读取从而使得 tsafe 阻塞没有正常执行。 (此外，值得注意的是选择了 sread 可能会使 smax 产生不相关性。) 为了减少阻塞的产生，Spanner 应该分配最旧的时间戳以保持外部一致性。 4.2.2 节解释了如何选择这样的时间戳。 4.2 实现细节 本节解释了之前省略的读写事务和快照事务的一些实际细节，以及用于实现原子模式更改的特殊事务类型的实现。然后描述了对基本方案的一些改进。 4.2.1 读取和写入事务 与 Bigtable 一样，事务中发生的写入在客户端缓存，直到提交。 因此，事务中的读取不会受到事务写入的影响。 这种设计在 Spanner 中运行良好，因为读取返回任何读取数据的时间戳，而未提交的写入尚未分配时间戳。 读写事务中的读取使用伤害-等待方案 [Rosenkrantz et al. 1978] 以避免死锁。 客户端向相应组的领导副本发出读操作，该副本获取读锁，然后读取最新数据。 当客户端事务保持打开状态时，它会发送 keepalive 消息以防止参与的领导节点超时其事务。 当客户端完成所有读取和缓冲写入后，它开始两阶段提交。 客户端选择一个协调者组并向每个参与者的领导节点发送一条提交消息，其中包含协调者的身份和任何缓冲的写入。 让客户端驱动两阶段提交协议可以避免通过广域链接发送数据两次。 非协调者参与领导时首先需要获得写锁。 然后它会选择一个预先准备时间戳，这一时间戳必须大于它被之前分配的所有事务的时间戳(以保持单调性)，并通过 Paxos 记录准备好的数据内容。 之后每个参与者将其准备时间戳通知协调器。 协调器领导者也会首先获得写锁，但跳过准备阶段。 在听取所有其他参与者领导人的意见后，它为整个交易选择一个时间戳。 提交时间戳 s 必须大于或等于所有准备好的时间戳(以满足第 4.1.3 节中讨论的约束)，大于协调器收到其提交消息时的 TT.now().latest ，并且大于任何领导者分配给先前交易的时间戳(再次，为了保持单调性)。 协调器领导然后通过 Paxos 记录提交记录(如果在等待其他参与者时超时，则中止)。 只有 Paxos 领导节点获得锁。锁定状态仅在事务准备期间记录。 如果在准备之前丢失了任何锁(无论是由于避免死锁、超时或 Paxos 领导更改)，参与者将中止运行。 Spanner 确保仅在所有锁仍处于持有状态时才记录准备或提交记录。 在领导者更改的情况下，新领导者会在接受任何新事务之前为准备好的但未提交的事务恢复锁定状态。 在允许任何协调器副本应用提交记录之前，协调器领导者会等待 TT.after(s)，以遵守第 4.1.2 节中描述的提交-等待规则。 因为协调者领导者根据 TT.now ().latest 选择了 s，并且现在一直等到那个时间戳被保证是过去的，所以预期的等待时间至少为 2*⋷。 此等待通常与 Paxos 通信重叠。 提交等待后，协调器将提交时间戳发送给客户端和所有其他参与的领导节点。 每个参与者的领导节点通过 Paxos 记录交易的结果。 所有参与者在相同的时间戳申请，然后释放锁定。 4.2.2 快照事务 分配时间戳给多个不同的 Paxos 组需要一个协商阶段。 因此，Spanner 需要为每个快照事务提供一个范围表达式(scope)，该表达式汇总了整个事务将读取的键。 Spanner 自动推断独立查询的范围(scope)。 如果范围的值由单个 Paxos 组提供，则客户端向该组的领导节点发出快照事务。 (当前的 Spanner 实现只为 Paxos 领导者的快照事务选择时间戳。) 这一领导节点分配了 sread 并执行了读取操作。 对于单站点读取，Spanner 通常比 TT.now().latest 做得更好。 将 LastTS() 定义为 Paxos 组上最后一次提交写入的时间戳。 如果没有准备好的事务，赋值 sread=LastTS() 就简单地满足了外部一致性：事务将看到最后一次写入的结果，因此在它之后排序。 如果范围的值由多个 Paxos 组提供，则有多种选择。 最复杂的选择是与所有组的领导者进行一轮沟通以协商基于 LastTS() 确定 sread。Spanner 目前实现了一个更简单的选择。 客户端避免协商回合，只是在 sread=TT.now().latest(会等一个安全时间使得时间足够靠后)执行其读取。 事务中的所有读取都可以发送到足够最新的副本。 4.2.3 结构变化事务 TrueTime 使 Spanner 能够支持原子化的结构更改。 使用标准事务是不可行的，因为参与者的数量(数据库中的组数)可能达到以数百万计。 Bigtable 支持在一个数据中心内进行原子的结构更改，但其结构更改会阻止所有操作。 Spanner 结构更改事务通常是标准事务的非阻塞变体。 首先，它被明确分配了一个未来的时间戳，在准备阶段注册。 因此，数千台服务器之间的结构更改可以在对其他并发活动的干扰最小的情况下完成。 其次，读取和写入操作都隐式依赖于结构，并与在时间 t 时任何注册的结构变化进行同步：如果它们的时间戳在 t 之前，它们可以继续，但是如果它们的时间戳在 t 之后，它们必须阻塞在模式更改事务之后。 如果没有 TrueTime，定义在 t 发生的结构更改将毫无意义。 4.2.4 改进 在这里的定义时一个缺陷，在单个准备的事务中会阻止 增长。 因此，即使读取与事务没有冲突，在以后的时间戳也不会发生读取。 这样错误的冲突可以通过使用从键范围到准备交易时间戳的细粒度映射来增加 的方式消除。 此信息可以存储在锁表中，该表已将键范围映射到锁元数据。 当读取到达时，只需要检查与读取冲突的键范围的细粒度安全时间。 这里定义的 LastTS() 有一个类似的缺陷：如果一个事务刚刚提交，一个非冲突的快照事务仍然必须被分配 sread 以便跟随该事务。因此，读取的执行可能会延迟。 可以通过使用从键范围到锁定表中提交时间戳的细粒度映射来增加 LastTS() 来类似地修复此弱点。(我们还没有实现这个优化。) 当快照事务到达时，可以通过取与事务冲突的键范围的 LastTS() 的最大值来分配其时间戳，除非存在冲突的准备事务(可以从细粒度安全时间确定)。 这里定义的有一个弱点，它不能在没有 Paxos 写入的情况下推进。 也就是说，在 t 读取的快照不能在最后一次写入发生在 t 之前的 Paxos 组上执行。 每个 Paxos 领导者通过保持一个阈值来提高 Paxos 的安全性，高于该阈值将发生未来写入的时间戳：它维护从 Paxos 序列号 n 到可以分配给 Paxos 序列号 n + 1 的最小时间戳的映射 MinNextTS(n)。 当副本通过 n 应用时，它可以将 t Paxos 安全推进到 MinNextTS(n) − 1。 单个领导节点可以轻松执行其 MinNextTS() 承诺。 因为 MinNextTS() 承诺的时间戳位于领导者的租约内，不连接不变量在领导者之间强制执行 MinNextTS() 承诺。 如果领导节点希望在其领导者租约结束后推进 MinNextTS()，则必须首先延长其租期。 请注意，smax 始终前进到 MinNextTS() 中的最高值以保留不相交性。 默认情况下，领导者每 8 秒推进一次 MinNextTS() 值。 因此，在没有准备好的事务的情况下，空闲 Paxos 组中的健康从服务器可以在最坏的情况下提供大于 8 秒的 read at 时间戳。 领导节点也可以根据从属节点的要求推进 MinNextTS() 值。 4.2.5 Paxos 领导者-租赁管理 确保 Paxos-leader-lease 间隔不相交的最简单方法是，无论何时，Leader 都会发出租用间隔的同步 Paxos 写入。随后的领导节点将等待该间隔并过去读取。 TrueTime 可用于确保不相交，而无需这些额外的日志写入。 潜在的第 i 个领导节点在开始投票租期副本 r 处于 时保持一个较低的边界，计算在事件 之前(定义为在领导节点发送租期请求时)。 每个副本 r 都会在 时获得租期，这一事件会在 之后发生(当副本收到租期请求时); 租期结束于 ，在 完成。 一个副本 r 遵循单独投票的规则：它不会授予另一次租期投票，直到 TT.after() 返回为真时。 在 r 的不同实例中强制执行此规则，Spanner 在授予租期之前在授予副本上记录租期投票；此日志写入可以搭载在现有的 Paxos 协议日志写入上。 当第 i 个领导节点接收到法定人数的投票(关于事件 )，它会计算它的租期间隔为 &#123;% mathjax %&#125; lease_&#123;i&#125; = [TT.now().latest,min_&#123;r&#125;(v_&#123;i,r&#125;^&#123;leader&#125;) + lease\\_length] &#123;% endmathjax %&#125;。 若 返回为假时租期在领导节点处被视为已到期。 为了证明不相交，我们利用了这样一个事实，即第 i 个和第 (i+1) 个领导节点在他们的法定人数中必须有一个共同的副本。这个副本为 r0，证明如下： (经过定义) (最小值) (经过定义) (因果关系) (经过定义) (单项投票) (因果关系) (经过定义) 5 评估 我们首先衡量 Spanner 在复制、事务和可用性方面的性能。然后我们提供了一些关于 TrueTime 行为的数据，以及我们的第一个客户 F1 的案例研究。 5.1 微基准 副本数量 写入延迟(ms) 快照事务延迟(ms) 快照读取(ms) 写入吞吐量(千次操作/秒) 快照事务吞吐量 快照读取吞吐量 1D 10.1 ± 0.3 - - 4.2 ± 0.03 - - 1 14.1 ± 1.0 1.3 ± 0.04 1.3 ± 0.02 4.2 ± 0.07 10.7 ± 0.6 11.4 ± 0.2 3 14.3 ± 0.5 1.4 ± 0.08 1.4 ± 0.08 1.8 ± 0.03 11.2 ± 0.4 32.0 ± 1.8 5 15.4 ± 2.3 1.4 ± 0.07 1.6 ± 0.04 1.2 ± 0.2 11.1 ± 0.5 46.8 ± 5.8 表 3：操作微基准，10 次运行的平均值和标准偏差。1D 表示禁用提交等待的一个副本。 表 3 列出了 Spanner 的一些微基准测试。 这些测量是在分时机器上进行的：每个 spanserver 在 4GB RAM 和 4 个内核(AMD Barcelona 2200MHz)上运行。 客户端在不同的机器上。每个 zone 包含一个 spanserver。 客户端和 zone 被放置在一组网络距离小于 1ms 的数据中心中。 (这样的布局应该是司空见惯的：大多数应用程序不需要在全球范围内分发所有数据。) 测试数据库由 50 个 Paxos 组和 2500 个目录。 操作是独立的 4KB 读取和写入。 压缩后，所有读取都在内存外提供，因此我们只测量 Spanner 调用堆栈的开销。 此外，首先进行一轮未测量的读取操作以预热任何位置缓存。 对于延迟实验，客户端发出足够少的操作以避免在服务器上排队。 从单个副本的实验来看，提交(commit)等待时间大约为 4ms，Paxos 延迟大约为 10ms。 随着副本数量的增加，延迟会略有增加，因为 Paxos 必须提交更多副本。 因为 Paxos 在组的副本上并行执行，所以增加是次线性的：延迟取决于仲裁中最慢的成员。 我们可以在写入延迟中看到最多的测量噪声。 对于吞吐量实验，客户端发出足够多的操作以使服务器的 CPU 饱和。 此外，Paxos 领导者节点固定在一个 zone ，以便在整个实验中保持恒定的领导节点运行的机器数量。 快照读取可以在任何最新的副本上执行，因此它们的吞吐量随着副本数量的增加而增加。 单读快照事务仅在领导节点处执行，因为时间戳分配必须发生在领导节点处。 因此，快照事务吞吐量与副本数量大致保持不变。 最后，写入吞吐量随着副本数量的增加而降低，因为 Paxos 工作量随副本数量线性增加。 同样，由于我们在生产数据中心运行，因此我们的测量中存在大量噪声：我们不得不丢弃一些只能通过其他作业的干扰来解释的数据。 参与者数量 平均延迟 第 99 个百分位的延迟 1 14.6 ± 0.2 26.550 ± 6.2 2 20.7 ± 0.4 31.958 ± 4.1 5 23.9 ± 2.2 46.428 ± 8.0 10 22.8 ± 0.4 45.931 ± 4.2 25 26.0 ± 0.6 52.509 ± 4.3 50 33.8 ± 0.6 62.420 ± 7.1 100 55.9 ± 1.2 88.859 ± 5.9 200 122.5 ± 6.7 206.443 ± 15.8 表 4：两阶段提交可扩展性。10 次运行的平均偏差和标准偏差 表 4 显示了两阶段提交可以扩展到合理数量的参与者：它是对一系列实验的总结，这些实验运行在 3 个 zone 上，每个 zone 具有 25 个 spanserver。 扩展到 50 个参与者，无论在平均值还是第 99 个百分位方面，都是合理的。在100个参与者的情形下，延迟开始明显增加。 5.2 可用性 图 5 说明了在多个数据中心运行 Spanner 的可用性优势。 它显示了在存在数据中心故障的情况下对吞吐量进行的三个实验的结果，所有这些实验都覆盖在相同的时间尺度上。 测试域由 5 个 zone Zi 组成，每个 zone 有 25 个 span server。 测试数据库被分成 1250 个 Paxos 组，100 个测试客户端以 50K 读取/秒的聚合速率不断发出非快照读取。 所有的领导节点都明确地放在 Z1 中。 每次测试 5 秒后，一个区域内的所有服务器都被杀死：非领导者关闭 Z2；领导节点强制关闭 Z1；领导节点软关闭 Z1，但它会通知所有服务器，他们应该首先移交领导权。 关闭 Z2 不会影响读取吞吐量。 在给领导节点时间将领导权移交给不同 zone 的同时关闭 Z1 有一个较小的影响：吞吐量下降在图中不可见，但约为 3-4%。 另一方面，在没有警告的情况下关闭 Z1 会产生严重的影响：完成率几乎下降到 0。 然而，由于领导节点被重新选择，系统的吞吐量升至大约十万次读取/秒，因为我们的实验存在者两个特征：系统中存在额外的容量，并且在领导者不可用时，操作会排队。 结果就是系统的吞吐量上升，然后再次稳定在其稳态速率。 我们还可以看到 Paxos 领导节点租期设置为 10 秒这一事实的影响。 当我们杀死 zone 时，组内的领导节点租期时间应该在接下来的 10 秒内均匀分布。 在一个关闭的领导节点的租约到期后不久，就会选出一个新的领导节点。 在关闭后大约 10 秒，所有组都有领导节点并且吞吐量已经恢复。 更短的租期将减少服务器关闭对可用性的影响，但需要更多的租期更新网络流量。 我们正在设计和实施一种机制，该机制将导致从属节点在领导节点关闭时释放 Paxos 领导租约。 5.3 TrueTime 关于 TrueTime，有两个问题必须回答：ϵ 是否就是时钟不确定性的边界？ϵ在最差的情况下会怎么样？ 对于前者，最严重的问题是如果本地时钟的漂移大于 200 us/sec：这将打破 TrueTime 的假设。 我们的机器统计数据显示，CPU 损坏的可能性是时钟损坏的 6 倍。 也就是说，相对于更严重的硬件问题，时钟问题非常罕见。 因此，我们相信 TrueTime 的实现与 Spanner 所依赖的任何其他软件一样值得信赖。 图 6 显示了在相距最远 2200 公里的数据中心内的数千个 spanserver 机器上获取的 TrueTime 数据。 它绘制了 ϵ 的第 90 个、第 99 个和第 99.9 个百分位数，在轮询时间主进程后立即在时间从进程中采样。 由于本地时钟的不确定性，这种采样消除了 ϵ 中的锯齿波，因此测量时间主控器的不确定性（通常为 0）加上与时间主控器的通信延迟。 数据表明，这两个因素在确定 ϵ 的基值时一般不成问题。 然而，可能存在导致 ϵ 值较高的显着尾部延迟问题。 从 3 月 30 日开始，尾部延迟的减少是由于网络改进减少了瞬态网络链路拥塞。 4 月 13 日的 ϵ 出现了升高，持续时间约为 1 小时，原因是数据中心的 2 次主站关闭进行日常维护。 我们将继续调查并消除 TrueTime 峰值的原因。 5.4 F1 Spanner 于 2011 年初开始在生产工作负载下进行实验评估，作为重写 Google 广告后端 F1 的一部分 [Shute et al. 2012]。 这个后端最初是基于一个手动分片的 MySQL 数据库。 未压缩的数据集有数十 TB，与许多 NoSQL 实例相比很小，但大到足以导致分片 MySQL 出现问题。 MySQL 分片方案将每个客户和所有相关数据分配到一个固定分片。 这种布局允许在每个客户的基础上使用索引和复杂的查询处理，但需要一些应用程序业务逻辑中的分片知识。 随着客户数量和数据规模的增长，重新分配这个对收入至关重要的数据库的成本非常高。 上一次重新分片花费了两年的努力，涉及数十个团队之间的协调和测试，以最大限度地降低风险。 此操作过于复杂，无法定期执行：因此，团队不得不通过将一些数据存储在外部 Bigtable 中来限制 MySQL 数据库的增长，但这会损害事务行为和查询所有数据的能力。 F1 团队选择使用 Spanner 有几个原因。 首先，Spanner 无需手动重新分片。 其次，Spanner 提供同步复制和自动故障转移。 使用 MySQL 主从复制，故障转移很困难，并且有数据丢失和停机的风险。 第三，F1 需要强大的事务语义，这使得使用其他 NoSQL 系统不切实际。 应用程序语义需要跨任意数据的事务，和一致性的读取功能。 F1 团队还需要对他们的数据进行二级索引(因为 Spanner 尚未提供对二级索引的自动支持)，并且能够使用 Spanner 事务实现自己的一致全局索引。 所有应用程序写入现在默认通过 F1 发送到 Spanner，而不是基于 MySQL 的应用程序栈。 F1在美国西海岸有 2 个副本，在东海岸有 3 个。 选择副本站点是为了应对潜在的重大自然灾害造成的中断，以及他们前端站点的选择。 有趣的是，他们几乎看不到 Spanner 的自动故障转移。 在过去的几个月中，尽管有不在计划内的机群失效，但是，F1团队最需要做的工作仍然是更新他们的数据库模式，来告诉 Spanner 在哪里放置 Paxos 领导节点，从而使得它们尽量靠近应用前端。 Spanner 的时间戳语义使 F1 能够有效地维护从数据库状态计算出的内存中数据结构。 F1 维护所有更改的逻辑历史日志，该日志作为每个事务的一部分写入 Spanner 本身。 F1 在时间戳上获取数据的完整快照以初始化其数据结构，然后读取增量更改以更新它们。 片段 目录 1 &gt;100M 2-4 341 5-9 5336 10-14 232 15-99 34 100-500 7 表 5：F1 中目录片段计数的分布。 表 5 说明了 F1 中每个目录的片段数量分布。 每个目录通常对应于 F1 之上的应用程序堆栈中的一个客户。 绝大多数目录（以及客户）仅包含 1 个片段，这意味着对这些客户数据的读取和写入保证仅发生在单个服务器上。 超过 100 个分片的目录都是包含 F1 二级索引的表：写入这些表的多个分片是非常罕见的。 F1 团队仅在将批量数据加载作为事务进行调优时才看到这种行为。 操作 平均延迟(ms) 延迟标准差 总大小 所有读取 8.7 376.4 21.5B 单点提交 72.3 112.8 31.2M 多点提交 103.0 52.2 32.1M 表 6：F1-24 小时内测量的可感知操作延迟。 表 6 显示了从 F1 服务器测量的 Spanner 操作延迟。 东海岸数据中心的副本在选择 Paxos 领导节点时具有更高的优先级。 表中的数据是从这些数据中心的 F1 服务器测量的。 写入延迟的大标准偏差是由于锁冲突导致的肥尾效应(pretty fat tail)造成的。 读取延迟更大的标准偏差部分是由于 Paxos 的领导节点分布在两个数据中心，其中只有一个有配备 SSD 的机器。 此外，测量包括系统中来自两个数据中心的每次读取：读取的字节的平均值和标准差分别约为 1.6KB 和 119KB。 6 相关工作 跨数据中心的一致复制作为存储服务已由 Megastore 和 DynamoDB 提供 [Baker et al. 2011]。 DynamoDB 呈现一个key-value 接口，并且只在一个区域内复制。 Spanner 跟随 Megastore 提供了一个半关系数据模型，甚至是一种类似的模式语言。 Megastore 没有实现高性能。 它位于 Bigtable 之上，这带来了高昂的通信成本。 它也不支持长时间的领导节点：多个副本可能会启动写入。 来自不同副本的所有写入都必然会在 Paxos 协议中发生冲突，即使它们在逻辑上没有冲突：Paxos 组的吞吐量会以每秒多次写入的速度崩溃。 Spanner 提供更高的性能、通用事务和外部一致性。 Pavlo et al. [2009] 比较了数据库和 MapReduce [Dean and Ghemawat 2010] 的性能。 他们指出了为探索分布式键值存储分层的数据库功能而做出的其他一些努力 [Abouzeidet al. 2009; Armbrust et al. 2011; Brantner et al. 2008; Thusoo et al. 2010] 二者可以实现充分的融合。 我们同意这个结论，但证明集成多个层有其优势：例如，将并发控制与复制集成降低了 Spanner 中的提交等待成本。 在复制存储之上分层交易的概念至少可以追溯到 Gifford 的论文 [Gifford 1982]。 Scatter [Glendenning et al. 2011] 是最近的基于 DHT 的键值存储，它在一致复制之上分层事务。 Spanner 专注于提供比 Scatter 更高级别的接口。 Gray 和 Lamport [2006] 描述了一种基于 Paxos 的非阻塞提交协议。 他们的协议比两阶段提交产生更多的消息传递成本，这会加剧广泛分布的组的提交成本。 Walter [Sovran et al. 2011] 提供了一种在数据中心内但不能跨数据中心工作的快照隔离变体。 相比之下，我们的快照事务提供了更自然的语义，因为我们支持外部一致性整体操作。 最近有大量关于减少或消除锁定开销的工作。 Calvin [Thomson et al. 2012] 消除了并发控制：它会重新分配时间戳，然后以时间戳的顺序执行事务。 H-Store [Stonebraker et al.2007] 和 Granola [Cowling and Liskov 2012] 都支持他们自己的交易类型分类，其中一些可以避免锁定。 这些系统都没有提供外部一致性。 Spanner 通过提供对快照隔离的支持来解决争用问题。 VoltDB [2012] 是一个分片内存数据库，它支持大范围内的主从复制以进行灾难恢复，但不支持更通用的复制配置。 它是所谓的 NewSQL 的一个实例，这是实现可扩展的 SQL 的强大的市场推动力 [Stonebraker 2010a]。 许多商业数据库过去都实现了读取功能，例如 MarkLogic [2012] 和 Oracle 的 Total Recall [Oracle2012]。 Lomet 和 Li [2009] 描述了这种时态数据库的实现策略。 Farsite 衍生出的时钟不确定性边界(比 TrueTime 更宽松)相对于受信任的时钟参考 [Douceur and Howell 2003]：Farsite 中的服务器租用与 Spanner 维护 Paxos 租期的方式相同。 在先前的工作中，松散同步的时钟已被用于并发控制目的 [Adya et al.1995; Liskov 1993]。 我们已经证明 TrueTime 可以让一个关于跨 Paxos 状态机集的全局时间的原因。 7 未来的工作 我们去年大部分时间都在与 F1 团队合作，将 Google 的广告后端从 MySQL 过渡到 Spanner。 我们正在积极改进其监控和支持工具，并调整其性能。 此外，我们一直致力于改进备份/恢复系统的功能和性能。 Wear 目前正在实施 Spanner 模式语言、二级索引的自动维护和基于负载的自动重新分片。 从长远来看，我们计划研究几个功能。 乐观地并行读取可能是一种有价值的策略，但最初的实验表明，正确的实现并非易事。 此外，我们计划最终支持 Paxos 配置的直接更改 [Lamport et al. 2010; Shraer et al. 2012]。 鉴于我们预计许多应用程序会在彼此相对靠近的数据中心之间复制其数据，TrueTime ϵ 可能会显着影响性能。我们认为将 ϵ 降低到 1 毫秒以下没有不可逾越的障碍。 可以减少时间主查询间隔，并且更好的时钟晶体相对便宜。 时间主查询延迟可以通过改进的网络技术来减少，或者甚至可以通过替代时间分配技术来避免。 最后，还有明显需要改进的地方。 尽管 Spanner 在节点数量上具有可扩展性，但节点本地数据结构在复杂 SQL 查询上的性能相对较差，因为它们是为简单的键值访问而设计的。 DB 文献中的算法和数据结构可以大大提高单节点性能。 其次，在数据中心之间自动移动数据以响应客户端负载的变化一直是我们的目标，但为了使该目标有效，我们还需要能够以自动化、协调的方式在数据中心之间移动客户端应用程序进程。 移动进程带来了更困难的问题，即管理数据中心之间的资源获取和分配。 8 结论 总而言之，Spanner 结合并扩展了来自两个研究社区的想法：来自数据库社区、熟悉的、易于使用的半关系界面、事务和基于 SQL 的查询语言；来自系统社区、可扩展性、自动分片、容错、一致复制、外部一致性和广域分布。 自 Spanner 成立以来，我们已经用了 5 年多的时间来迭代到当前的设计和实现。 这个漫长的迭代阶段的一部分是因为人们缓慢地意识到 Spanner 不仅应该解决全局复制命名空间的问题，还应该关注 Bigtable 缺少的数据库功能。 我们设计的一个方面脱颖而出：Spanner 功能集的关键是 TrueTime。 我们已经证明，在时间 API 中具体化时钟不确定性使得构建具有更强时间语义的分布式系统成为可能。 此外，随着底层系统对时钟不确定性实施更严格的界限，更强语义的开销减少。 作为一个社区，我们在设计分布式算法时不应再依赖松散同步的时钟和弱时间 API。 致谢 1234567891011Many people have helped to improve this article: our shepherd Jon Howell, who went above and beyond hisresponsibilities; the anonymous referees; and many Googlers: Atul Adya, Fay Chang, Frank Dabek, SeanDorward, Bob Gruber, David Held, Nick Kline, Alex Thomson, and Joel Wein. Our management has beenvery supportive of both our work and of publishing this article: Aristotle Balogh, Bill Coughran, Urs H¨olzle,Doron Meyer, Cos Nicolaou, Kathy Polizzi, Sridhar Ramaswany, and Shivakumar Venkataraman. We havebuilt upon the work of the Bigtable and Megastore teams. The F1 team, and Jeff Shute in particular, workedclosely with us in developing our data model and helped immensely in tracking down performance andcorrectness bugs. The Platforms team, and Luiz Barroso and Bob Felderman in particular, helped to makeTrueTime happen. Finally, a lot of Googlers used to be on our team: Ken Ashcraft, Paul Cychosz, KrzysztofOstrowski, Amir Voskoboynik, Matthew Weaver, Theo Vassilakis, and Eric Veach; or have joined our teamrecently: Nathan Bales, Adam Beberg, Vadim Borisov, Ken Chen, Brian Cooper, Cian Cullinan, Robert-JanHuijsman, Milind Joshi, Andrey Khorlin, Dawid Kuroczko, Laramie Leavitt, Eric Li, Mike Mammarella,Sunil Mushran, Simon Nielsen, Ovidiu Platon, Ananth Shrinivas, Vadim Suvorov, and Marcel van der Holst. 参考资料 [1] Azza Abouzeid et al. “HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads”. Proc. of VLDB. 2009, pp. 922–933. [2] A. Adya et al. “Efficient optimistic concurrency control using loosely synchronized clocks”. Proc. of SIGMOD. 1995, pp. 23–34. [3] Amazon. Amazon DynamoDB. 2012. [4] Michael Armbrust et al. “PIQL: Success-Tolerant Query Processing in the Cloud”. Proc. of VLDB. 2011, pp. 181–192. [5] Jason Baker et al. “Megastore: Providing Scalable, Highly Available Storage for Interactive Services”. Proc. of CIDR. 2011, pp. 223–234. [6] Hal Berenson et al. “A critique of ANSI SQL isolation levels”. Proc. of SIGMOD. 1995, pp. 1–10. [7] Matthias Brantner et al. “Building a database on S3”. Proc. of SIGMOD. 2008, pp. 251–264. [8] A. Chan and R. Gray. “Implementing Distributed Read-Only Transactions”. IEEE TOSE SE-11.2 (Feb. 1985), pp. 205–212. [9] Fay Chang et al. “Bigtable: A Distributed Storage System for Structured Data”. ACM TOCS 26.2 (June 2008), 4:1–4:26. [10] Brian F. Cooper et al. “PNUTS: Yahoo!’s hosted data serving platform”. Proc. of VLDB. 2008, pp. 1277–1288. [11] James Cowling and Barbara Liskov. “Granola: Low-Overhead Distributed Transaction Coordination”. Proc. of USENIX ATC. 2012, pp. 223–236. [12] Jeffrey Dean and Sanjay Ghemawat. “MapReduce: a flexible data processing tool”. CACM 53.1 (Jan. 2010), pp. 72–77. [13] John Douceur and Jon Howell. Scalable Byzantine-Fault-Quantifying Clock Synchronization. Tech. rep. MSR-TR-2003-67. MS Research, 2003. [14] John R. Douceur and Jon Howell. “Distributed directory service in the Farsite file system”. Proc. of OSDI. 2006, pp. 321–334. [15] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. “The Google file system”. Proc. of SOSP. Dec. 2003, pp. 29–43. [16] David K. Gifford. Information Storage in a Decentralized Computer System. Tech. rep. CSL-81-8. PhD dissertation. Xerox PARC, July 1982. [17] Lisa Glendenning et al. “Scalable consistency in Scatter”. Proc. of SOSP. 2011. [18] Jim Gray and Leslie Lamport. “Consensus on transaction commit”. ACM TODS 31.1 (Mar. 2006), pp. 133–160. [19] Pat Helland. “Life beyond Distributed Transactions: an Apostate’s Opinion”. Proc. of CIDR. 2007, pp. 132–141. [20] Maurice P. Herlihy and Jeannette M. Wing. “Linearizability: a correctness condition for concurrent objects”. ACM TOPLAS 12.3 (July 1990), pp. 463–492. [21] Leslie Lamport. “The part-time parliament”. ACM TOCS 16.2 (May 1998), pp. 133–169. [22] Leslie Lamport, Dahlia Malkhi, and Lidong Zhou. “Reconfiguring a state machine”. SIGACT News 41.1 (Mar. 2010), pp. 63–73. [23] Barbara Liskov. “Practical uses of synchronized clocks in distributed systems”. Distrib. Comput. 6.4 (July 1993), pp. 211–219. [24] David B. Lomet and Feifei Li. “Improving Transaction-Time DBMS Performance and Functionality”. Proc. of ICDE (2009), pp. 581–591. [25] Jacob R. Lorch et al. “The SMART way to migrate replicated stateful services”. Proc. of EuroSys. 2006, pp. 103–115. [26] MarkLogic. MarkLogic 5 Product Documentation. 2012. [27] Keith Marzullo and Susan Owicki. “Maintaining the time in a distributed system”. Proc. of PODC. 1983, pp. 295–305. [28] Sergey Melnik et al. “Dremel: Interactive Analysis of Web-Scale Datasets”. Proc. of VLDB. 2010, pp. 330–339. [29] D.L. Mills. Time synchronization in DCNET hosts. Internet Project Report IEN–173. COMSAT Laboratories, Feb. 1981. [30] Oracle. Oracle Total Recall. 2012. [31] Andrew Pavlo et al. “A comparison of approaches to large-scale data analysis”. Proc. of SIGMOD. 2009, pp. 165–178. [32] Daniel Peng and Frank Dabek. “Large-scale incremental processing using distributed transactions and notifications”. Proc. of OSDI. 2010, pp. 1–15. [33] Daniel J. Rosenkrantz, Richard E. Stearns, and Philip M. Lewis II. “System level concurrency control for distributed database systems”. ACM TODS 3.2 (June 1978), pp. 178–198. [34] Alexander Shraer et al. “Dynamic Reconfiguration of Primary/Backup Clusters”. Proc. of SENIX ATC. 2012, pp. 425–438. [35] Jeff Shute et al. “F1—The Fault-Tolerant Distributed RDBMS Supporting Google’s Ad Business”. Proc. of SIGMOD. May 2012, pp. 777–778. [36] Yair Sovran et al. “Transactional storage for geo-replicated systems”. Proc. of SOSP. 2011, pp. 385–400. [37] Michael Stonebraker. Why Enterprises Are Uninterested in NoSQL. 2010. [38] Michael Stonebraker. Six SQL Urban Myths. 2010. [39] Michael Stonebraker et al. “The end of an architectural era: (it’s time for a complete rewrite)”. Proc. of VLDB. 2007, pp. 1150–1160. [40] Alexander Thomson et al. “Calvin: Fast Distributed Transactions for Partitioned Database Systems”. Proc. of SIGMOD.2012, pp. 1–12. [41] Ashish Thusoo et al. “Hive — A Petabyte Scale Data Warehouse Using Hadoop”. Proc. of ICDE. 2010, pp. 996–1005. [42] VoltDB. VoltDB Resources. 2012.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Spanner","slug":"Spanner","permalink":"https://wangqian0306.github.io/tags/Spanner/"}]},{"title":"createrepo","slug":"linux/createrepo","date":"2021-12-10T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/createrepo/","permalink":"https://wangqian0306.github.io/2021/createrepo/","excerpt":"","text":"createrepo 简介 createrepo 命令用于检索本地目录 rpm 包中的元数据并对其生成 repomd.xml 文件便于 Yum 使用。 注：在遇到找不到 repodata/repomd.xml 错误的时候可以使用本工具解决问题。 安装 1yum install createrepo -y 初步使用 针对当前路径生成索引文件 1createrepo -g repodata/repomd.xml . 参数详解 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950-u --baseurl &lt;url&gt; 指定Base URL的地址-o --outputdir &lt;url&gt; 指定元数据的输出位置-x --excludes &lt;packages&gt; 指定在形成元数据时需要排除的包-i --pkglist &lt;filename&gt; 指定一个文件，该文件内的包信息将被包含在即将生成的元数据中，格式为每个包信息独占一行，不含通配符、正则，以及范围表达式。-n --includepkg 通过命令行指定要纳入本地库中的包信息，需要提供URL或本地路径。-q --quiet 安静模式执行操作，不输出任何信息。-g --groupfile &lt;groupfile&gt; 指定本地软件仓库的组划分，范例如下：createrepo -g comps.xml /path/to/rpms 注意：组文件需要和rpm包放置于同一路径下。-v --verbose 输出详细信息。-c --cachedir &lt;path&gt; 指定一个目录，用作存放软件仓库中软件包的校验和信息。 当createrepo在未发生明显改变的相同仓库文件上持续多次运行时，指定cachedir会明显提高其性能。--update 如果元数据已经存在，且软件仓库中只有部分软件发生了改变或增减， 则可用update参数直接对原有元数据进行升级，效率比重新分析rpm包依赖并生成新的元数据要高很多。-p --pretty 以整洁的格式输出xml文件。-d --database 该选项指定使用SQLite来存储生成的元数据，默认项。 参考资料 createrepo 命令详解","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"图算法","slug":"algorithm/graph_algorithm","date":"2021-12-06T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/graph-algorithm/","permalink":"https://wangqian0306.github.io/2021/graph-algorithm/","excerpt":"","text":"图算法 广度优先算法(Breadth-First Search) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Node: def __init__(self, id: int, name): self.id = id self.name = name self.explored = False def __str__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot; def __repr__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot;class Relation: def __init__(self, id: int, from_node: Node, to_node: Node): self.from_node = from_node self.to_node = to_node self.id = id def __str__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot; def __repr__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot;class Graph: def __init__(self, node_list: list[Node], relation_list: list[Relation]): self.node_list = node_list self.relation_list = relation_list def bfs(self, root: Node, name: str): q = [] root.explored = True q.append(root) while len(q) != 0: v = q.pop(0) if v.name == name: return v for relation in self.relation_list: if relation.from_node == v: if not relation.to_node.explored: relation.to_node.explored = True q.append(relation.to_node) 深度优先算法(Depth-First Search) 略(参见拓扑排序文章) 联通分量(Connected Components) 注：要查找图的所有组件，请遍历其顶点，只要循环到达尚未包含在先前找到的组件中的顶点，就开始新的广度优先或深度优先搜索。 最短路径算法(Dijkstra’s Shortest Path) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116import sysclass Graph(object): def __init__(self, nodes, init_graph): self.nodes = nodes self.graph = self.construct_graph(nodes, init_graph) def construct_graph(self, nodes, init_graph): &#x27;&#x27;&#x27; This method makes sure that the graph is symmetrical. In other words, if there&#x27;s a path from node A to B with a value V, there needs to be a path from node B to node A with a value V. &#x27;&#x27;&#x27; graph = &#123;&#125; for node in nodes: graph[node] = &#123;&#125; graph.update(init_graph) for node, edges in graph.items(): for adjacent_node, value in edges.items(): if graph[adjacent_node].get(node, False) == False: graph[adjacent_node][node] = value return graph def get_nodes(self): &quot;Returns the nodes of the graph.&quot; return self.nodes def get_outgoing_edges(self, node): &quot;Returns the neighbors of a node.&quot; connections = [] for out_node in self.nodes: if self.graph[node].get(out_node, False) != False: connections.append(out_node) return connections def value(self, node1, node2): &quot;Returns the value of an edge between two nodes.&quot; return self.graph[node1][node2]def dijkstra_algorithm(graph, start_node): unvisited_nodes = list(graph.get_nodes()) # We&#x27;ll use this dict to save the cost of visiting each node and update it as we move along the graph shortest_path = &#123;&#125; # We&#x27;ll use this dict to save the shortest known path to a node found so far previous_nodes = &#123;&#125; # We&#x27;ll use max_value to initialize the &quot;infinity&quot; value of the unvisited nodes max_value = sys.maxsize for node in unvisited_nodes: shortest_path[node] = max_value # However, we initialize the starting node&#x27;s value with 0 shortest_path[start_node] = 0 # The algorithm executes until we visit all nodes while unvisited_nodes: # The code block below finds the node with the lowest score current_min_node = None for node in unvisited_nodes: # Iterate over the nodes if current_min_node == None: current_min_node = node elif shortest_path[node] &lt; shortest_path[current_min_node]: current_min_node = node # The code block below retrieves the current node&#x27;s neighbors and updates their distances neighbors = graph.get_outgoing_edges(current_min_node) for neighbor in neighbors: tentative_value = shortest_path[current_min_node] + graph.value(current_min_node, neighbor) if tentative_value &lt; shortest_path[neighbor]: shortest_path[neighbor] = tentative_value # We also update the best path to the current node previous_nodes[neighbor] = current_min_node # After visiting its neighbors, we mark the node as &quot;visited&quot; unvisited_nodes.remove(current_min_node) return previous_nodes, shortest_pathdef print_result(previous_nodes, shortest_path, start_node, target_node): path = [] node = target_node while node != start_node: path.append(node) node = previous_nodes[node] # Add the start node manually path.append(start_node) print(&quot;We found the following best path with a value of &#123;&#125;.&quot;.format(shortest_path[target_node])) print(&quot; -&gt; &quot;.join(reversed(path)))nodes = [&quot;Reykjavik&quot;, &quot;Oslo&quot;, &quot;Moscow&quot;, &quot;London&quot;, &quot;Rome&quot;, &quot;Berlin&quot;, &quot;Belgrade&quot;, &quot;Athens&quot;]init_graph = &#123;&#125;for node in nodes: init_graph[node] = &#123;&#125;init_graph[&quot;Reykjavik&quot;][&quot;Oslo&quot;] = 5init_graph[&quot;Reykjavik&quot;][&quot;London&quot;] = 4init_graph[&quot;Oslo&quot;][&quot;Berlin&quot;] = 1init_graph[&quot;Oslo&quot;][&quot;Moscow&quot;] = 3init_graph[&quot;Moscow&quot;][&quot;Belgrade&quot;] = 5init_graph[&quot;Moscow&quot;][&quot;Athens&quot;] = 4init_graph[&quot;Athens&quot;][&quot;Belgrade&quot;] = 1init_graph[&quot;Rome&quot;][&quot;Berlin&quot;] = 2init_graph[&quot;Rome&quot;][&quot;Athens&quot;] = 2graph = Graph(nodes, init_graph)previous_nodes, shortest_path = dijkstra_algorithm(graph=graph, start_node=&quot;Reykjavik&quot;)print_result(previous_nodes, shortest_path, start_node=&quot;Reykjavik&quot;, target_node=&quot;Belgrade&quot;) 最小生成树(Prim’s Minimum Cost Spanning Tree) 12345678910111213141516171819202122232425262728293031323334353637# Prim&#x27;s Algorithm in PythonINF = 9999999# number of vertices in graphN = 5#creating graph by adjacency matrix methodG = [[0, 19, 5, 0, 0], [19, 0, 5, 9, 2], [5, 5, 0, 1, 6], [0, 9, 1, 0, 1], [0, 2, 6, 1, 0]]selected_node = [0, 0, 0, 0, 0]no_edge = 0selected_node[0] = True# printing for edge and weightprint(&quot;Edge : Weight\\n&quot;)while (no_edge &lt; N - 1): minimum = INF a = 0 b = 0 for m in range(N): if selected_node[m]: for n in range(N): if ((not selected_node[n]) and G[m][n]): # not in selected and there is an edge if minimum &gt; G[m][n]: minimum = G[m][n] a = m b = n print(str(a) + &quot;-&quot; + str(b) + &quot;:&quot; + str(G[a][b])) selected_node[b] = True no_edge += 1 拓扑排序(Topological Sort) 略(参照独立文章) 弗洛伊德算法(Floyd-Warshall) 1234567891011121314151617181920212223242526272829nV = 4INF = 999def floyd_warshall(G): distance = list(map(lambda i: list(map(lambda j: j, i)), G)) for k in range(nV): for i in range(nV): for j in range(nV): distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j]) print_solution(distance)def print_solution(distance): for i in range(nV): for j in range(nV): if distance[i][j] == INF: print(&quot;INF&quot;, end=&quot; &quot;) else: print(distance[i][j], end=&quot; &quot;) print(&quot; &quot;)G = [[0, 3, INF, 5], [2, 0, INF, 4], [INF, 1, 0, INF], [INF, INF, 2, 0]]floyd_warshall(G) Kruskal 最小生成树(Kruskal Minimum Cost Spanning Tree Algorithm) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Graph: def __init__(self, vertices): self.V = vertices self.graph = [] def add_edge(self, u, v, w): self.graph.append([u, v, w]) # Search function def find(self, parent, i): if parent[i] == i: return i return self.find(parent, parent[i]) def apply_union(self, parent, rank, x, y): xroot = self.find(parent, x) yroot = self.find(parent, y) if rank[xroot] &lt; rank[yroot]: parent[xroot] = yroot elif rank[xroot] &gt; rank[yroot]: parent[yroot] = xroot else: parent[yroot] = xroot rank[xroot] += 1 # Applying Kruskal algorithm def kruskal_algo(self): result = [] i, e = 0, 0 self.graph = sorted(self.graph, key=lambda item: item[2]) parent = [] rank = [] for node in range(self.V): parent.append(node) rank.append(0) while e &lt; self.V - 1: u, v, w = self.graph[i] i = i + 1 x = self.find(parent, u) y = self.find(parent, v) if x != y: e = e + 1 result.append([u, v, w]) self.apply_union(parent, rank, x, y) for u, v, weight in result: print(&quot;%d - %d: %d&quot; % (u, v, weight))g = Graph(6)g.add_edge(0, 1, 4)g.add_edge(0, 2, 4)g.add_edge(1, 2, 2)g.add_edge(1, 0, 4)g.add_edge(2, 0, 4)g.add_edge(2, 1, 2)g.add_edge(2, 3, 3)g.add_edge(2, 5, 2)g.add_edge(2, 4, 4)g.add_edge(3, 2, 3)g.add_edge(3, 4, 3)g.add_edge(4, 2, 4)g.add_edge(4, 3, 3)g.add_edge(5, 2, 2)g.add_edge(5, 4, 3)g.kruskal_algo()","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Superset 安装","slug":"tools/superset","date":"2021-12-03T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/superset-install/","permalink":"https://wangqian0306.github.io/2021/superset-install/","excerpt":"","text":"Superset 安装 简介 Apache Superset 是一个现代的、企业就绪的商业智能 Web 应用程序。它快速、轻量、直观，并且可配置，任何用户都可以轻松的探索和可视化他们的数据。对于简单的饼图到高度详细的 deck.gl 地理空间图表都有良好的支持。 注：类似于 Grafana。 安装 Docker 安装 123git clone https://github.com/apache/superset.gitcd supersetdocker-compose -f docker-compose-non-dev.yml up -d 使用步骤 新建数据源(链接到数据库) 新建数据集(链接到数据表) 新建仪表板 新建图像 注：默认的检索时间为 60 秒，若 60 秒内还没能完成检索则图像无法正常展示。 参考资料 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Superset","slug":"Superset","permalink":"https://wangqian0306.github.io/tags/Superset/"}]},{"title":"斜堆","slug":"data_structure/skew_heap","date":"2021-12-02T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/skew_heap/","permalink":"https://wangqian0306.github.io/2021/skew_heap/","excerpt":"","text":"斜堆(Skew Heap) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class Node: def __init__(self, key): self.key = key self.left = None self.right = None def __str__(self): return self.key def recursiveMerge(self, rsh_oot): if rsh_oot is self or rsh_oot is None: return self root1 = None root2 = None if rsh_oot.key &lt; self.key: root1 = rsh_oot root2 = self else: root1 = self root2 = rsh_oot tmp_root = root2.recursiveMerge(root1.right) root1.right = root1.left root1.left = tmp_root return root1def iterativeMerge(root1: Node, root2: Node): if root1 is None: return root2 if root2 is None: return root1 stack = [] r1 = root1 r2 = root2 while r1 is not None and r2 is not None: if r1.key &lt; root2.key: stack.append(r1) r1 = r1.right else: stack.append(r2) r2 = r2.right if r1 is not None: r = r1 else: r = r2 while len(stack) != 0: node = stack.pop() node.right = node.left node.left = r r = node return rdef buildHeap(list_a): queue = [] for key in list_a: queue.append(Node(key)) while len(queue) &gt; 1: root1 = queue.pop() root2 = queue.pop() root_node = iterativeMerge(root1, root2) queue.append(root_node) root_node = queue.pop() return SkewHeap(root_node)class SkewHeap: def __init__(self, root: Node): self.root = root def recursiveMerge(self, root1: Node, root2: Node): if root1 is None: return root2 if root2 is None: return root1 return root1.recursiveMerge(root2) def merge(self, h1, h2): root_node = iterativeMerge(h1.root, h2.root) return SkewHeap(root_node) def insert(self, x: int): self.root = iterativeMerge(Node(x), self.root) def extractMin(self): if self.root is None: return None min = self.root.key self.root = iterativeMerge(self.root.left, self.root.right) return min 参考资料 斜堆(Skew Heap)","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"二项堆","slug":"data_structure/binomial_heap","date":"2021-11-30T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/binomial-heap/","permalink":"https://wangqian0306.github.io/2021/binomial-heap/","excerpt":"","text":"二项堆(Binomial Heap) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325class ItemRef(object): &quot;&quot;&quot;Reference to an item in the heap. Used for decreasing keys and deletion. Do not use this class directly; only use instances returned by BinomialHeap.insert()! You should only use ItemRef.delete() and ItemRef.decrease(new_priority). &quot;&quot;&quot; def __init__(self, node, get_heap): self.ref = node self.get_heap = get_heap self.in_tree = True def __str__(self): if self.in_tree: return &quot;&lt;BinomialHeap Reference to &#x27;%s&#x27;&gt;&quot; % str(self.ref.val) else: return &quot;&lt;stale BinomialHeap Reference&gt;&quot; def decrease(self, new_key): &quot;Update the priority of the referenced item to a lower value.&quot; assert self.in_tree assert self.ref.ref == self self.ref.decrease(new_key) def delete(self): &quot;&quot;&quot;Remove the referenced item from the heap. &quot;&quot;&quot; self.decrease(self) v = self.get_heap().extract_min() assert not self.in_tree assert v is self.ref.val def in_heap(self, heap): &quot;&quot;&quot;Returns True if the referenced item is part of the BinomialHeap &#x27;heap&#x27;; False otherwise. &quot;&quot;&quot; return self.in_tree and self.get_heap() == heap def __lt__(self, other): &quot;Behaves like negative infinity: always True.&quot; return True def __gt__(self, other): &quot;Behaves like negative infinity: always False.&quot; return Falseclass BinomialHeap(object): &quot;&quot;&quot;Usage: &gt; H1 = BinomialHeap() &gt; H1.insert(40, &quot;fast.&quot;) &gt; H1.insert(10, &quot;Merging&quot;) &gt; H2 = BinomialHeap([(30, &quot;quite&quot;), (20, &quot;is&quot;)]) &gt; H1 += H2 &gt; for x in H1: &gt; print x, =&gt; &quot;Merging is quite fast.&quot; &quot;&quot;&quot; class Node(object): &quot;Internal node of the heap. Don&#x27;t use directly.&quot; def __init__(self, get_heap, key, val=None): self.degree = 0 self.parent = None self.next = None self.child = None self.key = key self.ref = ItemRef(self, get_heap) if val == None: val = key self.val = val def __str__(self): k = lambda x: str(x.key) if x else &#x27;NIL&#x27; return &#x27;(%s, c:%s, n:%s)&#x27; % (k(self), k(self.child), k(self.next)) def link(self, other): &quot;Makes other a subtree of self.&quot; other.parent = self other.next = self.child self.child = other self.degree += 1 def decrease(self, new_key): node = self assert new_key &lt; node.key node.key = new_key cur = node parent = cur.parent while parent and cur.key &lt; parent.key: # need to bubble up # swap refs parent.ref.ref, cur.ref.ref = cur, parent parent.ref, cur.ref = cur.ref, parent.ref # now swap keys and payload parent.key, cur.key = cur.key, parent.key parent.val, cur.val = cur.val, parent.val # step up cur = parent parent = cur.parent @staticmethod def roots_merge(h1, h2): &quot;&quot;&quot;Merge two lists of heap roots, sorted by degree. Returns the new head. &quot;&quot;&quot; if not h1: return h2 if not h2: return h1 if h1.degree &lt; h2.degree: h = h1 h1 = h.next else: h = h2 h2 = h2.next p = h while h2 and h1: if h1.degree &lt; h2.degree: p.next = h1 h1 = h1.next else: p.next = h2 h2 = h2.next p = p.next if h2: p.next = h2 else: p.next = h1 return h @staticmethod def roots_reverse(h): &quot;&quot;&quot;Reverse the heap root list. Returns the new head. Also clears parent references. &quot;&quot;&quot; if not h: return None tail = None next = h h.parent = None while h.next: next = h.next h.next = tail tail = h h = next h.parent = None h.next = tail return h class __Ref(object): def __init__(self, h): self.heap = h self.ref = None def get_heap_ref(self): if not self.ref: return self else: # compact self.ref = self.ref.get_heap_ref() return self.ref def get_heap(self): return self.get_heap_ref().heap def __init__(self, lst=[]): &quot;&quot;&quot;Populate a new heap with the (key, value) pairs in &#x27;lst&#x27;. If the elements of lst are not subscriptable, then they are treated as opaque elements and inserted into the heap themselves. &quot;&quot;&quot; self.head = None self.size = 0 self.ref = BinomialHeap.__Ref(self) for x in lst: try: self.insert(x[0], x[1]) except TypeError: self.insert(x) def insert(self, key, value=None): &quot;&quot;&quot;Insert &#x27;value&#x27; in to the heap with priority &#x27;key&#x27;. If &#x27;value&#x27; is omitted, then &#x27;key&#x27; is used as the value. Returns a reference (of type ItemRef) to the internal node in the tree. Use this reference to delete the key or to change its priority. &quot;&quot;&quot; n = BinomialHeap.Node(self.ref.get_heap, key, value) self.__union(n) self.size += 1 return n.ref def union(self, other): &quot;&quot;&quot;Merge &#x27;other&#x27; into &#x27;self&#x27;. Returns None. Note: This is a destructive operation; &#x27;other&#x27; is an empty heap afterwards. &quot;&quot;&quot; self.size = self.size + other.size h2 = other.head self.__union(h2) other.ref.ref = self.ref other.__init__() def min(self): &quot;&quot;&quot;Returns the value with the minimum key (= highest priority) in the heap without removing it, or None if the heap is empty. &quot;&quot;&quot; pos = self.__min() return pos[0].val if pos else None def extract_min(self): &quot;&quot;&quot;Returns the value with the minimum key (= highest priority) in the heap AND removes it from the heap, or None if the heap is empty. &quot;&quot;&quot; # find mininum pos = self.__min() if not pos: return None else: (x, prev) = pos # remove from list if prev: prev.next = x.next else: self.head = x.next kids = BinomialHeap.Node.roots_reverse(x.child) self.__union(kids) x.ref.in_tree = False self.size -= 1 return x.val def __nonzero__(self): &quot;&quot;&quot;True if the heap is not empty; False otherwise.&quot;&quot;&quot; return self.head != None def __iter__(self): &quot;&quot;&quot;Returns a _destructive_ iterator over the values in the heap. This violates the iterator protocol slightly, but is very useful. &quot;&quot;&quot; return self def __len__(self): &quot;&quot;&quot;Returns the number of items in this heap.&quot;&quot;&quot; return self.size def __setitem__(self, key, value): &quot;&quot;&quot;Insert. H[key] = value is equivalent to H.insert(key, value) &quot;&quot;&quot; self.insert(key, value) def __iadd__(self, other): &quot;&quot;&quot;Merge. a += b is equivalent to a.union(b). &quot;&quot;&quot; self.union(other) return self def next(self): &quot;&quot;&quot;Returns the value with the minimum key (= highest priority) in the heap AND removes it from the heap; raises StopIteration if the heap is empty. &quot;&quot;&quot; if self.head: return self.extract_min() else: raise StopIteration def __contains__(self, ref): &quot;&quot;&quot;Test whether a given reference &#x27;ref&#x27; (of ItemRef) is in this heap. &quot;&quot;&quot; if type(ref) != ItemRef: print(&quot;TypeError Expected an ItemRef&quot;) else: return ref.in_heap(self) def __min(self): if not self.head: return None min = self.head min_prev = None prev = min cur = min.next while cur: if cur.key &lt; min.key: min = cur min_prev = prev prev = cur cur = cur.next return (min, min_prev) def __union(self, h2): if not h2: # nothing to do return h1 = self.head if not h1: self.head = h2 return h1 = BinomialHeap.Node.roots_merge(h1, h2) prev = None x = h1 next = x.next while next: if x.degree != next.degree or \\ (next.next and next.next.degree == x.degree): prev = x x = next elif x.key &lt;= next.key: # x becomes the root of next x.next = next.next x.link(next) else: # next becomes the root of x if not prev: # update the &quot;master&quot; head h1 = next else: # just update previous link prev.next = next next.link(x) # x is not toplevel anymore, update ref by advancing x = next next = x.next self.head = h1def heap(lst=[]): &quot;&quot;&quot;Create a new heap. lst should be a sequence of (key, value) pairs. Shortcut for BinomialHeap(lst) &quot;&quot;&quot; return BinomialHeap(lst) 参考资料","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"斐波那契堆","slug":"data_structure/fibonacci_heap","date":"2021-11-30T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/fibonacci-heap/","permalink":"https://wangqian0306.github.io/2021/fibonacci-heap/","excerpt":"","text":"斐波那契堆(Fibonacci Heap) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188import mathclass FibonacciHeap: # internal node class class Node: def __init__(self, key, value): self.key = key self.value = value self.parent = self.child = self.left = self.right = None self.degree = 0 self.mark = False # function to iterate through a doubly linked list def iterate(self, head): node = stop = head flag = False while True: if node == stop and flag is True: break elif node == stop: flag = True yield node node = node.right # pointer to the head and minimum node in the root list root_list, min_node = None, None # maintain total node count in full fibonacci heap total_nodes = 0 # return min node in O(1) time def find_min(self): return self.min_node # extract (delete) the min node from the heap in O(log n) time # amortized cost analysis can be found here (http://bit.ly/1ow1Clm) def extract_min(self): z = self.min_node if z is not None: if z.child is not None: # attach child nodes to root list children = [x for x in self.iterate(z.child)] for i in range(0, len(children)): self.merge_with_root_list(children[i]) children[i].parent = None self.remove_from_root_list(z) # set new min node in heap if z == z.right: self.min_node = self.root_list = None else: self.min_node = z.right self.consolidate() self.total_nodes -= 1 return z # insert new node into the unordered root list in O(1) time # returns the node so that it can be used for decrease_key later def insert(self, key, value=None): n = self.Node(key, value) n.left = n.right = n self.merge_with_root_list(n) if self.min_node is None or n.key &lt; self.min_node.key: self.min_node = n self.total_nodes += 1 return n # modify the key of some node in the heap in O(1) time def decrease_key(self, x, k): if k &gt; x.key: return None x.key = k y = x.parent if y is not None and x.key &lt; y.key: self.cut(x, y) self.cascading_cut(y) if x.key &lt; self.min_node.key: self.min_node = x # merge two fibonacci heaps in O(1) time by concatenating the root lists # the root of the new root list becomes equal to the first list and the second # list is simply appended to the end (then the proper min node is determined) def merge(self, h2): H = FibonacciHeap() H.root_list, H.min_node = self.root_list, self.min_node # fix pointers when merging the two heaps last = h2.root_list.left h2.root_list.left = H.root_list.left H.root_list.left.right = h2.root_list H.root_list.left = last H.root_list.left.right = H.root_list # update min node if needed if h2.min_node.key &lt; H.min_node.key: H.min_node = h2.min_node # update total nodes H.total_nodes = self.total_nodes + h2.total_nodes return H # if a child node becomes smaller than its parent node we # cut this child node off and bring it up to the root list def cut(self, x, y): self.remove_from_child_list(y, x) y.degree -= 1 self.merge_with_root_list(x) x.parent = None x.mark = False # cascading cut of parent node to obtain good time bounds def cascading_cut(self, y): z = y.parent if z is not None: if y.mark is False: y.mark = True else: self.cut(y, z) self.cascading_cut(z) # combine root nodes of equal degree to consolidate the heap # by creating a list of unordered binomial trees def consolidate(self): A = [None] * int(math.log(self.total_nodes) * 2) nodes = [w for w in self.iterate(self.root_list)] for w in range(0, len(nodes)): x = nodes[w] d = x.degree while A[d] != None: y = A[d] if x.key &gt; y.key: temp = x x, y = y, temp self.heap_link(y, x) A[d] = None d += 1 A[d] = x # find new min node - no need to reconstruct new root list below # because root list was iteratively changing as we were moving # nodes around in the above loop for i in range(0, len(A)): if A[i] is not None: if A[i].key &lt; self.min_node.key: self.min_node = A[i] # actual linking of one node to another in the root list # while also updating the child linked list def heap_link(self, y, x): self.remove_from_root_list(y) y.left = y.right = y self.merge_with_child_list(x, y) x.degree += 1 y.parent = x y.mark = False # merge a node with the doubly linked root list def merge_with_root_list(self, node): if self.root_list is None: self.root_list = node else: node.right = self.root_list.right node.left = self.root_list self.root_list.right.left = node self.root_list.right = node # merge a node with the doubly linked child list of a root node def merge_with_child_list(self, parent, node): if parent.child is None: parent.child = node else: node.right = parent.child.right node.left = parent.child parent.child.right.left = node parent.child.right = node # remove a node from the doubly linked root list def remove_from_root_list(self, node): if node == self.root_list: self.root_list = node.right node.left.right = node.right node.right.left = node.left # remove a node from the doubly linked child list def remove_from_child_list(self, parent, node): if parent.child == parent.child.right: parent.child = None elif parent.child == node: parent.child = node.right node.right.parent = parent node.left.right = node.right node.right.left = node.left 参考资料 Fibonacci heaps","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"左倾堆","slug":"data_structure/leftist_heap","date":"2021-11-30T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/liftist-heap/","permalink":"https://wangqian0306.github.io/2021/liftist-heap/","excerpt":"","text":"左倾堆(Leftist Heap) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class LeftistNode(object): def __init__(self, val): self.val = val self.dist = 0 self.right = None self.left = None self.prnt = Noneclass LeftistHeap(object): def inorder(self, root): if root: myHeap.inorder(root.left) print(root.val) myHeap.inorder(root.right) def distance(self, root): if (root is None): return -1 else: return root.dist def merge(self, rootA, rootB): if (rootA is None): return rootB if (rootB is None): return rootA if (rootB.val &gt; rootA.val): temp = rootB rootB = rootA rootA = temp rootA.right = myHeap.merge(rootA.right, rootB) if (myHeap.distance(rootA.right) &gt; myHeap.distance(rootA.left)): temp = rootA.right rootA.right = rootA.left rootA.left = temp if (rootA.right is None): rootA.dist = 0 else: rootA.dist = 1 + (rootA.right.dist) return (rootA) def deletion(self, root): print(&quot;deleted element is &quot;, root.val) root = myHeap.merge(root.right, root.left) return root def insert(self, root): newnode = LeftistNode(int(input(&quot;enter value\\n&quot;))) root = myHeap.merge(root, newnode) print(&quot;root element is &quot;, root.val, &quot; inorder traversal of tree is:&quot;) myHeap.inorder(root) return (root)root = LeftistNode(1)myHeap = LeftistHeap() 参考资料 LeftistHeap-In-Python","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"小顶堆","slug":"data_structure/min_heap","date":"2021-11-26T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/min-heap/","permalink":"https://wangqian0306.github.io/2021/min-heap/","excerpt":"","text":"小顶堆(Min Heap) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class MinHeap: def __init__(self): self.__values: list = [] def __swap(self, i, j): self.__values[int(i)], self.__values[int(j)] = self.__values[int(j)], self.__values[int(i)] def push(self, data): self.__values.append(data) i = len(self.__values) - 1 while i != 0 and self.__values[int(i)] &lt; self.__values[int((i - 1) / 2)]: self.__swap(i, (i - 1) / 2) i -= 1 i /= 2 def top(self): if len(self.__values) == 0: return None return self.__values[0] def pop(self): top = self.__values[0] self.__swap(0, len(self.__values) - 1) self.__values.pop() i = 0 while True: left = 2 * i + 1 right = 2 * i + 2 max_ = i if left &lt; len(self.__values) and self.__values[left] &lt; self.__values[max_]: max_ = left if right &lt; len(self.__values) and self.__values[right] &lt; self.__values[max_]: max_ = right if max_ == i: break self.__swap(i, max_) i = max_ return top def isEmpty(self): return len(self.__values) == 0 def __len__(self): return len(self.__values) @staticmethod def fromList(data_list: list): heap = MinHeap() for i in data_list: heap.push(i) return heap 参考资料 python实现小顶堆MinHeap和哈夫曼树HaffumanTree","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"CDH 集成 Atlas","slug":"bigdata/atlas","date":"2021-11-25T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/atlas/","permalink":"https://wangqian0306.github.io/2021/atlas/","excerpt":"","text":"CDH 集成 Atlas 简介 Atlas 是一组可扩展和可扩展的核心基础治理服务——使企业能够有效和高效地满足其在 Hadoop 中的合规性要求，可以与整个企业数据生态系统集成。 版本说明 CDH 版本：6.3.2 Atlas 版本：2.1.0 注：构建时请使用部署了服务的 CDH 集群内的任意主机。 前置依赖 软件依赖 Java 8 Maven 3.5+ Python 2 Maven 配置 关闭 Maven HTTP 源安全检测，注释 setting.xml 中的以下行 1234567&lt;mirror&gt; &lt;id&gt;maven-default-http-blocker&lt;/id&gt; &lt;mirrorOf&gt;external:http:*&lt;/mirrorOf&gt; &lt;name&gt;Pseudo repository to mirror external repositories initially using HTTP.&lt;/name&gt; &lt;url&gt;http://0.0.0.0/&lt;/url&gt; &lt;blocked&gt;true&lt;/blocked&gt;&lt;/mirror&gt; 源码编译 获取并解压源码 12wget https://dlcdn.apache.org/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz --no-check-certificatetar -zxvf apache-atlas-2.1.0-sources.tar.gz 进入解压后的文件夹，并编辑 pom.xml 文件 1cd apache-atlas-sources-2.1.0/ 在 pom.xml 文件的 properties 部分，修改如下版本包 12345&lt;hadoop.version&gt;3.0.0-cdh6.3.2&lt;/hadoop.version&gt;&lt;hbase.version&gt;2.1.0-cdh6.3.2&lt;/hbase.version&gt;&lt;hive.version&gt;2.1.1-cdh6.3.2&lt;/hive.version&gt;&lt;kafka.version&gt;2.1.0-cdh6.3.2&lt;/kafka.version&gt;&lt;zookeeper.version&gt;3.4.5-cdh6.3.2&lt;/zookeeper.version&gt; 在 pom.xml 文件的 repository 部分，新增 Cloudera 软件源 1234&lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;&lt;/repository&gt; 修改配置文件 distro/src/conf/atlas-env.sh 并新增如下内容 123export HBASE_CONF_DIR=/etc/hbase/confexport MANAGE_LOCAL_SOLR=falseexport MANAGE_LOCAL_HBASE=false 修改 Hive Hook 适配代码 org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java(577) 1String catalogName = hiveDB.getCatalogName() != null ? hiveDB.getCatalogName().toLowerCase() : null; 改为 1String catalogName = null; org/apache/atlas/hive/hook/AtlasHiveHookContext.java(81) 1this.metastoreHandler = (listenerEvent != null) ? metastoreEvent.getIHMSHandler() : null; 1this.metastoreHandler = null; 编译打包 注：如果此处编译出现配置相关问题，则查看软件安装部分中的修改配置文件小节并编辑 distro/src/conf/atlas-application.properties 文件 12mvn clean -DskipTests installmvn clean -DskipTests package -Pdist 查看安装包 1ll distro/target 软件安装 创建相应目录并解压软件 1234mkdir -p /opt/atlascp distro/target/apache-atlas-2.1.0-bin.tar.gz /opt/atlascd /opt/atlastar -zxvf apache-atlas-2.1.0-bin.tar.gz 修改配置文件 conf/atlas-application.properties 123456789101112131415atlas.graph.storage.backend=hbaseatlas.graph.storage.hostname=&lt;zookeeper-1&gt;:2181,&lt;zookeeper-2&gt;:2181,&lt;zookeeper-3&gt;:2181atlas.graph.storage.hbase.table=apache_atlas_janusatlas.graph.index.search.solr.mode=cloudatlas.graph.index.search.solr.wait-searcher=trueatlas.graph.index.search.solr.zookeeper-url=&lt;zookeeper-1&gt;:2181,&lt;zookeeper-2&gt;:2181,&lt;zookeeper-3&gt;:2181/solratlas.graph.index.search.solr.zookeeper-connect-timeout=60000atlas.graph.index.search.solr.zookeeper-session-timeout=60000atlas.notification.embedded=falseatlas.kafka.zookeeper.connect=&lt;zookeeper-1&gt;:2181,&lt;zookeeper-2&gt;:2181,&lt;zookeeper-3&gt;:2181atlas.kafka.bootstrap.servers=&lt;zookeeper-1&gt;:2181,&lt;zookeeper-2&gt;:2181,&lt;zookeeper-3&gt;:2181atlas.kafka.zookeeper.session.timeout.ms=60000atlas.kafka.zookeeper.connection.timeout.ms=30000atlas.kafka.enable.auto.commit=trueatlas.audit.hbase.zookeeper.quorum=&lt;zookeeper-1&gt;:2181,&lt;zookeeper-2&gt;:2181,&lt;zookeeper-3&gt;:2181 同步 hbase,solr,zookeeper 配置文件至 conf 目录下 与各组件集成 此处需要将 Atals 配置文件放入各个组件中，但是由于 CDH 每次运行的配置文件是动态生成的所以需要将配置文件使用如下命令压缩至 jar 包中 1zip -u &lt;atlas_home&gt;/hook/hive/atlas-plugin-classloader-2.1.0.jar atlas-application.properties 注：此条压缩命令不可改变，如果变更会导致 jar 包内的文件路径映射问题，导致 Hook 无法读取配置文件以至于组件启动失败。 在如下配置项中新增配置： hive-site.xml 的 Hive 服务高级配置代码段（安全阀） hive-site.xml 的 Hive 客户端高级配置代码段（安全阀） hive-site.xml 的 HiveServer2 高级配置代码段（安全阀） 配置如下内容： 1234&lt;property&gt; &lt;name&gt;hive.exec.post.hooks&lt;/name&gt; &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;&lt;/property&gt; 在 hive-env.sh 的 Gateway 客户端环境高级配置代码段（安全阀）中新增如下内容: 1HIVE_AUX_JARS_PATH=&lt;atlas_path&gt;/hook/hive 在 Hive 辅助 JAR 目录中新增如下内容: 1&lt;atlas_path&gt;/hook/hive 重启 Hive 和 Atlas 即可 如果遇到问题请参照官方文档 与 Hive 集成 注：其他组件请在左侧导航栏中寻找 手动添加元数据 请求方式： POST 请求地址： http://&lt;atlas_host&gt;:21000/api/atlas/v2/entity 创建数据库实例 12345678910111213141516171819&#123; &quot;entity&quot;: &#123; &quot;attributes&quot;: &#123; &quot;owner&quot;: &quot;&lt;user&gt;&quot;, &quot;ownerName&quot;: &quot;&lt;user&gt;&quot;, &quot;name&quot;: &quot;mysql_instance&quot;, &quot;qualifiedName&quot;: &quot;mysql_instance@&lt;mysql_host&gt;&quot;, &quot;rdbms_type&quot;: &quot;mysql&quot;, &quot;description&quot;: &quot;dashboard database&quot;, &quot;contact_info&quot;: &quot;jdbc://&lt;mysql_host&gt;:3306/&lt;mysql_db&gt;&quot;, &quot;platform&quot;: &quot;Linux&quot;, &quot;hostname&quot;: &quot;&lt;mysql_host&gt;&quot;, &quot;protocol&quot;: &quot;mysql protocol&quot;, &quot;port&quot;: &quot;3306&quot; &#125;, &quot;typeName&quot;: &quot;rdbms_instance&quot;, &quot;status&quot;: &quot;ACTIVE&quot; &#125;&#125; 创建数据库 12345678910111213141516171819&#123; &quot;entity&quot;: &#123; &quot;attributes&quot;: &#123; &quot;owner&quot;: &quot;&lt;user&gt;&quot;, &quot;ownerName&quot;: &quot;&lt;user&gt;&quot;, &quot;name&quot;: &quot;&lt;database&gt;&quot;, &quot;displayText&quot;: &quot;&quot;, &quot;qualifiedName&quot;: &quot;&lt;database&gt;@&lt;mysql_host&gt;&quot;, &quot;description&quot;: &quot;&quot;, &quot;instance&quot;: &#123; &quot;guid&quot;: &quot;&lt;数据库实例 ID&gt;&quot;, &quot;typeName&quot;: &quot;rdbms_instance&quot;, &quot;entityStatus&quot;: &quot;ACTIVE&quot; &#125; &#125;, &quot;typeName&quot;: &quot;rdbms_db&quot;, &quot;status&quot;: &quot;ACTIVE&quot; &#125;&#125; 创建数据表 12345678910111213141516171819&#123; &quot;entity&quot;: &#123; &quot;attributes&quot;: &#123; &quot;owner&quot;: &quot;&lt;user&gt;&quot;, &quot;ownerName&quot;: &quot;&lt;user&gt;&quot;, &quot;name&quot;: &quot;&lt;table_name&gt;&quot;, &quot;db&quot;: &#123; &quot;guid&quot;: &quot;&lt;数据库 ID&gt;&quot;, &quot;typeName&quot;: &quot;rdbms_db&quot; &#125;, &quot;qualifiedName&quot;: &quot;&lt;database&gt;.&lt;table&gt;@&lt;host&gt;&quot;, &quot;description&quot;: &quot;&quot;, &quot;replicatedTo&quot;: null, &quot;replicatedFrom&quot;: null &#125;, &quot;typeName&quot;: &quot;rdbms_table&quot;, &quot;status&quot;: &quot;ACTIVE&quot; &#125;&#125; 创建列 12345678910111213141516171819202122&#123; &quot;entity&quot;: &#123; &quot;attributes&quot;: &#123; &quot;owner&quot;: &quot;&lt;user&gt;&quot;, &quot;ownerName&quot;: &quot;&lt;user&gt;&quot;, &quot;name&quot;: &quot;&lt;column&gt;&quot;, &quot;qualifiedName&quot;: &quot;&lt;database&gt;.&lt;table&gt;.&lt;column&gt;@&lt;host&gt;&quot;, &quot;default_value&quot;: null, &quot;isPrimaryKey&quot;: false, &quot;indexes&quot;: [], &quot;isNullable&quot;: false, &quot;data_type&quot;: &quot;datetime&quot;, &quot;comment&quot;: &quot;&quot;, &quot;table&quot;: &#123; &quot;guid&quot;: &quot;&lt;table&gt;&quot;, &quot;typeName&quot;: &quot;rdbms_table&quot; &#125; &#125;, &quot;typeName&quot;: &quot;rdbms_column&quot;, &quot;status&quot;: &quot;ACTIVE&quot; &#125;&#125; 创建变化关系 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;entity&quot;: &#123; &quot;attributes&quot;: &#123; &quot;owner&quot;: null, &quot;ownerName&quot;: null, &quot;outputs&quot;: [ &#123; &quot;guid&quot;: &quot;&lt;output_id&gt;&quot;, &quot;typeName&quot;: &quot;&lt;output_type&gt;&quot; &#125; ], &quot;queryGraph&quot;: null, &quot;replicatedTo&quot;: null, &quot;userDescription&quot;: null, &quot;replicatedFrom&quot;: null, &quot;recentQueries&quot;: [], &quot;qualifiedName&quot;: &quot;&lt;name&gt;&quot;, &quot;displayName&quot;: null, &quot;inputs&quot;: [ &#123; &quot;guid&quot;: &quot;&lt;input_id&gt;&quot;, &quot;typeName&quot;: &quot;&lt;input_type&gt;&quot; &#125; ], &quot;description&quot;: null, &quot;userName&quot;: &quot;&quot;, &quot;queryId&quot;: &quot;&quot;, &quot;clusterName&quot;: &quot;primary&quot;, &quot;name&quot;: &quot;&lt;name&gt;&quot;, &quot;queryText&quot;: &quot;&quot;, &quot;startTime&quot;: 1638773124750, &quot;operationType&quot;: &quot; &quot;, &quot;queryPlan&quot;: &quot;Not Supported&quot;, &quot;endTime&quot;: 1638773124750 &#125;, &quot;typeName&quot;: &quot;Process&quot;, &quot;status&quot;: &quot;ACTIVE&quot; &#125;&#125; 参考资料 环境篇：Atlas2.0.0兼容CDH6.2.0部署 CDH6.3.2 Atlas-2.1.0安装打包使用，亲测可用","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Atlas","slug":"Atlas","permalink":"https://wangqian0306.github.io/tags/Atlas/"}]},{"title":"Flink","slug":"bigdata/flink","date":"2021-11-25T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/flink/","permalink":"https://wangqian0306.github.io/2021/flink/","excerpt":"","text":"Flink 简介 Apache Flink 是一个框架和分布式处理引擎，用于在无界和有界数据流上进行有状态的计算。 Flink 能在所有常见集群环境中运行，并能以内存速度为限制进行任意规模的计算。 运行方式 Flink 可以通过如下三种方式运行程序： 应用模式(Application Mode) 集群生命周期和应用进行绑定，当应用执行完成才会停止集群。 应用程序公用公共资源。 任务模式(Per-Job Mode) 集群生命周期和任务周期绑定。 单个任务独享所需资源。 节点需要一定的启动时间，适合长时间运行的程序。 资源利用率相对低。 会话模式(Session Mode) 集群生命周期不受任务影响，只有手动关闭会话，集群才会被停止。 所有任务争抢一套系统资源。 集群所有节点都预先启动，无需每次启动作业都申请资源、启动节点，适合对于作业执行时间段、对任务启动时间敏感的任务。 资源充分共享，资源利用率高。 所以一般情况下建议使用应用模式运行。 部署方式 独立(适合试用 Flink) Kubernetes Yarn 命令行工具安装及配置 从官网下载对应版本的安装包，然后进行解压。 编辑环境变量配置文件，然后填入如下内容 1vim /etc/profile.d/flink.sh 1export PATH=&lt;flink_path&gt;/bin:$PATH 修改配置文件 flink-conf.yaml 12rest.bind-port: 8080-8090rest.bind-address: 0.0.0.0 独立部署(Docker-Compose,Session Mode) 编写 docker-compose.yaml 配置文件 1234567891011121314151617181920212223services: jobmanager: image: flink:1.14.0-scala_2.12-java11 ports: - &quot;8081:8081&quot; - &quot;6123:6123&quot; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager: image: flink:1.14.0-scala_2.12-java11 depends_on: - jobmanager command: taskmanager scale: 1 environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 8 启动集群 1docker-compose up -d 基础命令 远程执行 Jar 包 1flink run --detached &lt;jar_path&gt; 获取正在执行的任务 1flink list 创建保存点 1flink savepoint &lt;job_id&gt; /tmp/flink-savepoints 删除保存点 1flink savepoint --dispose &lt;savepoint_path&gt; &lt;job_id&gt; 终止工作 1flink stop --savepointPath &lt;savepoint_path&gt; &lt;job_id&gt; 取消作业 1flink cancel &lt;job_id&gt; 从保存点启动作业 1flink run --detached --fromSavepoint &lt;savepoint_path&gt; &lt;jar_path&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"},{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"}]},{"title":"B 树","slug":"data_structure/b_tree","date":"2021-11-25T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/b/","permalink":"https://wangqian0306.github.io/2021/b/","excerpt":"","text":"B树(B-tree) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192class BTreeNode: def __init__(self, leaf=False): self.leaf = leaf self.keys = [] self.child = []class BTree: def __init__(self, t): self.root = BTreeNode(True) self.t = t # Insert a key def insert(self, k): root = self.root if len(root.keys) == (2 * self.t) - 1: temp = BTreeNode() self.root = temp temp.child.insert(0, root) self.split_child(temp, 0) self.insert_non_full(temp, k) else: self.insert_non_full(root, k) # Insert non full def insert_non_full(self, x, k): i = len(x.keys) - 1 if x.leaf: x.keys.append((None, None)) while i &gt;= 0 and k[0] &lt; x.keys[i][0]: x.keys[i + 1] = x.keys[i] i -= 1 x.keys[i + 1] = k else: while i &gt;= 0 and k[0] &lt; x.keys[i][0]: i -= 1 i += 1 if len(x.child[i].keys) == (2 * self.t) - 1: self.split_child(x, i) if k[0] &gt; x.keys[i][0]: i += 1 self.insert_non_full(x.child[i], k) # Split the child def split_child(self, x, i): t = self.t y = x.child[i] z = BTreeNode(y.leaf) x.child.insert(i + 1, z) x.keys.insert(i, y.keys[t - 1]) z.keys = y.keys[t: (2 * t) - 1] y.keys = y.keys[0: t - 1] if not y.leaf: z.child = y.child[t: 2 * t] y.child = y.child[0: t - 1] # Delete a node def delete(self, x, k): t = self.t i = 0 while i &lt; len(x.keys) and k[0] &gt; x.keys[i][0]: i += 1 if x.leaf: if i &lt; len(x.keys) and x.keys[i][0] == k[0]: x.keys.pop(i) return return if i &lt; len(x.keys) and x.keys[i][0] == k[0]: return self.delete_internal_node(x, k, i) elif len(x.child[i].keys) &gt;= t: self.delete(x.child[i], k) else: if i != 0 and i + 2 &lt; len(x.child): if len(x.child[i - 1].keys) &gt;= t: self.delete_sibling(x, i, i - 1) elif len(x.child[i + 1].keys) &gt;= t: self.delete_sibling(x, i, i + 1) else: self.delete_merge(x, i, i + 1) elif i == 0: if len(x.child[i + 1].keys) &gt;= t: self.delete_sibling(x, i, i + 1) else: self.delete_merge(x, i, i + 1) elif i + 1 == len(x.child): if len(x.child[i - 1].keys) &gt;= t: self.delete_sibling(x, i, i - 1) else: self.delete_merge(x, i, i - 1) self.delete(x.child[i], k) # Delete internal node def delete_internal_node(self, x, k, i): t = self.t if x.leaf: if x.keys[i][0] == k[0]: x.keys.pop(i) return return if len(x.child[i].keys) &gt;= t: x.keys[i] = self.delete_predecessor(x.child[i]) return elif len(x.child[i + 1].keys) &gt;= t: x.keys[i] = self.delete_successor(x.child[i + 1]) return else: self.delete_merge(x, i, i + 1) self.delete_internal_node(x.child[i], k, self.t - 1) # Delete the predecessor def delete_predecessor(self, x): if x.leaf: return x.pop() n = len(x.keys) - 1 if len(x.child[n].keys) &gt;= self.t: self.delete_sibling(x, n + 1, n) else: self.delete_merge(x, n, n + 1) self.delete_predecessor(x.child[n]) # Delete the successor def delete_successor(self, x): if x.leaf: return x.keys.pop(0) if len(x.child[1].keys) &gt;= self.t: self.delete_sibling(x, 0, 1) else: self.delete_merge(x, 0, 1) self.delete_successor(x.child[0]) # Delete resolution def delete_merge(self, x, i, j): cnode = x.child[i] if j &gt; i: rsnode = x.child[j] cnode.keys.append(x.keys[i]) for k in range(len(rsnode.keys)): cnode.keys.append(rsnode.keys[k]) if len(rsnode.child) &gt; 0: cnode.child.append(rsnode.child[k]) if len(rsnode.child) &gt; 0: cnode.child.append(rsnode.child.pop()) new = cnode x.keys.pop(i) x.child.pop(j) else: lsnode = x.child[j] lsnode.keys.append(x.keys[j]) for i in range(len(cnode.keys)): lsnode.keys.append(cnode.keys[i]) if len(lsnode.child) &gt; 0: lsnode.child.append(cnode.child[i]) if len(lsnode.child) &gt; 0: lsnode.child.append(cnode.child.pop()) new = lsnode x.keys.pop(j) x.child.pop(i) if x == self.root and len(x.keys) == 0: self.root = new # Delete the sibling def delete_sibling(self, x, i, j): cnode = x.child[i] if i &lt; j: rsnode = x.child[j] cnode.keys.append(x.keys[i]) x.keys[i] = rsnode.keys[0] if len(rsnode.child) &gt; 0: cnode.child.append(rsnode.child[0]) rsnode.child.pop(0) rsnode.keys.pop(0) else: lsnode = x.child[j] cnode.keys.insert(0, x.keys[i - 1]) x.keys[i - 1] = lsnode.keys.pop() if len(lsnode.child) &gt; 0: cnode.child.insert(0, lsnode.child.pop()) # Print the tree def print_tree(self, x, l=0): print(&quot;Level &quot;, l, &quot; &quot;, len(x.keys), end=&quot;:&quot;) for i in x.keys: print(i, end=&quot; &quot;) print() l += 1 if len(x.child) &gt; 0: for i in x.child: self.print_tree(i, l) 参考资料 B tree","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"B+树","slug":"data_structure/b+tree","date":"2021-11-25T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/b+/","permalink":"https://wangqian0306.github.io/2021/b+/","excerpt":"","text":"B+树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363import random # for demo testsplits = 0parent_splits = 0fusions = 0parent_fusions = 0class Node(object): &quot;&quot;&quot;Base node object. It should be index node Each node stores keys and children. Attributes: parent &quot;&quot;&quot; def __init__(self, parent=None): &quot;&quot;&quot;Child nodes are stored in values. Parent nodes simply act as a medium to traverse the tree. :type parent: Node&quot;&quot;&quot; self.keys: list = [] self.values: list[Node] = [] self.parent: Node = parent def index(self, key): &quot;&quot;&quot;Return the index where the key should be. :type key: str &quot;&quot;&quot; for i, item in enumerate(self.keys): if key &lt; item: return i return len(self.keys) def __getitem__(self, item): return self.values[self.index(item)] def __setitem__(self, key, value): i = self.index(key) self.keys[i:i] = [key] self.values.pop(i) self.values[i:i] = value def split(self): &quot;&quot;&quot;Splits the node into two and stores them as child nodes. extract a pivot from the child to be inserted into the keys of the parent. @:return key and two children &quot;&quot;&quot; global splits, parent_splits splits += 1 parent_splits += 1 left = Node(self.parent) mid = len(self.keys) // 2 left.keys = self.keys[:mid] left.values = self.values[:mid + 1] for child in left.values: child.parent = left key = self.keys[mid] self.keys = self.keys[mid + 1:] self.values = self.values[mid + 1:] return key, [left, self] def __delitem__(self, key): i = self.index(key) del self.values[i] if i &lt; len(self.keys): del self.keys[i] else: del self.keys[i - 1] def fusion(self): global fusions, parent_fusions fusions += 1 parent_fusions += 1 index = self.parent.index(self.keys[0]) # merge this node with the next node if index &lt; len(self.parent.keys): next_node: Node = self.parent.values[index + 1] next_node.keys[0:0] = self.keys + [self.parent.keys[index]] for child in self.values: child.parent = next_node next_node.values[0:0] = self.values else: # If self is the last node, merge with prev prev: Node = self.parent.values[-2] prev.keys += [self.parent.keys[-1]] + self.keys for child in self.values: child.parent = prev prev.values += self.values def borrow_key(self, minimum: int): index = self.parent.index(self.keys[0]) if index &lt; len(self.parent.keys): next_node: Node = self.parent.values[index + 1] if len(next_node.keys) &gt; minimum: self.keys += [self.parent.keys[index]] borrow_node = next_node.values.pop(0) borrow_node.parent = self self.values += [borrow_node] self.parent.keys[index] = next_node.keys.pop(0) return True elif index != 0: prev: Node = self.parent.values[index - 1] if len(prev.keys) &gt; minimum: self.keys[0:0] = [self.parent.keys[index - 1]] borrow_node = prev.values.pop() borrow_node.parent = self self.values[0:0] = [borrow_node] self.parent.keys[index - 1] = prev.keys.pop() return True return Falseclass Leaf(Node): def __init__(self, parent=None, prev_node=None, next_node=None): &quot;&quot;&quot; Create a new leaf in the leaf link :type prev_node: Leaf :type next_node: Leaf &quot;&quot;&quot; super(Leaf, self).__init__(parent) self.next: Leaf = next_node if next_node is not None: next_node.prev = self self.prev: Leaf = prev_node if prev_node is not None: prev_node.next = self def __getitem__(self, item): return self.values[self.keys.index(item)] def __setitem__(self, key, value): i = self.index(key) if key not in self.keys: self.keys[i:i] = [key] self.values[i:i] = [value] else: self.values[i - 1] = value def split(self): global splits splits += 1 left = Leaf(self.parent, self.prev, self) mid = len(self.keys) // 2 left.keys = self.keys[:mid] left.values = self.values[:mid] self.keys: list = self.keys[mid:] self.values: list = self.values[mid:] # When the leaf node is split, set the parent key to the left-most key of the right child node. return self.keys[0], [left, self] def __delitem__(self, key): i = self.keys.index(key) del self.keys[i] del self.values[i] def fusion(self): global fusions fusions += 1 if self.next is not None and self.next.parent == self.parent: self.next.keys[0:0] = self.keys self.next.values[0:0] = self.values else: self.prev.keys += self.keys self.prev.values += self.values if self.next is not None: self.next.prev = self.prev if self.prev is not None: self.prev.next = self.next def borrow_key(self, minimum: int): index = self.parent.index(self.keys[0]) if index &lt; len(self.parent.keys) and len(self.next.keys) &gt; minimum: self.keys += [self.next.keys.pop(0)] self.values += [self.next.values.pop(0)] self.parent.keys[index] = self.next.keys[0] return True elif index != 0 and len(self.prev.keys) &gt; minimum: self.keys[0:0] = [self.prev.keys.pop()] self.values[0:0] = [self.prev.values.pop()] self.parent.keys[index - 1] = self.keys[0] return True return Falseclass BPlusTree(object): &quot;&quot;&quot;B+ tree object, consisting of nodes. Nodes will automatically be split into two once it is full. When a split occurs, a key will &#x27;float&#x27; upwards and be inserted into the parent node to act as a pivot. Attributes: maximum (int): The maximum number of keys each node can hold. &quot;&quot;&quot; root: Node def __init__(self, maximum=4): self.root = Leaf() self.maximum: int = maximum if maximum &gt; 2 else 2 self.minimum: int = self.maximum // 2 self.depth = 0 def find(self, key) -&gt; Leaf: &quot;&quot;&quot; find the leaf Returns: Leaf: the leaf which should have the key &quot;&quot;&quot; node = self.root # Traverse tree until leaf node is reached. while type(node) is not Leaf: node = node[key] return node def __getitem__(self, item): return self.find(item)[item] def query(self, key): &quot;&quot;&quot;Returns a value for a given key, and None if the key does not exist.&quot;&quot;&quot; leaf = self.find(key) return leaf[key] if key in leaf.keys else None def change(self, key, value): &quot;&quot;&quot;change the value Returns: (bool,Leaf): the leaf where the key is. return False if the key does not exist &quot;&quot;&quot; leaf = self.find(key) if key not in leaf.keys: return False, leaf else: leaf[key] = value return True, leaf def __setitem__(self, key, value, leaf=None): &quot;&quot;&quot;Inserts a key-value pair after traversing to a leaf node. If the leaf node is full, split the leaf node into two. &quot;&quot;&quot; if leaf is None: leaf = self.find(key) leaf[key] = value if len(leaf.keys) &gt; self.maximum: self.insert_index(*leaf.split()) def insert(self, key, value): &quot;&quot;&quot; Returns: (bool,Leaf): the leaf where the key is inserted. return False if already has same key &quot;&quot;&quot; leaf = self.find(key) if key in leaf.keys: return False, leaf else: self.__setitem__(key, value, leaf) return True, leaf def insert_index(self, key, values: list[Node]): &quot;&quot;&quot;For a parent and child node, Insert the values from the child into the values of the parent.&quot;&quot;&quot; parent = values[1].parent if parent is None: values[0].parent = values[1].parent = self.root = Node() self.depth += 1 self.root.keys = [key] self.root.values = values return parent[key] = values # If the node is full, split the node into two. if len(parent.keys) &gt; self.maximum: self.insert_index(*parent.split()) # Once a leaf node is split, it consists of a internal node and two leaf nodes. # These need to be re-inserted back into the tree. def delete(self, key, node: Node = None): if node is None: node = self.find(key) del node[key] if len(node.keys) &lt; self.minimum: if node == self.root: if len(self.root.keys) == 0 and len(self.root.values) &gt; 0: self.root = self.root.values[0] self.root.parent = None self.depth -= 1 return elif not node.borrow_key(self.minimum): node.fusion() self.delete(key, node.parent) # Change the left-most key in node # if i == 0: # node = self # while i == 0: # if node.parent is None: # if len(node.keys) &gt; 0 and node.keys[0] == key: # node.keys[0] = self.keys[0] # return # node = node.parent # i = node.index(key) # # node.keys[i - 1] = self.keys[0] def show(self, node=None, file=None, _prefix=&quot;&quot;, _last=True): &quot;&quot;&quot;Prints the keys at each level.&quot;&quot;&quot; if node is None: node = self.root print(_prefix, &quot;`- &quot; if _last else &quot;|- &quot;, node.keys, sep=&quot;&quot;, file=file) _prefix += &quot; &quot; if _last else &quot;| &quot; if type(node) is Node: # Recursively print the key of child nodes (if these exist). for i, child in enumerate(node.values): _last = (i == len(node.values) - 1) self.show(child, file, _prefix, _last) def output(self): return splits, parent_splits, fusions, parent_fusions, self.depth def readfile(self, reader): i = 0 for i, line in enumerate(reader): s = line.decode().split(maxsplit=1) self[s[0]] = s[1] if i % 1000 == 0: print(&#x27;Insert &#x27; + str(i) + &#x27;items&#x27;) return i + 1 def leftmost_leaf(self) -&gt; Leaf: node = self.root while type(node) is not Leaf: node = node.values[0] return nodedef demo(): bplustree = BPlusTree() random_list = random.sample(range(1, 100), 20) for i in random_list: bplustree[i] = &#x27;test&#x27; + str(i) print(&#x27;Insert &#x27; + str(i)) bplustree.show() random.shuffle(random_list) for i in random_list: print(&#x27;Delete &#x27; + str(i)) bplustree.delete(i) bplustree.show()if __name__ == &#x27;__main__&#x27;: demo() 参考资料 bplustree","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"基数树","slug":"data_structure/radix_tree","date":"2021-11-24T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/radix/","permalink":"https://wangqian0306.github.io/2021/radix/","excerpt":"","text":"基数树(Radix Tree) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from collections import defaultdictclass RadixTreeNode(): def __init__(self, is_word=False, keys=None): self.is_word = is_word self.children = keys if keys else defaultdict(RadixTreeNode)class Trie(): def __init__(self): self.root = RadixTreeNode() def insert(self, word): return self.insert_helper(self.root, word) def insert_helper(self, node, word): for key, child in node.children.items(): prefix, split, rest = self.match(key, word) if not split: # key complete match if not rest: # word matched child.is_word = True return True else: # match rest of word return self.insert_helper(child, rest) if prefix: # key partial match, need to split new_node = RadixTreeNode(is_word=not rest, keys=&#123;split: child&#125;) node.children[prefix] = new_node del node.children[key] return self.insert_helper(new_node, rest) node.children[word] = RadixTreeNode(is_word=True) def search(self, word): return self.search_helper(self.root, word) def search_helper(self, node, word): for key, child in node.children.items(): prefix, split, rest = self.match(key, word) if not split and not rest: return child.is_word if not split: return self.search_helper(child, rest) return False def startsWith(self, word): return self.startsWith_helper(self.root, word) def startsWith_helper(self, node, word): for key, child in node.children.items(): prefix, split, rest = self.match(key, word) if not rest: return True if not split: return self.startsWith_helper(child, rest) return False def match(self, key, word): i = 0 for k, w in zip(key, word): if k != w: break i += 1 return key[:i], key[i:], word[i:] 参考资料 python-radix-tree-memory-efficient","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"三分搜索树","slug":"data_structure/ternary_search_tree","date":"2021-11-24T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/ternary/","permalink":"https://wangqian0306.github.io/2021/ternary/","excerpt":"","text":"三分搜索树(Ternary Search Tree) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107class Node: lo = None hi = None eq = None endpoint = False def __init__(self, char): self.char = char def __repr__(self): # useful in debugging return &#x27;&#x27;.join([&#x27;[&#x27;, self.char, (&#x27;&#x27; if not self.endpoint else &#x27; &lt;end&gt;&#x27;), (&#x27;&#x27; if self.lo is None else &#x27; lo: &#x27; + self.lo.__repr__()), (&#x27;&#x27; if self.eq is None else &#x27; eq: &#x27; + self.eq.__repr__()), (&#x27;&#x27; if self.hi is None else &#x27; hi: &#x27; + self.hi.__repr__()), &#x27;]&#x27;])def insert(node, string): if len(string) == 0: return node head = string[0] tail = string[1:] if node is None: node = Node(head) if head &lt; node.char: node.lo = insert(node.lo, string) elif head &gt; node.char: node.hi = insert(node.hi, string) else: if len(tail) == 0: node.endpoint = True else: node.eq = insert(node.eq, tail) return nodedef search(node, string): if node is None or len(string) == 0: return False head = string[0] tail = string[1:] if head &lt; node.char: return search(node.lo, string) elif head &gt; node.char: return search(node.hi, string) else: # use &#x27;and&#x27; for matches on complete words only, # versus &#x27;or&#x27; for matches on string prefixes if len(tail) == 0 or node.endpoint: return True return search(node.eq, tail)def suffixes(node): if node is not None: if node.endpoint: yield node.char if node.lo: for s in suffixes(node.lo): yield s if node.hi: for s in suffixes(node.hi): yield s if node.eq: for s in suffixes(node.eq): yield node.char + sdef autocompletes(node, string): if node is None or len(string) == 0: return [] head = string[0] tail = string[1:] if head &lt; node.char: return autocompletes(node.lo, string) elif head &gt; node.char: return autocompletes(node.hi, string) else: if len(tail) == 0: # found the node containing the prefix string, # so get all the possible suffixes underneath return suffixes(node.eq) return autocompletes(node.eq, string[1:])class Trie: # a simple wrapper root = None def __init__(self, string): self.append(string) def append(self, string): self.root = insert(self.root, string) def __contains__(self, string): return search(self.root, string) def autocomplete(self, string): return map(lambda x: string + x, autocompletes(self.root, string)) 参考资料 tire","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"哈希表","slug":"data_structure/hash_table","date":"2021-11-23T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/hash_table/","permalink":"https://wangqian0306.github.io/2021/hash_table/","excerpt":"","text":"哈希表 拉链法(Open Hash Tables (Closed Addressing)) 12345678910111213141516171819class HashTable: def __init__(self, size: int): self.size = size self.table = &#123;&#125; for i in range(size): self.table[i] = [] def insert(self, content): self.table[hash(content) % self.size].append(content) def search(self, content): for cache in self.table[hash(content) % self.size]: if content == cache: return True return False def pretty_print(self): for key in self.table.keys(): print(f&quot;key:&#123;key&#125; values:&#123;self.table[key]&#125;&quot;) 开地址法(Closed Hash Tables, using buckets) 1234567891011121314151617181920212223242526272829303132333435363738class HashTable: def __init__(self, size: int, bucket_size: int): self.size = size self.table = [] self.bucket_size = bucket_size self.extra = [] for i in range(size): for j in range(bucket_size): self.table.append(None) def insert(self, content): flag = True index = hash(content) % self.size for i in range(index * self.bucket_size, (index + 1) * self.bucket_size): if self.table[i] is None or self.table[i] == content: self.table[i] = content flag = False break if flag: self.extra.append(content) def search(self, content): index = hash(content) % self.size for i in range(index * self.bucket_size, (index + 1) * self.bucket_size): if self.table[i] == content: return True for cache in self.extra: if cache == content: return True return False def pretty_print(self): for index in range(self.size): cache = [] for i in range(index * self.bucket_size, (index + 1) * self.bucket_size): cache.append(self.table[i]) print(f&quot;bucket:&#123;index&#125; value:&#123;cache&#125;&quot;) print(f&quot;extra: &#123;self.extra&#125;&quot;)","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"字典树","slug":"data_structure/tire","date":"2021-11-23T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/tire_tree/","permalink":"https://wangqian0306.github.io/2021/tire_tree/","excerpt":"","text":"字典树 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class TrieNode: # Trie node class def __init__(self): self.children = [None] * 26 # isEndOfWord is True if node represent the end of the word self.isEndOfWord = Falseclass Trie: # Trie data structure class def __init__(self): self.root = self.getNode() def getNode(self): # Returns new trie node (initialized to NULLs) return TrieNode() def _charToIndex(self, ch): # private helper function # Converts key current character into index # use only &#x27;a&#x27; through &#x27;z&#x27; and lower case return ord(ch) - ord(&#x27;a&#x27;) def insert(self, key): # If not present, inserts key into trie # If the key is prefix of trie node, # just marks leaf node pCrawl = self.root length = len(key) for level in range(length): index = self._charToIndex(key[level]) # if current character is not present if not pCrawl.children[index]: pCrawl.children[index] = self.getNode() pCrawl = pCrawl.children[index] # mark last node as leaf pCrawl.isEndOfWord = True def search(self, key): # Search key in the trie # Returns true if key presents # in trie, else false pCrawl = self.root length = len(key) for level in range(length): index = self._charToIndex(key[level]) if not pCrawl.children[index]: return False pCrawl = pCrawl.children[index] return pCrawl.isEndOfWord 参考资料 Trie","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"红黑树","slug":"data_structure/red_black","date":"2021-11-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/red_black_tree/","permalink":"https://wangqian0306.github.io/2021/red_black_tree/","excerpt":"","text":"红黑树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365import sys# data structure that represents a node in the treeclass Node: def __init__(self, data): self.data = data # holds the key self.parent = None # pointer to the parent self.left = None # pointer to left child self.right = None # pointer to right child self.color = 1 # 1 . Red, 0 . Black# class RedBlackTree implements the operations in Red Black Treeclass RedBlackTree: def __init__(self): self.null_node = Node(0) self.null_node.color = 0 self.null_node.left = None self.null_node.right = None self.root = self.null_node def __pre_order_helper(self, node): if node != self.null_node: sys.stdout.write(node.data + &quot; &quot;) self.__pre_order_helper(node.left) self.__pre_order_helper(node.right) def __in_order_helper(self, node): if node != self.null_node: self.__in_order_helper(node.left) sys.stdout.write(node.data + &quot; &quot;) self.__in_order_helper(node.right) def __post_order_helper(self, node): if node != self.null_node: self.__post_order_helper(node.left) self.__post_order_helper(node.right) sys.stdout.write(node.data + &quot; &quot;) def __search_tree_helper(self, node, key): if node == self.null_node or key == node.data: return node if key &lt; node.data: return self.__search_tree_helper(node.left, key) return self.__search_tree_helper(node.right, key) # fix the rb tree modified by the delete operation def __fix_delete(self, x): while x != self.root and x.color == 0: if x == x.parent.left: s = x.parent.right if s.color == 1: # case 3.1 s.color = 0 x.parent.color = 1 self.left_rotate(x.parent) s = x.parent.right if s.left.color == 0 and s.right.color == 0: # case 3.2 s.color = 1 x = x.parent else: if s.right.color == 0: # case 3.3 s.left.color = 0 s.color = 1 self.right_rotate(s) s = x.parent.right # case 3.4 s.color = x.parent.color x.parent.color = 0 s.right.color = 0 self.left_rotate(x.parent) x = self.root else: s = x.parent.left if s.color == 1: # case 3.1 s.color = 0 x.parent.color = 1 self.right_rotate(x.parent) s = x.parent.left if s.left.color == 0 and s.right.color == 0: # case 3.2 s.color = 1 x = x.parent else: if s.left.color == 0: # case 3.3 s.right.color = 0 s.color = 1 self.left_rotate(s) s = x.parent.left # case 3.4 s.color = x.parent.color x.parent.color = 0 s.left.color = 0 self.right_rotate(x.parent) x = self.root x.color = 0 def __rb_transplant(self, u, v): if u.parent is None: self.root = v elif u == u.parent.left: u.parent.left = v else: u.parent.right = v v.parent = u.parent def __delete_node_helper(self, node, key): # find the node containing key z = self.null_node while node != self.null_node: if node.data == key: z = node if node.data &lt;= key: node = node.right else: node = node.left if z == self.null_node: print(&quot;Couldn&#x27;t find key in the tree&quot;) return y = z y_original_color = y.color if z.left == self.null_node: x = z.right self.__rb_transplant(z, z.right) elif z.right == self.null_node: x = z.left self.__rb_transplant(z, z.left) else: y = self.minimum(z.right) y_original_color = y.color x = y.right if y.parent == z: x.parent = y else: self.__rb_transplant(y, y.right) y.right = z.right y.right.parent = y self.__rb_transplant(z, y) y.left = z.left y.left.parent = y y.color = z.color if y_original_color == 0: self.__fix_delete(x) # fix the red-black tree def __fix_insert(self, k): while k.parent.color == 1: if k.parent == k.parent.parent.right: u = k.parent.parent.left # uncle if u.color == 1: # case 3.1 u.color = 0 k.parent.color = 0 k.parent.parent.color = 1 k = k.parent.parent else: if k == k.parent.left: # case 3.2.2 k = k.parent self.right_rotate(k) # case 3.2.1 k.parent.color = 0 k.parent.parent.color = 1 self.left_rotate(k.parent.parent) else: u = k.parent.parent.right # uncle if u.color == 1: # mirror case 3.1 u.color = 0 k.parent.color = 0 k.parent.parent.color = 1 k = k.parent.parent else: if k == k.parent.right: # mirror case 3.2.2 k = k.parent self.left_rotate(k) # mirror case 3.2.1 k.parent.color = 0 k.parent.parent.color = 1 self.right_rotate(k.parent.parent) if k == self.root: break self.root.color = 0 def __print_helper(self, node, indent, last): # print the tree structure on the screen if node != self.null_node: sys.stdout.write(indent) if last: sys.stdout.write(&quot;R----&quot;) indent += &quot; &quot; else: sys.stdout.write(&quot;L----&quot;) indent += &quot;| &quot; s_color = &quot;RED&quot; if node.color == 1 else &quot;BLACK&quot; print(str(node.data) + &quot;(&quot; + s_color + &quot;)&quot;) self.__print_helper(node.left, indent, False) self.__print_helper(node.right, indent, True) # Pre-Order traversal # Node.Left Subtree.Right Subtree def preorder(self): self.__pre_order_helper(self.root) # In-Order traversal # left Subtree . Node . Right Subtree def inorder(self): self.__in_order_helper(self.root) # Post-Order traversal # Left Subtree . Right Subtree . Node def postorder(self): self.__post_order_helper(self.root) # search the tree for the key k # and return the corresponding node def searchTree(self, k): return self.__search_tree_helper(self.root, k) # find the node with the minimum key def minimum(self, node): while node.left != self.null_node: node = node.left return node # find the node with the maximum key def maximum(self, node): while node.right != self.null_node: node = node.right return node # find the successor of a given node def successor(self, x): # if the right subtree is not None, # the successor is the leftmost node in the # right subtree if x.right != self.null_node: return self.minimum(x.right) # else it is the lowest ancestor of x whose # left child is also an ancestor of x. y = x.parent while y != self.null_node and x == y.right: x = y y = y.parent return y # find the predecessor of a given node def predecessor(self, x): # if the left subtree is not None, # the predecessor is the rightmost node in the # left subtree if x.left != self.null_node: return self.maximum(x.left) y = x.parent while y != self.null_node and x == y.left: x = y y = y.parent return y # rotate left at node x def left_rotate(self, x): y = x.right x.right = y.left if y.left != self.null_node: y.left.parent = x y.parent = x.parent if x.parent is None: self.root = y elif x == x.parent.left: x.parent.left = y else: x.parent.right = y y.left = x x.parent = y # rotate right at node x def right_rotate(self, x): y = x.left x.left = y.right if y.right != self.null_node: y.right.parent = x y.parent = x.parent if x.parent is None: self.root = y elif x == x.parent.right: x.parent.right = y else: x.parent.left = y y.right = x x.parent = y # insert the key to the tree in its appropriate position # and fix the tree def insert(self, key): # Ordinary Binary Search Insertion node = Node(key) node.parent = None node.data = key node.left = self.null_node node.right = self.null_node node.color = 1 # new node must be red y = None x = self.root while x != self.null_node: y = x if node.data &lt; x.data: x = x.left else: x = x.right # y is parent of x node.parent = y if y is None: self.root = node elif node.data &lt; y.data: y.left = node else: y.right = node # if new node is a root node, simply return if node.parent is None: node.color = 0 return # if the grandparent is None, simply return if node.parent.parent is None: return # Fix the tree self.__fix_insert(node) def get_root(self): return self.root # delete the node from the tree def delete_node(self, data): self.__delete_node_helper(self.root, data) # print the tree structure on the screen def pretty_print(self): self.__print_helper(self.root, &quot;&quot;, True) 参考资料 Red Black Trees","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"伸展树","slug":"data_structure/splay","date":"2021-11-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/splay_tree/","permalink":"https://wangqian0306.github.io/2021/splay_tree/","excerpt":"","text":"伸展树(Splay Tree) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266import sysclass Node: def __init__(self, data): self.data = data self.parent = None self.left = None self.right = Noneclass SplayTree: def __init__(self): self.root = None def __print_helper(self, current_print, indent, last): # print the tree structure on the screen if current_print is not None: sys.stdout.write(indent) if last: sys.stdout.write(&quot;R----&quot;) indent += &quot; &quot; else: sys.stdout.write(&quot;L----&quot;) indent += &quot;| &quot; print(current_print.data) self.__print_helper(current_print.left, indent, False) self.__print_helper(current_print.right, indent, True) def __search_tree_helper(self, node, key): if node is None or key == node.data: return node if key &lt; node.data: return self.__search_tree_helper(node.left, key) return self.__search_tree_helper(node.right, key) def __delete_node_helper(self, node, key): x = None t = None s = None while node is not None: if node.data == key: x = node if node.data &lt;= key: node = node.right else: node = node.left if x is None: print(&quot;Couldn&#x27;t find key in the tree&quot;) return # split operation self.__splay(x) if x.right is not None: t = x.right t.parent = None else: t = None s = x s.right = None x = None # join operation if s.left is not None: s.left.parent = None self.root = self.__join(s.left, t) s = None # rotate left at node x def __left_rotate(self, x): y = x.right x.right = y.left if y.left is not None: y.left.parent = x y.parent = x.parent if x.parent is None: self.root = y elif x == x.parent.left: x.parent.left = y else: x.parent.right = y y.left = x x.parent = y # rotate right at node x def __right_rotate(self, x): y = x.left x.left = y.right if y.right is not None: y.right.parent = x y.parent = x.parent if x.parent is None: self.root = y elif x == x.parent.right: x.parent.right = y else: x.parent.left = y y.right = x x.parent = y # Splaying operation. It moves x to the root of the tree def __splay(self, x): while x.parent is not None: if x.parent.parent is None: if x == x.parent.left: # zig rotation self.__right_rotate(x.parent) else: # zag rotation self.__left_rotate(x.parent) elif x == x.parent.left and x.parent == x.parent.parent.left: # zig-zig rotation self.__right_rotate(x.parent.parent) self.__right_rotate(x.parent) elif x == x.parent.right and x.parent == x.parent.parent.right: # zag-zag rotation self.__left_rotate(x.parent.parent) self.__left_rotate(x.parent) elif x == x.parent.right and x.parent == x.parent.parent.left: # zig-zag rotation self.__left_rotate(x.parent) self.__right_rotate(x.parent) else: # zag-zig rotation self.__right_rotate(x.parent) self.__left_rotate(x.parent) # joins two trees s and t def __join(self, s, t): if s is None: return t if t is None: return s x = self.maximum(s) self.__splay(x) x.right = t t.parent = x return x def __pre_order_helper(self, node): if node is not None: sys.stdout.write(node.data + &quot; &quot;) self.__pre_order_helper(node.left) self.__pre_order_helper(node.right) def __in_order_helper(self, node): if node is not None: self.__in_order_helper(node.left) sys.stdout.write(node.data + &quot; &quot;) self.__in_order_helper(node.right) def __post_order_helper(self, node): if node is not None: self.__post_order_helper(node.left) self.__post_order_helper(node.right) sys.stdout.write(node.data + &quot; &quot;) # Pre-Order traversal # Node-&gt;Left Subtree-&gt;Right Subtree def preorder(self): self.__pre_order_helper(self.root) # In-Order traversal # Left Subtree -&gt; Node -&gt; Right Subtree def inorder(self): self.__in_order_helper(self.root) # Post-Order traversal # Left Subtree -&gt; Right Subtree -&gt; Node def postorder(self): self.__post_order_helper(self.root) # search the tree for the key k # and return the corresponding node def search_tree(self, k): x = self.__search_tree_helper(self.root, k) if x is not None: self.__splay(x) # find the node with the minimum key @staticmethod def minimum(node): while node.left is not None: node = node.left return node # find the node with the maximum key @staticmethod def maximum(node): while node.right is not None: node = node.right return node # find the successor of a given node def successor(self, x): # if the right subtree is not null, # the successor is the leftmost node in the # right subtree if x.right is not None: return self.minimum(x.right) # else it is the lowest ancestor of x whose # left child is also an ancestor of x. y = x.parent while y is not None and x == y.right: x = y y = y.parent return y # find the predecessor of a given node def predecessor(self, x): # if the left subtree is not null, # the predecessor is the rightmost node in the # left subtree if x.left is not None: return self.maximum(x.left) y = x.parent while y is not None and x == y.left: x = y y = y.parent return y # insert the key to the tree in its appropriate position def insert(self, key): node = Node(key) y = None x = self.root while x is not None: y = x if node.data &lt; x.data: x = x.left else: x = x.right # y is parent of x node.parent = y if y is None: self.root = node elif node.data &lt; y.data: y.left = node else: y.right = node # splay the node self.__splay(node) # delete the node from the tree def delete_node(self, data): self.__delete_node_helper(self.root, data) # print the tree structure on the screen def pretty_print(self): self.__print_helper(self.root, &quot;&quot;, True) 参考资料 Splay Trees","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"平衡二叉树","slug":"data_structure/avl","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/balanced_binary_tree/","permalink":"https://wangqian0306.github.io/2021/balanced_binary_tree/","excerpt":"","text":"平衡二叉树(Balanced Binary Tree,AVL Tree) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122class TreeNode(object): def __init__(self, val): self.val = val self.left = None self.right = None self.height = 1class AVLTree(object): def insert(self, root, key): if not root: return TreeNode(key) elif key &lt; root.val: root.left = self.insert(root.left, key) else: root.right = self.insert(root.right, key) root.height = 1 + max(self.get_height(root.left), self.get_height(root.right)) balance = self.get_balance(root) if balance &gt; 1 and key &lt; root.left.val: return self.right_rotate(root) if balance &lt; -1 and key &gt; root.right.val: return self.left_rotate(root) if balance &gt; 1 and key &gt; root.left.val: root.left = self.left_rotate(root.left) return self.right_rotate(root) if balance &lt; -1 and key &lt; root.right.val: root.right = self.right_rotate(root.right) return self.left_rotate(root) return root def delete(self, root, key): if not root: return root elif key &lt; root.val: root.left = self.delete(root.left, key) elif key &gt; root.val: root.right = self.delete(root.right, key) else: if root.left is None: temp = root.right root = None return temp elif root.right is None: temp = root.left root = None return temp temp = self.get_min_value_node(root.right) root.val = temp.val root.right = self.delete(root.right, temp.val) if root is None: return root root.height = 1 + max(self.get_height(root.left), self.get_height(root.right)) balance = self.get_balance(root) if balance &gt; 1 and self.get_balance(root.left) &gt;= 0: return self.get_height(root) if balance &lt; -1 and self.get_balance(root.right) &lt;= 0: return self.left_rotate(root) if balance &gt; 1 and self.get_balance(root.left) &lt; 0: root.left = self.left_rotate(root.left) return self.get_height(root) if balance &lt; -1 and self.get_balance(root.right) &gt; 0: root.right = self.get_height(root.right) return self.left_rotate(root) return root def left_rotate(self, z): y = z.right t2 = y.left y.left = z z.right = t2 z.height = 1 + max(self.get_height(z.left), self.get_height(z.right)) y.height = 1 + max(self.get_height(y.left), self.get_height(y.right)) return y def right_rotate(self, z): y = z.left t3 = y.right y.right = z z.left = t3 z.height = 1 + max(self.get_height(z.left), self.get_height(z.right)) y.height = 1 + max(self.get_height(y.left), self.get_height(y.right)) return y def get_height(self, root): if not root: return 0 return root.height def get_balance(self, root): if not root: return 0 return self.get_height(root.left) - self.get_height(root.right) def get_min_value_node(self, root): if root is None or root.left is None: return root return self.get_min_value_node(root.left) def preOrder(self, root): if not root: return print(&quot;&#123;0&#125; &quot;.format(root.val)) self.preOrder(root.left) self.preOrder(root.right)def search(root, key): if root is None: return None if key &gt; root.val: return search(root.right, key) elif key &lt; root.val: return search(root.left, key) else: return root","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"栈和队列","slug":"data_structure/basic","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-14T08:19:10.020Z","comments":true,"path":"2021/basic/","permalink":"https://wangqian0306.github.io/2021/basic/","excerpt":"","text":"栈和队列 栈 123456789101112131415class Stack: def __init__(self): self.stack = [] def pop(self): if len(self.stack) &lt; 1: return None return self.stack.pop() def push(self, item): self.stack.append(item) def size(self): return len(self.stack) 队列 123456789101112131415class Queue: def __init__(self): self.queue = [] def enqueue(self, item): self.queue.append(item) def dequeue(self): if len(self.queue) &lt; 1: return None return self.queue.pop(0) def size(self): return len(self.queue) 单调栈 在遇到求取比此数据大或小的下一个数据场景下可以使用单调栈的思路解决问题： 123456789101112131415161718192021222324252627import java.util.Arrays;import java.util.Deque;import java.util.LinkedList;public class Solution &#123; public int[] nextGreaterElements(int[] nums) &#123; int n = nums.length; int[] ret = new int[n]; Arrays.fill(ret, -1); Deque&lt;Integer&gt; stack = new LinkedList&lt;&gt;(); for (int i = 0; i &lt; n * 2 - 1; i++) &#123; while (!stack.isEmpty() &amp;&amp; nums[stack.peek()] &lt; nums[i % n]) &#123; ret[stack.pop()] = nums[i % n]; &#125; stack.push(i % n); &#125; return ret; &#125; public static void main(String[] args) &#123; Solution solution = new Solution(); int[] nums = &#123;1, 5, 3, 4&#125;; int[] ret = solution.nextGreaterElements(nums); System.out.println(Arrays.toString(ret)); &#125;&#125; 基本的处理流程如下： 判断栈是否为空，且栈顶部的索引和要比对的索引谁大谁小 如果符合规则，循环更新结果数据 入栈 此处 LeetCode 官方采用了 Deque 而不是 Stack 的原因是： Java官方文档建议用 Deque 代替旧的 Stack 类，因为 Deque 接口提供了更多的灵活性和更强的功能集。 旧的 Stack 类继承自 Vector，这意味着它是线程安全的，但是这种安全性是通过同步方法实现的，这会导致不必要的性能损失。 参考资料 Stacks and Queues in Python","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"二叉搜索树","slug":"data_structure/binary_search_tree","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/binary_search_tree/","permalink":"https://wangqian0306.github.io/2021/binary_search_tree/","excerpt":"","text":"二叉搜索树(Binary Search Tree) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Node: def __init__(self, key): self.left = None self.right = None self.val = keydef insert(root, key): if root is None: return Node(key) else: if root.val == key: return root elif root.val &lt; key: root.right = insert(root.right, key) else: root.left = insert(root.left, key) return rootdef inorder(root): if root: inorder(root.left) print(root.val) inorder(root.right)def min_value_node(node): current = node while current.left is not None: current = current.left return currentdef delete_node(root, key): if root is None: return root if key &lt; root.key: root.left = delete_node(root.left, key) elif key &gt; root.key: root.right = delete_node(root.right, key) else: if root.left is None: temp = root.right root = None return temp elif root.right is None: temp = root.left root = None return temp temp = min_value_node(root.right) root.key = temp.key root.right = delete_node(root.right, temp.key) return rootdef search(root, key): if root is None: return None if key &gt; root.val: return search(root.right, key) elif key &lt; root.val: return search(root.left, key) else: return root","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"链表","slug":"data_structure/list","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/list/","permalink":"https://wangqian0306.github.io/2021/list/","excerpt":"","text":"链表 单向链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107class LinkListNode(object): &quot;&quot;&quot; 链表结点 &quot;&quot;&quot; def __init__(self, data=None, next_node=None): self.data = data self.next = next_node def __str__(self): return str(self.data)class LinkList(object): &quot;&quot;&quot; 单链表类 &quot;&quot;&quot; def __init__(self): self.head = LinkListNode() # 表头的空结点指针 Dummy node self.tail = self.head def __str__(self): head = self.head.next output_str = &quot;LinkList[&quot; while head: output_str += str(head.data) + str(&quot;, &quot;) head = head.next output_str += &quot;]&quot; return output_str def push_back(self, data): self.tail.next = LinkListNode(data) self.tail = self.tail.next def back(self): if self.tail: return self.tail.data else: return None def pop_back(self): if self.tail != self.head: node_to_remove = self.head.next prev_node = self.head while node_to_remove and node_to_remove.next: prev_node = node_to_remove node_to_remove = node_to_remove.next prev_node.next = None self.tail = prev_node del node_to_remove def front(self): if self.head.next: return self.head.next.data else: return None def push_front(self, data): self.head.next = LinkListNode(data, self.head.next) if self.tail == self.head: self.tail = self.head.next def pop_front(self): if self.head != self.tail: first_node = self.head.next self.head.next = first_node.next if not self.head.next: self.tail = self.head del first_node def remove_at(self, index): if index &lt; 0 or self.tail == self.head: return False node_to_remove = self.head.next prev_node = self.head for i in range(index): prev_node = node_to_remove node_to_remove = node_to_remove.next if not node_to_remove: return False prev_node.next = node_to_remove.next if not prev_node.next: self.tail = prev_node del node_to_remove return True def is_empty(self): return self.tail == self.head def size(self): count = 0 cur_node = self.head.next while cur_node: count += 1 cur_node = cur_node.next return count @staticmethod def print_list(list_head): &quot;&quot;&quot; 打印链表 :param list_head: 链表头结点 :return:None &quot;&quot;&quot; output_str = str(list_head) print(output_str) 双向链表 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class Node(object): &quot;&quot;&quot;双向链表节点&quot;&quot;&quot; def __init__(self, item): self.item = item self.next = None self.prev = Noneclass DLinkList(object): &quot;&quot;&quot;双向链表&quot;&quot;&quot; def __init__(self): self._head = None def is_empty(self): &quot;&quot;&quot;判断链表是否为空&quot;&quot;&quot; return self._head is None def length(self): &quot;&quot;&quot;返回链表的长度&quot;&quot;&quot; cur = self._head count = 0 while cur is not None: count += 1 cur = cur.next return count def travel(self): &quot;&quot;&quot;遍历链表&quot;&quot;&quot; cur = self._head while cur is not None: print(cur.item) cur = cur.next def add(self, item): &quot;&quot;&quot;头部插入元素&quot;&quot;&quot; node = Node(item) if self.is_empty(): # 如果是空链表，将_head指向node self._head = node else: # 将node的next指向_head的头节点 node.next = self._head # 将_head的头节点的prev指向node self._head.prev = node # 将_head 指向node self._head = node def append(self, item): &quot;&quot;&quot;尾部插入元素&quot;&quot;&quot; node = Node(item) if self.is_empty(): # 如果是空链表，将_head指向node self._head = node else: # 移动到链表尾部 cur = self._head while cur.next is not None: cur = cur.next # 将尾节点cur的next指向node cur.next = node # 将node的prev指向cur node.prev = cur def search(self, item): &quot;&quot;&quot;查找元素是否存在&quot;&quot;&quot; cur = self._head while cur is not None: if cur.item == item: return True cur = cur.next return False 参考资料 常见数据结构Part1(数组和链表) 链表","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"查找算法","slug":"data_structure/search","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/search/","permalink":"https://wangqian0306.github.io/2021/search/","excerpt":"","text":"查找算法 顺序查找(Liner Search) 12345def liner_search(arr: list, x) -&gt; int: for i in range(arr.__len__()): if arr[i] == x: return i return -1 二分查找(Binary Search) 123456789101112def binary_search(arr: list, l_index: int, r_index: int, x): if r_index &gt;= l_index: mid = int(l_index + (r_index - l_index) / 2) if arr[mid] == x: return mid elif arr[mid] &gt; x: return binary_search(arr, l_index, mid - 1, x) # 元素大于中间位置的元素，只需要再比较右边的元素 else: return binary_search(arr, mid + 1, r_index, x) else: return -1","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"排序算法","slug":"data_structure/sort","date":"2021-11-18T12:32:58.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/sort/","permalink":"https://wangqian0306.github.io/2021/sort/","excerpt":"","text":"排序算法 冒泡排序(Bubble Sort) 12345def bubble_sort(content_list: list[int]): for j in range(len(content_list) - 1, 0, -1): for i in range(j): if content_list[i] &gt; content_list[i + 1]: content_list[i], content_list[i + 1] = content_list[i + 1], content_list[i] 选择排序(Selection sort) 12345678910111213def selection_sort(content_list: list[int]): n = len(content_list) # 需要进行n-1次选择操作 for i in range(n - 1): # 记录最小位置 min_index = i # 从i+1位置到末尾选择出最小数据 for j in range(i + 1, n): if content_list[j] &lt; content_list[min_index]: min_index = j # 如果选择出的数据不在正确位置，进行交换 if min_index != i: content_list[i], content_list[min_index] = content_list[min_index], content_list[i] 插入排序(Insertion Sort) 1234567def insert_sort(content_list: list[int]): # 从第二个位置，即下标为1的元素开始向前插入 for i in range(1, len(content_list)): # 从第i个元素开始向前比较，如果小于前一个元素，交换位置 for j in range(i, 0, -1): if content_list[j] &lt; content_list[j - 1]: content_list[j], content_list[j - 1] = content_list[j - 1], content_list[j] 希尔排序(Shell Sort) 1234567891011121314def shell_sort(content_list: list[int]): n = len(content_list) # 初始步长 gap = n / 2 while gap &gt; 0: # 按步长进行插入排序 for i in range(gap, n): j = i # 插入排序 while j &gt;= gap and content_list[j - gap] &gt; content_list[j]: content_list[j - gap], content_list[j] = content_list[j], content_list[j - gap] j -= gap # 得到新的步长 gap = gap / 2 归并排序(Merge Sort) 1234567891011121314151617181920212223242526def merge_sort(content_list: list[int]): if len(content_list) &lt;= 1: return content_list # 二分分解 num = len(content_list) / 2 left = merge_sort(content_list[:num]) right = merge_sort(content_list[num:]) # 合并 return merge(left, right)def merge(left: list[int], right: list[int]) -&gt; list[int]: &quot;&quot;&quot;合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组&quot;&quot;&quot; # left与right的下标指针 l, r = 0, 0 result = [] while l &lt; len(left) and r &lt; len(right): if left[l] &lt; right[r]: result.append(left[l]) l += 1 else: result.append(right[r]) r += 1 result += left[l:] result += right[r:] return result 快速排序(Quick Sort) 1234567891011121314151617181920212223242526272829303132333435363738def quick_sort(content_list: list[int], start_index: int, end_index: int) -&gt; None: &quot;&quot;&quot;快速排序&quot;&quot;&quot; # 递归的退出条件 if start_index &gt;= end_index: return # 设定起始元素为要寻找位置的基准元素 mid = content_list[start_index] # low为序列左边的由左向右移动的游标 low = start_index # high为序列右边的由右向左移动的游标 high = end_index while low &lt; high: # 如果low与high未重合，high指向的元素不比基准元素小，则high向左移动 while low &lt; high and content_list[high] &gt;= mid: high -= 1 # 将high指向的元素放到low的位置上 content_list[low] = content_list[high] # 如果low与high未重合，low指向的元素比基准元素小，则low向右移动 while low &lt; high and content_list[low] &lt; mid: low += 1 # 将low指向的元素放到high的位置上 content_list[high] = content_list[low] # 退出循环后，low与high重合，此时所指位置为基准元素的正确位置 # 将基准元素放到该位置 content_list[low] = mid # 对基准元素左边的子序列进行快速排序 quick_sort(content_list, start_index, low - 1) # 对基准元素右边的子序列进行快速排序 quick_sort(content_list, low + 1, end_index) 桶排序(Bucket Sort) 12345678910111213141516def bucket_sort(content_list: list[int]) -&gt; None: &quot;&quot;&quot;桶排序&quot;&quot;&quot; min_num = min(content_list) max_num = max(content_list) # 桶的大小 bucket_range = (max_num - min_num) / len(content_list) # 桶数组 count_list = [[] for _ in range(len(content_list) + 1)] # 向桶数组填数 for i in content_list: count_list[int((i - min_num) // bucket_range)].append(i) content_list.clear() # 回填，这里桶内部排序直接调用了sorted for i in count_list: for j in sorted(i): content_list.append(j) 计数排序(Counting Sort) 1234567891011121314151617def counting_sort(content_list: list[int]) -&gt; list[int]: # 桶的个数，这一是取决于，这一个列表中最大的那个数字 bucket = [0] * (max(content_list) + 1) for num in content_list: # 用num下标的位置，记录其出现的次数 bucket[num] += 1 i = 0 # nums 的索引 # 循环遍历整个桶 for j in range(len(bucket)): # 找到第j个元素，就是出现j的次数，如果不是0次没代表出现 while bucket[j] &gt; 0: # 出现了，就把这一个写入到原本的nums列表中， # 同时出现的次数减一，nums列表加一 content_list[i] = j bucket[j] -= 1 i += 1 return content_list 基数排序(Radix Sort) 123456789101112131415161718192021222324252627282930313233343536373839404142434445def counting_sort_for_radix(input_array, place_value): count_array = [0] * 10 input_size = len(input_array) for i in range(input_size): place_element = (input_array[i] // place_value) % 10 count_array[place_element] += 1 for i in range(1, 10): count_array[i] += count_array[i - 1] output_array = [0] * input_size i = input_size - 1 while i &gt;= 0: current_el = input_array[i] place_element = (input_array[i] // place_value) % 10 count_array[place_element] -= 1 new_position = count_array[place_element] output_array[new_position] = current_el i -= 1 return output_arraydef radix_sort(input_array): # Step 1 -&gt; Find the maximum element in the input array max_el = max(input_array) # Step 2 -&gt; Find the number of digits in the `max` element digit = 1 while max_el &gt; 0: max_el /= 10 digit += 1 # Step 3 -&gt; Initialize the place value to the least significant place place_val = 1 # Step 4 output_array = input_array while digit &gt; 0: output_array = counting_sort_for_radix(output_array, place_val) place_val *= 10 digit -= 1 return output_array 堆排序(HeapSort) 1234567891011121314151617181920212223242526272829303132333435def heapify(arr, n, i): largest = i # Initialize largest as root l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 # See if left child of root exists and is # greater than root if l &lt; n and arr[largest] &lt; arr[l]: largest = l # See if right child of root exists and is # greater than root if r &lt; n and arr[largest] &lt; arr[r]: largest = r # Change root, if needed if largest != i: arr[i], arr[largest] = arr[largest], arr[i] # swap # Heapify the root. heapify(arr, n, largest) # The main function to sort an array of given sizedef heap_sort(arr): n = len(arr) # Build a maxheap. for i in range(n//2 - 1, -1, -1): heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # swap heapify(arr, i, 0) 参考资料 RadixSort 排序与搜索 排序可视化","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"}]},{"title":"N 皇后问题","slug":"algorithm/nqueens","date":"2021-11-15T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/nqueens/","permalink":"https://wangqian0306.github.io/2021/nqueens/","excerpt":"","text":"N 皇后问题 简介 N 皇后问题 研究的是如何将 N 个皇后放置在 N*N 大小的棋盘上，并且使皇后彼此之间不能相互攻击。 皇后的走法是：可以横直斜走，格数不限。因此要求皇后彼此之间不能相互攻击，等价于要求任何两个皇后都不能在同一行、同一列以及同一条斜线上。 例如： 12345 0 1 2 30 | | |Q| |1 |Q| | | |2 | | | |Q|3 | |Q| | | 解答方法 Leetcode 上有两种回溯法分别如下 基于集合的回溯 为了防止混淆，下面的内容使用按列填入皇后的方式进行说明。 问题分析： 总列数等于皇后棋子的数量且皇后不能处于同一列，所以每一列都需要有一个皇后。 两个皇后不能处于同一斜线上，所以 行下标与列下标之差不能相等(右下方向)。 行下标与列下标之和不能相等(左下方向)。 确保上述过程重复执行直至遍历整个棋盘即可。 简化注释版代码： 12345678910111213141516171819202122232425262728293031323334353637383940# 确定输入参数queens = 4# 确保皇后不能处于同一行column = set()# 确保皇后不能处于同一斜线(右下方向)d1 = set()# 确保皇后不能处于同一斜线(左下方向)d2 = set()# 皇后落子的每一列的坐标result = [0, 0, 0, 0]# 递归遍历方法def backtrack(row: int): # 若所有棋子已经放下则输出结果 if row == queens: print(result) # 遍历当前列 for i in range(queens): # 若当前位置已经无法落子则跳往下一位置 if (i in column) or (row - i in d1) or (row + i in d2): continue # 将皇后放置在此位置 result[row] = i # 记录此位置的冲突数据 column.add(i) d1.add(row - i) d2.add(row + i) # 前往下一行进行遍历 backtrack(row + 1) # 移除此位置的冲突数据 column.remove(i) d1.remove(row - i) d2.remove(row + i)# 开始遍历第 0 行backtrack(0) 官方解法： 1234567891011121314151617181920212223242526272829303132333435class Solution: def solveNQueens(self, n: int) -&gt; List[List[str]]: def generateBoard(): board = list() for i in range(n): row[queens[i]] = &quot;Q&quot; board.append(&quot;&quot;.join(row)) row[queens[i]] = &quot;.&quot; return board def backtrack(row: int): if row == n: board = generateBoard() solutions.append(board) else: for i in range(n): if i in columns or row - i in diagonal1 or row + i in diagonal2: continue queens[row] = i columns.add(i) diagonal1.add(row - i) diagonal2.add(row + i) backtrack(row + 1) columns.remove(i) diagonal1.remove(row - i) diagonal2.remove(row + i) solutions = list() queens = [-1] * n columns = set() diagonal1 = set() diagonal2 = set() row = [&quot;.&quot;] * n backtrack(0) return solutions 基于位运算的回溯 此方法采用按行填写的方案完成。 问题分析： 总行数等于皇后棋子的数量且皇后不能处于同一行，所以每一行都需要有一个皇后。 两个皇后不能处于同一斜线上，所以左右位移落子之后的点来确定跳过的目标位置。 将不能放置棋子的位置标记设置为 1 将可以放置棋子的位置标记为 0，就可以利用二进制记录放置信息 左移取得左下不能放置的点。 右移取得右下不能放置的点。 记录当前放置的点。 使用二进制的或操作就可以确定不能放置棋子的位置。 在获取到可以放置的位置之后可以通过如下手段获取单个棋子的位置。 x&amp;(-x) 可以获得 x 的二进制表示中的最低位的 1 的位置。 x&amp;(x-1) 可以将二进制中最低位的 1 置成 0。 确保上述过程重复执行直至遍历整个棋盘即可。 简化注释版代码： 12345678910111213141516171819202122232425262728293031# 确定输入参数n = 4# 定义结果缓存queens = [-1] * n# 递归遍历方法def solve(row: int, columns: int, diagonals1: int, diagonals2: int): # 若所有棋子已经放下则输出结果 if row == n: print(queens) else: # 获取可以放置棋子的位置 availablePositions = ((1 &lt;&lt; n) - 1) &amp; (~(columns | diagonals1 | diagonals2)) # 遍历可以放置棋子的位置 while availablePositions: # 获取最低位的位置 position = availablePositions &amp; (-availablePositions) # 将最低位放置为 0 availablePositions = availablePositions &amp; (availablePositions - 1) # 确定列的位置 column = bin(position - 1).count(&quot;1&quot;) # 记录缓存结果 queens[row] = column # 完成递归调用 solve(row + 1, columns | position, (diagonals1 | position) &lt;&lt; 1, (diagonals2 | position) &gt;&gt; 1)# 递归遍历方法solve(0, 0, 0, 0) 注：如果需要打印中间结果可以采用此语句 bin(availablePositions)[2:].zfill(n) 官方解法： 12345678910111213141516171819202122232425262728class Solution: def solveNQueens(self, n: int) -&gt; List[List[str]]: def generateBoard(): board = list() for i in range(n): row[queens[i]] = &quot;Q&quot; board.append(&quot;&quot;.join(row)) row[queens[i]] = &quot;.&quot; return board def solve(row: int, columns: int, diagonals1: int, diagonals2: int): if row == n: board = generateBoard() solutions.append(board) else: availablePositions = ((1 &lt;&lt; n) - 1) &amp; (~(columns | diagonals1 | diagonals2)) while availablePositions: position = availablePositions &amp; (-availablePositions) availablePositions = availablePositions &amp; (availablePositions - 1) column = bin(position - 1).count(&quot;1&quot;) queens[row] = column solve(row + 1, columns | position, (diagonals1 | position) &lt;&lt; 1, (diagonals2 | position) &gt;&gt; 1) solutions = list() queens = [-1] * n row = [&quot;.&quot;] * n solve(0, 0, 0, 0) return solutions 参考资料 Leetcode 官方题解 交互过程试验","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"ncu","slug":"frount/ncu","date":"2021-11-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/ncu/","permalink":"https://wangqian0306.github.io/2021/ncu/","excerpt":"","text":"ncu 简介 ncu(npm-check-updates) 插件，用于更新 NPM 配置文件中的依赖软件包的版本。 安装方式 1npm i -g npm-check-updates 使用方式 更新当前目录对应的软件依赖包配置文件 1ncu -u 常见问题 1ncu : 无法加载文件 ...ncu.ps1，因为在此系统上禁止运行脚本。 有关详细信息，请参阅 https:/go.microsoft.com/fwlink/?LinkID=135170 中的 about_Execution_Policies 部分。","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"},{"name":"ncu","slug":"ncu","permalink":"https://wangqian0306.github.io/tags/ncu/"}]},{"title":"Node.js 环境准备","slug":"frount/nodejs","date":"2021-11-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/nodejs/","permalink":"https://wangqian0306.github.io/2021/nodejs/","excerpt":"","text":"Node.js 环境准备 简介 简单的说 Node.js 就是运行在服务端的 JavaScript。Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。 安装 Windows 平台 访问 官网 下载安装包即可。 Linux 平台 CentOS 7 访问 官网 下载二进制文件，然后使用如下命令进行安装 123sudo mkdir -p /usr/local/lib/nodejssudo tar -xJvf node-&lt;$VERSION-$DISTRO&gt;.tar.xz -C /usr/local/lib/nodejs sudo vim /etc/profile.d/nodejs.sh 填入如下内容： 1export PATH=/usr/local/lib/nodejs/node-&lt;$VERSION-$DISTRO&gt;/bin:$PATH 刷新配置 1source /etc/profile.d/nodejs.sh CentOS 8/ Fedora 查看软件源及版本： 1dnf module list nodejs 安装： 1dnf install nodejs 配置国内软件源 配置源 1npm config set registry http://registry.npm.taobao.org/ 获取当前源的配置情况 1npm config get registry 安装 Yarn 包管理工具 1npm install -g yarn 常见问题 ENOENT: no such file or directory, lstat ‘C:\\Users\\xxx\\AppData\\Roaming\\npm’ 重新安装 npm 即可： 1npm install -g npm@latest","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"}]},{"title":"Turborepo","slug":"frount/turbo","date":"2021-11-11T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/turborepo/","permalink":"https://wangqian0306.github.io/2021/turborepo/","excerpt":"","text":"Turborepo 简介 Turborepo 是适用于 JavaScript 和 TypeScript 代码库的高性能构建系统。它可以组合不同服务进行构建。 使用方式 可以创建一个项目来试用： 1npx create-turbo@latest --example xxx 参考资料 官方文档 turboRepo介绍","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"}]},{"title":"keytool","slug":"java/keytool","date":"2021-11-10T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/keytool/","permalink":"https://wangqian0306.github.io/2021/keytool/","excerpt":"","text":"keytool 简介 keytool 是 JDK 自带的管理加密密钥、X.509 证书链和可信证书密钥库(数据库)的命令行工具。 命令相关参数 创建或者新增数据进入 Keystore getcert genkeypair genseckey importcert importpassword 从另一个 Keystore 引入内容 importkeystore 生成证书请求 certreq 导出数据 exportcert 列出数据 list printcert printcertreq printcrl 管理 Keystore storepasswd keypasswd delete changealias 使用方式 SSL/TLS 生成服务器 keystore 文件 1keytool -genkey -alias tomcat -keypass 123456 -keyalg RSA -keysize 1024 -validity 365 -keystore ./tomcat.keystore -storepass 123456 生成客户端 p12 文件 1keytool -genkey -alias client -keyalg RSA -storetype PKCS12 -keypass 123456 -storepass 123456 -keystore ./client.p12 生成客户端 cer 证书 1keytool -export -alias client -keystore ./client.p12 -storetype PKCS12 -keypass 123456 -file ./client.cer 将客户端 cer 证书导入到 keystore 文件中 1keytool -import -v -file ./client.cer -keystore ./tomcat.keystore 导出服务器 cer 证书(可选) 1keytool -keystore ./tomcat.keystore -export -alias tomcat -file ./server.cer 证书生成完毕后可以在 SpringBoot 中进行如下配置，配置后即可使用 HTTPS 接口： 1234server.ssl.key-store=classpath:tomcat.keystoreserver.ssl.key-store-password=123456server.ssl.keyStoreType=JKSserver.ssl.keyAlias:tomcat","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"keytool","slug":"keytool","permalink":"https://wangqian0306.github.io/tags/keytool/"}]},{"title":"域控","slug":"tools/freeipa","date":"2021-11-09T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/freeipa/","permalink":"https://wangqian0306.github.io/2021/freeipa/","excerpt":"","text":"域控 简介 域控制器是指在“域”模式下，至少有一台服务器负责每一台联入网络的电脑和用户的验证工作。 相关产品 FreeIPA NIS Windows AD 相关技术 LDAP Kerberos DNS NTP 适用情况 需要统一管理 Windows 设备则使用 Windows AD。 需要开源免费且适用性交广则采用 FreeIPA 公网试用 目前 FreeIPA 提供了一个公网的试用版本，可以进行一些基本的测试。 访问 ipa.demo1.freeipa.org 使用如下账号都可以进行登录： admin: 超级管理员 helpdesk: 普通用户，具有运维权限，可以编辑用户或者编辑组 employee: 普通用户 manger: 普通用户，被设定成 employee 的管理员 密码都是统一的 Secret123。 注：在 Chrome 浏览器弹出的登录框需要手动关闭，然后在页面的登录表单中进行登录。","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"FreeIPA","slug":"FreeIPA","permalink":"https://wangqian0306.github.io/tags/FreeIPA/"},{"name":"NIS","slug":"NIS","permalink":"https://wangqian0306.github.io/tags/NIS/"},{"name":"Windows AD","slug":"Windows-AD","permalink":"https://wangqian0306.github.io/tags/Windows-AD/"}]},{"title":"HTTP/2 请求相关的知识整理","slug":"protocol/http2","date":"2021-11-03T15:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/http/","permalink":"https://wangqian0306.github.io/2021/http/","excerpt":"","text":"HTTP/2 请求相关的知识整理 简介 HTTP 2.0 是在 SPDY 基础上形成的下一代互联网通信协议。HTTP/2 的目的是通过支持请求与响应的多路复用来较少延迟，通过压缩 HTTPS 首部字段将协议开销降低，同时增加了请求优先级和服务器端推送的支持。 主要变化 使用 HTTP 帧的方式传输数据 报文格式替换为： 123456789+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+ Length: 载荷的长度，无符号24位整型。对于发送长度大于2^14 (16384字节)的载荷, 只有在接收方设置 SETTINGS_MAX_FRAME_SIZE 为更大的值时才被允许传输。 Type: 8位的值表示帧类型，决定了帧的格式和语义。协议实现上必须忽略任何未知类型的帧。 Flags: 为 Type 保留的布尔标识，大小是8位。对确定的帧类型赋予特定的语义，否则发送时必须忽略(设置为0x0)。 R: 1位的保留字段，尚未定义语义。 发送和接收必须忽略(0x0)。 Stream Identifier: 31位无符号整型的流标示符。 其中0x0作为保留值，表示与连接相关的frames作为一个整体而不是一个单独的流。 使用流和复用机制增强性能 在一个 HTTP/2 的连接中, 流是服务器与客户端之间用于帧交换的一个独立双向序列. 流有几个重要的特点: 一个 HTTP/2 连接可以包含多个并发的流，各个端点从多个流中交换帧。 流可以被客户端或服务器单方面建立，使用或共享。 流也可以被任意一方关闭。 帧在一个流上的发送顺序很重要。接收方将按照他们的接收顺序处理这些帧。特别是 HEADERS 和DATA 帧的顺序，在协议的语义上显得尤为重要。 流用一个整数(流标识符)标记。端点初始化流的时候就为其分配了标识符。 下面是流的生命周期： 123456789101112131415161718192021222324252627282930313233343536 +--------+ send PP | | recv PP ,--------| idle |--------. / | | \\ v +--------+ v +----------+ | +----------+ | | | send H / | |,------| reserved | | recv H | reserved |------.| | (local) | | | (remote) | || +----------+ v +----------+ || | +--------+ | || | recv ES | | send ES | || send H | ,-------| open |-------. | recv H || | / | | \\ | || v v +--------+ v v || +----------+ | +----------+ || | half | | | half | || | closed | | send R / | closed | || | (remote) | | recv R | (local) | || +----------+ | +----------+ || | | | || | send ES / | recv ES / | || | send R / v send R / | || | recv R +--------+ recv R | || send R / `-----------&gt;| |&lt;-----------&#x27; send R / || recv R | closed | recv R |`-----------------------&gt;| |&lt;----------------------&#x27; +--------+ send: 发送这个frame的终端 recv: 接受这个frame的终端 H: HEADERS帧 (隐含CONTINUATION帧) PP: PUSH_PROMISE帧 (隐含CONTINUATION帧) ES: END_STREAM标记 R: RST_STREAM帧 引入流的依赖关系和优先级机制 客户端可以给一个新的流分配一个优先级，做法是在用来打开流的 HEADERS 帧中包含优先次序信息。在其他任何时间，可以用 PRIORITY 帧改变一个流的优先级。 每个流可以显式指定依赖其他的流。如果流被其他流所依赖，这就表明这个流在资源的分配上优先于它的从属流。 新增服务器推送机制(抢先推送与相应内容相关的请求) HTTP/2 允许服务器先行发送（或推送）与先前客户端初始请求相关的响应（对应&quot;许诺&quot;的请求）到客户端。这在服务端知道客户端将会须要这些响应来完全地处理原始请求的响应时会很有用。 许诺的请求必须是可缓存的(参见 [RFC7231], 4.2.3 节），必须是安全的（参见 [RFC7231], 4.2.1 节），必须不能包含请求体。 客户端收到不是可缓存的，不能被识别为安全的或者是表明存在请求体的许诺请求必须重置许诺流并设置 PROTOCOL_ERROR 类型的流错误。 注意这可能导致在客户端不能识别一个新的方法是不是安全的时候重置流。 参考资料 协议原文 协议中文译版","categories":[{"name":"Web","slug":"Web","permalink":"https://wangqian0306.github.io/categories/Web/"}],"tags":[{"name":"HTTP/2","slug":"HTTP-2","permalink":"https://wangqian0306.github.io/tags/HTTP-2/"}]},{"title":"CDC 工具对比","slug":"tools/cdc","date":"2021-11-01T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/cdc/","permalink":"https://wangqian0306.github.io/2021/cdc/","excerpt":"","text":"CDC 工具对比 简介 Change Data Capture (缩写为 CDC)—— 大概可以机翻为 “变动数据捕获”—— 你可以将它视为和数据库有关的架构设计模式的一种。它的核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入，更新，删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。 目前市面上常见的处理方式大致分为如下三种： 基于日志的解析模式； 基于增量条件查询模式； 数据源主动 Push 模式。 开源项目对比 项目名 支持数据库 工作方式 Sqoop JDBC 链接均可 增量条件查询 Kafka Connector JDBC JDBC 链接均可 增量条件查询 Maxwell MySQL 日志解析 Canal MySQL 日志解析 DataBus Oracle,MySQL 日志解析 Debezium MySQL, MongoDB, Postgre SQL, Oracle, SQL Server, Cassandra, Vitess 日志解析 参考资料 Sqoop Kafka Connector JDBC Maxwell Canal DataBus Debezium","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"CDC","slug":"CDC","permalink":"https://wangqian0306.github.io/tags/CDC/"}]},{"title":"Spring 注入 Bean","slug":"spring/import","date":"2021-10-28T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2021/import/","permalink":"https://wangqian0306.github.io/2021/import/","excerpt":"","text":"Spring 注入 Bean 简介 我们谈到 Spring 的时候一定会提到 IOC 容器、DI 依赖注入，Spring通过将一个个类标注为 Bean 的方法注入到 IOC 容器中，达到了控制反转的效果。 注入方式 Spring 有如下种注入方式 属性注入 构造函数注入 工厂方法注入 而在一般的使用过程中可以选择使用 @Autowired 注解或者 @Resource 注解进行注入。 Autowired 与 Resource 的区别 Spring 不但支持自己定义的 @Autowired 注解，还支持几个由 JSR-250 规范定义的注解，它们分别是 @Resource、@PostConstruct 以及 @PreDestroy。 @Resource 的作用相当于 @Autowired，只不过 @Autowired 按 byType 自动注入，而 @Resource 默认按 byName 自动注入罢了。 @Resource 有两个属性是比较重要的，分是 name 和 type，Spring 将 @Resource 注解的 name 属性解析为 bean 的名字，而 type 属性则解析为 bean 的类型。 所以如果使用 name 属性，则使用 byName 的自动注入策略，而使用 type 属性时则使用 byType 自动注入策略。 如果既不指定 name 也不指定 type 属性，这时将通过反射机制使用 byName 自动注入策略。 参考资料 Spring @Autowired @autowired和@resource注解的区别是什么？","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Transactional","slug":"spring/transactional","date":"2021-10-28T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2021/transactional/","permalink":"https://wangqian0306.github.io/2021/transactional/","excerpt":"","text":"Transactional 简介 事务(transaction)是指业务逻辑上对数据库进行的一系列持久化操作，要么全部成功，要么全部失败。 @Transactional 是 SpringBoot 中一种常见的事务实现方式。 配置建议 建议将此注解采用如下写法： 1@Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED) 参考资料 SpringBoot中@Transactional事务控制实现原理及事务无效问题排查","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"Open Feign","slug":"spring/open-feign","date":"2021-10-27T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2021/openfeign/","permalink":"https://wangqian0306.github.io/2021/openfeign/","excerpt":"","text":"Open Feign 使用 配置 pom 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 编写调用类 123456789101112import org.springframework.cloud.openfeign.FeignClient;import org.springframework.stereotype.Component;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(name = &quot;&lt;service_name&gt;&quot;, path = &quot;&lt;url&gt;&quot;)public interface FeignTest &#123; @GetMapping String getVersion();&#125; 在主类启用 OpenFeign 123456789@SpringBootApplication@EnableFeignClientspublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置请求方式 123456789spring: cloud: openfeign: client: config: default: connectTimeout: 5000 readTimeout: 5000 loggerLevel: basic 自定义异常类 在默认情况下 Feign 只会抛出 FeignException 异常，如有需求可以使用 ErrorDecoder 抛出异常。 12345678910111213141516171819public class StashErrorDecoder implements ErrorDecoder &#123; @Override public Exception decode(String methodKey, Response response) &#123; if (response.status() &gt;= 400 &amp;&amp; response.status() &lt;= 499) &#123; return new StashClientException( response.status(), response.reason() ); &#125; if (response.status() &gt;= 500 &amp;&amp; response.status() &lt;= 599) &#123; return new StashServerException( response.status(), response.reason() ); &#125; return errorStatus(methodKey, response); &#125;&#125; 1234567public class MyFeignClientConfiguration &#123; @Bean public ErrorDecoder errorDecoder() &#123; return new StashErrorDecoder(); &#125;&#125; fallback Spring Cloud CircuitBreaker 支持回退的概念：当远程地址无法链接或出现错误时执行的默认代码。要为给定的 @FeignClient 启用 fallback，需要将该属性设置为实现回退的类名。 12345678910111213141516171819202122232425@FeignClient(name = &quot;test&quot;, url = &quot;http://localhost:$&#123;server.port&#125;/&quot;, fallback = Fallback.class)protected interface TestClient &#123; @RequestMapping(method = RequestMethod.GET, value = &quot;/hello&quot;) Hello getHello(); @RequestMapping(method = RequestMethod.GET, value = &quot;/hellonotfound&quot;) String getException();&#125;@Componentstatic class Fallback implements TestClient &#123; @Override public Hello getHello() &#123; throw new NoFallbackAvailableException(&quot;Boom!&quot;, new RuntimeException()); &#125; @Override public String getException() &#123; return &quot;Fixed response&quot;; &#125;&#125; 并且配置如下参数： 1234567spring: cloud: openfeign: circuitbreaker: enabled: true alphanumeric-ids: enabled: true 参考资料 官方文档","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud/"}]},{"title":"Spring Security","slug":"spring/security","date":"2021-10-27T13:32:58.000Z","updated":"2025-01-13T02:30:03.411Z","comments":true,"path":"2021/spring-security/","permalink":"https://wangqian0306.github.io/2021/spring-security/","excerpt":"","text":"Spring Security 简介 Spring Security 是一款安全框架。 基本使用 引入依赖包： Developer Tools Lombok Web Spring Web Template Engines Thymeleaf Security Spring Security OAuth2 Resource Server SQL Spring Data JPA / MyBatis Framework MySQL Driver / … 书写配置文件： 123456spring.datasource.url=$&#123;MYSQL_URI:jdbc:mysql://xxx.xxx.xxx:xxxx/xxx&#125;spring.datasource.username=$&#123;JDBC_USERNAME:xxx&#125;spring.datasource.password=$&#123;JDBC_PASSWORD:xxx&#125;spring.datasource.driver-class-name=$&#123;JDBC_DRIVER:com.mysql.cj.jdbc.Driver&#125;spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImplspring.jpa.hibernate.ddl-auto=update 新建用户模型 user/User.java ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import com.fasterxml.jackson.annotation.JsonIgnore;import jakarta.persistence.*;import lombok.*;import org.hibernate.proxy.HibernateProxy;import java.util.Objects;@Getter@Setter@ToString@RequiredArgsConstructor@Entity@Table(name=&quot;T_USER&quot;)public class User &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; private String password; private String roles; public User(String username, String password, String roles) &#123; this.username = username; this.password = password; this.roles = roles; &#125; @Override public final boolean equals(Object o) &#123; if (this == o) return true; if (o == null) return false; Class&lt;?&gt; oEffectiveClass = o instanceof HibernateProxy ? ((HibernateProxy) o).getHibernateLazyInitializer().getPersistentClass() : o.getClass(); Class&lt;?&gt; thisEffectiveClass = this instanceof HibernateProxy ? ((HibernateProxy) this).getHibernateLazyInitializer().getPersistentClass() : this.getClass(); if (thisEffectiveClass != oEffectiveClass) return false; User user = (User) o; return getId() != null &amp;&amp; Objects.equals(getId(), user.getId()); &#125; @Override public final int hashCode() &#123; return this instanceof HibernateProxy ? ((HibernateProxy) this).getHibernateLazyInitializer().getPersistentClass().hashCode() : getClass().hashCode(); &#125;&#125; 新建用户存储库 user/UserRepository.java： 123456789101112import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.JpaSpecificationExecutor;import org.springframework.stereotype.Repository;import java.util.Optional;@Repositorypublic interface UserRepository extends JpaRepository&lt;User, Long&gt;, JpaSpecificationExecutor&lt;User&gt; &#123; Optional&lt;User&gt; findByUsername(String username);&#125; 新建用户初始化类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import org.springframework.context.ApplicationListener;import org.springframework.context.event.ContextRefreshedEvent;import org.springframework.lang.Nullable;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.stereotype.Component;import org.springframework.transaction.annotation.Transactional;import java.util.Optional;@Componentpublic class SetupUserLoader implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123; boolean alreadySetup = false; private final UserRepository userRepository; private final PasswordEncoder passwordEncoder; public SetupUserLoader(UserRepository userRepository, PasswordEncoder passwordEncoder) &#123; this.userRepository = userRepository; this.passwordEncoder = passwordEncoder; &#125; @Override @Transactional public void onApplicationEvent(@Nullable ContextRefreshedEvent event) &#123; if (alreadySetup) return; createUserIfNotFound(&quot;admin&quot;, &quot;admin&quot;, &quot;ROLE_ADMIN,ROLE_USER&quot;); alreadySetup = true; &#125; @Transactional public void createUserIfNotFound(String username, String password, String role) &#123; Optional&lt;User&gt; optional = userRepository.findByUsername(username); if (optional.isEmpty()) &#123; User user = new User(); user.setUsername(username); user.setPassword(passwordEncoder.encode(password)); user.setRoles(role); userRepository.save(user); &#125; &#125;&#125; 新建登录请求类 auth/LoginRequest.java： 12345678910import lombok.Data;@Datapublic class LoginRequest &#123; public String username; public String password;&#125; 新建用户细节类 security/CustomUserDetail.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.springframework.security.core.GrantedAuthority;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.UserDetails;import java.util.Arrays;import java.util.Collection;public class CustomUserDetail implements UserDetails &#123; private final User user; public CustomUserDetail(User user) &#123; this.user = user; &#125; @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() &#123; return Arrays.stream(user .getRoles() .split(&quot;,&quot;)) .map(SimpleGrantedAuthority::new) .toList(); &#125; @Override public String getPassword() &#123; return user.getPassword(); &#125; @Override public String getUsername() &#123; return user.getUsername(); &#125; @Override public boolean isAccountNonExpired() &#123; return true; &#125; @Override public boolean isAccountNonLocked() &#123; return true; &#125; @Override public boolean isCredentialsNonExpired() &#123; return true; &#125; @Override public boolean isEnabled() &#123; return true; &#125;&#125; 新建登录验证程序 security/CustomUserDetailsService： 123456789101112131415161718192021222324import jakarta.annotation.Resource;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.core.userdetails.UsernameNotFoundException;import org.springframework.stereotype.Service;import java.util.Optional;@Servicepublic class CustomUserDetailsService implements UserDetailsService &#123; @Resource private UserRepository userRepository; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; Optional&lt;User&gt; optional = userRepository.findByUsername(username); if (optional.isEmpty()) &#123; throw new UsernameNotFoundException(username); &#125; return new CustomUserDetail(optional.get()); &#125;&#125; Session 新建登出处理类 security/NoRedirectLogoutSuccessHandler.java : 12345678910111213141516import jakarta.servlet.ServletException;import jakarta.servlet.http.HttpServletRequest;import jakarta.servlet.http.HttpServletResponse;import org.springframework.security.core.Authentication;import org.springframework.security.web.authentication.logout.LogoutSuccessHandler;import java.io.IOException;public class NoRedirectLogoutSuccessHandler implements LogoutSuccessHandler &#123; @Override public void onLogoutSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; response.setStatus(200); &#125;&#125; 新建权限配置程序 security/SecurityConfig.java ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import jakarta.annotation.Resource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.authentication.ProviderManager;import org.springframework.security.authentication.dao.DaoAuthenticationProvider;import org.springframework.security.config.Customizer;import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity;import org.springframework.security.config.annotation.rsocket.EnableRSocketSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.web.SecurityFilterChain;import org.springframework.security.web.authentication.logout.HeaderWriterLogoutHandler;import org.springframework.security.web.header.writers.ClearSiteDataHeaderWriter;import static jakarta.servlet.DispatcherType.ERROR;import static jakarta.servlet.DispatcherType.FORWARD;import static org.springframework.security.web.header.writers.ClearSiteDataHeaderWriter.Directive.ALL;@Configuration@EnableWebSecurity@EnableMethodSecurity@EnableRSocketSecuritypublic class SecurityConfig &#123; @Resource private CustomUserDetailsService customUserDetailsService; private static final String ADMIN_ROLE_NAME = &quot;ROLE_ADMIN&quot;; private static final String[] AUTH_WHITELIST = &#123; // -- Swagger UI v3 (OpenAPI) &quot;/v3/api-docs/**&quot;, &quot;/swagger-ui/**&quot;, // other &quot;/api/auth/login&quot; &#125;; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize .dispatcherTypeMatchers(FORWARD, ERROR).permitAll() .requestMatchers(AUTH_WHITELIST).permitAll() .requestMatchers(&quot;/api/admin/**&quot;).hasAuthority(ADMIN_ROLE_NAME) .anyRequest().authenticated() ) .csrf(AbstractHttpConfigurer::disable) .httpBasic(Customizer.withDefaults()) .cors(AbstractHttpConfigurer::disable) .formLogin(Customizer.withDefaults()) .logout((logout) -&gt; logout .logoutUrl(&quot;/api/auth/logout&quot;) .addLogoutHandler(new HeaderWriterLogoutHandler(new ClearSiteDataHeaderWriter(ALL))) .deleteCookies(&quot;JSESSIONID&quot;) .logoutSuccessHandler(new NoRedirectLogoutSuccessHandler()) ); return http.build(); &#125; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; @Bean public AuthenticationManager authenticationManager() &#123; DaoAuthenticationProvider daoAuthenticationProvider = new DaoAuthenticationProvider(); daoAuthenticationProvider.setUserDetailsService(customUserDetailsService); daoAuthenticationProvider.setPasswordEncoder(passwordEncoder()); return new ProviderManager(daoAuthenticationProvider); &#125;&#125; 新建用户验证服务 auth/AuthService.java： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import jakarta.annotation.Resource;import jakarta.servlet.http.HttpServletRequest;import jakarta.servlet.http.HttpServletResponse;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;import org.springframework.security.core.Authentication;import org.springframework.security.core.AuthenticationException;import org.springframework.security.core.context.SecurityContext;import org.springframework.security.core.context.SecurityContextHolder;import org.springframework.security.core.context.SecurityContextHolderStrategy;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.web.authentication.logout.SecurityContextLogoutHandler;import org.springframework.security.web.context.HttpSessionSecurityContextRepository;import org.springframework.security.web.context.SecurityContextRepository;import org.springframework.stereotype.Service;@Servicepublic class AuthService &#123; @Resource private AuthenticationManager authenticationManager; private final SecurityContextRepository securityContextRepository = new HttpSessionSecurityContextRepository(); SecurityContextLogoutHandler logoutHandler = new SecurityContextLogoutHandler(); public void login(HttpServletRequest request, HttpServletResponse response, LoginRequest body ) throws AuthenticationException &#123; UsernamePasswordAuthenticationToken token = UsernamePasswordAuthenticationToken.unauthenticated(body.getUsername(), body.getPassword()); Authentication authentication = authenticationManager.authenticate(token); SecurityContextHolderStrategy securityContextHolderStrategy = SecurityContextHolder.getContextHolderStrategy(); SecurityContext context = securityContextHolderStrategy.createEmptyContext(); context.setAuthentication(authentication); securityContextHolderStrategy.setContext(context); securityContextRepository.saveContext(context, request, response); &#125; public String getUsername() &#123; Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal(); if (principal instanceof User user) &#123; return user.getUsername(); &#125; else if (principal instanceof UserDetails userDetails) &#123; return userDetails.getUsername(); &#125; else &#123; throw new ReportBadException(ErrorEnum.PARAM_EXCEPTION); &#125; &#125;&#125; 新建登录控制器 auth/AuthController.java： 1234567891011121314151617181920212223import jakarta.annotation.Resource;import jakarta.servlet.http.HttpServletRequest;import jakarta.servlet.http.HttpServletResponse;import org.springframework.web.bind.annotation.*;@RestController@RequestMapping(&quot;/api/auth&quot;)public class AuthController &#123; @Resource private AuthService authService; @PostMapping(&quot;/login&quot;) public void login(@RequestBody LoginRequest loginRequest, HttpServletRequest request, HttpServletResponse response) &#123; authService.login(request,response,loginRequest); &#125; @GetMapping(&quot;/user&quot;) public String getUser() &#123; return authService.getUsername(); &#125;&#125; 使用方式 在 IDEA 中则可以使用如下方式进行请求： 12345678910111213141516### LoginPOST http://localhost:8080/api/auth/loginContent-Type: application/json&#123; &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;&#125;### Get UserGET http://localhost:8080/api/auth/userContent-Type: application/json### LogoutGET http://localhost:8080/api/auth/logoutContent-Type: application/json 获取用户相关信息的基本方式 12345678@RestControllerpublic class HelloController &#123; @GetMapping(&quot;/hello&quot;) public String hello() &#123; return &quot;当前登录用户：&quot; + SecurityContextHolder.getContext().getAuthentication().getName(); &#125;&#125; JWT 新增如下配置： 12jwt.public.key=classpath:pub.keyjwt.private.key=classpath:pri.key 新建权限配置程序 security/SecurityConfig.java： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import com.nimbusds.jose.jwk.JWK;import com.nimbusds.jose.jwk.JWKSet;import com.nimbusds.jose.jwk.RSAKey;import com.nimbusds.jose.jwk.source.ImmutableJWKSet;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.Customizer;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configurers.oauth2.server.resource.OAuth2ResourceServerConfigurer;import org.springframework.security.config.http.SessionCreationPolicy;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.data.repository.query.SecurityEvaluationContextExtension;import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer;import org.springframework.security.oauth2.jwt.JwtDecoder;import org.springframework.security.oauth2.jwt.JwtEncoder;import org.springframework.security.oauth2.jwt.NimbusJwtDecoder;import org.springframework.security.oauth2.jwt.NimbusJwtEncoder;import org.springframework.security.oauth2.server.resource.web.BearerTokenAuthenticationEntryPoint;import org.springframework.security.oauth2.server.resource.web.access.BearerTokenAccessDeniedHandler;import org.springframework.security.web.SecurityFilterChain;import java.security.interfaces.RSAPrivateKey;import java.security.interfaces.RSAPublicKey;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Value(&quot;$&#123;jwt.public.key&#125;&quot;) RSAPublicKey publicKey; @Value(&quot;$&#123;jwt.private.key&#125;&quot;) RSAPrivateKey privateKey; private static final String ADMIN_ROLE_NAME = &quot;ROLE_ADMIN&quot;; private static final String USER_ROLE_NAME = &quot;ROLE_USER&quot;; private static final String[] AUTH_WHITELIST = &#123; // -- Swagger UI v3 (OpenAPI) &quot;/v3/api-docs/**&quot;, &quot;/swagger-ui/**&quot;, // other &quot;/api/v1/user/login&quot; &#125;; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize .dispatcherTypeMatchers(FORWARD, ERROR).permitAll() .requestMatchers(AUTH_WHITELIST).permitAll() .requestMatchers(AUTH_WHITELIST).permitAll() .requestMatchers(&quot;/api/v1/admin/**&quot;).hasAuthority(ADMIN_ROLE_NAME) .requestMatchers(&quot;/api/v1/admin&quot;).hasAuthority(ADMIN_ROLE_NAME) .requestMatchers(&quot;/api/v1/notice&quot;).hasAnyAuthority(USER_ROLE_NAME, ADMIN_ROLE_NAME) .anyRequest().authenticated() ) .csrf((csrf) -&gt; csrf.ignoringRequestMatchers(&quot;/token&quot;)) .httpBasic(Customizer.withDefaults()) .oauth2ResourceServer(oauth2 -&gt; oauth2 .jwt(jwt -&gt; jwt .decoder(jwtDecoder()) ) ) .sessionManagement((session) -&gt; session.sessionCreationPolicy(SessionCreationPolicy.STATELESS)) .exceptionHandling((exceptions) -&gt; exceptions.authenticationEntryPoint(new BearerTokenAuthenticationEntryPoint()) .accessDeniedHandler(new BearerTokenAccessDeniedHandler()) ).cors(AbstractHttpConfigurer::disable); return http.build(); &#125; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; @Bean JwtDecoder jwtDecoder() &#123; return NimbusJwtDecoder.withPublicKey(this.publicKey).build(); &#125; @Bean JwtEncoder jwtEncoder() &#123; JWK jwk = new RSAKey.Builder(this.publicKey).privateKey(this.privateKey).build(); return new NimbusJwtEncoder(new ImmutableJWKSet&lt;&gt;(new JWKSet(jwk))); &#125;&#125; 新建登录返回类 auth/LoginResponse.java： 123456import lombok.Data;@Datapublic class LoginResponse &#123; private String token;&#125; 新建用户验证服务 auth/AuthService.java： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Value;import org.springframework.security.core.GrantedAuthority;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.jwt.JwtClaimsSet;import org.springframework.security.oauth2.jwt.JwtEncoder;import org.springframework.security.oauth2.jwt.JwtEncoderParameters;import org.springframework.stereotype.Service;import java.time.Instant;import java.util.Optional;import java.util.stream.Collectors;@Slf4j@Servicepublic class UserService &#123; @Value(&quot;$&#123;jwt.expiry:36000&#125;&quot;) Long expiry; @Resource private JwtEncoder jwtEncoder; @Resource private UserDetailsService userDetailsService; @Resource private PasswordEncoder passwordEncoder; @Resource private UserRepository userRepository; public LoginResponse login(LoginRequest loginRequest) &#123; LoginResponse loginResponse = new LoginResponse(); UserDetails userDetails = userDetailsService.loadUserByUsername(loginRequest.getUsername()); if (passwordEncoder.matches(loginRequest.getPassword(), userDetails.getPassword())) &#123; loginResponse.setToken(generateToken(userDetails)); &#125; else &#123; throw new RuntimeException(&quot;Invalid password&quot;); &#125; return loginResponse; &#125; public User getUserByUsername(String username) &#123; Optional&lt;User&gt; optionalUser = userRepository.findByUsername(username); if (optionalUser.isEmpty())&#123; throw new RuntimeException(&quot;Not Found&quot;); &#125; return optionalUser.get(); &#125; private String generateToken(UserDetails userDetails) &#123; Instant now = Instant.now(); String scope = userDetails.getAuthorities().stream() .map(GrantedAuthority::getAuthority) .collect(Collectors.joining(&quot; &quot;)); JwtClaimsSet claims = JwtClaimsSet.builder() .issuer(&quot;self&quot;) .issuedAt(now) .expiresAt(now.plusSeconds(expiry)) .subject(userDetails.getUsername()) .claim(&quot;scope&quot;, scope) .build(); return this.jwtEncoder.encode(JwtEncoderParameters.from(claims)).getTokenValue(); &#125;&#125; 新建登录控制器 auth/AuthController.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import io.swagger.v3.oas.annotations.Operation;import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.springframework.http.ResponseEntity;import org.springframework.security.core.context.SecurityContextHolder;import org.springframework.security.oauth2.jwt.Jwt;import org.springframework.web.bind.annotation.*;@Slf4j@RestController@RequestMapping(&quot;/api/auth&quot;)public class AuthController &#123; @Resource private UserService userService; @GetMapping @Operation(summary = &quot;get current user&quot;) public ResponseEntity&lt;User&gt; user() &#123; Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal(); if (principal instanceof Jwt) &#123; String username = ((Jwt) principal).getSubject(); return ResponseEntity.ok(userService.getUserByUsername(username)); &#125; else &#123; throw new RuntimeException(&quot;Token error&quot;); &#125; &#125; @PostMapping(&quot;/login&quot;) @Operation(summary = &quot;login&quot;) public ResponseEntity&lt;LoginResponse&gt; login(@RequestBody LoginRequest loginRequest) &#123; return ResponseEntity.ok(userService.login(loginRequest)); &#125; @GetMapping(&quot;/roles&quot;) public ResponseEntity&lt;List&lt;String&gt;&gt; authorities() &#123; Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); List&lt;String&gt; authorities = new ArrayList&lt;&gt;(); for (GrantedAuthority authority : authentication.getAuthorities()) &#123; authorities.add(authority.getAuthority()); &#125; return ResponseEntity.ok(authorities); &#125;&#125; 使用方式 123456curl --location --request POST &#x27;localhost:8080/api/auth/login&#x27; \\--header &#x27;Content-Type: application/json&#x27; \\--data-raw &#x27;&#123; &quot;username&quot;:&quot;admin&quot;, &quot;password&quot;:&quot;admin&quot;&#125;&#x27; 1curl --request GET &#x27;http://localhost:8080/api/auth&#x27; --header &#x27;Authorization: Bearer &lt;token&gt;&#x27; 在 Get 参数或 Form 中携带 token 首先需要参照如下样例修改配置类： 12345678910111213141516171819202122232425262728293031import org.springframework.security.oauth2.server.resource.web.DefaultBearerTokenResolver;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http, MvcRequestMatcher.Builder mvc) throws Exception &#123; DefaultBearerTokenResolver resolver = new DefaultBearerTokenResolver(); resolver.setAllowUriQueryParameter(true); resolver.setAllowFormEncodedBodyParameter(true); http .authorizeHttpRequests((authorize) -&gt; authorize .dispatcherTypeMatchers(FORWARD, ERROR).permitAll() .requestMatchers(antMatcher(&quot;/api/auth/login&quot;)).permitAll() .anyRequest().authenticated() ) .csrf(AbstractHttpConfigurer::disable) .httpBasic(Customizer.withDefaults()) .oauth2ResourceServer(oauth2 -&gt; oauth2 .bearerTokenResolver(resolver) .jwt(jwt -&gt; jwt .decoder(jwtDecoder()) ) ) .sessionManagement((session) -&gt; session.sessionCreationPolicy(SessionCreationPolicy.STATELESS)) .exceptionHandling((exceptions) -&gt; exceptions .authenticationEntryPoint(new BearerTokenAuthenticationEntryPoint())); return http.build(); &#125;&#125; 然后即可在请求中添加 access_token 参数即可，GET 请求样例如下： 1curl --request GET &#x27;http://localhost:8080/api/user?access_token=&lt;token&gt;&#x27; 注：无需添加 Bearer 字段 OpenAPI 相关配置 可以加入如下配置，可以在 OpenAPI 中自动携带 JWT Token。 1234567891011121314151617181920212223242526import io.swagger.v3.oas.models.Components;import io.swagger.v3.oas.models.OpenAPI;import io.swagger.v3.oas.models.security.SecurityRequirement;import io.swagger.v3.oas.models.security.SecurityScheme;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class OpenAPIConf &#123; @Bean public OpenAPI customizeOpenAPI() &#123; String securitySchemeName = &quot;bearerAuth&quot;; return new OpenAPI() .addSecurityItem(new SecurityRequirement() .addList(securitySchemeName)) .components(new Components() .addSecuritySchemes(securitySchemeName, new SecurityScheme() .name(securitySchemeName) .type(SecurityScheme.Type.HTTP) .scheme(&quot;bearer&quot;) .description( &quot;Provide the JWT token. JWT token can be obtained from the Login API. For testing, use the credentials &lt;strong&gt;john/password&lt;/strong&gt;&quot;) .bearerFormat(&quot;JWT&quot;))); &#125;&#125; 与 Spring Data 集成 可以在依赖中添加 org.springframework.security:spring-security-data 来与 SpringData 集成，通过下面的查询直接返回用户所拥有的内容： 12345@Repositorypublic interface MessageRepository extends PagingAndSortingRepository&lt;Message,Long&gt; &#123; @Query(&quot;select m from Message m where m.to.id = ?#&#123; principal?.id &#125;&quot;) Page&lt;Message&gt; findInbox(Pageable pageable);&#125; 自定义登录页 编写如下 resources/templates/login.html ： 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh&quot; xmlns:th=&quot;https://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;title&gt;Please Log In&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Here is Custom Login page. Please Log In: &lt;/h1&gt;&lt;div th:if=&quot;$&#123;param.error&#125;&quot;&gt; Invalid username and password.&lt;/div&gt;&lt;div th:if=&quot;$&#123;param.logout&#125;&quot;&gt; You have been logged out.&lt;/div&gt;&lt;form th:action=&quot;@&#123;/login&#125;&quot; method=&quot;post&quot;&gt; &lt;div&gt; &lt;label&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;Username&quot;/&gt; &lt;/label&gt; &lt;/div&gt; &lt;div&gt; &lt;label&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; placeholder=&quot;Password&quot;/&gt; &lt;/label&gt; &lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Log in&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 编写 security/SecurityConfig.java 配置文件： 123456789101112131415161718import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.web.SecurityFilterChain;@Configuration@EnableWebSecuritypublic class SecurityConfig &#123; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception &#123; http .authorizeHttpRequests((authorize) -&gt; authorize.anyRequest().authenticated()) .formLogin(form -&gt; form.loginPage(&quot;/login&quot;).permitAll()); return http.build(); &#125;&#125; 编写 auth/LoginController.java ： 123456789101112import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;@Controllerpublic class LoginController &#123; @GetMapping(&quot;/login&quot;) public String login() &#123; return &quot;login&quot;; &#125;&#125; 注：此处如果单个页面样式上需要使用 Tailwind CSS 可以参照参考资料样例。 单元测试 在测试中可以使用如下方式指定测试用户： 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class UserControllerTest &#123; @Autowired private MockMvc mockMvc; @Test @WithMockUser(username = &quot;user&quot;, roles = &quot;USER&quot;) public void testGetUser() throws Exception &#123; mockMvc.perform(get(&quot;/api/users/1&quot;)) .andExpect(status().isOk()) .andExpect(jsonPath(&quot;$.id&quot;).value(1)) .andExpect(jsonPath(&quot;$.username&quot;).value(&quot;user&quot;)); &#125;&#125; 方法鉴权 除了在接口层面上做安全之外，还可以在方法层面上进行补充和完善，确保数据安全。 开启下面的注解后即可使用方法鉴权： 123import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity;@EnableMethodSecurity 在通过 Spring 调用相应需要鉴权的方法时就会触发安全检查，抛出相应异常。 参考资料 Spring Security 例程 baeldung 教程 RSA 密钥生成 spring-boot-3-jwt-security spring-boot-tailwind Spring Tips: Spring Security method security with special guest Rob Winch method-security 样例项目 Let’s Explore Spring Security 6.4 (SpringOne 2024) Bootiful Spring Boot 3.4: Spring Security spring-security-64 Testing with CSRF Protection","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Spring Security","slug":"Spring-Security","permalink":"https://wangqian0306.github.io/tags/Spring-Security/"},{"name":"JWT","slug":"JWT","permalink":"https://wangqian0306.github.io/tags/JWT/"}]},{"title":"导出 Excel","slug":"java/excel","date":"2021-10-25T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/export-excel/","permalink":"https://wangqian0306.github.io/2021/export-excel/","excerpt":"","text":"导出 Excel Maven 依赖 123456&lt;!-- Excel Export --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt;&lt;/dependency&gt; 编码 编写工具包： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import org.apache.poi.ss.usermodel.Cell;import org.apache.poi.ss.usermodel.Row;import org.apache.poi.ss.usermodel.Sheet;import org.apache.poi.xssf.streaming.SXSSFWorkbook;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.OutputStream;import java.io.UnsupportedEncodingException;import java.util.List;public class ExcelUtil &#123; /** * Excel 导出类 * * @param response 响应 * @param fileName 文件名 * @param columnList 每列的标题名 * @param dataList 导出的数据 */ public static void exportExcel(HttpServletResponse response, String fileName, List&lt;String&gt; columnList, List&lt;List&lt;String&gt;&gt; dataList) &#123; //声明输出流 OutputStream os = null; //设置响应头 setResponseHeader(response, fileName); try &#123; //获取输出流 os = response.getOutputStream(); //内存中保留1000条数据，以免内存溢出，其余写入硬盘 SXSSFWorkbook wb = new SXSSFWorkbook(1000); //获取该工作区的第一个sheet Sheet sheet1 = wb.createSheet(&quot;sheet1&quot;); int excelRow = 0; //创建标题行 Row titleRow = sheet1.createRow(excelRow++); for (int i = 0; i &lt; columnList.size(); i++) &#123; //创建该行下的每一列，并写入标题数据 Cell cell = titleRow.createCell(i); cell.setCellValue(columnList.get(i)); &#125; //设置内容行 if (dataList != null &amp;&amp; dataList.size() &gt; 0) &#123; //序号是从1开始的 int count = 1; //外层for循环创建行 for (List&lt;String&gt; strings : dataList) &#123; Row dataRow = sheet1.createRow(excelRow++); //内层for循环创建每行对应的列，并赋值 for (int j = -1; j &lt; dataList.get(0).size(); j++) &#123;//由于多了一列序号列所以内层循环从-1开始 Cell cell = dataRow.createCell(j + 1); if (j == -1) &#123;//第一列是序号列，不是在数据库中读取的数据，因此手动递增赋值 cell.setCellValue(count++); &#125; else &#123;//其余列是数据列，将数据库中读取到的数据依次赋值 cell.setCellValue(strings.get(j)); &#125; &#125; &#125; &#125; //将整理好的excel数据写入流中 wb.write(os); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; // 关闭输出流 if (os != null) &#123; os.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 设置浏览器下载响应头 */ private static void setResponseHeader(HttpServletResponse response, String fileName) &#123; try &#123; try &#123; fileName = new String(fileName.getBytes(), &quot;ISO8859-1&quot;); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; response.setContentType(&quot;application/octet-stream;charset=UTF-8&quot;); response.setHeader(&quot;Content-Disposition&quot;, &quot;attachment;filename=&quot; + fileName); response.addHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 编写业务类 123456789101112131415161718192021222324252627282930313233343536import com.rainbowfish.health.physical.util.ExcelUtil;import io.swagger.annotations.Api;import io.swagger.annotations.ApiOperation;import org.springframework.http.HttpEntity;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import javax.servlet.http.HttpServletResponse;import java.util.ArrayList;import java.util.List;@Api(tags = &quot;Excel 下载接口&quot;)@RestController@RequestMapping(&quot;/api/v1/excel&quot;)public class ExcelController &#123; @GetMapping(&quot;/test&quot;) @ApiOperation(value = &quot;测试下载 Excel &quot;) public HttpEntity&lt;ResultResponse&gt; download(HttpServletResponse response) &#123; List&lt;String&gt; record = new ArrayList&lt;&gt;(); List&lt;List&lt;String&gt;&gt; recordList = new ArrayList&lt;&gt;(); record.add(&quot;A&quot;); record.add(&quot;B&quot;); record.add(&quot;C&quot;); recordList.add(record); List&lt;String&gt; columnNameList = new ArrayList&lt;&gt;(); columnNameList.add(&quot;编号&quot;); columnNameList.add(&quot;姓名&quot;); columnNameList.add(&quot;性别&quot;); columnNameList.add(&quot;住址&quot;); ExcelUtil.exportExcel(response, &quot;download.xlsx&quot;, columnNameList, recordList); return new HttpEntity&lt;&gt;(null); &#125;&#125; 参考资料 官方文档 手把手教你springboot中导出数据到excel中","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Excel","slug":"Excel","permalink":"https://wangqian0306.github.io/tags/Excel/"}]},{"title":"KafkaAdminClient","slug":"bigdata/kafka-admin-client","date":"2021-10-19T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/kafka-admin-client/","permalink":"https://wangqian0306.github.io/2021/kafka-admin-client/","excerpt":"","text":"KafkaAdminClient 简介 虽然可以使用命令行脚本来管理 Kafka，但是如果想在应用程序，运维框架或是监控平台中集成它们就会很困难。而且在服务器端的脚本通常是使用Kafka服务器端代码运行的，不会有权限相关的限制。在这种情况下就可以使用 KafkaAdminClient。 注：服务器端也有一个 AdminClient 但是社区已经不再推荐使用它了。 功能 主题管理：包括主题的创建、删除和查询。 权限管理：包括具体权限的配置与删除。 配置参数管理：包括 Kafka 各种资源的参数设置、详情查询。所谓的 Kafka 资源，主要有 Broker、主题、用户、Client-id 等。 副本日志管理：包括副本底层日志路径的变更和详情查询。 分区管理：即创建额外的主题分区。 消息删除：即删除指定位移之前的分区消息。 Delegation Token 管理：包括 Delegation Token 的创建、更新、过期和详情查询。 消费者组管理：包括消费者组的查询、位移查询和删除。 Preferred 领导者选举：推选指定主题分区的 Preferred Broker 为领导者。 设计 AdminClient 是一个双线程的设计：分为前端主线程和后端 I/O 线程。 前端线程负责将用户要执行的操作转换成对应的请求，然后再将请求发送到后端 I/O 线程的队列中；而后端 I/O 线程从队列中读取相应的请求，然后发送到对应的 Broker 节点上，之后把执行结果保存起来，以便等待前端线程的获取。 链接例程 1234567Properties props = new Properties();props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;kafka-host:port&quot;);props.put(&quot;request.timeout.ms&quot;, 600000);try (AdminClient client = AdminClient.create(props)) &#123; // 执行你要做的操作……&#125; 参考资料 Kafka 核心技术与实战","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"}]},{"title":"ureplicator-apache-kafka-replicator","slug":"reference/kafka-replicator","date":"2021-10-19T14:43:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/ureplicator-apache-kafka-replicator/","permalink":"https://wangqian0306.github.io/2021/ureplicator-apache-kafka-replicator/","excerpt":"","text":"uReplicator: Uber Engineering’s Robust Apache Kafka Replicator(中文翻译版) 简介 Uber 罗列了社区的 MirrorMaker 工具的一些缺陷以及自研工具(uReplicator)的应对实现方法。所以此处将此博文进行翻译整理。 Uber 的数据分析工作流 在 Uber，使用 Apache Kafka 作为连接生态系统中不同部分的数据管道。从乘客和驾驶员应用程序中收集系统和应用程序日志以及事件数据。然后，通过 Kafka 将这些数据提供给各种下游消费者。 Kafka 中的数据同时提供实时管道和批处理管道。前一个数据用于计算业务指标、调试、警报和仪表板等活动。批处理管道数据更具探索性，例如将 ETL 转换为 Apache Hadoop 和 HP Vertica。 在本文中，将介绍 Uber 的开源解决方案 UreReplicator，该解决方案用于以健壮可靠的方式复制 Apache Kafka 数据。该系统扩展了 Kafka MirrorMaker 的原始设计，专注于极高的可靠性、零数据丢失保证和易操作性。uReplicator 自2015年11月开始投入生产，是 Uber 多数据中心基础设施的关键组成部分。 什么是 MirrorMaker，为什么需要它 鉴于 Uber 内部大量使用 Kafka，在不同的数据中心使用多个集群。一些用例需要查看这些数据的全局视图。例如，为了计算与出行次数相关的业务指标，需要从所有数据中心收集信息，并在一个地方进行分析。为了实现这一点，之前一直使用 Kafka 软件包附带的开源 MirrorMaker 工具跨数据中心复制数据，如下所示。 MirrorMaker(首次在 Kafka 0.8.2 版本引入)本身非常简单。它使用高级 Kafka 消费者从源集群获取数据，然后将该数据馈送到 Kafka 生产者以将其转储到目标集群。 在 Uber 遇到的 Kafka MirrorMaker 缺陷 尽管初始的 MirrorMaker 设置已经满足了需求，但很快就遇到了可伸缩性问题。随着主题数量和数据速率(bytes/second)的增长，就开始出现数据交付延迟或丢失进入聚合集群的数据，从而导致生产环境问题，降低了数据质量。针对 Uber 特定用例的现有 MirrorMaker 工具(从 0.8.2 开始)的一些主要问题如下所示： 不可接受的重平衡(rebalancing) 如前所述，每个 MirrorMaker 工作线程使用一个高级消费者。这些消费者经常经历一个再平衡的过程。他们相互协商决定谁拥有哪个主题分区(通过 Apache ZooKeeper完成)。这个过程可能需要很长时间；在某些情况下，我们观察到大约 5-10 分钟的重平衡过程。这是一个问题，因为它违反了端到端延迟保证。此外，消费者可以在 32 次再平衡尝试后放弃，永远陷入停止状态。不幸的是，我们亲眼看到这种情况发生过几次。在每次尝试重新平衡后都会看到类似的流量模式： 在重新平衡期间的停止活动之后，MirrorMaker 有大量积压的数据需要处理。这导致目标集群和所有下游消费者的流量激增，导致生产中断和增加端到端延迟。 添加主题困难 在 Uber，必须在 MirrorMaker 中指定一个主题白名单，以控制通过 WAN 链路的数据流量。对于 Kafka MirrorMaker 来说这个白名单是完全静态的，需要重新启动 MirrorMaker 集群来添加新的主题。重启成本过于高昂，因为它迫使高层消费者进行重新平衡。 可能造成数据丢失 旧的 MirrorMaker 有一个问题(在最新版本中似乎已经被修复了)，自动的位移提交可能会导致数据丢失。高级消费者会自动提交已经获取到的消息的偏移量。如果在 MirrorMaker 未能验证其是否将消息正常写入目标集群之前发生故障，则这些消息则会被丢弃。 元数据同步问题 Uber 的工程师在更新配置的方式上也遇到了操作问题。要从白名单中添加或删除主题，Uber 的工程师在一个配置文件中列出了所有最终的主题名称，该文件在 MirrorMaker 初始化期间读取。有时，配置无法在其中一个节点上更新。这导致了整个集群的崩溃，因为不同的 MirrorMaker 工作人员在要复制的主题列表上没有达成一致。 为什么研发 uReplicator Uber 的工程师构思了以下几种方案来解决上述问题： 方案 A：将数据分发进多个 MirrorMaker 集群中 上面列出的大多数问题都是由高级消费者再平衡过程造成的。减少其影响的一种方法是限制一个 MirrorMaker 集群复制的主题分区的数量。因此，可以拆分成几个 MirrorMaker 集群，每个集群复制要聚合的主题子集。 优点： 新增主题简单，只需要新增一个集群就可以了。 重启速度也会很快 缺点： 这是另一个运维噩梦：必须部署和维护多个集群。 方案 B：使用 Apache Samza 进行复制 由于问题在于高级消费者(从0.8.2开始)，另一种解决方案是使用 Kafka SimpleConsumer 并添加缺少的领导人选举和分区分配功能。Apache Samza 是一个流处理框架，它已经静态地将分区分配给工作线程。然后，可以简单地使用 Samza 作业将数据复制和聚合到目标。 优点： 稳定可靠 便于维护，使用一个任务可以复制多个主题 重启任务时造成的影响小 缺点： 实现模式比较僵化，需要重启任务才能新增或删除话题 需要重启任务才能新增工作线程 主题扩展需要显式处理。 方案 C：使用 Apache Helix-based Kafka consumer 最终方案是使用基于 Helix-based Kafka consumer。在本例中，使用 Apache Helix 将分区分配给工作线程，每个工作线程使用 Simple Consumer 复制数据。 优点： 添加和删除主题非常简单 向 MirrorMaker 集群添加和删除节点非常简单。 永远不需要出于操作原因重新启动集群(仅在升级时重启)。 可靠稳定 缺点： 引入了对 Helix 的依赖。(这是可以接受的，因为 Helix 本身非常稳定，可以将一个 Helix 集群用于多个 MirrorMaker 集群。) uReplicator 概览 uReplicator 的各种组件以不同的方式工作，以实现可靠性和稳定性： Helix uReplicator 控制器实际上是一个节点群集，它有几个职责： 为每个工作进程分配和分配主题分区 处理主题/分区的添加/删除 处理 uReplicator 工作进程的添加/删除 检测节点故障并重新分配那些特定的主题分区 控制器使用 ZooKeeper 来完成所有这些任务。它还公开了一个简单的 REST API，以便添加/删除/修改要镜像的主题。 uReplicator 工作线程，类似于Kafka MirrorMaker 功能中的工作进程，将一组主题和分区从源集群复制到目的集群。uReplicator 控制器决定 uReplicator 的分配，而不是重新平衡过程。此外，抛弃了 Kafka 高级消费者，而是使用称为 DynamicKafkaConsumer 的简化版本。 只要发生了变化(话题分区的增加/删除) Helix 客户端会通知每个 uReplicator 工作节点。反之一样，Helix 也会通知 DynamicKafkaConsumer 添加/删除主题分区。 每个 uReplicator 工作线程上都存在一个 DynamicKafkaConsumer 实例，它是 Kafka 高级消费者的魔改版本。它删除了重新平衡部分，并添加了一种动态添加/删除主题分区的机制。 假设向现有的 uReplicator 集群添加一个新主题。事件流程如下所示： Kafka 管理员使用以下命令将新主题添加到控制器： 1curl -X POST http://localhost:9000/topics/testTopic uReplicator 控制器计算 testTopic 的分区数量，并将主题分区映射到活动的工作线程。然后更新 ZooKeeper 元数据以反映此映射。 每个相应的 Helix 客户端都会收到一个回调，通知添加这些主题分区。反过来，客户端也会引用DynamicKafkaConsumer 的 addFetcherForPartitions 功能 。 该 DynamicKafkaConsumer 随后注册这些新的分区，找到相应的领导经纪人，并将它们添加到提取器线程来启动数据镜像。 有关实现的更多详细信息，请参阅 uReplicator Design wiki。 对整体稳定性的影响 自从大约八个月前 Uber 上首次推出 uReplicator 以来，还没有看到一个产品出现问题(与实施前几乎每周都会出现的某种停机形成对比)。下图描述了在生产环境中将新主题添加到 MirrorMaker 白名单的场景。第一个图显示每个 uReplicator 工作人员拥有的总主题分区。每添加一个新主题，此计数就会增加。 第二个图显示流向目标群集的相应 uReplicator 流量。没有出现 Kafka MirrorMaker 一样的有一段时间的停止消费或负载峰值： 总的来说，uReplicator 的优势如下： 稳定：重新平衡现在仅在启动期间以及添加或删除节点时发生。此外，它只影响主题分区的一个子集，而不是像以前那样导致完全不活动。 更好的扩展性：现在向现有集群添加新节点要简单得多。由于分区分配现在是静态的，可以智能地仅将分区的子集移动到新节点。其他主题分区不受影响。 操作更简单：uReplicator 支持动态白名单。现在在添加/删除/扩展 Kafka 主题时不需要重新启动集群。 零数据丢失：uReplicator 保证零数据丢失，因为它仅在数据被持久化到目标集群后才提交检查点。 参考资料 uReplicator: Uber Engineering’s Robust Apache Kafka Replicator","categories":[{"name":"参考资料","slug":"参考资料","permalink":"https://wangqian0306.github.io/categories/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"JMX 使用","slug":"tools/jmx-exporter","date":"2021-10-14T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/jmx-exporter/","permalink":"https://wangqian0306.github.io/2021/jmx-exporter/","excerpt":"","text":"JMX 使用 简介 JMX（Java Management Extensions，即Java管理扩展）是一个为应用程序、设备、系统等植入管理功能的框架。 本机使用 JDK 中默认附带了 jconsul 工具，可以使用 jconsul 命令打开图形界面。 Prometheus 监控 Prometheus 提供了 JMX Exporter 工具提取 JMX 数据。 Java Agent 方式 访问 官方网站 获得最新版本的 jar 包，然后编写如下配置文件: 1234567891011121314151617181920startDelaySeconds: 0hostPort: 127.0.0.1:1234username: password: jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:1234/jmxrmissl: falselowercaseOutputName: falselowercaseOutputLabelNames: falsewhitelistObjectNames: [&quot;org.apache.cassandra.metrics:*&quot;]blacklistObjectNames: [&quot;org.apache.cassandra.metrics:type=ColumnFamily,*&quot;]rules: - pattern: &#x27;org.apache.cassandra.metrics&lt;type=(\\w+), name=(\\w+)&gt;&lt;&gt;Value: (\\d+)&#x27; name: cassandra_$1_$2 value: $3 valueFactor: 0.001 labels: &#123;&#125; help: &quot;Cassandra metric $1 $2&quot; cache: false type: GAUGE attrNameSnakeCase: false 使用如下命令启动程序： 1java -javaagent:./jmx_prometheus_javaagent-0.16.1.jar=8080:config.yaml -jar yourJar.jar HTTP Server 方式 使用如下命令进行调试 1234git clone https://github.com/prometheus/jmx_exporter.gitcd jmx_exporter./mvnw packagejava -cp collector/target/collector*.jar io.prometheus.jmx.JmxScraper service:jmx:rmi:///jndi/rmi://&lt;host&gt;:&lt;port&gt;/jmxrmi 注：若输出参数则证明程序运行正常，配置无误。 调试完成后可以在 jmx_prometheus_httpserver 构建出的文件夹内找到可安装的 deb 包和 jar 包，或者使用如下脚本直接运行服务器： 1java -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=&lt;port&gt; -jar jmx_prometheus_httpserver/target/jmx_prometheus_httpserver-$&#123;version&#125;-jar-with-dependencies.jar &lt;port&gt; example_configs/httpserver_sample_config.yml","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"},{"name":"JConsul","slug":"JConsul","permalink":"https://wangqian0306.github.io/tags/JConsul/"},{"name":"JMX Exporter","slug":"JMX-Exporter","permalink":"https://wangqian0306.github.io/tags/JMX-Exporter/"}]},{"title":"Prometheus 安装","slug":"tools/prometheus-install","date":"2021-10-13T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/prometheus-install/","permalink":"https://wangqian0306.github.io/2021/prometheus-install/","excerpt":"","text":"Prometheus 安装 简介 Prometheus 是一个开源系统监控和警报工具包。Prometheus 将其指标收集并存储为时间序列数据，即指标信息与记录的时间戳一起存储，以及称为标签的可选键值对。 本地容器化部署 1234567891011services: prometheus: image: prom/prometheus:latest environment: TZ: Asia/Shanghai ports: - 9090:9090 volumes: - type: bind source: ./prometheus.yml target: /etc/prometheus/prometheus.yml 监控自身(测试) 编写配置文件 1234567891011121314151617global: scrape_interval: 15s # 默认情况下，每 15 秒提取一次数据 # 在与外部系统（联邦集群、远程存储、报警系统）通信时，将这些标签附加到任何时间序列或警报。 external_labels: monitor: &#x27;codelab-monitor&#x27;# 目标采集点配置scrape_configs: # 作业名称将作为标签 “job=&lt;job_name&gt;” 添加到此配置中的读取到的数据上。 - job_name: &#x27;prometheus&#x27; # 覆盖全局默认值，并每隔 5 秒从该作业中读取数据。 scrape_interval: 5s static_configs: - targets: [ &#x27;localhost:9090&#x27; ] 启动服务 编写查询语句 1prometheus_target_interval_length_seconds 执行查询进行测试 监控 JMX Prometheus 官方提供了 JMX 监控的导出工具 JMX exporter，可以访问官网下载 jar 包： 然后即可在 prometheus.yml 文件下添加检测任务： 1234- job_name: &#x27;&lt;name&gt;&#x27; scrape_interval: &lt;time_interval&gt; static_configs: - targets: [ &#x27;&lt;host&gt;:&lt;port&gt;&#x27; ] 在项目中编写导出参数项的配置文件 config.yaml 内容样例如下： 12rules: - pattern: &quot;.*&quot; 然后通过 Java Agent 运行需要监控的程序即可： 1java -javaagent:./jmx_prometheus_javaagent-0.17.0.jar=&lt;port&gt;:config.yaml -jar &lt;jar_name&gt;.jar 在程序启动后即可访问 http://localhost:port 看到监控数据。 监控 Mysql 新建配置文件 prometheus.yml ： 1234567global: scrape_interval: 15sscrape_configs: - job_name: &#x27;check-web&#x27; scrape_interval: 5s static_configs: - targets: [ &#x27;mysqlexporter:9104&#x27; ] 新建 docker-compose.yaml ： 12345678910111213141516171819202122232425262728services: mysql: image: mysql:latest command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: demo ports: - &quot;3306:3306&quot; mysqlexporter: image: prom/mysqld-exporter:latest ports: - &quot;9104:9104&quot; environment: - DATA_SOURCE_NAME=root:123456@(mysql:3306)/demo depends_on: - mysql prometheus: image: prom/prometheus:latest environment: TZ: Asia/Shanghai ports: - &quot;9090:9090&quot; volumes: - type: bind source: ./prometheus.yml target: /etc/prometheus/prometheus.yml 参考资料 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"}]},{"title":"Grafana","slug":"tools/grafana","date":"2021-10-11T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/grafana/","permalink":"https://wangqian0306.github.io/2021/grafana/","excerpt":"","text":"Grafana 简介 Grafana 开源是开源可视化和分析软件。可以接入多种数据源，并对数据进行展示和检索。 部署 Docker 12345services: grafana: image: grafana/grafana:latest ports: - 3000:3000 注：在安装时可以指定环境变量的方式附带插件 GF_INSTALL_PLUGINS=grafana-clock-panel, grafana-simple-json-datasource 官方 Docker 部署说明 Kubernetes 在 Kubernetes 上可以使用 Helm 部署服务。 插件 监控 CDH 安装 CDH 插件 配置数据源 填写配置信息 进行连接测试 新建 panel 填入如下 SQL 进行测试 1select total_read_bytes_rate_across_disks, total_write_bytes_rate_across_disks where category = CLUSTER 参考资料 官方文档","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://wangqian0306.github.io/tags/Grafana/"}]},{"title":"Kafka 性能和高可用性调整","slug":"bigdata/kafka-tuning","date":"2021-09-15T14:43:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/kafka-tuning/","permalink":"https://wangqian0306.github.io/2021/kafka-tuning/","excerpt":"","text":"Kafka 性能和高可用性调整 操作系统部分 文件描述符限制 ulimit -n 1000000 vim /etc/sysctl.conf 1vm.max_map_count=655360 交换内存(swap) 打开交换内存 vm.swapiness 应当设置为 1 设置内存可以填充脏页的百分比 vm.dirty_background_ratio 应当设置为小于 10 大部分情况下可以直接设为 5 设置脏页填充的绝对最大系统内存量vm.dirty_ratio 应当设置为大于 20，60~80 是一个比较合理的区间 在使用的过程中可以针对 swap 内的脏页数量进行监控，防止集群崩溃造成数据丢失。 1cat /proc/vmstat | grep &quot;dirty|writeback&quot; 磁盘及挂载参数 建议采用 XFS 文件系统 在挂载 Kafka 数据盘时建议采用 noatime 参数(屏蔽最后访问时间的更改，提高性能) 网络配置 socket 读写缓冲区配置为 131072 (128 KB) net.core.wmem_default net.core.wmem_default socket 读写缓冲最大值为 2097152 (2 MB) net.core.wmen_max net.core.rmem_max TCP socket 读写缓冲区大小设置为 4096 65536 2048000 (最小值 默认值 最大值) net.ipv4.tcp_wmem net.ipv4.tcp_rmem 打开 TCP 时间窗扩展 net.ipv4.tcp_window_scaling 设置为 1 提升并发量 net.ipv4.tcp_max_syn_backlog 设置为比 1024 更大的值 允许更多的数据包进入内核 net.core.netdev_max_backlog 设置为比 1000 更大的值 JVM 部分 建议使用 Java 11 配置部分 Broker 启动参数 123-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC-XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M-XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -XX:+ExplicitGCInvokesConcurrent 机架名 为了防止单个机架的故障导致服务不可用，可以设置 broker.rack 参数。这样一来 Kafka 会保证分区的副本被分布在多个机架上，从而获得更高的可用性。 副本选举策略 unclean.leader.election 参数默认为 true，表示允许不同步的副本成为首领，可能会造成消息丢失。如果业务场景不能接受消息丢失则需要修改为 false。 auto.leader.rebalance.enable 参数设置为 false，表示禁止定期进行的重新选举。 监控部分 对于服务器需要监控如下参数： CPU 使用率 网络输入/输出吞吐量 磁盘平均等待时间 磁盘剩余空间 内存使用率 TCP 链接数 打开文件数 inode 使用情况 对于 JVM 需要监控如下内容： Full GC 发生频率和时间长度 活跃对象大小 应用线程总数 对于生产者来说需要监控如下参数： error-rate retry-rate 注：通过这两项参数明确生产者的错误率。 对于消费者来说需要监控如下参数： consumer-lag records-lag-max records-lead-min 注：Lag 表示距离最新消息还有多少积压。Lead 值是指消费者最新消费消息的位移与分区当前第一条消息位移的差值。一旦你监测到 Lead 越来越小，甚至是快接近于 0 了，你就一定要小心了，这可能预示着消费者端要丢消息了 对于 broker 来说需要监控如下参数： UnderReplicatedPartitions(未同步的分区) ActiveControllerCount(活跃度控制器数量) RequestHandlerAvgIdlePercent(请求处理器空闲率) BytesInPerSec(主题输入字节/秒) BytesOutPerSec(主题输出字节/秒) MessagesInPerSec(主题接收消息/秒) PartitionCount(分区数量) LeaderCount(首领数量) OfflinePartitionsCount(离线分区数量) 注：详情参阅 官方文档。 性能指标调优 调优吞吐量 参数位置 参数描述 Broker 端 适当增加 num.replica.fetchers 参数值，但不用超过 CPU 核心数 Broker 端 调优 GC 参数以避免经常性的 Full GC Producer 端 适当增加 batch.size 参数值，比如从默认的 16 KB 增加到 512 KB 或 1MB Producer 端 适当增加 linger.ms 参数值，比如 10~100 Producer 端 设置 compression.type=lz4 或者 zstd Producer 端 设置 acks=0 或 1 Producer 端 设置 retries=0 Producer 端 如果多线程共享同一个 Producer 实例，就增加 buffer.memory 参数值 Consumer 端 采用多 Consumer 进程或线程同时消费数据 Consumer 端 增加 fetch.min.bytes 参数值，比如设置成 1KB 或更大 调优延时 参数位置 参数描述 Broker 端 适当增加 num.replica.fetchers 参数值 Producer 端 设置 linger.ms=0 Producer 端 不启用压缩，即设置 compression.type=none Producer 端 设置 acks=1 Consumer 端 设置 fetch.min.bytes=1 批处理指标监控 batch-size-avg ：此指标是批处理的实际大小。如果一切顺利，这将非常接近 batch.size。如果 batch-size-avg 始终低于设置的批量大小，则 linger.ms 可能不够高。同时，如果 linger.ms 很高，而批次仍然很小，则可能是记录生成速度不够快。如果已经很高了可以再调整回原来的值。 records-per-request-avg ：每个请求跨批次的平均记录数。 record-size-avg ：注意不要接近或超过 batch.size buffer-available-bytes ：内存余量。 record-queue-time-avg ：在发送记录之前等待填充的时间。 参考资料 Kafka 官方文档 Kafka 核心技术与实战 Kafka Producer and Consumer Internals","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"Docker Swarm 初步使用","slug":"docker/docker-swarm","date":"2021-09-09T12:26:13.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/docker-swarm 初步使用/","permalink":"https://wangqian0306.github.io/2021/docker-swarm%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Docker Swarm 初步使用 简介 Docker Swarm 是 Docker 的集群管理工具。它将 Docker 主机池转变为单个虚拟 Docker 主机。 安装 具体方案请参照 官方文档 使用 在 Docker Swarm 集群中部署服务采用的是 Docker Compose 文件的格式。 但是某些特定的参数项将不再生效，例如： container_name expose restart depends_on 常用命令 部署或更新服务 1docker stack deploy -c docker-compose.yaml &lt;name&gt; --with-registry-auth 删除服务 1docker stack rm &lt;name&gt; 注意事项 Docker Swarm 默认使用了两个网络 Docker 原始的内部网络用于服务间通信 Ingress 网络用于对外提供服务","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://wangqian0306.github.io/tags/Docker-Compose/"},{"name":"Docker Swarm","slug":"Docker-Swarm","permalink":"https://wangqian0306.github.io/tags/Docker-Swarm/"}]},{"title":"Kafka API","slug":"bigdata/kafka-api","date":"2021-08-24T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/kafka-api/","permalink":"https://wangqian0306.github.io/2021/kafka-api/","excerpt":"","text":"Kafka API 简介 Kafka 存在以下五种 API： Producer API Consumer API Streams API Connect API Admin API Producer API Producer API 允许应用程序向 Kafka 集群中的主题发送数据流。 Produce API 使用了异步发送消息的方式，在发送过程中涉及的线程是 main 线程和 Sender 线程，以及一个线程共享变量——RecordAccumulator。 在发送数据时可以采用以下三种方式: 发送并忘记(fire-and-forget) 我们把消息发送给服务器，但井不关心它是否正常到达。大多数情况下，消息会正常到达，因为Kafka 是高可用的，而且生产者会自动尝试重发。不过，使用这种方式有时候也会丢失一些消息。 注：在需要严格的数据顺序时不建议采用此种方式。 同步发送 我们使用 send() 方怯发送消息，它会返回一个 Future 对象，调用 get() 方法进行等待，就可以知道悄息是否发送成功。 异步发送 我们调用 send() 方怯，并指定一个回调函数，服务器在返回响应时调用该函数。 注：只有在不改变主题分区数量的情况下，键与分区之间的映射才能保持不变。在采用新的包写入数据时记得检查分区器 The Importance of Standardized Hashing Across Producers。 简单使用 12345from kafka import KafkaProducerproducer = KafkaProducer(bootstrap_servers=&#x27;xxx:xxx,xxx:xxx&#x27;)producer.send(&#x27;&lt;topic&gt;&#x27;, key=b&#x27;&lt;key&gt;&#x27;, value=b&quot;&lt;value&gt;&quot;)producer.close() 注：尽量使用域名而不要写 IP 地址！因为此问题在之前的使用过程中遇到了程序正常运行却没有输出的麻瓜问题。 Consumer API Producer API 允许应用程序从 Kafka 集群中的主题拉取数据流。 注：在同一个群组里，我们无法让一个线程运行多个消费者，也无法让多个线程安全的共享一个消费者。如有需要可以使用 Java 的 Executor Service 启动多个线程，使每个消费者运行在自己的线程上。 在接收数据时也可以采用以下几种种方式提交偏移量(offset): 自动提交 在 enable.auto.commit 参数设置为 true 时，每过 5 秒(auto.commit.interval.ms)，消费者会把 poll() 方法接收到的最大偏移量提交上去。 可能会有数据重复，但一般情况下不会有什么问题，不过在处理异常或提前退出轮询时要格外小心。 手动提交 在手动提交时需要将 enable.auto.commit 参数设置为 false，然后使用 commitSync() 方法提交偏移量。 异步提交 commitSync() 方法在成功提交或碰到无怯恢复的错误之前都会一直重试，但 commitAsync() 方法不会。 与此同时带来的问题是可能会造成消息重复。需要尤其注意偏移量的提交顺序。 同步和异步组合提交 消费者关闭前一般会组合使用 commitSync() 和 commitAsync()。 如果一切正常，我们使用 commitAsync() 方法提交，若如果直接关闭消费者则会使用 commitSync() 方法。 简单使用 12345from kafka import KafkaConsumerconsumer = KafkaConsumer(&#x27;&lt;topic&gt;&#x27;, bootstrap_servers=&#x27;xxxx:xxxx,xxxx:xxxx&#x27;, group_id=&#x27;&lt;group&gt;&#x27;)for msg in consumer: print (msg) 消费特定位移并打印时间戳： 1234567891011121314151617181920212223from kafka import KafkaConsumerdef consume_specific_message(topic, partition, offset): consumer = KafkaConsumer( bootstrap_servers=&#x27;&lt;kafka_broker_host&gt;:9092&#x27;, auto_offset_reset=&#x27;earliest&#x27;, enable_auto_commit=False # 禁用自动提交偏移量，以确保我们可以手动控制偏移量的位置 ) # 指定要消费的主题、分区和偏移量 consumer.assign([&#123;&#x27;topic&#x27;: topic, &#x27;partition&#x27;: partition, &#x27;offset&#x27;: offset&#125;]) for message in consumer: print(&#x27;Received message: &#123;&#125;, timestamp: &#123;&#125;&#x27;.format(message.value.decode(&#x27;utf-8&#x27;), message.timestamp)) break # 一旦找到消息，就停止消费 consumer.close()if __name__ == &#x27;__main__&#x27;: topic_name = &#x27;&lt;topic_name&gt;&#x27; partition_id = 0 # 指定要消费的分区 message_offset = 12345 # 指定要查找的消息的偏移量 consume_specific_message(topic_name, partition_id, message_offset) Streams API Kafka Streams 是用于构建应用程序和微服务的客户端库，其中输入和输出数据存储在 Kafka 集群中。 Connect API Connect API 允许实现从某个源数据系统不断拉入 Kafka 或从 Kafka 推送到某个接收器数据系统的连接器。 在此时我们可以把 Kafka 看成一个数据管道。 Admin API Admin API 支持管理和检查主题、broker、acl 和其他 Kafka 对象。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"Linux 系统单个磁盘变为只读问题","slug":"linux/only-read","date":"2021-08-16T15:52:33.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/only-read/","permalink":"https://wangqian0306.github.io/2021/only-read/","excerpt":"","text":"Linux 系统单个磁盘变为只读问题 简介 访问文件系统中的特定目录之后无法创建和写入文件，结合 df -TH 命令进行检查发现问题出现在对应磁盘。 解决方法 重启 怀疑是挂载与操作系统相关问题，尝试使用重启的方式解决问题。 在重启过后问题依旧存在。 修复磁盘 使用此种方式完成了修复 强制卸载磁盘 1umount -vl &lt;mount_path&gt; 修复磁盘 1fsck -t &lt;type&gt; -y /dev/&lt;disk_name&gt; 注：type 字段指的是磁盘格式，例如 ext4。且此命令的执行时间会比较长。 重新挂载磁盘 1mount /dev/&lt;disk_name&gt; &lt;mount_path&gt;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"配置时区","slug":"linux/tz","date":"2021-08-16T15:52:33.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/tz/","permalink":"https://wangqian0306.github.io/2021/tz/","excerpt":"","text":"配置时区 系统设置 如果本身存在时区文件 /usr/share/zoneinfo/Asia/Shanghai 则可以进行如下操作： 12mv /etc/localtime /etc/localtime.bkln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 若找不到时区文件则可以使用如下命令生成： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@localhost ~]# tzselectPlease identify a location so that time zone rules can be set correctly.Please select a continent or ocean. 1) Africa 2) Americas 3) Antarctica 4) Arctic Ocean 5) Asia 6) Atlantic Ocean 7) Australia 8) Europe 9) Indian Ocean10) Pacific Ocean11) none - I want to specify the time zone using the Posix TZ format.#? 5Please select a country. 1) Afghanistan 18) Israel 35) Palestine 2) Armenia 19) Japan 36) Philippines 3) Azerbaijan 20) Jordan 37) Qatar 4) Bahrain 21) Kazakhstan 38) Russia 5) Bangladesh 22) Korea (North) 39) Saudi Arabia 6) Bhutan 23) Korea (South) 40) Singapore 7) Brunei 24) Kuwait 41) Sri Lanka 8) Cambodia 25) Kyrgyzstan 42) Syria 9) China 26) Laos 43) Taiwan10) Cyprus 27) Lebanon 44) Tajikistan11) East Timor 28) Macau 45) Thailand12) Georgia 29) Malaysia 46) Turkmenistan13) Hong Kong 30) Mongolia 47) United Arab Emirates14) India 31) Myanmar (Burma) 48) Uzbekistan15) Indonesia 32) Nepal 49) Vietnam16) Iran 33) Oman 50) Yemen17) Iraq 34) Pakistan#? 9Please select one of the following time zone regions.1) Beijing Time2) Xinjiang Time#? 1The following information has been given: China Beijing TimeTherefore TZ=&#x27;Asia/Shanghai&#x27; will be used.Local time is now: Sat Aug 29 10:33:56 CST 2020.Universal Time is now: Sat Aug 29 02:33:56 UTC 2020.Is the above information OK?1) Yes2) No#? 1You can make this change permanent for yourself by appending the line TZ=&#x27;Asia/Shanghai&#x27;; export TZto the file &#x27;.profile&#x27; in your home directory; then log out and log in again.Here is that TZ value again, this time on standard output so that youcan use the /usr/bin/tzselect command in shell scripts:Asia/Shanghai 在容器中使用 当在切换容器中的时区的时候可以依照 tzselect 命令当中的输出，配置 TZ 环境变量： 1ENV TZ Asia/Shanghai","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"MySQL","slug":"database/mysql-community","date":"2021-08-04T14:12:59.000Z","updated":"2025-01-08T02:59:32.129Z","comments":true,"path":"2021/mysql/","permalink":"https://wangqian0306.github.io/2021/mysql/","excerpt":"","text":"MySQL 单机安装 访问 Yum Repository 下载仓库包，然后进行安装： 12yum localinstall mysql80-community-release-el7-3.noarch.rpmyum install mysql-community-server 或者直接使用 yum 安装 Source distribution 版本 ： 1yum install mysql mysql-server -y 启动服务： 1systemctl enable mysqld --now 检查临时密码(社区版) 1grep &#x27;temporary password&#x27; /var/log/mysqld.log 进行登录(社区版) 1mysql -u root -p 更新 ROOT 用户密码 1ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;MyNewPass4!&#x27;; 创建用户并赋予用户指定库的访问权限： 123CREATE USER &#x27;rbfish&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Rbfish123..&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;rbfish&#x27;@&#x27;%&#x27;;FLUSH PRIVILEGES; 开放用户远程访问： 123USE mysql;UPDATE USER SET host = &#x27;%&#x27; WHERE user = &#x27;&lt;user&gt;&#x27;;FLUSH PRIVILEGES; 创建数据库： 1CREATE DATABASE `&lt;name&gt;` CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 注：本文以 RockyLinux 7 为例，详情参照 官方文档，记得进入页面后切换至当前版本。 容器化安装 12345678910services: db: image: mysql:latest restart: always environment: MYSQL_ROOT_PASSWORD: example volumes: - &lt;dir&gt;:/var/lib/mysql ports: - &quot;3306:3306&quot; 注：详细配置信息请参照 DockerHub 文档或官方文档。 存储引擎 InnoDB 在 InnoDB 中的数据结构分为以下两个部分： 内存 磁盘 内存架构 在内存中的数据类型有以下四种： Buffer Pool ：缓冲池是主内存中的一个区域，用于 InnoDB 在访问数据时缓存表和索引数据。缓冲池允许直接从内存访问常用数据，从而加快处理速度。 注：在 Buffer Pool 中采用 LRU 算法来逐出最近最少使用的页。 Change Buffer ：更改缓冲区是一种特殊的数据结构，二级索引不在缓冲池中时，它会缓存对这些页所做的更改，并在稍后通过其他读取操作将页加载到缓冲池中时进行合并。 Adaptive Hash Index ：自适应哈希索引是基于经常访问的索引页中键的前缀构建的，用于加速用户的查询操作。 Log Buffer ：日志缓冲区是保存要写入磁盘上日志文件的数据的内存区域。日志缓冲区的内容会定期刷写到磁盘。大型日志缓冲区可以使大型事务能够运行，而无需在事务提交之前将重做日志数据写入磁盘。 磁盘架构 在磁盘中的数据类型有以下六种： Tables idb 文件：表结构，索引，数据等 cfg 文件：元数据文件，例如锁和加密信息 cfp 文件：密钥文件 Indexes 聚簇索引：每个表都有一个 InnoDB 称为聚簇索引的特殊索引，用于存储行数据。通常，聚簇索引与主键同义。 二级索引：聚集索引以外的索引称为二级索引。在 InnoDB 中，二级索引中的每条记录都包含行的主键列，以及为二级索引指定的列。 Tablespaces The Systsem Tablespace ：系统表空间是更改缓冲区的存储区域。如果表是在系统表空间中创建的，而不是在每个表的文件或常规表空间中创建的，则它还可能包含表和索引数据。 File-Per-Table Tablespaces ：独占表空间包含单个表的数据和索引，并存储在文件系统上的单个 InnoDB 数据文件中。 General Tablespaces ：通用表空间是使用 CREATE TABLESPACE 语法创建的共享 InnoDB 表空间。由于共享所以消耗的空间会稍小。 Undo Tablespaces ：撤消表空间包含撤消日志，撤消日志是记录的集合，其中包含有关如何撤消事务对聚集索引记录的最新更改的信息。 Temporary Tablespaces Session Temporary Tablespaces ：会话临时表空间存储用户创建的临时表和优化程序在配置为磁盘上内部临时表的存储引擎时 InnoDB 创建的内部临时表。 Global Temporary Tablespace ：全局临时表空间存储对用户创建的临时表所做的更改的回滚段。 Doublewrite Buffer ：双重写入缓冲区是一个存储区域，用于在将页面写入 InnoDB 数据文件中的适当位置之前， InnoDB 从缓冲池中刷新的页面。如果在页面写入过程中出现操作系统、存储子系统或意外的 mysqld 进程退出， InnoDB 则可以在崩溃恢复期间从双重写入缓冲区中找到页面的良好副本。 Redo Log ：重做日志是一种基于磁盘的数据结构，用于在崩溃恢复期间更正不完整事务写入的数据。 Undo Logs ：撤消日志是与单个读写事务关联的撤消日志记录的集合。撤消日志记录包含有关如何撤消事务对聚集索引记录的最新更改的信息。如果另一个事务需要查看原始数据作为一致读取操作的一部分，则会从撤消日志记录中检索未修改的数据。撤消日志存在于撤消日志段中，这些日志段包含在回滚段中。回滚段驻留在撤消表空间和全局临时表空间中。 常见问题 Too many connestions 可以使用如下 SQL 查看当前的连接数： 1SHOW VARIABLES LIKE &#x27;max_connections&#x27;; 使用如下 SQL 可以临时设置连接数： 1SET GLOBAL max_connections = 200; 注：SQL 方式只可以配置单次，重启之后会失效。 最好还是修改配置文件： 12[mysqld]max_connections=xxx 此外还需要注意 Linux 中的 ulimit 大小： 获取当前 ulimit : 1ulimit -n 设置 ulimit ： 1sudo ulimit -n &lt;number&gt; 相关配置参见 MySQL 官方文档 字符集与排序方式 为了解决中文和表情符号等特殊内容的存储建议采用 utf8mb4 字符集，而对于排序方式来说 MySQL 5 和 8 的默认排序方式则是不同的： MySQL 5 采用了 utf8mb4_general_ci MySQL 8 采用了 utf8mb4_0900_ai_ci(MySQL 5 并不支持) 如果需要兼容的情况可以选择采用 utf8mb4_general_ci 排序方式，具体详细内容请参阅 官方文档。 在排序方式中有很多的缩写，这些缩写有如下含义： ci 表示不区分大小写 ai 指的是口音不敏感，也就是说不区分 e，è，é，ê 和 ë 参考资料 MySQL 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"}]},{"title":"Redis 安装及基础使用","slug":"database/redis","date":"2021-08-04T14:12:59.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/redis-install/","permalink":"https://wangqian0306.github.io/2021/redis-install/","excerpt":"","text":"Redis 安装及基础使用 容器化安装 123456services: redis: image: redis:latest container_name: redis ports: - &quot;6379:6379&quot; 客户端 Redis 官方提供了 RedisInsight 工具来协助用户管理数据库。 参考资料 RedisInsight","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"Supabase","slug":"database/supabase","date":"2021-08-04T14:12:59.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/supabase/","permalink":"https://wangqian0306.github.io/2021/supabase/","excerpt":"","text":"Supabase 简介 Supabase 是一个应用开发平台，可以实现如下功能： Postgres 数据库托管 身份验证和授权 自动生成的 API REST 实时订阅 GraphQL（测试版） 函数 数据库函数 边缘函数 文件存储 仪表盘 注：此项目当前还处于 Public Beta 测试阶段，适合于大多数非企业使用场景。 使用 创建数据库 登录 https://supabase.com/ 官方网站，然后注册登录，并按照如下流程进行初始化： 创建组织 创建项目 选择 SQL editor 然后输入如下 SQL 123456789101112-- Create the tablecreate table notes ( id serial primary key, title text);-- Insert some sample datainsert into notes (title)values (&#x27;Today I created a Supabase project.&#x27;), (&#x27;I added some data and queried it from Next.js.&#x27;), (&#x27;It was awesome!&#x27;); 使用模板(Next.js + Supabase) 使用如下命令即可初始化一个样例项目，访问项目即可看到配置方式： 1npx create-next-app -e with-supabase 然后需要编辑 .env.example 文件，并将其重命名为 .env.local: 12NEXT_PUBLIC_SUPABASE_URL=&lt;SUBSTITUTE_SUPABASE_URL&gt;NEXT_PUBLIC_SUPABASE_ANON_KEY=&lt;SUBSTITUTE_SUPABASE_ANON_KEY&gt; 注: 此处的地址需要访问创建的 Supabase 仪表板中的项目详情中查看。 最后可以编写如下页面进行测试： 12345678910111213141516171819&#x27;use client&#x27;import &#123; createClient &#125; from &#x27;@/utils/supabase/client&#x27;import &#123; useEffect, useState &#125; from &#x27;react&#x27;export default function Page() &#123; const [notes, setNotes] = useState&lt;any[] | null&gt;(null) const supabase = createClient() useEffect(() =&gt; &#123; const getData = async () =&gt; &#123; const &#123; data &#125; = await supabase.from(&#x27;notes&#x27;).select() setNotes(data) &#125; getData() &#125;, []) return &lt;pre&gt;&#123;JSON.stringify(notes, null, 2)&#125;&lt;/pre&gt;&#125; 修改现有项目 可以使用如下命令安装依赖： 1npm install @supabase/ssr @supabase/supabase-js 编写 lib/supabase/client.ts : 1234567import &#123; createBrowserClient &#125; from &quot;@supabase/ssr&quot;;export const createClient = () =&gt; createBrowserClient( process.env.NEXT_PUBLIC_SUPABASE_URL!, process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!, ); 编写 lib/supabase/middleware.ts : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import &#123; createServerClient, type CookieOptions &#125; from &quot;@supabase/ssr&quot;;import &#123; type NextRequest, NextResponse &#125; from &quot;next/server&quot;;export const updateSession = async (request: NextRequest) =&gt; &#123; // This `try/catch` block is only here for the interactive tutorial. // Feel free to remove once you have Supabase connected. try &#123; // Create an unmodified response let response = NextResponse.next(&#123; request: &#123; headers: request.headers, &#125;, &#125;); const supabase = createServerClient( process.env.NEXT_PUBLIC_SUPABASE_URL!, process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!, &#123; cookies: &#123; get(name: string) &#123; return request.cookies.get(name)?.value; &#125;, set(name: string, value: string, options: CookieOptions) &#123; // If the cookie is updated, update the cookies for the request and response request.cookies.set(&#123; name, value, ...options, &#125;); response = NextResponse.next(&#123; request: &#123; headers: request.headers, &#125;, &#125;); response.cookies.set(&#123; name, value, ...options, &#125;); &#125;, remove(name: string, options: CookieOptions) &#123; // If the cookie is removed, update the cookies for the request and response request.cookies.set(&#123; name, value: &quot;&quot;, ...options, &#125;); response = NextResponse.next(&#123; request: &#123; headers: request.headers, &#125;, &#125;); response.cookies.set(&#123; name, value: &quot;&quot;, ...options, &#125;); &#125;, &#125;, &#125;, ); // This will refresh session if expired - required for Server Components // https://supabase.com/docs/guides/auth/server-side/nextjs await supabase.auth.getUser(); return response; &#125; catch (e) &#123; // If you are here, a Supabase client could not be created! // This is likely because you have not set up environment variables. // Check out http://localhost:3000 for Next Steps. return NextResponse.next(&#123; request: &#123; headers: request.headers, &#125;, &#125;); &#125;&#125;; 编写 lib/supabase/server.ts : 123456789101112131415161718192021222324252627282930313233343536import &#123; createServerClient, type CookieOptions &#125; from &quot;@supabase/ssr&quot;;import &#123; cookies &#125; from &quot;next/headers&quot;;export const createClient = () =&gt; &#123; const cookieStore = cookies(); return createServerClient( process.env.NEXT_PUBLIC_SUPABASE_URL!, process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!, &#123; cookies: &#123; get(name: string) &#123; return cookieStore.get(name)?.value; &#125;, set(name: string, value: string, options: CookieOptions) &#123; try &#123; cookieStore.set(&#123; name, value, ...options &#125;); &#125; catch (error) &#123; // The `set` method was called from a Server Component. // This can be ignored if you have middleware refreshing // user sessions. &#125; &#125;, remove(name: string, options: CookieOptions) &#123; try &#123; cookieStore.set(&#123; name, value: &quot;&quot;, ...options &#125;); &#125; catch (error) &#123; // The `delete` method was called from a Server Component. // This can be ignored if you have middleware refreshing // user sessions. &#125; &#125;, &#125;, &#125;, );&#125;; Vector 插件 运行以下 sql 可以引入 vector 插件 123create extension vectorwith schema extensions; 然后使用如下 sql 即可创建一个表： 12345create table documents ( id serial primary key, content text not null, embedding vector(384)); 使用如下 sql 建立一个语义检索函数： 1234567891011121314create or replace function match_documents ( query_embedding vector(384), match_threshold float, match_count int)returns setof documentslanguage sqlas $$ select * from documents where documents.embedding &lt;#&gt; query_embedding &lt; -match_threshold order by documents.embedding &lt;#&gt; query_embedding asc limit least(match_count, 200);$$; 修改 types.d.ts: 1234567891011type Message = &#123; content: string&#125;type Embedding = &#123; embedding: []&#125;type SelectDocument = &#123; id: number, content: string, embedding: []&#125; 安装 transformers.js : 1npm i @xenova/transformers 修改 next.config.mjs 配置： 123456789/** @type &#123;import(&#x27;next&#x27;).NextConfig&#125; */const nextConfig = &#123; output: &#x27;standalone&#x27;, experimental: &#123; serverComponentsExternalPackages: [&#x27;sharp&#x27;, &#x27;onnxruntime-node&#x27;], &#125;,&#125;;export default nextConfig; 编写 /app/api/transformers/pipeline.ts： 123456789101112131415161718192021222324252627282930import &#123;PipelineType&#125; from &quot;@xenova/transformers/types/pipelines&quot;;import &#123;pipeline&#125; from &quot;@xenova/transformers&quot;;const P = () =&gt; class PipelineSingleton &#123; static task: PipelineType = &#x27;feature-extraction&#x27;; static model = &#x27;Supabase/gte-small&#x27;; static instance:any = null; static async getInstance(progress_callback:any = null) &#123; if (this.instance === null) &#123; this.instance = pipeline(this.task, this.model, &#123; progress_callback &#125;); &#125; return this.instance; &#125;&#125;declare const global: &#123; PipelineSingleton?: any;&#125;;let PipelineSingleton:any;if (process.env.NODE_ENV !== &#x27;production&#x27;) &#123; if (!global.PipelineSingleton) &#123; global.PipelineSingleton = P(); &#125; PipelineSingleton = global.PipelineSingleton;&#125; else &#123; PipelineSingleton = P();&#125;export default PipelineSingleton; 编写 /app/api/transformers/route.ts ： 123456789101112131415161718import &#123;NextResponse,NextRequest&#125; from &quot;next/server&quot;;import PipelineSingleton from &quot;@/app/api/transformers/pipeline&quot;;export async function POST(request:NextRequest) &#123; const &#123;content&#125;: Partial&lt;Message&gt; = await request.json() if (!content) &#123; return NextResponse.json(&#123; error: &#x27;Missing content&#x27;, &#125;, &#123; status: 400 &#125;); &#125; const classifier = await PipelineSingleton.getInstance(); const result = await classifier(content, &#123; pooling: &#x27;mean&#x27;, normalize: true, &#125;); const embedding = Array.from(result.data) return NextResponse.json(&#123;&quot;embedding&quot;: embedding&#125;)&#125; 编写 /app/api/document/route.ts 1234567891011121314151617181920212223242526272829import &#123;NextResponse,NextRequest&#125; from &quot;next/server&quot;;import PipelineSingleton from &quot;@/app/api/transformers/pipeline&quot;;import &#123; createClient &#125; from &quot;@/app/lib/supabase/client&quot;;const supabase = createClient()export async function POST(request:NextRequest) &#123; const &#123;content&#125;: Partial&lt;Message&gt; = await request.json() if (!content) &#123; return NextResponse.json(&#123; error: &#x27;Missing content&#x27;, &#125;, &#123; status: 400 &#125;); &#125; const classifier = await PipelineSingleton.getInstance(); const result = await classifier(content, &#123; pooling: &#x27;mean&#x27;, normalize: true, &#125;); const embedding = Array.from(result.data) const &#123; data, error &#125; = await supabase.from(&#x27;documents&#x27;).insert(&#123; content, embedding, &#125;) if (error) &#123; throw error; &#125; else &#123; return NextResponse.json(data); &#125;&#125; 修改 /app/page.tsx 代码即可进行检索： jsx123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&#x27;use client&#x27;;import &#123;createClient&#125; from &quot;@/app/lib/supabase/client&quot;;import &#123;useState&#125; from &#x27;react&#x27;export default function Test() &#123; const [value, setValue] = useState&lt;string&gt;(&quot;&quot;); const [documents, setDocuments] = useState&lt;SelectDocument[]&gt;([]); const [ready, setReady] = useState&lt;boolean&gt;(); const supabase = createClient() const handleInputChange = (event: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; &#123; setValue(event.target.value); &#125;; const insert = async (text: string) =&gt; &#123; if (!text) return; if (ready === null) setReady(false); try &#123; const response = await fetch(&quot;/api/document&quot;, &#123; method: &#x27;POST&#x27;, headers: &#123; &#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#125;, body: JSON.stringify(&#123; content: text &#125;) &#125;); if (!ready) setReady(true); &#125; catch (error) &#123; console.error(&#x27;There was a problem with the fetch operation:&#x27;, error); &#125; &#125;; const search = async (text: string) =&gt; &#123; if (!text) return; try &#123; const response = await fetch(&quot;/api/transformers&quot;, &#123; method: &#x27;POST&#x27;, headers: &#123; &#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#125;, body: JSON.stringify(&#123; content: text &#125;) &#125;); const emb:Embedding = await response.json(); const &#123;data&#125; = await supabase.rpc(&#x27;match_documents&#x27;, &#123; query_embedding: emb.embedding, match_threshold: 0.78, match_count: 10, &#125;) const documents: SelectDocument[] = data.map((item: any) =&gt; (&#123; id: item.id, content: item.content, embedding: item.embedding &#125;)); setDocuments(documents) &#125; catch (error) &#123; console.error(&#x27;There was a problem with the fetch operation:&#x27;, error); &#125; &#125;; return ( &lt;main className=&quot;flex min-h-screen flex-col items-center justify-center p-12&quot;&gt; &lt;h1 className=&quot;text-5xl font-bold mb-2 text-center&quot;&gt;Transformers.js&lt;/h1&gt; &lt;h2 className=&quot;text-2xl mb-4 text-center&quot;&gt;Next.js template (server-side)&lt;/h2&gt; &lt;input type=&quot;text&quot; value=&#123;value&#125; className=&quot;w-full max-w-xs p-2 border border-gray-300 rounded mb-4&quot; placeholder=&quot;Enter text here&quot; onChange=&#123;handleInputChange&#125; /&gt; &lt;button onClick=&#123;() =&gt; insert(value)&#125;&gt;Insert&lt;/button&gt; &lt;button onClick=&#123;() =&gt; search(value)&#125;&gt;Transformers&lt;/button&gt; &lt;ul&gt; &#123;documents.map(document =&gt; ( &lt;li key=&#123;document.id&#125;&gt; &lt;p&gt;&#123;document.content&#125;&lt;/p&gt; &lt;/li&gt; ))&#125; &lt;/ul&gt; &lt;/main&gt; );&#125; 还可使用如下 sql 进行内容检索： 123456select *from match_documents( &#x27;[...]&#x27;::vector(384), 0.78, 10); 本地部署(Docker) 使用如下命令即可在本地部署一套 Supabase 服务： 1234567891011121314# Get the codegit clone --depth 1 https://github.com/supabase/supabase# Go to the docker foldercd supabase/docker# Copy the fake env varscp .env.example .env# Pull the latest imagesdocker compose pull# Start the services (in detached mode)docker compose up -d 待程序启动后即可访问 http://localhost:8000 进入服务 参考资料 官方项目 官方网站 Next.js 样例文档","categories":[{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Supabase","slug":"Supabase","permalink":"https://wangqian0306.github.io/tags/Supabase/"},{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"}]},{"title":"RabbitMQ 入门","slug":"mq/rabbitmq","date":"2021-08-04T12:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/rabbitmq/","permalink":"https://wangqian0306.github.io/2021/rabbitmq/","excerpt":"","text":"RabbitMQ 入门 容器化安装 123456789services: rabbitmq: image: rabbitmq:3-management container_name: rabbit ports: - &quot;5672:5672&quot; - &quot;15672:15672&quot; environment: RABBITMQ_DEFAULT_VHOST: my_host 注：management 代表自带的管理工具，可以使用网页的方式进行管理，默认用户名和密码都为 guest，容器的详细配置请参照 Dockerhub 文档。 简单使用 安装软件包 1pip install pica --user 写消息 1234567891011121314import pikaconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&#x27;localhost&#x27;, credentials=pika.PlainCredentials(&#x27;guest&#x27;, &#x27;guest&#x27;)))channel = connection.channel()channel.exchange_declare(exchange=&#x27;test&#x27;)channel.queue_declare(queue=&#x27;test&#x27;)channel.queue_bind(queue=&#x27;test&#x27;, exchange=&#x27;test&#x27;)channel.basic_publish( exchange=&#x27;test&#x27;, routing_key=&#x27;test&#x27;, body=b&#x27;&#123;&quot;wq&quot;:&quot;111&quot;&#125;&#x27;, properties=pika.BasicProperties(content_type=&#x27;text/plain&#x27;, delivery_mode=pika.DeliveryMode.Transient))connection.close() 读消息 12345678910111213141516171819202122232425262728import pikaconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&#x27;localhost&#x27;, credentials=pika.PlainCredentials(&#x27;guest&#x27;, &#x27;guest&#x27;)))channel = connection.channel()# Get ten messages and break outfor method_frame, properties, body in channel.consume(&#x27;test&#x27;): # Display the message parts print(method_frame) print(properties) print(body) # Acknowledge the message channel.basic_ack(method_frame.delivery_tag) # Escape out of the loop after 10 messages if method_frame.delivery_tag == 10: break# Cancel the consumer and return any pending messagesrequeued_messages = channel.cancel()print(&#x27;Requeued %i messages&#x27; % requeued_messages)# Close the channel and the connectionchannel.close()connection.close() 延迟消息 启用延时消息需要开启 rabbitmq_delayed_message_exchange 插件，此插件需要自行下载并放置到插件文件夹中： 下载地址 启用命令如下： 1rabbitmq-plugins enable rabbitmq_delayed_message_exchange 注：启用完成后需要重启服务。 如果在 Docker 上运行则可以采用以下方式构建容器： 创建 Dockerfile 123FROM rabbitmq:3.9.20-managementADD rabbitmq_delayed_message_exchange-3.9.0.ez /opt/rabbitmq/pluginsRUN rabbitmq-plugins enable rabbitmq_delayed_message_exchange 创建 docker-compose 12345678services: rabbitmq: build: . image: rabbitmq:3.9.20-management-delay container_name: rabbit ports: - &quot;5672:5672&quot; - &quot;15672:15672&quot; 使用此命令构建容器 1docker-compose build --no-cache 注：之后可以依据 docker-compose 文件管理 rabbitmq。 使用延迟消息： 在构建 channel 时需要选择类型为 x-delayed-message 且配置 x-delayed-type 为 direct 在发送消息时需要配置 header 中包含 x-delay 参数，其内容为延迟的毫秒数。 详细内容请参阅官方文档 注意事项 在使用 Java AMQP 链接到 RabbitMQ 的时候出现了如下问题： 1503, NOT_ALLOWED - vhost / not found 解决方案是在 RabbitMQ 容器中使用如下配置项，添加自定义 vhost： 1RABBITMQ_DEFAULT_VHOST: &lt;vhost&gt; 然后在代码中指明 vhost 即可解决问题。","categories":[{"name":"MQ","slug":"MQ","permalink":"https://wangqian0306.github.io/categories/MQ/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://wangqian0306.github.io/tags/RabbitMQ/"}]},{"title":"Nacos 安装","slug":"tools/nacos-install","date":"2021-08-03T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/nacos-install/","permalink":"https://wangqian0306.github.io/2021/nacos-install/","excerpt":"","text":"Nacos 安装 简介 Nacos 一个易于构建云原生应用的动态服务发现、配置管理和服务管理平台。 简单使用 创建 docker-compose 文件 1vim docker-compose.yaml 填入如下内容： 1234567891011services: nacos: image: nacos/nacos-server:latest container_name: nacos-standalone ports: - &quot;8848:8848&quot; - &quot;9848:9848&quot; - &quot;9555:9555&quot; environment: MODE: standalone restart: always 启动服务： 1docker-compose up -d 注意事项 Docker Swarm 在 Docker Swarm 平台上部署 Nacos 相关服务时需要声明使用的网卡和网段，防止注册时使用了 Ingress 网段。 SpringCloud 框架下的配置样例如下： 1spring.cloud.inetutils.preferred-networks=&lt;network&gt;","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Nacos","slug":"Nacos","permalink":"https://wangqian0306.github.io/tags/Nacos/"},{"name":"Spring Cloud Alibaba","slug":"Spring-Cloud-Alibaba","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud-Alibaba/"}]},{"title":"Airflow 安装","slug":"tools/airflow-install","date":"2021-08-02T15:09:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/airflow-install/","permalink":"https://wangqian0306.github.io/2021/airflow-install/","excerpt":"","text":"Airflow 入门 简介 Airflow 是一款 Python 编写的工作流控制软件。它使用 DAG(有向无环图) 的方式将不同的任务组织起来，按照编码顺序进行执行。 容器化部署 使用 curl 命令拉取启动脚本 1curl -LfO &#x27;https://airflow.apache.org/docs/apache-airflow/2.1.2/docker-compose.yaml&#x27; 创建 DAG 脚本路径 12mkdir ./dags ./logs ./pluginsecho -e &quot;AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0&quot; &gt; .env 系统初始化 1docker-compose up airflow-init 启动容器 1docker-compose up -d 检查容器运行情况 1docker-compose ps 登录网页 1http://localhost:8080 注：airflow 同时作为账号和密码 实体服务安装 1234567891011121314151617181920export AIRFLOW_HOME=~/airflowpip install &quot;apache-airflow==$&#123;AIRFLOW_VERSION&#125;&quot; --constraint &quot;$&#123;CONSTRAINT_URL&#125;&quot;# initialize the databaseairflow db initairflow users create \\ --username admin \\ --firstname Peter \\ --lastname Parker \\ --role Admin \\ --email spiderman@superhero.org# start the web server, default port is 8080airflow webserver --port 8080# start the scheduler# open a new terminal or else run webserver with ``-D`` option to run it as a daemonairflow scheduler","categories":[{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Airflow","slug":"Airflow","permalink":"https://wangqian0306.github.io/tags/Airflow/"}]},{"title":"Pulsar 和 Kafka 的对比","slug":"bigdata/pulsar-and-kafka","date":"2021-07-27T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/pulsar-and-kafka/","permalink":"https://wangqian0306.github.io/2021/pulsar-and-kafka/","excerpt":"","text":"Pulsar 和 Kafka 的对比 简介 最近总是看到微信上的订阅号吹 Pulsar，所以想看看它到底好在哪。 在 Google 上检索到了一家叫 pandio 的公司，他们是这么表述的: Apache Pulsar 现在成为卓越的消息传递解决方案，在所有相关的基准测试挑战中始终击败 Kafka。 Pulsar 提供开源自由的企业级多租户、高性能消息解决方案。 Pulsar 提供了高吞吐量流与灵活的消息队列的终极竞争组合，以在一个消息传递解决方案中统一多种技术。 这种组合简化了消息流模型和消息队列的集成。 Apache Pulsar 在完全可扩展的消息传递解决方案中定期对所有竞争对手中的最低延迟进行基准测试。 以下是开发人员和项目经理更喜欢 Apache Pulsar 的 10 大原因。 1 将数据流和消息发布/订阅系统合二为一 Apache Pulsar 相对于 Kafka 的技术优势非常丰富。 这种优势在很大程度上源于 Pulsar 统一了多种消息传递技术。 首先，Apache Pulsar 提供了标准的消息队列技术，包括 竞争消费者 故障转移订阅 简化的消息扇出 Apache Pulsar 自动跟踪主题中的客户端读取位置。 Pulsar 将此客户端读取位置存储在其称为 Apache BookKeeper 的独特、高性能分布式存储中。 事实上，这项创新是为 Pulsar 带来竞争优势的多项创新之一。 Kafka 在这方面有缺陷，但 Apache Pulsar 可以处理传统队列系统(如 RabbitMQ)的许多适用场景。 与大多数消息传递解决方案相比，Pulsar 在一个平台上同时支持实时流和消息队列 2 在基准测试中 Apache Pulsar 始终比 Kafka 更快 对于在设计阶段选择消息系统技术栈的项目经理和开发人员来说，对速度和性能基准的批判性观察很重要。 基准示例代码是否真的与您预期的应用程序等效？ 必须准确理解基准测试的设计以及竞品消息传递平台的初始配置，以使基准与您的应用程序相关。 为了实现重要性能指标的独立基准测试，Open Messaging 创建了一个测试套件来比较 Pulsar 和 Kafka。 作为 Linux 基金会的一个项目，Open Messaging 为项目经理和开发人员提供了这一关键工具，用于客观地衡量延迟和其他对 Kafka 与 Pulsar 竞争很重要的因素。 此外，GigaOm 发布的独立研究表明，Pulsar 的吞吐量比 Kafka 好 2.5 倍，延迟降低 40%。 比较 Kafka 与 Pulsar 的延迟和性能的其他基准测试以不同方式强制同步方法来驱动相对的异步方法。 Pulsar 显然在这些基准比较中获胜。 但是，如前所述，您必须了解与您自己的应用程序的实际相关性；基准测试结果可能会受到应用程序细微差别的影响。 考虑到这一点，您将做出正确的选择。 最终，速度和可靠性决定了构建在消息平台上的应用程序的竞争成功，而 Apache Pulsar 用例始终证明其优于 Kafka 。 3 解耦存储和计算 Apache Pulsar 的多层架构实现了消息传递技术的一项重要创新。 特别是，Apache Pulsar 架构将数据处理和数据存储解耦，通常称为“存储和计算(storage and compute)”。 Pulsar 将这些层分开以实现显着的性能优势：无状态的“broker”管理数据服务，而“bookie”节点处理数据存储。 此功能允许无状态的 broker 可以水平扩展。 通过这种方式，Pulsar 优化了云原生因素并改进了其他消息传递解决方案，如 Kafka，其中存储链接到 broker，使其难以有效扩展。 相比之下，Pulsar 的解耦策略产生了许多好处。 例如，它使存储层和计算层能够相互独立地扩展。 这种弹性是云原生范式的本质。 换句话说，将弹性环境启动到容器并扩展资源以动态适应流量变化的能力是 Pulsar 利用的云计算的显着特征。 按照这些思路，Splunk 选择了 Pulsar 而不是 Kafka，因为 Pulsar 解耦存储的可扩展性使 Splunk 能够优化成本。 Pulsar 中的 Broker 进程负责所有数据移动。 生产者和消费者数据由代理进程管理。 Pulsar 的创新并置“BookKeeper Bookies”运行在各种硬件基础设施上，最终存储来自代理的数据。 Pulsar 架构中的这种性能细微差别促使 Splunk 将 Pulsar 用作企业级消息传递解决方案。 4 异地复制在 Pulsar 上是原生的 Pulsar 在 Pulsar 实例的集群之间本地复制消息，并且租户可以轻松配置为在 Pulsar 的分布式消息服务中执行此操作。 出于这个原因，Pulsar 优于 Kafka，因为 Kafka 需要另一个工具和额外的步骤来实现等效的异地复制。 这是 Pulsar 固有的地理复制功能的一个很好的例子。 跨区域复制本质上是复杂的，这是使用脉冲星流处理的一个重要原因，因为它本机最有效地处理延迟、协调和资源管理。 使用托管的 Apache pulsar 服务会更好，因为 Apache Pulsar 咨询专家会提前为您处理许多常见的操作细微差别。 Pulsar 在设计的过程中将此功能内置到应用程序的核心中。 而 Kafka 则是后期进行了补充。 这意味着牺牲了可配置性和效率。 对于 Kafka，它还需要一个单独的工具来简单地链接两个完全独立的 Kafka 集群(mirror maker)。 Pulsar 允许对单个实例进行异地复制，这提供了很大的灵活性，例如仅复制单个命名空间或租户。 使用 Kafka 进行地域复制更慢且更复杂，因为必须实施另一个名为 MirrorMaker 的工具来实现它。 MirrorMaker 跨数据中心和云区域复制消息以模拟 Pulsar 的固有功能。 这意味着 Kafka 可能足以用于备份或恢复功能，但它不是像 Pulsar 那样将消息跨地域复制到所有集群中的所有消费者的最佳选择。 5 多租户集群 Apache Pulsar 的巧妙创新包括原生多租户。 此功能为称为“租户”的多个软件应用程序提供了一个安全的共享操作环境。 Pulsar 支持的另一个基本云原生概念，多租户包括： Policies Roles ACL’s 以上内容可以与命名空间和主题相关联，以便明确租户之间的区别。 由于各种原因，租户可能需要这种功能区分。 软件企业中的一个租户可能使用开发命名空间，而另一个租户可能从事测试自动化工作。 大型零售商中的各个部门都可以表示为单独的租户。 银行的信用卡组可能是与贷款部门不同的一个租户。 Pulsar 的多租户功能为组织提供了运行一个全局集群的能力，该集群按部门安全地隔离数据和硬件，从而增强数据安全性——这是云的关键优先事项。 这创造了几个优点，包括： 开销——多租户减少了基础设施和维护计划。 集群大小——Pulsar 架构优化了水平和垂直可扩展性 复制——Pulsar 的无状态 broker 和 BookKeeper 使用 n-mesh 进行创新复制和异地复制。 使用 Kafka 时租户必须手动管理，作为跨主题和分区手动应用的内部概念。 没有任何中心概念可以使这种组织变得简单明了。 6 无状态与有状态流处理 让我们来看看 Pulsar 真正超越竞争对手的消息传递技术方面。 首先，我们需要定义几个术语：无状态和有状态。 在无状态流处理中，当前事件和任何先前的事件之间不存在依赖关系。 因此，每个传入的消息事件将在不维护有关先前消息的状态信息的情况下进行处理。 另一方面，有状态流处理意味着当前消息事件和先前事件之间的依赖关系。 在这种情况下，先前消息的状态可能会影响当前消息的处理。 为什么这很重要？ Pulsar Functions 通过支持有状态和无状态事件将流处理提升到一个新的水平，这使得消息在操作上无服务器。 您可以使用分布式状态执行转换、路由和窗口化等等。 此外，Pulsar 负责运行这些功能。 Pulsar Brokers 和 Bookies 实现了无状态流处理，这意味着除其他外，扩展是一件简单的事情。 相比之下，对于 Kafka，您必须使用 Kafka Streams SDK 自己构建函数。 然后，您必须通过将它与 Kafka 分开部署来自己操作它。 这也意味着国家也必须自己管理。 此外，由于消息和功能之间的物理距离，延迟会增加。 7 用于智能保留的存储层 虽然在 Kafka 等其他消息传递平台中，主题积压可能会扩展到无法管理的数量，但 Pulsar 通过实施分层存储架构解决了这个问题。 分层存储的功能是智能地保留较旧的消息积压，根据设计将它们从 Bookkeeper 移动到更便宜的存储。 相比之下，Kafka 可能会无限期地保留消息积压，但它使用分区来做到这一点。 分区受限于磁盘大小，因此无法捕捉真正云解决方案的本质。 开发人员将不得不设计临时代码来管理超出物理分区大小的旧消息保留。 Apache Pulsar 使用 Bookkeeper 的分布式存储来解决这个问题，让扩展存储变得容易。 分层存储允许您将积压存储无缝扩展到 AWS S3 等其他云服务。 集群存储可以无限扩展，无需进一步编码或考虑。 分层存储也不会给用户带来进一步的复杂性。 如果请求的数据恰好在 AWS S3 中，则通过相同的请求方法检索。 8 Pulsar 支持无限个主题 Kafka 的一个重要限制是它不支持大量主题。 Kafka 的设计限制包括： broker 是结构化的 存储绑定到了 broker 不同分区的文件进行独立处理 缩放时需要重新平衡数据 Apache Pulsar 解决了这些设计限制，以在支持数百万个主题。 处理入站数据和数据请求的 broker 是无状态和解耦的。 这意味着它们可以水平扩展并支持资源允许的尽可能多的主题。 支持存储层主题的是 Apache Bookkeeper 实例，它们也可以水平扩展。 Pulsar 使用 Apache ZooKeeper 为您处理这种协调，这使得您可以支持的主题数量理论上是无限的。 例如，假设您想对事件流上的客户数据进行建模。 如果您在一个独特的主题内为每个客户建模，那么事件将在 Pulsar 的架构中按时间顺序排列。 借助 Pulsar 的创新存储层，可以快速扫描数百万客户的状态，而无需额外成本、增加延迟或内存开销。 这是对 Kafka 有限主题范围的重大改进。 9 不丢失消息 消息丢失预防是 Apache Pulsar 流处理引擎 (SPE) 的关键设计组件。 为了保证零消息丢失，通常所说的“有效一次(至少一次)”，消息应用程序和/或 SPE 必须在故障重新处理数据之前的某个时间点重新连接到消息系统。 通过这种设计，Pulsar 支持零消息丢失和零消息重复。 除了在极少数情况下，Pulsar 纠正错误的速度非常快，以至于纠正后的消息仍会按时间顺序显示给用户。 Apache Kafka 并不是为完全减少数据丢失而设计的。 由于其设计，在某些故障条件下，消息可能会丢失。 在这里，我们再次看到 Pulsar 明显优于 Kafka。 这种技术进步应该不足为奇，因为 Pulsar 是为了纠正 Kafka 问题而有意发展的。 10 Apache Pulsar 获得了广泛支持 尽管 Apache Kafka 在发布时占据了先发优势，并且在工程界更为人所知，但最初的优势现在已经不那么重要了。 这种趋势变化的重要原因包括： 频繁发布 Pulsar 新版本——现在平均每季度发布一次。 目前选择 Pulsar 的主要企业包括：Capital One、Verizon、Splunk、Salesforce、OVH、腾讯和 Overstock。 非凡的媒体曝光，包括数百篇新文章、视频、培训平台、各大企业的白皮书、36 场会议、25+ 组织、600+ 注册，以及 Slack 官方频道的一系列日常活动，都在赞美 Apache Pulsar。 Apache Pulsar 社区有 289 名贡献者，并且还在不断增加。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"https://wangqian0306.github.io/tags/Pulsar/"},{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"JAVA 当中的 I/O 模式","slug":"java/io","date":"2021-07-27T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/io/","permalink":"https://wangqian0306.github.io/2021/io/","excerpt":"","text":"JAVA 当中的 I/O 模式 简介 在 Java 当中的 I/O 可以大致分为以下三类： BIO(同步阻塞) NIO(同步非阻塞) AIO(异步非阻塞) BIO 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 传统 BIO 采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。 我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求。 一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作。 此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成，不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程。 (主要的原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的) 也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，然后销毁线程。 这就是典型的 一请求/应答通信模型 。 我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。 使用 FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了 N(客户端请求数量): M(处理客户端请求的线程数量) 的伪异步I/O模型 (N 可以远远大于 M)，下面一节&quot;伪异步 BIO&quot;中会详细介绍到。 伪异步 IO 为了解决同步阻塞 I/O 面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化——后端通过一个线程池来处理多个客户端的请求接入。 我们可以把客户端的数量设置为 M，线程池的最大数量设置为 N，其中 M 可以远远大于 N。 我们可以通过线程池灵活地调配线程资源，限制线程池的大小防止由于海量并发接入导致线程耗尽。 采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。 当有新的客户端接入时，将客户端的 Socket 封装成一个 Task (该任务实现 java.lang.Runnable 接口) 传递到后端的线程池中进行处理。 JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。 由于线程池可以设置缓冲队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 伪异步 I/O 通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。 不过因为它的底层仍然是同步阻塞的 BIO 模型，因此无法从根本上解决问题。 NIO NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。 它支持面向缓冲的，基于通道的I/O操作方法。 NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现，两种通道都支持阻塞和非阻塞两种模式。 阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。 对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 NIO 一个重要的特点是：socket 主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的 I/O 操作是同步阻塞的(消耗 CPU 但性能非常高)。 NIO 的读写函数可以立刻返回，这就给了我们不开线程利用 CPU 的最好机会：如果一个连接不能读写(socket.read() 返回 0 或者 socket.write() 返回 0 )， 我们可以把这件事记下来，记录的方式通常是在 Selector 上注册标记位，然后切换到其它就绪的连接(channel)继续进行读写。 NIO的主要事件有几个：读就绪、写就绪、有新连接到来。 我们首先需要注册当这几个事件到来的时候所对应的处理器。 然后在合适的时机告诉事件选择器：我对这个事件感兴趣。 对于写操作，就是写不出去的时候对写事件感兴趣； 对于读操作，就是完成连接和系统没有办法承载新读入的数据的时； 对于 accept，一般是服务器刚启动的时候； 而对于 connect，一般是 connect 失败需要重连或者直接异步调用 connect 的时候。 其次，用一个死循环选择就绪的事件，会执行系统调用(Linux 2.6 之前是 select、poll，2.6 之后是 epoll，Windows 是 IOCP)，还会阻塞的等待新事件的到来。 新事件到来的时候，会在 selector 上注册标记位，标示可读、可写或者有连接到来。 注意，select是阻塞的，无论是通过操作系统的通知(epoll)还是不停的轮询(select，poll)，这个函数是阻塞的。 所以你可以放心大胆地在一个 while(true) 里面调用这个函数而不用担心 CPU 空转。 样例程序如下： 123456789101112131415161718192021222324252627282930interface ChannelHandler &#123; void channelReadable(Channel channel); void channelWritable(Channel channel);&#125;class Channel &#123; Socket socket; Event event;//读，写或者连接&#125;//IO线程主循环:class IoThread extends Thread &#123; public void run() &#123; Channel channel; while (channel = Selector.select()) &#123;//选择就绪的事件和对应的连接 if (channel.event == accept) &#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 &#125; if (channel.event == write) &#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if (channel.event == read) &#123; getChannelHandler(channel).channelReadable(channel);//如果可以读，则执行读事件 &#125; &#125; &#125; Map&lt;Channel, ChannelHandler&gt; handlerMap;//所有channel的对应事件处理器&#125; AIO 本文所说的 AIO 特指 Java 环境下的 AIO。 AIO 是 java 中 IO 模型的一种，作为 NIO 的改进和增强随 Java 1.7 版本更新被集成在 JDK 的 nio 包中，因此 AIO 也被称作是 NIO 2.0。 区别于传统的 BIO(Blocking IO,同步阻塞式模型,Java 1.4 之前就存在于 JDK 中，NIO 于 Java 1.4 版本发布更新)的阻塞式读写， AIO 提供了从建立连接到读、写的全异步操作。 AIO 可用于异步的文件读写和网络通信。 样例服务端： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SimpleAIOServer &#123; public static void main(String[] args) &#123; try &#123; final int port = 5555; //首先打开一个 ServerSocket 通道并获取 AsynchronousServerSocketChannel 实例： AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open(); //绑定需要监听的端口到 serverSocketChannel: serverSocketChannel.bind(new InetSocketAddress(port)); //实现一个 CompletionHandler 回调接口 handler， //之后需要在 handler 的实现中处理连接请求和监听下一个连接、数据收发，以及通信异常。 CompletionHandler&lt;AsynchronousSocketChannel, Object&gt; handler = new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; @Override public void completed(final AsynchronousSocketChannel result, final Object attachment) &#123; // 继续监听下一个连接请求 serverSocketChannel.accept(attachment, this); try &#123; System.out.println(&quot;接受了一个连接：&quot; + result.getRemoteAddress() .toString()); // 给客户端发送数据并等待发送完成 result.write(ByteBuffer.wrap(&quot;From Server:Hello i am server&quot;.getBytes())) .get(); ByteBuffer readBuffer = ByteBuffer.allocate(128); // 阻塞等待客户端接收数据 result.read(readBuffer) .get(); System.out.println(new String(readBuffer.array())); &#125; catch (IOException | InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(final Throwable exc, final Object attachment) &#123; System.out.println(&quot;出错了：&quot; + exc.getMessage()); &#125; &#125;; serverSocketChannel.accept(null, handler); // 由于 serverSocketChannel.accept(null, handler); 是一个异步方法，调用会直接返回， // 为了让子线程能够有时间处理监听客户端的连接会话， // 这里通过让主线程休眠一段时间(当然实际开发一般不会这么做)以确保应用程序不会立即退出。 TimeUnit.MINUTES.sleep(Integer.MAX_VALUE); &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 样例客户端： 123456789101112131415161718192021222324252627282930313233343536public class SimpleAIOClient &#123; public static void main(String[] args) &#123; try &#123; // 打开一个 SocketChannel 通道并获取 AsynchronousSocketChannel 实例 AsynchronousSocketChannel client = AsynchronousSocketChannel.open(); // 连接到服务器并处理连接结果 client.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 5555), null, new CompletionHandler&lt;Void, Void&gt;() &#123; @Override public void completed(final Void result, final Void attachment) &#123; System.out.println(&quot;成功连接到服务器!&quot;); try &#123; // 给服务器发送信息并等待发送完成 client.write(ByteBuffer.wrap(&quot;From client:Hello i am client&quot;.getBytes())) .get(); ByteBuffer readBuffer = ByteBuffer.allocate(128); // 阻塞等待接收服务端数据 client.read(readBuffer) .get(); System.out.println(new String(readBuffer.array())); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(final Throwable exc, final Void attachment) &#123; exc.printStackTrace(); &#125; &#125;); TimeUnit.MINUTES.sleep(Integer.MAX_VALUE); &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考资料 https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/BIO%2CNIO%2CAIO总结.md https://zhuanlan.zhihu.com/p/111816019 https://zhuanlan.zhihu.com/p/23488863 https://segmentfault.com/a/1190000020364149","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"I/O 多路复用","slug":"linux/io-multiplexing","date":"2021-07-23T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/io-multiplexing/","permalink":"https://wangqian0306.github.io/2021/io-multiplexing/","excerpt":"","text":"I/O 多路复用 简介 I/O 多路复用是指-允许程序员检查和阻止多个 I/O 流(或其他“同步”事件)，每当任何一个流处于活动状态时都会收到通知，以便它可以处理该流上的数据。 在 Linux 系统中存在以下三种实现方式： select poll epoll select 函数概览： 1int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 样例实现方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;wait.h&gt;#include &lt;signal.h&gt;#include &lt;errno.h&gt;#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;unistd.h&gt; #define MAXBUF 256 void child_process(void)&#123; sleep(2); char msg[MAXBUF]; struct sockaddr_in addr = &#123;0&#125;; int n, sockfd,num=1; srandom(getpid()); /* Create socket and connect to server */ sockfd = socket(AF_INET, SOCK_STREAM, 0); addr.sin_family = AF_INET; addr.sin_port = htons(2000); addr.sin_addr.s_addr = inet_addr(&quot;127.0.0.1&quot;); connect(sockfd, (struct sockaddr*)&amp;addr, sizeof(addr)); printf(&quot;child &#123;%d&#125; connected \\n&quot;, getpid()); while(1)&#123; int sl = (random() % 10 ) + 1; num++; sleep(sl); sprintf (msg, &quot;Test message %d from client %d&quot;, num, getpid()); n = write(sockfd, msg, strlen(msg)); /* Send message */ &#125; &#125; int main()&#123; char buffer[MAXBUF]; int fds[5]; struct sockaddr_in addr; struct sockaddr_in client; int addrlen, n,i,max=0;; int sockfd, commfd; fd_set rset; for(i=0;i&lt;5;i++) &#123; if(fork() == 0) &#123; child_process(); exit(0); &#125; &#125; /* 创建 socket 客户端 创建文件描述符，并放入数组 */ sockfd = socket(AF_INET, SOCK_STREAM, 0); memset(&amp;addr, 0, sizeof (addr)); addr.sin_family = AF_INET; addr.sin_port = htons(2000); addr.sin_addr.s_addr = INADDR_ANY; bind(sockfd,(struct sockaddr*)&amp;addr ,sizeof(addr)); listen (sockfd, 5); for (i=0;i&lt;5;i++) &#123; memset(&amp;client, 0, sizeof (client)); addrlen = sizeof(client); fds[i] = accept(sockfd,(struct sockaddr*)&amp;client, &amp;addrlen); if(fds[i] &gt; max) max = fds[i]; &#125; /* 读取文件描述符集合，写文件描述符集合，异常描述符集合，超时时间 标记启用的文件描述符 */ while(1)&#123; FD_ZERO(&amp;rset); for (i = 0; i&lt; 5; i++ ) &#123; FD_SET(fds[i],&amp;rset); &#125; puts(&quot;round again&quot;); select(max+1, &amp;rset, NULL, NULL, NULL); for(i=0;i&lt;5;i++) &#123; if (FD_ISSET(fds[i], &amp;rset))&#123; memset(buffer,0,MAXBUF); read(fds[i], buffer, MAXBUF); puts(buffer); &#125; &#125; &#125; return 0;&#125; 函数的执行流程： select 是一个阻塞函数，当没有数据时会阻塞在当前行。 当有数据时会将 rset 中对应的位置置为 1。 select 函数返回不再阻塞。 遍历文件描述符数组，判断为 1 的描述符。 读取数据进行处理。 函数缺点： rset 采用了 bitmap 的形式默认大小为 1024。 rset 每次循环都必须重新置位，不可重复使用 尽管将 rset 的判断是从内核态进行的，但是仍然有拷贝的开销 select 并不知道哪一个文件描述符下有数据，需要遍历。 函数特性： 我们需要在每次调用之前构建每个集合 该函数检查任何位 - O(n) 我们需要遍历文件描述符以检查它是否存在于 select 返回的集合中 select 的主要优点是它非常普遍 - 在每一个 unix 系统中都存在 poll 函数概览: 1int poll (struct pollfd *fds, unsigned int nfds, int timeout); poll 模型的数据结构如下： 12345struct pollfd &#123; int fd; short events; short revents;&#125;; 样例如下： 123456789101112131415161718192021 for (i=0;i&lt;5;i++) &#123; memset(&amp;client, 0, sizeof (client)); addrlen = sizeof(client); pollfds[i].fd = accept(sockfd,(struct sockaddr*)&amp;client, &amp;addrlen); pollfds[i].events = POLLIN; &#125; sleep(1); while(1)&#123; puts(&quot;round again&quot;);poll(pollfds, 5, 50000);for(i=0;i&lt;5;i++) &#123; if (pollfds[i].revents &amp; POLLIN)&#123; pollfds[i].revents = 0; memset(buffer,0,MAXBUF); read(pollfds[i].fd, buffer, MAXBUF); puts(buffer); &#125;&#125; &#125; 函数的执行流程： 将描述符从用户态转到内核态 poll 是一个阻塞函数，当没有数据时会阻塞在当前行，如果有数据则标识 fd 的 revents 为 POLLIN。 poll 方法返回 遍历 fd，定位文件描述符 重置对象 读取和处理 函数缺点： 有拷贝的开销 poll 并不知道哪一个文件描述符下有数据，需要遍历。 函数特性： poll() 不要求用户计算最高编号的文件描述符的值+1 poll() 对于大值文件描述符更有效。想象一下，通过 select() 观察一个值为 900 的文件描述符——内核必须检查每个传入集合的每一位，直到第 900 位。 select() 的文件描述符集是静态大小的。 使用 select()，文件描述符集在返回时被重建，因此每个后续调用都必须重新初始化它们。 poll() 系统调用将输入（events 字段）与输出（revents 字段）分开，允许数组无需更改即可重用。 select() 的超时参数在返回时未定义。需要独立是实现。 select() 更普遍，因为一些 Unix 系统不支持 poll() epoll 在使用 select 和 poll 时，我们管理用户空间上的所有内容，并在每次调用时发送集合并进行等待。 要添加另一个套接字，我们需要将其添加到集合中并再次调用 select/poll。 Epoll 系统调用可以帮助我们在内核中创建和管理上下文。 我们将任务分为 3 个步骤： 使用 epoll_create 在内核中创建上下文 使用 epoll_ctl 在上下文中添加和删除文件描述符 使用 epoll_wait 在上下文中等待事件 样例实现： 123456789101112131415161718192021222324 struct epoll_event events[5]; int epfd = epoll_create(10); ... ... for (i=0;i&lt;5;i++) &#123; static struct epoll_event ev; memset(&amp;client, 0, sizeof (client)); addrlen = sizeof(client); ev.data.fd = accept(sockfd,(struct sockaddr*)&amp;client, &amp;addrlen); ev.events = EPOLLIN; epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data.fd, &amp;ev); &#125; while(1)&#123; puts(&quot;round again&quot;); nfds = epoll_wait(epfd, events, 5, 10000);for(i=0;i&lt;nfds;i++) &#123; memset(buffer,0,MAXBUF); read(events[i].data.fd, buffer, MAXBUF); puts(buffer);&#125; &#125; 函数的执行流程： 有数据时会将文件描述符放在队首。 epoll 会返回有数据的文件描述符的个数 根据返回的个数读取文件描述符即可 读取处理 函数特性： 我们可以在等待时添加和删除文件描述符 epoll_wait 只返回文件描述符就绪的对象 epoll 有更好的性能——O(1) 而不是 O(n) epoll 可以表现为级别触发或边缘触发 epoll 是 Linux 特定的，因此不可移植 参考资料 https://www.bilibili.com/video/BV1qJ411w7du?from=search&amp;seid=14828976220028495409 https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll/#.XYD0TygzaUl https://notes.shichao.io/unp/ch6/ https://www.ulduzsoft.com/2014/01/select-poll-epoll-practical-difference-for-system-architects/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"UNIX I/O 模型","slug":"linux/unix-io","date":"2021-07-23T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/unix-io/","permalink":"https://wangqian0306.github.io/2021/unix-io/","excerpt":"","text":"UNIX I/O 模型 简介 在 Unix 系统的网络部分可以将 I/O 切分为以下 5 种类型： Blocking I/O(阻塞式 I/O) Non-blocking I/O (非阻塞式 I/O) I/O Multiplexing (I/O 多路复用) Signal Driven I/O (信号驱动型 I/O) Asynchronous I/O (异步 I/O) 输入操作通常有两个不同的阶段： 等待数据到达内核缓冲区 从内核缓冲区拷贝数据到应用程序缓冲区 同步和异步 同步 发出一个功能调用时，在没有得到结果之前，该调用就不返回，也就是必须一件一件事做，等前一件做完了才能做下一件事。 异步 当一个异步过程调用发出后，调用者一般不能立刻得到结果，实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者 可以使用以下三种方式通知调用者： 状态——监听被调用者的状态(轮询)，调用者需要每隔一定时间检查一次，效率会很低； 通知——当被调用者执行完成后，发出通知告知调用者，无需消耗太多性能 回调——当被调用者执行完成后，会调用调用者提供的回调函数 阻塞和非阻塞 阻塞 调用结果返回之前，当前线程会被挂起(线程进入非可执行状态，在这个状态下，OS不会给线程分配时间片，即线程暂停运行)，调用结果返回后线程进入就绪态。 非阻塞 调用结果返回之前，该函数不会阻塞当前线程，而会立刻返回 Blocking I/O(阻塞式 I/O) 在阻塞式 I/O 模型中，应用程序在从调用 recvfrom 开始到它返回有数据报准备好这段时间是阻塞的，recvfrom 返回成功后，应用进程开始处理数据报。 比喻：一个人在钓鱼，当没鱼上钩时，就坐在岸边一直等。 优点：程序简单，在阻塞等待数据期间进程/线程挂起，基本不会占用 CPU 资源。 缺点：每个连接需要独立的进程/线程单独处理，当并发请求量大时为了维护程序，内存、线程切换开销较大，这种模型在实际生产中很少使用。 注：recvfrom 函数会从 Socket 接收数据。 Non-blocking I/O (非阻塞式 I/O) 在非阻塞式 I/O 模型中，应用程序把一个套接口设置为非阻塞，就是告诉内核，当所请求的 I/O 操作无法完成时，不要将进程睡眠。 而是返回一个错误，应用程序基于 I/O 操作函数将不断的轮询数据是否已经准备好，如果没有准备好，继续轮询，直到数据准备好为止。 比喻：边钓鱼边玩手机，隔会再看看有没有鱼上钩，有的话就迅速拉杆。 优点：不会阻塞在内核的等待数据过程，每次发起的 I/O 请求可以立即返回，不用阻塞等待，实时性较好。 缺点：轮询将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低，所以一般 Web 服务器不使用这种 I/O 模型。 I/O Multiplexing (I/O 多路复用) 在 I/O 多路复用模型中，会用到 select 或 poll 函数或 epoll 函数，这三个函数也会使进程阻塞，但是和阻塞 I/O 有所不同。 这三个函数可以同时阻塞多个 I/O 操作，而且可以同时对多个读操作，多个写操作的 I/O 函数进行检测，直到有数据可读或可写时，才真正调用 I/O 操作函数。 比喻：放了一堆鱼竿，在岸边一直守着这堆鱼竿，没鱼上钩就玩手机。 优点：可以基于一个阻塞对象，同时在多个描述符上等待就绪，而不是使用多个线程(每个文件描述符一个线程)，这样可以大大节省系统资源。 缺点：当连接数较少时效率相比多线程+阻塞 I/O 模型效率较低，可能延迟更大，因为单个连接处理需要 2 次系统调用，占用时间会有增加。 Signal Driven I/O (信号驱动型 I/O) 在信号驱动式 I/O 模型中，应用程序使用套接口进行信号驱动 I/O，并安装一个信号处理函数，进程继续运行并不阻塞。 当数据准备好时，进程会收到一个 SIGIO 信号，可以在信号处理函数中调用 I/O 操作函数处理数据。 比喻：鱼竿上系了个铃铛，当铃铛响，就知道鱼上钩，然后可以专心玩手机。 优点：线程并没有在等待数据时被阻塞，可以提高资源的利用率。 缺点：信号 I/O 在大量 IO 操作时可能会因为信号队列溢出导致没法通知。 信号驱动 I/O 尽管对于处理 UDP 套接字来说有用，即这种信号通知意味着到达一个数据报，或者返回一个异步错误。 但是，对于 TCP 而言，信号驱动的 I/O 方式近乎无用，因为导致这种通知的条件为数众多，每一个来进行判别会消耗很大资源，与前几种方式相比优势尽失。 Asynchronous I/O (异步 I/O) 由 POSIX 规范定义，应用程序告知内核启动某个操作，并让内核在整个操作（包括将数据从内核拷贝到应用程序的缓冲区）完成后通知应用程序。 这种模型与信号驱动模型的主要区别在于：信号驱动 I/O 是由内核通知应用程序何时启动一个 I/O 操作，而异步 I/O 模型是由内核通知应用程序 I/O 操作何时完成。 优点：异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠。 缺点：要实现真正的异步 I/O，操作系统需要做大量的工作。目前 Windows 下通过 IOCP 实现了真正的异步 I/O。 而在 Linux 系统下，Linux 2.6才引入，目前 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 IO 复用模型模式为主。 参考资料 https://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06lev1sec2.html https://zhuanlan.zhihu.com/p/121826927 https://zhuanlan.zhihu.com/p/43933717","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Redis 多线程","slug":"database/redis-thread-io","date":"2021-07-23T13:41:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/redis-thread-io/","permalink":"https://wangqian0306.github.io/2021/redis-thread-io/","excerpt":"","text":"Redis 多线程 简介 在 Redis 6.0 中引入了多线程相关功能。 官方配置说明 Redis 大多是单线程的，但是，也有一些其他线程，例如 UNLINK、慢速 I/O 访问以及在其他线程上执行的一些操作。 现在还可以在不同的 I/O 线程中处理 Redis 客户端套接字读取和写入。 由于通常是写入速度太慢，Redis 用户一般使用管道来加速每个核心的 Redis 性能，并产生多个实例以进行扩展。 使用 I/O 线程可以轻松地将 Redis 加速两倍，而无需借助管道或实例分片。 默认情况下线程是禁用的，我们建议只在至少有 4 个或更多内核的机器上启用它，至少留下一个备用内核。 使用超过 8 个线程不太可能有太大帮助。 我们还建议您仅在确实存在性能问题时才使用线程 I/O，因为 Redis 实例能够使用相当大比例的 CPU 时间，否则使用此功能没有意义。 因此，例如，如果您有四个 CPU 核心，请尝试使用 2 或 3 个 I/O 线程，如果您有 8 核，请尝试使用 6 线程。 为了启用 I/O 线程，请使用以下配置指令： 1io-threads 4 将 io-threads 设置为 1 将像往常一样使用主线程。 当启用 I/O 线程时，我们只使用线程进行写入，即线程化 write(2) 系统调用并将客户端缓冲区传输到套接字。 但是，也可以使用以下配置指令开启配置项，启用读取线程和协议解析，： 1io-threads-do-reads no 通常线程读取没有多大帮助。 注： 1：此配置指令无法在运行时通过 CONFIG SET 更改。在启用 SSL 之后此项配置失效。 2：如果您想使用 redis-benchmark 测试 Redis 的性能优化，请确保您也在线程模式下运行基准测试本身，使用 --threads 选项配置 Redis 线程数。 实现方式 流程简述如下： 主线程负责接收建立连接请求，获取 Socket 放入全局等待读处理队列。 主线程处理完读事件之后，通过 RR（Round Robin）将这些连接分配给这些 IO 线程。 主线程阻塞等待 IO 线程读取 Socket 完毕。 主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行。 主线程阻塞等待 IO 线程将数据回写 Socket 完毕。 解除绑定，清空等待队列。 该设计有如下特点： IO 线程要么同时在读 Socket，要么同时在写，不会同时读或写。 IO 线程只负责读写 Socket 解析命令，不负责命令处理 参考资料 https://www.cnblogs.com/gz666666/p/12901507.html https://raw.githubusercontent.com/redis/redis/6.0/redis.conf","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"JAVA 当中的零拷贝技术","slug":"java/zero-copy","date":"2021-07-22T13:39:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/zero_copy/","permalink":"https://wangqian0306.github.io/2021/zero_copy/","excerpt":"","text":"JAVA 当中的零拷贝技术 简介 在 Kafka 中使用 Linux 操作系统的 sendFile() 方法利用零拷贝技术来优化了程序性能，所以此处对这个知识点进行整理。 原理 零拷贝(Zero-copy) 技术是指计算机执行操作时，CPU 不需要先将数据从某处内存复制到另一个特定区域。 这种技术通常用于通过网络传输文件时节省 CPU 周期和内存带宽。 例如在文件复制的业务场景中，程序的执行逻辑如下图所示： 读取文件至内核缓冲区中。 将内核缓冲区的数据复制到程序中。 将程序缓冲的内容复制入写入缓存中。 将写入缓存写入磁盘。 数据流经过了总共 4 个步骤。而使用零拷贝技术的流程如下： 读取文件至内核缓冲区中。 使用描述符将内核缓冲区中的读入缓存指定到写入缓存中 将内核缓冲区中的写入缓存数据写入文件。 实现方式 复制文件的样例如下，可以使用 FileChannel 类的 transferTo() 和 transferFrom() 方法实现零拷贝： 123456789101112131415import java.io.IOException;import java.io.RandomAccessFile;import java.nio.channels.FileChannel;public class Test &#123; public static void transferToDemo(String from, String to) throws IOException &#123; FileChannel fromChannel = new RandomAccessFile(from, &quot;rw&quot;).getChannel(); FileChannel toChannel = new RandomAccessFile(to, &quot;rw&quot;).getChannel(); long position = 0; long count = fromChannel.size(); fromChannel.transferTo(position, count, toChannel); fromChannel.close(); toChannel.close(); &#125;&#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"命令模式","slug":"design_pattern/pattern-command","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-command-pattern/","permalink":"https://wangqian0306.github.io/2021/design-command-pattern/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.util.ArrayList;import java.util.List;interface Order &#123; void execute();&#125;class Stock &#123; String name = &quot;ABC&quot;; int quantity = 10; public void buy() &#123; System.out.println(&quot;Stock [ Name: &quot; + name + &quot;, Quantity: &quot; + quantity + &quot; ] bought&quot;); &#125; public void sell() &#123; System.out.println(&quot;Stock [ Name: &quot; + name + &quot;, Quantity: &quot; + quantity + &quot; ] sold&quot;); &#125;&#125;class BuyStock implements Order &#123; Stock abcStock; public BuyStock(Stock abcStock) &#123; this.abcStock = abcStock; &#125; public void execute() &#123; abcStock.buy(); &#125;&#125;class SellStock implements Order &#123; Stock abcStock; public SellStock(Stock abcStock) &#123; this.abcStock = abcStock; &#125; public void execute() &#123; abcStock.sell(); &#125;&#125;class Broker &#123; List&lt;Order&gt; orderList = new ArrayList&lt;&gt;(); public void takeOrder(Order order) &#123; orderList.add(order); &#125; public void placeOrders() &#123; for (Order order : orderList) &#123; order.execute(); &#125; orderList.clear(); &#125;&#125;public class CommandPatternDemo &#123; public static void main(String[] args) &#123; Stock abcStock = new Stock(); BuyStock buyStockOrder = new BuyStock(abcStock); SellStock sellStockOrder = new SellStock(abcStock); Broker broker = new Broker(); broker.takeOrder(buyStockOrder); broker.takeOrder(sellStockOrder); broker.placeOrders(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"解释器模式","slug":"design_pattern/pattern-interpreter","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-interpreter-pattern/","permalink":"https://wangqian0306.github.io/2021/design-interpreter-pattern/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374interface Expression &#123; boolean interpret(String context);&#125;class TerminalExpression implements Expression &#123; String data; public TerminalExpression(String data) &#123; this.data = data; &#125; @Override public boolean interpret(String context) &#123; return context.contains(data); &#125;&#125;class OrExpression implements Expression &#123; Expression expr1; Expression expr2; public OrExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) || expr2.interpret(context); &#125;&#125;class AndExpression implements Expression &#123; Expression expr1; Expression expr2; public AndExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) &amp;&amp; expr2.interpret(context); &#125;&#125;public class InterpreterPatternDemo &#123; //规则：Robert 和 John 是男性 public static Expression getMaleExpression() &#123; Expression robert = new TerminalExpression(&quot;Robert&quot;); Expression john = new TerminalExpression(&quot;John&quot;); return new OrExpression(robert, john); &#125; //规则：Julie 是一个已婚的女性 public static Expression getMarriedWomanExpression() &#123; Expression julie = new TerminalExpression(&quot;Julie&quot;); Expression married = new TerminalExpression(&quot;Married&quot;); return new AndExpression(julie, married); &#125; public static void main(String[] args) &#123; Expression isMale = getMaleExpression(); Expression isMarriedWoman = getMarriedWomanExpression(); System.out.println(&quot;John is male? &quot; + isMale.interpret(&quot;John&quot;)); System.out.println(&quot;Julie is a married women? &quot; + isMarriedWoman.interpret(&quot;Married Julie&quot;)); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"迭代器模式","slug":"design_pattern/pattern-iterator","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-iterator-pattern/","permalink":"https://wangqian0306.github.io/2021/design-iterator-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546interface Iterator &#123; boolean hasNext(); Object next();&#125;interface Container &#123; Iterator getIterator();&#125;class NameRepository implements Container &#123; public String[] names = &#123;&quot;Robert&quot;, &quot;John&quot;, &quot;Julie&quot;, &quot;Lora&quot;&#125;; @Override public Iterator getIterator() &#123; return new NameIterator(); &#125; private class NameIterator implements Iterator &#123; int index; @Override public boolean hasNext() &#123; return index &lt; names.length; &#125; @Override public Object next() &#123; if (this.hasNext()) &#123; return names[index++]; &#125; return null; &#125; &#125;&#125;public class IteratorPatternDemo &#123; public static void main(String[] args) &#123; NameRepository namesRepository = new NameRepository(); for (Iterator iter = namesRepository.getIterator(); iter.hasNext(); ) &#123; String name = (String) iter.next(); System.out.println(&quot;Name : &quot; + name); &#125; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"中介者模式","slug":"design_pattern/pattern-mediator","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-mediator-pattern/","permalink":"https://wangqian0306.github.io/2021/design-mediator-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233import java.util.Date;class User &#123; String name; public String getName() &#123; return name; &#125; public User(String name) &#123; this.name = name; &#125; public void sendMessage(String message) &#123; ChatRoom.showMessage(this, message); &#125;&#125;class ChatRoom &#123; public static void showMessage(User user, String message) &#123; System.out.println(new Date() + &quot; [&quot; + user.getName() + &quot;] : &quot; + message); &#125;&#125;public class MediatorPatternDemo &#123; public static void main(String[] args) &#123; User robert = new User(&quot;Robert&quot;); User john = new User(&quot;John&quot;); robert.sendMessage(&quot;Hi! John!&quot;); john.sendMessage(&quot;Hello! Robert!&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"备忘录模式","slug":"design_pattern/pattern-memento","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-memento-pattern/","permalink":"https://wangqian0306.github.io/2021/design-memento-pattern/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.ArrayList;import java.util.List;class Memento &#123; String state; public Memento(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125;&#125;class Originator &#123; private String state; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public Memento saveStateToMemento() &#123; return new Memento(state); &#125; public void getStateFromMemento(Memento Memento) &#123; state = Memento.getState(); &#125;&#125;class CareTaker &#123; List&lt;Memento&gt; mementoList = new ArrayList&lt;&gt;(); public void add(Memento state) &#123; mementoList.add(state); &#125; public Memento get(int index) &#123; return mementoList.get(index); &#125;&#125;public class MementoPatternDemo &#123; public static void main(String[] args) &#123; Originator originator = new Originator(); CareTaker careTaker = new CareTaker(); originator.setState(&quot;State #1&quot;); originator.setState(&quot;State #2&quot;); careTaker.add(originator.saveStateToMemento()); originator.setState(&quot;State #3&quot;); careTaker.add(originator.saveStateToMemento()); originator.setState(&quot;State #4&quot;); System.out.println(&quot;Current State: &quot; + originator.getState()); originator.getStateFromMemento(careTaker.get(0)); System.out.println(&quot;First saved State: &quot; + originator.getState()); originator.getStateFromMemento(careTaker.get(1)); System.out.println(&quot;Second saved State: &quot; + originator.getState()); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"空对象模式","slug":"design_pattern/pattern-null-object","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-null-object-pattern/","permalink":"https://wangqian0306.github.io/2021/design-null-object-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667abstract class AbstractCustomer &#123; protected String name; public abstract boolean isNil(); public abstract String getName();&#125;class RealCustomer extends AbstractCustomer &#123; public RealCustomer(String name) &#123; this.name = name; &#125; @Override public String getName() &#123; return name; &#125; @Override public boolean isNil() &#123; return false; &#125;&#125;class NullCustomer extends AbstractCustomer &#123; @Override public String getName() &#123; return &quot;Not Available in Customer Database&quot;; &#125; @Override public boolean isNil() &#123; return true; &#125;&#125;class CustomerFactory &#123; public static final String[] names = &#123;&quot;Rob&quot;, &quot;Joe&quot;, &quot;Julie&quot;&#125;; public static AbstractCustomer getCustomer(String name) &#123; for (String s : names) &#123; if (s.equalsIgnoreCase(name)) &#123; return new RealCustomer(name); &#125; &#125; return new NullCustomer(); &#125;&#125;public class NullObjectPatternDemo &#123; public static void main(String[] args) &#123; AbstractCustomer customer1 = CustomerFactory.getCustomer(&quot;Rob&quot;); AbstractCustomer customer2 = CustomerFactory.getCustomer(&quot;Bob&quot;); AbstractCustomer customer3 = CustomerFactory.getCustomer(&quot;Julie&quot;); AbstractCustomer customer4 = CustomerFactory.getCustomer(&quot;Laura&quot;); System.out.println(&quot;Customers&quot;); System.out.println(customer1.getName()); System.out.println(customer2.getName()); System.out.println(customer3.getName()); System.out.println(customer4.getName()); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"观察者模式","slug":"design_pattern/pattern-observer","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-observer-pattern/","permalink":"https://wangqian0306.github.io/2021/design-observer-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import java.util.ArrayList;import java.util.List;abstract class Observer &#123; protected Subject subject; public abstract void update();&#125;class Subject &#123; List&lt;Observer&gt; observers = new ArrayList&lt;&gt;(); int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; notifyAllObservers(); &#125; public void attach(Observer observer) &#123; observers.add(observer); &#125; public void notifyAllObservers() &#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125;&#125;class BinaryObserver extends Observer &#123; public BinaryObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println(&quot;Binary String: &quot; + Integer.toBinaryString(subject.getState())); &#125;&#125;class OctalObserver extends Observer &#123; public OctalObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println(&quot;Octal String: &quot; + Integer.toOctalString(subject.getState())); &#125;&#125;class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println(&quot;Hex String: &quot; + Integer.toHexString(subject.getState()).toUpperCase()); &#125;&#125;public class ObserverPatternDemo &#123; public static void main(String[] args) &#123; Subject subject = new Subject(); new HexaObserver(subject); new OctalObserver(subject); new BinaryObserver(subject); System.out.println(&quot;First state change: 15&quot;); subject.setState(15); System.out.println(&quot;Second state change: 10&quot;); subject.setState(10); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"策略模式","slug":"design_pattern/pattern-strategy","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-observer-pattern/","permalink":"https://wangqian0306.github.io/2021/design-observer-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849interface Strategy &#123; int doOperation(int num1, int num2);&#125;class OperationAdd implements Strategy &#123; @Override public int doOperation(int num1, int num2) &#123; return num1 + num2; &#125;&#125;class OperationSubtract implements Strategy &#123; @Override public int doOperation(int num1, int num2) &#123; return num1 - num2; &#125;&#125;class OperationMultiply implements Strategy &#123; @Override public int doOperation(int num1, int num2) &#123; return num1 * num2; &#125;&#125;class Context &#123; Strategy strategy; public Context(Strategy strategy) &#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2) &#123; return strategy.doOperation(num1, num2); &#125;&#125;public class StrategyPatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(new OperationAdd()); System.out.println(&quot;10 + 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationSubtract()); System.out.println(&quot;10 - 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationMultiply()); System.out.println(&quot;10 * 5 = &quot; + context.executeStrategy(10, 5)); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"模板模式","slug":"design_pattern/pattern-template","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-template-pattern/","permalink":"https://wangqian0306.github.io/2021/design-template-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263abstract class Game &#123; abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板 public final void play() &#123; //初始化游戏 initialize(); //开始游戏 startPlay(); //结束游戏 endPlay(); &#125;&#125;class Cricket extends Game &#123; @Override void endPlay() &#123; System.out.println(&quot;Cricket Game Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Cricket Game Initialized! Start playing.&quot;); &#125; @Override void startPlay() &#123; System.out.println(&quot;Cricket Game Started. Enjoy the game!&quot;); &#125;&#125;class Football extends Game &#123; @Override void endPlay() &#123; System.out.println(&quot;Football Game Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Football Game Initialized! Start playing.&quot;); &#125; @Override void startPlay() &#123; System.out.println(&quot;Football Game Started. Enjoy the game!&quot;); &#125;&#125;public class TemplatePatternDemo &#123; public static void main(String[] args) &#123; Game game = new Cricket(); game.play(); System.out.println(); game = new Football(); game.play(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"访问者模式","slug":"design_pattern/pattern-visitor","date":"2021-07-22T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-visitor-pattern/","permalink":"https://wangqian0306.github.io/2021/design-visitor-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485interface ComputerPartVisitor &#123; void visit(Computer computer); void visit(Mouse mouse); void visit(Keyboard keyboard); void visit(Monitor monitor);&#125;interface ComputerPart &#123; void accept(ComputerPartVisitor computerPartVisitor);&#125;class Keyboard implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125;class Monitor implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125;class Mouse implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125;class Computer implements ComputerPart &#123; ComputerPart[] parts; public Computer() &#123; parts = new ComputerPart[]&#123;new Mouse(), new Keyboard(), new Monitor()&#125;; &#125; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; for (ComputerPart part : parts) &#123; part.accept(computerPartVisitor); &#125; computerPartVisitor.visit(this); &#125;&#125;class ComputerPartDisplayVisitor implements ComputerPartVisitor &#123; @Override public void visit(Computer computer) &#123; System.out.println(&quot;Displaying Computer.&quot;); &#125; @Override public void visit(Mouse mouse) &#123; System.out.println(&quot;Displaying Mouse.&quot;); &#125; @Override public void visit(Keyboard keyboard) &#123; System.out.println(&quot;Displaying Keyboard.&quot;); &#125; @Override public void visit(Monitor monitor) &#123; System.out.println(&quot;Displaying Monitor.&quot;); &#125;&#125;public class VisitorPatternDemo &#123; public static void main(String[] args) &#123; ComputerPart computer = new Computer(); computer.accept(new ComputerPartDisplayVisitor()); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"适配器模式","slug":"design_pattern/pattern-adapter","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-adapter-pattern/","permalink":"https://wangqian0306.github.io/2021/design-adapter-pattern/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889interface MediaPlayer &#123; void play(String audioType, String fileName);&#125;interface AdvancedMediaPlayer &#123; void playVlc(String fileName); void playMp4(String fileName);&#125;class VlcPlayer implements AdvancedMediaPlayer &#123; @Override public void playVlc(String fileName) &#123; System.out.println(&quot;Playing vlc file. Name: &quot; + fileName); &#125; @Override public void playMp4(String fileName) &#123; //什么也不做 &#125;&#125;class Mp4Player implements AdvancedMediaPlayer &#123; @Override public void playVlc(String fileName) &#123; //什么也不做 &#125; @Override public void playMp4(String fileName) &#123; System.out.println(&quot;Playing mp4 file. Name: &quot; + fileName); &#125;&#125;class MediaAdapter implements MediaPlayer &#123; AdvancedMediaPlayer advancedMusicPlayer; public MediaAdapter(String audioType) &#123; if (audioType.equalsIgnoreCase(&quot;vlc&quot;)) &#123; advancedMusicPlayer = new VlcPlayer(); &#125; else if (audioType.equalsIgnoreCase(&quot;mp4&quot;)) &#123; advancedMusicPlayer = new Mp4Player(); &#125; &#125; @Override public void play(String audioType, String fileName) &#123; if (audioType.equalsIgnoreCase(&quot;vlc&quot;)) &#123; advancedMusicPlayer.playVlc(fileName); &#125; else if (audioType.equalsIgnoreCase(&quot;mp4&quot;)) &#123; advancedMusicPlayer.playMp4(fileName); &#125; &#125;&#125;class AudioPlayer implements MediaPlayer &#123; MediaAdapter mediaAdapter; @Override public void play(String audioType, String fileName) &#123; //播放 mp3 音乐文件的内置支持 if (audioType.equalsIgnoreCase(&quot;mp3&quot;)) &#123; System.out.println(&quot;Playing mp3 file. Name: &quot; + fileName); &#125; //mediaAdapter 提供了播放其他文件格式的支持 else if (audioType.equalsIgnoreCase(&quot;vlc&quot;) || audioType.equalsIgnoreCase(&quot;mp4&quot;)) &#123; mediaAdapter = new MediaAdapter(audioType); mediaAdapter.play(audioType, fileName); &#125; else &#123; System.out.println(&quot;Invalid media. &quot; + audioType + &quot; format not supported&quot;); &#125; &#125;&#125;public class AdapterPatternDemo &#123; public static void main(String[] args) &#123; AudioPlayer audioPlayer = new AudioPlayer(); audioPlayer.play(&quot;mp3&quot;, &quot;beyond the horizon.mp3&quot;); audioPlayer.play(&quot;mp4&quot;, &quot;alone.mp4&quot;); audioPlayer.play(&quot;vlc&quot;, &quot;far far away.vlc&quot;); audioPlayer.play(&quot;avi&quot;, &quot;mind me.avi&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"桥接模式","slug":"design_pattern/pattern-bridge","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-bridge-pattern/","permalink":"https://wangqian0306.github.io/2021/design-bridge-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354interface DrawAPI &#123; void drawCircle(int radius, int x, int y);&#125;class RedCircle implements DrawAPI &#123; @Override public void drawCircle(int radius, int x, int y) &#123; System.out.println(&quot;Drawing Circle[ color: red, radius: &quot; + radius + &quot;, x: &quot; + x + &quot;, &quot; + y + &quot;]&quot;); &#125;&#125;class GreenCircle implements DrawAPI &#123; @Override public void drawCircle(int radius, int x, int y) &#123; System.out.println(&quot;Drawing Circle[ color: green, radius: &quot; + radius + &quot;, x: &quot; + x + &quot;, &quot; + y + &quot;]&quot;); &#125;&#125;abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI) &#123; this.drawAPI = drawAPI; &#125; public abstract void draw();&#125;class Circle extends Shape &#123; private int x, y, radius; public Circle(int x, int y, int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; this.radius = radius; &#125; public void draw() &#123; drawAPI.drawCircle(radius, x, y); &#125;&#125;public class BridgePatternDemo &#123; public static void main(String[] args) &#123; Shape redCircle = new Circle(100, 100, 10, new RedCircle()); Shape greenCircle = new Circle(100, 100, 10, new GreenCircle()); redCircle.draw(); greenCircle.draw(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"责任链模式","slug":"design_pattern/pattern-chain-of-responsibility","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-chain-of-responsibility-pattern/","permalink":"https://wangqian0306.github.io/2021/design-chain-of-responsibility-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384abstract class AbstractLogger &#123; public static int INFO = 1; public static int DEBUG = 2; public static int ERROR = 3; protected int level; //责任链中的下一个元素 protected AbstractLogger nextLogger; public void setNextLogger(AbstractLogger nextLogger) &#123; this.nextLogger = nextLogger; &#125; public void logMessage(int level, String message) &#123; if (this.level &lt;= level) &#123; write(message); &#125; if (nextLogger != null) &#123; nextLogger.logMessage(level, message); &#125; &#125; abstract protected void write(String message);&#125;class ConsoleLogger extends AbstractLogger &#123; public ConsoleLogger(int level) &#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;Standard Console::Logger: &quot; + message); &#125;&#125;class ErrorLogger extends AbstractLogger &#123; public ErrorLogger(int level) &#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;Error Console::Logger: &quot; + message); &#125;&#125;class FileLogger extends AbstractLogger &#123; public FileLogger(int level) &#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;File::Logger: &quot; + message); &#125;&#125;public class ChainPatternDemo &#123; private static AbstractLogger getChainOfLoggers() &#123; AbstractLogger errorLogger = new ErrorLogger(AbstractLogger.ERROR); AbstractLogger fileLogger = new FileLogger(AbstractLogger.DEBUG); AbstractLogger consoleLogger = new ConsoleLogger(AbstractLogger.INFO); errorLogger.setNextLogger(fileLogger); fileLogger.setNextLogger(consoleLogger); return errorLogger; &#125; public static void main(String[] args) &#123; AbstractLogger loggerChain = getChainOfLoggers(); loggerChain.logMessage(AbstractLogger.INFO, &quot;This is an information.&quot;); loggerChain.logMessage(AbstractLogger.DEBUG, &quot;This is a debug level information.&quot;); loggerChain.logMessage(AbstractLogger.ERROR, &quot;This is an error information.&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"组合模式","slug":"design_pattern/pattern-composite","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-composite-pattern/","permalink":"https://wangqian0306.github.io/2021/design-composite-pattern/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.ArrayList;import java.util.List;class Employee &#123; String name; String dept; int salary; List&lt;Employee&gt; subordinates; //构造函数 public Employee(String name, String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public List&lt;Employee&gt; getSubordinates() &#123; return subordinates; &#125; public String toString() &#123; return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary + &quot; ]&quot;); &#125;&#125;public class CompositePatternDemo &#123; public static void main(String[] args) &#123; Employee CEO = new Employee(&quot;John&quot;, &quot;CEO&quot;, 30000); Employee headSales = new Employee(&quot;Robert&quot;, &quot;Head Sales&quot;, 20000); Employee headMarketing = new Employee(&quot;Michel&quot;, &quot;Head Marketing&quot;, 20000); Employee clerk1 = new Employee(&quot;Laura&quot;, &quot;Marketing&quot;, 10000); Employee clerk2 = new Employee(&quot;Bob&quot;, &quot;Marketing&quot;, 10000); Employee salesExecutive1 = new Employee(&quot;Richard&quot;, &quot;Sales&quot;, 10000); Employee salesExecutive2 = new Employee(&quot;Rob&quot;, &quot;Sales&quot;, 10000); CEO.add(headSales); CEO.add(headMarketing); headSales.add(salesExecutive1); headSales.add(salesExecutive2); headMarketing.add(clerk1); headMarketing.add(clerk2); //打印该组织的所有员工 System.out.println(CEO); for (Employee headEmployee : CEO.getSubordinates()) &#123; System.out.println(headEmployee); for (Employee employee : headEmployee.getSubordinates()) &#123; System.out.println(employee); &#125; &#125; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"装饰器模式","slug":"design_pattern/pattern-decorator","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-decorator-pattern/","permalink":"https://wangqian0306.github.io/2021/design-decorator-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364interface Shape &#123; void draw();&#125;class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Shape: Rectangle&quot;); &#125;&#125;class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Shape: Circle&quot;); &#125;&#125;abstract class ShapeDecorator implements Shape &#123; protected Shape decoratedShape; public ShapeDecorator(Shape decoratedShape) &#123; this.decoratedShape = decoratedShape; &#125; public void draw() &#123; decoratedShape.draw(); &#125;&#125;class RedShapeDecorator extends ShapeDecorator &#123; public RedShapeDecorator(Shape decoratedShape) &#123; super(decoratedShape); &#125; @Override public void draw() &#123; decoratedShape.draw(); setRedBorder(decoratedShape); &#125; private void setRedBorder(Shape decoratedShape) &#123; System.out.println(&quot;Border Color: Red&quot;); &#125;&#125;public class DecoratorPatternDemo &#123; public static void main(String[] args) &#123; Shape circle = new Circle(); ShapeDecorator redCircle = new RedShapeDecorator(new Circle()); ShapeDecorator redRectangle = new RedShapeDecorator(new Rectangle()); System.out.println(&quot;Circle with normal border&quot;); circle.draw(); System.out.println(&quot;\\nCircle of red border&quot;); redCircle.draw(); System.out.println(&quot;\\nRectangle of red border&quot;); redRectangle.draw(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"外观模式","slug":"design_pattern/pattern-facade","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-facade-pattern/","permalink":"https://wangqian0306.github.io/2021/design-facade-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061interface Shape &#123; void draw();&#125;class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125;class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Square::draw()&quot;); &#125;&#125;class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125;class ShapeMaker &#123; Shape circle; Shape rectangle; Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; public void drawCircle() &#123; circle.draw(); &#125; public void drawRectangle() &#123; rectangle.draw(); &#125; public void drawSquare() &#123; square.draw(); &#125;&#125;public class FacadePatternDemo &#123; public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"过滤器模式","slug":"design_pattern/pattern-filter","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-filter-pattern/","permalink":"https://wangqian0306.github.io/2021/design-filter-pattern/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154import java.util.ArrayList;import java.util.List;class Person &#123; String name; String gender; String maritalStatus; public Person(String name, String gender, String maritalStatus) &#123; this.name = name; this.gender = gender; this.maritalStatus = maritalStatus; &#125; public String getName() &#123; return name; &#125; public String getGender() &#123; return gender; &#125; public String getMaritalStatus() &#123; return maritalStatus; &#125;&#125;interface Criteria &#123; List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons);&#125;class CriteriaMale implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; malePersons = new ArrayList&lt;&gt;(); for (Person person : persons) &#123; if (person.getGender().equalsIgnoreCase(&quot;MALE&quot;)) &#123; malePersons.add(person); &#125; &#125; return malePersons; &#125;&#125;class CriteriaFemale implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; femalePersons = new ArrayList&lt;&gt;(); for (Person person : persons) &#123; if (person.getGender().equalsIgnoreCase(&quot;FEMALE&quot;)) &#123; femalePersons.add(person); &#125; &#125; return femalePersons; &#125;&#125;class CriteriaSingle implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; singlePersons = new ArrayList&lt;&gt;(); for (Person person : persons) &#123; if (person.getMaritalStatus().equalsIgnoreCase(&quot;SINGLE&quot;)) &#123; singlePersons.add(person); &#125; &#125; return singlePersons; &#125;&#125;class AndCriteria implements Criteria &#123; Criteria criteria; Criteria otherCriteria; public AndCriteria(Criteria criteria, Criteria otherCriteria) &#123; this.criteria = criteria; this.otherCriteria = otherCriteria; &#125; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; firstCriteriaPersons = criteria.meetCriteria(persons); return otherCriteria.meetCriteria(firstCriteriaPersons); &#125;&#125;class OrCriteria implements Criteria &#123; Criteria criteria; Criteria otherCriteria; public OrCriteria(Criteria criteria, Criteria otherCriteria) &#123; this.criteria = criteria; this.otherCriteria = otherCriteria; &#125; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; firstCriteriaItems = criteria.meetCriteria(persons); List&lt;Person&gt; otherCriteriaItems = otherCriteria.meetCriteria(persons); for (Person person : otherCriteriaItems) &#123; if (!firstCriteriaItems.contains(person)) &#123; firstCriteriaItems.add(person); &#125; &#125; return firstCriteriaItems; &#125;&#125;public class CriteriaPatternDemo &#123; public static void main(String[] args) &#123; List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); persons.add(new Person(&quot;Robert&quot;, &quot;Male&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;John&quot;, &quot;Male&quot;, &quot;Married&quot;)); persons.add(new Person(&quot;Laura&quot;, &quot;Female&quot;, &quot;Married&quot;)); persons.add(new Person(&quot;Diana&quot;, &quot;Female&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;Mike&quot;, &quot;Male&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;Bobby&quot;, &quot;Male&quot;, &quot;Single&quot;)); Criteria male = new CriteriaMale(); Criteria female = new CriteriaFemale(); Criteria single = new CriteriaSingle(); Criteria singleMale = new AndCriteria(single, male); Criteria singleOrFemale = new OrCriteria(single, female); System.out.println(&quot;Males: &quot;); printPersons(male.meetCriteria(persons)); System.out.println(&quot;\\nFemales: &quot;); printPersons(female.meetCriteria(persons)); System.out.println(&quot;\\nSingle Males: &quot;); printPersons(singleMale.meetCriteria(persons)); System.out.println(&quot;\\nSingle Or Females: &quot;); printPersons(singleOrFemale.meetCriteria(persons)); &#125; public static void printPersons(List&lt;Person&gt; persons) &#123; for (Person person : persons) &#123; System.out.println(&quot;Person : [ Name : &quot; + person.getName() + &quot;, Gender : &quot; + person.getGender() + &quot;, Marital Status : &quot; + person.getMaritalStatus() + &quot; ]&quot;); &#125; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"享元模式","slug":"design_pattern/pattern-flyweight","date":"2021-07-21T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2021/design-flyweight-pattern/","permalink":"https://wangqian0306.github.io/2021/design-flyweight-pattern/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.util.HashMap;interface Shape &#123; void draw();&#125;class Circle implements Shape &#123; String color; int x; int y; int radius; public Circle(String color) &#123; this.color = color; &#125; public void setX(int x) &#123; this.x = x; &#125; public void setY(int y) &#123; this.y = y; &#125; public void setRadius(int radius) &#123; this.radius = radius; &#125; @Override public void draw() &#123; System.out.println(&quot;Circle: Draw() [Color : &quot; + color + &quot;, x : &quot; + x + &quot;, y :&quot; + y + &quot;, radius :&quot; + radius); &#125;&#125;class ShapeFactory &#123; private static final HashMap&lt;String, Shape&gt; circleMap = new HashMap&lt;&gt;(); public static Shape getCircle(String color) &#123; Circle circle = (Circle) circleMap.get(color); if (circle == null) &#123; circle = new Circle(color); circleMap.put(color, circle); System.out.println(&quot;Creating circle of color : &quot; + color); &#125; return circle; &#125;&#125;public class FlyweightPatternDemo &#123; private static final String[] colors = &#123;&quot;Red&quot;, &quot;Green&quot;, &quot;Blue&quot;, &quot;White&quot;, &quot;Black&quot;&#125;; public static void main(String[] args) &#123; for (int i = 0; i &lt; 20; ++i) &#123; Circle circle = (Circle) ShapeFactory.getCircle(getRandomColor()); circle.setX(getRandomX()); circle.setY(getRandomY()); circle.setRadius(100); circle.draw(); &#125; &#125; private static String getRandomColor() &#123; return colors[(int) (Math.random() * colors.length)]; &#125; private static int getRandomX() &#123; return (int) (Math.random() * 100); &#125; private static int getRandomY() &#123; return (int) (Math.random() * 100); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"拓扑排序","slug":"algorithm/topological_sorting","date":"2021-07-19T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/topological_sorting/","permalink":"https://wangqian0306.github.io/2021/topological_sorting/","excerpt":"","text":"拓扑排序 简介 拓扑排序是遍历 DAG 的方式。实现此算法有如下几种方式： Kahn’s algorithm Depth-first search Parallel algorithms Kahn’s algorithm 概念原理： 123456789101112131415L ← Empty list that will contain the sorted elementsS ← Set of all nodes with no incoming edgewhile S is not empty do remove a node n from S add n to L for each node m with an edge e from n to m do remove edge e from the graph if m has no other incoming edges then insert m into Sif graph has edges then return error (graph has at least one cycle)else return L (a topologically sorted order) 简单实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899class Node: def __init__(self, id: int, name): self.id = id self.name = name def __str__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot; def __repr__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot;class Relation: def __init__(self, id: int, from_node: Node, to_node: Node): self.from_node = from_node self.to_node = to_node self.id = id def __str__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot; def __repr__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot;class Graph: def __init__(self, node_list: list[Node], relation_list: list[Relation]): self.require_map = &#123;&#125; self.node_list = node_list self.relation_list = relation_list for node in self.node_list: self.require_map[node] = set() for relation in self.relation_list: values = self.require_map.get(relation.to_node) values.add(relation.from_node) def topological_sort(self): l = [] s = set(self.get_no_incoming_node()) edges_copy = self.relation_list.copy() cache = [] while s: n = s.pop() if n not in l: l.append(n) cache.append(n) for m in self.get_each_node_m_with_an_edge_e_from_n_to_m(n): self.remove_edge(edges_copy, n, m) if self.check_no_incoming_edges(m, l): if m not in l: l.append(m) qualify_nodes = set(self.get_qualified_node(l)) s = qualify_nodes.difference(cache) if edges_copy: raise RuntimeError(&quot;图中至少含有一个环&quot;) else: return l def get_no_incoming_node(self) -&gt; set[Node]: for key in self.require_map.keys(): if len(self.require_map.get(key)) == 0: yield key def get_qualified_node(self, node_list: list[Node]): for key in self.require_map.keys(): if (len(self.require_map.get(key)) == 0) or (set(self.require_map.get(key)).issubset(set(node_list))): yield key def get_each_node_m_with_an_edge_e_from_n_to_m(self, n: Node) -&gt; list[Node]: for relation in self.relation_list: if relation.from_node == n: yield relation.to_node @staticmethod def remove_edge(edge_list: list[Relation], from_node: Node, to_node: Node): for edge in edge_list: if (edge.from_node == from_node) and (edge.to_node == to_node): edge_list.remove(edge) break def check_no_incoming_edges(self, m: Node, node_list: list[Node]) -&gt; bool: return set(self.require_map.get(m)).issubset(set(node_list))if __name__ == &quot;__main__&quot;: y = Node(1, &quot;y&quot;) x = Node(2, &quot;x&quot;) b = Node(3, &quot;b&quot;) a = Node(4, &quot;a&quot;) r1 = Relation(1, y, x) r2 = Relation(2, y, b) r3 = Relation(3, x, a) r4 = Relation(4, b, a) g = Graph([x, y, b, a], [r1, r2, r3, r4]) print(g.topological_sort()) Depth-first search 概念原理： 12345678910111213141516171819L ← Empty list that will contain the sorted nodeswhile exists nodes without a permanent mark do select an unmarked node n visit(n)function visit(node n) if n has a permanent mark then return if n has a temporary mark then stop (not a DAG) mark n with a temporary mark for each node m with an edge from n to m do visit(m) remove temporary mark from n mark n with a permanent mark add n to head of L 简单实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class Node: def __init__(self, id: int, name): self.id = id self.name = name self.temporary_mark = False self.permanent_mark = False def __str__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot; def __repr__(self): return f&quot;Node(&#123;self.id&#125;,&#123;self.name&#125;)&quot;class Relation: def __init__(self, id: int, from_node: Node, to_node: Node): self.from_node = from_node self.to_node = to_node self.id = id def __str__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot; def __repr__(self): return f&quot;Relation(&#123;self.id&#125;,&#123;self.from_node&#125;,&#123;self.to_node&#125;)&quot;class Graph: def __init__(self, node_list: list[Node], relation_list: list[Relation]): self.node_list = node_list self.relation_list = relation_list self.l = [] def topological_sort(self): # Depth-first search unmarked_node = self.node_list.copy() while unmarked_node: n = unmarked_node.pop() self.visit(n) return self.l def visit(self, n: Node): if n.permanent_mark: return if n.temporary_mark: raise RuntimeError(&quot;图中至少含有一个环&quot;) n.temporary_mark = True for m in self.get_each_node_m_with_an_edge_e_from_n_to_m(n): self.visit(m) n.temporary_mark = False n.permanent_mark = True self.l.insert(0, n) def get_each_node_m_with_an_edge_e_from_n_to_m(self, n: Node) -&gt; list[Node]: for relation in self.relation_list: if relation.from_node == n: yield relation.to_nodeif __name__ == &quot;__main__&quot;: y = Node(1, &quot;y&quot;) x = Node(2, &quot;x&quot;) b = Node(3, &quot;b&quot;) a = Node(4, &quot;a&quot;) r1 = Relation(1, y, x) r2 = Relation(2, y, b) r3 = Relation(3, x, a) r4 = Relation(4, b, a) g = Graph([x, y, b, a], [r1, r2, r3, r4]) print(g.topological_sort()) Parallel algorithms 12345678910111213141516171819202122232425p processing elements with IDs from 0 to p-1Input: G = (V, E) DAG, distributed to PEs, PE index j = 0, ..., p - 1Output: topological sorting of Gfunction traverseDAGDistributed δ incoming degree of local vertices V Q = &#123;v ∈ V | δ[v] = 0&#125; // All vertices with in degree 0 nrOfVerticesProcessed = 0 do global build prefix sum over size of Q // get offsets and total amount of vertices in this step offset = nrOfVerticesProcessed + sum(Qi, i = 0 to j - 1) // j is the processor index foreach u in Q localOrder[u] = index++; foreach (u,v) in E do post message (u, v) to PE owning vertex v nrOfVerticesProcessed += sum(|Qi|, i = 0 to p - 1) deliver all messages to neighbors of vertices in Q receive messages for local vertices V remove all vertices in Q foreach message (u, v) received: if --δ[v] = 0 add v to Q while global size of Q &gt; 0 return localOrder 参考资料 维基百科","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"大数据平台常见文件格式","slug":"bigdata/filetype","date":"2021-07-16T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/filetype/","permalink":"https://wangqian0306.github.io/2021/filetype/","excerpt":"","text":"大数据平台常见文件格式 简介 在大数据平台中除了日常使用的 CSV, JSON, XML 等格式外，还有一些经常使用的文件类型，例如： Parquet Avro RCFile ORC File SequenceFile … 下面我们针对每种类型的数据格式进行简单说明。 Parquet 项目官网 Parquet 是一种支持嵌套数据的列式存储格式。元数据使用 Apache Thrift 进行编码，如果想要理解这部分内容需要参照 Thrift 相关文档。 Parquet 使用了 Google 的 Dremel: Interactive Analysis of Web-Scale Datasets 论文中的记录的拆散和组装算法(record shredding and assembly algorithm)。 注：论文原文 Parquet 将文件分割成了 N 列，然后将每一列分成 M 和行组。 使用文件元数据记录所有列元数据的起始未知。 读取段可以先查找元数据来定位想要读取的列块，然后顺序读取内容。 注：Parquet 是由 Cloudera 和 Twitter 研发的。 Avro Apache Avro 是一个数据序列化系统。 Avro 提供： 丰富的数据结构。 一种紧凑、快速的二进制数据格式。 一个容器文件，用于存储持久数据。 远程过程调用 (RPC)。 与动态语言的简单集成。代码生成不需要读取或写入数据文件，也不需要使用或实现 RPC 协议。代码生成作为一种可选的优化，只值得为静态类型语言实现。 Avro 文件带有结构。 读取 Avro 数据时，写入时配置的结构将始终存在。 这种方式使得应用可以不花费额外的开销写入数据，从而使快速高效完成序列化。 同样也便于使用动态脚本语言，因为数据及其结构已经事先规定好了。 当 Avro 数据存储在文件中时，它的结构也随之存储，以便任何程序稍后可以处理文件。 如果读取程序需要不同的数据结构(类型)，这样的问题也很容易解决，我们可以让数据可以支持用户定义的类型。 在 RPC 中使用 Avro 时，客户端和服务器在连接握手中改变结构。 (此处内容可以优化，在大多数情况下，没有实际的类型经过了传输。) 由于客户端和服务器都具有对方的完整结构，因此可以轻松解决相同命名字段之间的对应关系、缺失字段、额外字段等问题. Avro 的数据结构是用 JSON 定义的。 注：Avro 是由 Hortonworks 和 Facebook 研发的。 RCFile 和 ORC File (Hive) RCFile RCFile(Record Columnar File)是一种数据结构，它决定了如何在计算机集群上存储关系表。 它是为使用 MapReduce 框架的系统设计的。 RCFile 结构包括数据存储格式、数据压缩方法和数据读取优化技术。 它能够满足数据放置的四个要求： 数据加载速度快 查询处理速度快 存储空间利用率高 对动态数据访问模式的适应性强。 ORC Files ORC(Optimized Row Columnar) File 提供了一种高效存储 Hive 数据的文件格式。 这种文件格式的设计初衷是打破现有 Hive 数据文件的相关限制。 使得在读取，写入和处理数据时能提供更高的效率。 与原始的 RCFile 格式相比，ORC 有很多的优点： 单个文件作为每个任务的输出，减少了 NameNode 的负载 Hive 类型支持，包括日期时间、十进制和复杂类型（结构、列表、映射和联合） 存储在文件中的轻量级索引 跳过未通过谓词过滤的行组 寻找给定的行 基于数据类型的块模式压缩 整数列的运行长度编码 字符串列的字典编码 使用单独的 RecordReaders 并发读取同一文件 无需扫描标记即可拆分文件的能力 限制读取或写入所需的内存量 使用协议缓冲区存储的元数据，允许添加和删除字段 ORC 文件包含称为条带的行数据组，以及文件页脚中的辅助信息。在文件末尾，附言包含压缩参数和压缩页脚的大小。 默认条带大小为 250 MB。大条带大小支持从 HDFS 进行大而高效的读取。 文件页脚包含文件中的条带列表、每个条带的行数以及每列的数据类型。它还包含列级聚合计数、最小值、最大值和总和。 SequenceFile (MapReduce) SequenceFile 是一个由二进制键/值对组成的平面文件。 它在 MapReduce 中广泛用作输入/输出格式。 还值得注意的是，在内部，Map 操作的临时输出是使用 SequenceFile 进行存储的。 该 SequenceFile 提供了一个写入者，读取者和分拣机类来分别实现写入，读取和排序。 共有 3 种不同的 SequenceFile 格式： 未压缩的键/值记录 记录压缩的键/值记录——这里只压缩“值” 块压缩键/值记录 - 键和值都分别收集在“块”中并进行压缩。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Parquet","slug":"Parquet","permalink":"https://wangqian0306.github.io/tags/Parquet/"},{"name":"Avro","slug":"Avro","permalink":"https://wangqian0306.github.io/tags/Avro/"}]},{"title":"Paxos Made Simple 中文翻译","slug":"treatise/paxos_made_simple","date":"2021-07-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/paxos_made_simple/","permalink":"https://wangqian0306.github.io/2021/paxos_made_simple/","excerpt":"","text":"Paxos Made Simple 中文翻译 作者：Leslie Lamport 英文原文 摘要 我们可以用简单的语言来描述 Paxos 算法。 注：本文是关于 The Part-Time Parliament 的补充说明，这篇文章实在是太难看懂了。 1 引言 用于实现容错分布式系统的 Paxos 算法一直被认为难以理解，可能是因为最初的介绍对许多读者来说是希腊语 [5]。 事实上，它是最简单和最明显的分布式算法之一。 它的核心是一个共识算法——“synod”算法 [5]。 下一节表明，这种共识算法几乎不可避免地遵循我们希望它满足的属性。 最后一节解释了完整的 Paxos 算法，它是通过直接将共识应用于状态机方法来构建分布式系统而获得的。 这种方法应该是众所周知的，因为它可能是最常见的主题-引用关于分布式系统理论的文章 [4]。 2 共识算法 2.1 问题 假设有一组可以提出(propose)值的流程(processes)。 共识算法会确保在提议的值中选择其中的一个。 如果未提出任何值，则不应选择任何值。 如果已经选择了一个值，那么流程应该能够知道(learn)到所选的值。 共识的安全要求是： 只能选择(chosen)已提议的值 仅选择一个值 一个过程永远不会知道一个值已经被选择，除非它实际上已经被选择 我们不会尝试指定精确的生命状态(liveness)要求。 但是，目标是确保最终选择某个提议的值，如果已选择某个值，则流程最终可以知道该值。 注：此处的 liveness 指的是保证集群中某些节点一直可用。 我们让共识算法中的三个角色由三类客户端执行：提议者(proposers)、接受者(acceptors)和学习者(learners)。 在一个实现中，单个进程可能充当多个客户端，但是我们在这里不涉及从客户端到进程的映射。 假设客户端可以通过发送消息相互通信。 我们使用惯用的异步非拜占庭(non-Byzantine)模型，其中： 客户端以任意速度运行，可能会因停止而失败，也可能会重新启动。 由于所有客户端在选择一个值后可能会失败然后重新启动，因此除非某些信息可以被失败并重新启动的客户端记住，否则解决方案是不可能的。 消息的传递时间可以任意长，可以复制，也可以丢失，但它们不会损坏。 2.2 选择一个值 选择值的最简单方法是拥有一个单一的接受者客户端。 提议者向接受者发送提议，接受者选择它收到的第一个提议值。 虽然简单，但这个解决方案并不令人满意，因为接受者的失败使得任何进一步的进展都变得不可能。 因此，让我们尝试另一种选择值的方法。 代替单个接受者，让我们使用多个接受者客户端。 提议者将提议的值发送给一组接受者。 接受者可以接受提议的值。 当足够多的接受者集接受它时，就会选择该值。 那么多少节点才算足够多？ 为了确保只选择一个值，我们可以让一个足够多的集合由大多数客户端组成这些客户端可以是系统中的任意客户端。 因为任何两个提议发送至大多数的客户端都有至少一个共同的接受者，如果一个接受者最多可以接受一个值，那算法就可以使用了。 (在许多论文中已经观察到大多数人的明显概括，显然是从 [3] 开始的。) 在没有失败或消息丢失的情况下，即使单个提议者仅提出一个值，我们也希望选择一个值。 这表明了如下要求： 1P1. An acceptor must accept the first proposal that it receives. 条件 1：接受者必须接受它收到的第一个提案 但是这个条件提出了一个问题。 不同的提议者可能同时提出多个值，导致每个接受者都接受了一个值，但大多数接受者都没有接受单个值的情况。 即使只有两个提议的值，如果每个值都被大约一半的接受者接受，单个接受者的失败可能会导致无法了解到具体选择了哪个值。 在条件 1 以及只有在大多数接受者接受时才选择一个值的要求意味着必须允许接受者接受多个提案。 我们通过为每个提案分配一个(自然)编号来跟踪接受者可能接受的不同提案，因此提案由提案编号和值组成。 为了防止混淆，我们要求不同的提案有不同的编号。 实现是另外的问题，所以现在我们只是假设它。 当具有该值的单个提案被大多数接受者接受时，就会选择一个值。 在这种情况下，我们说提案(及其值)已被选择。 我们可以允许选择多个提案，但我们必须保证所有被选择的提案都具有相同的值。 通过对提案编号的归纳，足以保证： 1P2. If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v. 条件 2：如果选择了值为 v 的提案，则选择的每个编号较高的提案都具有值 v。 由于数字是完全有序的，条件 2 保证了仅选择单个值的关键安全属性。 要被选择，提案必须被至少一个接受者接受。 因此，我们可以通过满足以下条件来满足条件 2： 1P2a. If a proposal with value v is chosen, then every higher-numbered proposal accepted by any acceptor has value v. 条件 2 a ：如果选择了值为 v 的提案，则任何接受者接受的每个编号较高的提案都具有值为 v。 我们仍然保持条件 1 以确保选择某些提案。 因为通信是异步的，所以可以选择一个从未收到任何提案的特定接受者 c 的提案。 假设一个新的提议者“醒来”并发出一个具有不同值的更高编号的提议。 条件 1 要求 c 接受这个提议，违反了条件 2 a。 为了维持条件 1 和条件 2 a 我们需要对条件 2 a 进行一些补充。 1P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v. 条件 2 b ：如果选择了值为 v 的提案，则任何提议者发布的每个编号较高的提案都具有值为 v。 由于提议必须由提议者发出才能被接受者接受，因此条件 2 b 隐含条件 2 a ，而条件 2 a又隐含 条件 2。 为了发现如何满足条件 2 b，让我们考虑如何证明它成立。 我们假设某个编号为 m 且值为 v 的提案被选择，并表明任何编号为 n &gt; m 的提案也具有值 v。 我们可以通过对 n 使用归纳让证明更容易，因此我们可以证明提案编号 n 的值是 v， 前提是每个提案都在 m…(n − 1) 中发布了值为 v 的数字，其中 i…j 表示从 i 到 j 的数字集合。 对于要选择的编号为 m 的提案，必须有一个由大多数接受者组成的集合 C，使得 C 中的每个接受者都接受它。 将此与归纳假设相结合，选择 m 的假设意味着： C 中的每个接受者都接受了一个编号为 m…(n − 1) 的提案。 以及每一个在 m…(n − 1) 的提案被任何接受者接受的值为 v。 由于由大多数接受者组成的任何集合 S 至少包含 C 的一个成员，我们可以通过确保以下不变式被维护来得出编号为 n 的提案具有值 v 的结论。 1234P2c. For any v and n, if a proposal with value v and number n is issued,then there is a set S consisting of a majority of acceptors such that either (a) no acceptor in S has accepted anyproposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered lessthan n accepted by the acceptors in S. 条件 2 c ：对于任何 v 和 n，如果发布了一个值为 v 和编号为 n 的提案，则存在一个由大多数接受者组成的集合 S， 使得 (a) S 中没有接受者接受任何编号小于 n 的提案，或者 (b) v 是 S 中接受者接受的所有编号小于 n 的提案中编号最高的提案的值。 因此，我们可以通过保持条件 2 c 的不变性来满足条件 2 b 。 为了保持条件 2 c 的不变性，想要发布编号为 n 的提案的提案者必须知道编号小于 n 的最高编号提案(如果有)， 该提案已经或将被某些多数接受者中的每个接受者接受。 了解已经接受的提案很容易但很难预测未来的接受度。 提议者不是试图预测未来，而是通过提取不会有任何此类接受的承诺来控制它。 换句话说，提议者要求接受者不再接受任何编号小于 n 的提议。 这导致了以下用于发布提案的算法。 提议者选择一个新的提议编号 n 并向一组接受者的每个成员发送请求，要求其响应： 承诺不再接受编号小于 n 的提案 其已接受的最大数目小于 n 的提案(如果有) 我将这样的请求称为编号为 n 的准备请求。 如果提议者收到来自大多数接受者的请求响应，那么它可以发出编号为 n 且值为 v 的提议，其中 v 是响应中编号最高的提议的值， 或者是提议者选择的任何值 如果响应者报告没有提案。 提议者通过向某组接受者发送提议被接受的请求来发布提议。 (这不必是响应初始请求的同一组接受者。) 我们称之为接受请求。 这描述了提议者的算法。 那接受者呢？ 它可以接收来自提议者的两种请求：准备请求和接受请求。 接受者可以忽略任何请求而不会影响安全性。 所以，我们只需要说明什么时候允许响应请求。 它总是可以响应准备请求。 它可以响应接受请求，接受提议，如果它没有承诺不接受。 换句话说： 1P1a. An acceptor can accept a proposal numbered n iff it has not responded to a prepare request having a number greater than n. 条件 1 a ：接受者可以接受编号为 n 的提议，如果它没有响应编号大于 n 的准备请求。 我们可以发现条件 1 包含条件 1 a 。 假设一个接受者收到一个编号为 n 的准备请求，但它已经响应了一个编号大于 n 的准备请求，从而承诺不接受任何编号为 n 的新提案。 那么接受者就没有理由响应新的准备请求，因为它不会接受提议者想要发布的编号为 n 的提案。 所以我们让接受者忽略这样的准备请求。 我们还让它忽略对已经接受的提案的准备请求。 通过这种优化，接受者只需要记住它曾经接受过的最高编号的提议和它已经响应的最高编号的准备请求的编号。 因为无论失败如何条件 2 c 都必须保持不变，所以即使经历失败然后重新启动，接受者也必须记住此信息。 将提议者和接受者的动作放在一起，我们看到该算法在以下两个阶段运行。 第一阶段 (a) 提议者选择一个提议编号 n 并向大多数接受者发送编号为 n 的准备请求。 (b) 如果接受者收到的准备请求的数量 n 大于它已经响应的任何准备请求的数量， 那么它会以承诺不再接受任何编号小于 n 的提案和编号最高的提案来响应该请求(如果有)它已接受。 第二阶段 (a) 如果提议者从大多数接受者那里收到对其准备请求（编号为 n）的响应，那么它会向这些接受者中的每一个发送接受请求， 以获得编号为 n 且值为 v 的提议，其中 v 是最高的值 - 在响应中编号的提案，或者如果响应没有报告提案，则为任何值。 (b) 如果接受者收到对编号为 n 的提案的接受请求，则它接受该提案，除非它已经响应了编号大于 n 的准备请求。 一个提议者可以提出多个提议，只要它遵循每个提议的算法即可。 它可以随时放弃协议中间的提案。 (即使对提案的请求和/或响应可能在提案被放弃很久之后才到达目的地，但仍保持正确性。) 如果某个提议者已经开始尝试发布更高编号的提议，那么放弃提议可能是个好主意。 因此，如果一个接受者因为已经收到了一个更高编号的准备请求而忽略了一个准备或接受请求，那么它可能应该通知提议者，然后提议者应该放弃它的提议。 这是不影响正确性的性能优化。 2.3 知道被选择的值 要了解已选择一个值，学习者必须发现提案已被大多数接受者接受。 显而易见的算法是让每个接受者，无论何时接受一个提议，响应所有学习者，向他们发送提议。 这允许学习者尽快找到一个选择的值，但它需要每个接受者对每个学习者做出回应——回应的数量等于接受者数量和学习者数量的乘积。 非拜占庭失败的假设使一个学习者很容易从另一个学习者那里发现一个值已被接受。 我们可以让接受者用他们的接受来回应一个杰出的学习者，当一个值被选择时，它反过来通知其他学习者。 这种方法需要额外的一轮让所有学习者发现所选值。 它也不太可靠，因为杰出的学习者可能会失败。 但它需要的响应数量仅等于接受者数量和学习者数量之和 更多情况下，接受者可以用他们的接受来回应一些杰出的学习者，然后每个学习者都可以在选择了一个值时通知所有学习者。 使用更大的杰出学习者集合以更高的通信复杂性为代价提供更高的可靠性。 由于消息丢失，可以在没有学习者发现的情况下选择一个值。 学习者可以询问接受者他们接受了哪些提议，但接受者失败可能无法知道大多数人是否接受了特定提议。 在这种情况下，只有在选择新提案时，学习者才会发现选择了什么值。 如果学习者需要知道某个值是否已被选择，它可以让提议者使用上述算法发布提议。 2.4 进度 很容易构建一个场景，其中两个提议者每个都不断发布一系列数量不断增加的提议，但没有一个被选中。 提案人 p 完成了提案编号 n1 的阶段 1。 另一个提议者 q 然后为提议编号 n2 &gt; n1 完成阶段 1。 提议者 p 的第 2 阶段接受对编号为 n1 的提议的请求被忽略，因为接受者都承诺不接受任何编号小于 n2 的新提议。 因此，提议者 p 然后开始并完成第 1 阶段的新提议编号 n3 &gt; n2，导致第二阶段 2 接受提议者 q 的请求被忽略，等步骤。 为了保证进度，必须选择一个杰出的提议者作为唯一一个尝试发布提议的人。 如果杰出的提议者可以与大多数接受者成功通信，并且如果它使用的提议编号大于任何已经使用的提议，那么它将成功发布被接受的提议。 通过放弃一个提议并在得知某个具有更高提议编号的请求时重试，杰出的提议者最终将选择一个足够高的提议编号。 如果足够多的系统(提议者、接受者和通信网络)正常工作，则可以通过选举单个杰出的提议者来进行恢复。 Fischer、Lynch 和 Patterson [1] 的著名结果表明，用于选举提议者的可靠算法必须使用随机性或实时性——例如，使用超时。 但是，无论选举成败，安全都是有保障的。 2.5 实施 Paxos 算法 [5] 假设了一个进程间的网络。 在其共识算法中，每个进程都扮演着提议者、接受者和学习者的角色。 算法选择一个领导者，领导者扮演杰出提议者和杰出学习者的角色。 Paxos 共识算法正是上述算法，其中请求和响应作为普通消息发送。 (响应消息标有相应的提案编号，以防止混淆。) 在故障期间保留的稳定存储用于维护接受者必须记住的信息。 在实际发送响应之前，接受器将其预期响应记录在稳定存储中。 剩下的就是描述保证没有两个提案以相同的编号发布的机制。 不同的提议者从不相交的数字集中选择他们的数字，因此两个不同的提议者永远不会发布具有相同数字的提议。 每个提议者都记住(在稳定存储中)它尝试发出的编号最高的提议，并以比它已经使用的提议编号更高的提议编号开始阶段 1。 3 实现状态机 实现分布式系统的一种简单方法是作为向中央服务器发出命令的客户端集合。 服务器可以被描述为一个确定性状态机，它以某种顺序执行客户端命令。 状态机有一个当前状态；它通过将命令作为输入并产生输出和新状态来执行步骤。 例如，分布式银行系统的客户端可能是柜员，状态机状态可能由所有用户的账户余额组成。 取款将通过执行状态机命令来执行，当且仅当余额大于取款金额时，该命令会减少帐户余额，并生成旧余额和新余额作为输出。 如果该服务器发生故障，则使用单个中央服务器的实现将导致集群故障。 因此，我们改为使用一组服务器，每个服务器独立实现状态机。 因为状态机是确定性的，如果所有服务器都执行相同的命令序列，它们将产生相同的状态序列和输出。 然后，发出命令的客户端可以使用任何服务器为其生成的输出。 为了保证所有服务器执行相同的状态机命令序列，我们实现了 Paxos 共识算法的一系列单独实例， 第 i 个实例选择的值是序列中的第 i 个状态机命令。 每个服务器在算法的每个实例中扮演所有角色(提议者、接受者和学习者)。 现在，我假设服务器集是固定的，因此共识算法的所有实例都使用相同的角色组。 在正常操作中，单个服务器被选为领导者，在共识算法的所有实例中充当杰出的提议者(唯一一个尝试发布提议的人)。 客户端向领导者发送命令，领导者决定每个命令出现的顺序。 如果领导者决定某个客户端命令应该是第 135 个命令，它会尝试将该命令选为共识算法的第 135 个实例的值。 它通常会成功。 它可能因为失败而失败，或者因为另一个服务器也认为自己是领导者，并且对第 135 个命令应该是什么有不同的想法。 但共识算法确保最多可以选择一个命令作为第 135 个命令。 我现在将描述 Paxos 状态机实现在正常操作期间是如何工作的。 稍后，我将讨论可能出错的地方。 我考虑了当前任领导刚刚失败并选择了新领导时会发生什么。 (系统启动是一种特殊情况，尚未提出任何命令。) 作为共识算法所有实例的学习者，新的领导者应该知道大多数已经选择的命令。 假设它知道命令 1-134、138 和 139，即在共识算法的实例 1-134、138 和 139 中选择的值。 (我们稍后会看到命令序列中的这种差距是如何产生的。) 然后它执行实例 135-137 和所有大于 139 的实例的阶段 1。 (我在下面描述了这是如何完成的。) 假设这些执行的结果决定了在实例 135 和 140 中建议的值，但在所有其他实例中不限制建议值。 然后领导者为实例 135 和 140 执行阶段 2，从而选择命令 135 和 140。 领导者以及学习领导者知道的所有命令的任何其他服务器现在可以执行命令 1-135。 但是，它不能执行命令 138-140，它也知道，因为命令 136 和 137 尚未被选择。 领导者可以将客户端请求的下两个命令作为命令 136 和 137。 相反，我们通过建议(作为命令 136 和 137)一个特殊的 “noop” 命令来让状态保持不变，让它立即填补空白。 (它通过执行共识算法的实例 136 和 137 的第 2 阶段来实现这一点。) 一旦选择了这些无操作命令，就可以执行命令 138-140。 现在已选择命令 1–140。 领导者也已经完成了所有大于 140 的共识算法的实例的阶段 1，并且可以在这些实例的阶段 2 中自由提出任何值。 它将命令编号 141 分配给客户端请求的下一个命令，将其作为共识算法实例 141 的阶段 2 中的值。 它建议它接收的下一个客户端命令作为命令 142，依此类推。 领导者可以在得知其提议的命令 141 已被选择之前提议命令 142。 它可能会丢失在建议命令 141 中发送的所有消息，并且在任何其他服务器了解领导者建议作为命令 141 的内容之前选择命令 142。 当领导者未能在实例 141 中收到对其第 2 阶段消息的预期响应时，它将重新传输这些消息。 如果一切顺利，将选择其建议的命令。 但是，它可能首先失败，在所选命令的序列中留下空白。 一般来说，假设领导者可以提前获得 α 个命令——也就是说，它可以在选择命令 1 到 i 后提出命令 i+1 到 i+α。 然后可能会出现多达 α-1 个命令的差距。 新选择的领导者为无限多个共识算法实例执行阶段 1——在上面的场景中，实例 135-137 和所有大于 139 的实例。 对所有实例使用相同的提议编号，它可以通过向其他服务器发送一条合理的短消息来实现这一点。 在第 1 阶段，只有当接受者已经收到来自某个提议者的第 2 阶段消息时，接受者才会响应一个简单的 OK。 (在该场景中，仅实例 135 和 140 是这种情况。) 因此，服务器(充当接受者)可以使用单个合理的短消息对所有实例进行响应。 因此，执行阶段 1 的这些无限多个实例没有问题。 由于领导者的失败和新领导者的选举应该是罕见的事件，执行状态机命令的有效成本——即在命令/值上达成共识——是仅执行共识算法的第 2 阶段的成本。 可以证明，Paxos 共识算法的第 2 阶段在出现故障的情况下达成一致的任何算法的成本可能最低 [2]。 因此，Paxos 算法本质上是最优的。 对系统正常运行的讨论假设总是有一个领导者，除了当前领导者失败和选举新领导者之间的短暂时间。 在异常情况下，leader 选举可能会失败。 如果没有服务器充当领导者，则不会提出新命令。 如果多个服务器认为他们是领导者，那么他们都可以在共识算法的同一个实例中提出值，这可能会阻止任何值被选中。 但是，安全性得到了保护——两个不同的服务器永远不会在选择作为第 I 个状态机命令的值上存在分歧。 只需要选举一个领导人来确保进展。 如果服务器集可以更改，那么必须有某种方法来确定哪些服务器实现了共识算法的哪些实例。 最简单的方法是通过状态机本身。 当前的服务器集可以成为状态的一部分，并且可以使用普通的状态机命令进行更改。 通过让执行第 i 个状态机命令后的状态指定执行共识算法的实例 i + α 的服务器集，我们可以允许领导者提前获得 α 个命令。 这允许任意复杂的重新配置算法的简单实现。 参考文献 [1] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM, 32(2):374–382, April 1985. [2] Idit Keidar and Sergio Rajsbaum. On the cost of fault-tolerant consensus when there are no faults—a tutorial. TechnicalReport MIT-LCS-TR-821, Laboratory for Computer Science, Massachusetts Institute Technology, Cambridge, MA, 02139, May 2001. also published in SIGACT News 32(2) (June 2001). [3] Leslie Lamport. The implementation of reliable distributed multiprocess systems. Computer Networks, 2:95–114, 1978. [4] Leslie Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM, 21(7):558–565, July 1978. [5] Leslie Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133–169, May 1998.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Paxos","slug":"Paxos","permalink":"https://wangqian0306.github.io/tags/Paxos/"}]},{"title":"Apache Flink: Stream and Batch Processing in a Single Engine 中文翻译","slug":"treatise/apache_flink_stream_and_batch_processing_in_a_single_engine","date":"2021-07-12T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/apache_flink_stream_and_batch_processing_in_a_single_engine/","permalink":"https://wangqian0306.github.io/2021/apache_flink_stream_and_batch_processing_in_a_single_engine/","excerpt":"","text":"Apache Flink: Stream and Batch Processing in a Single Engine 中文翻译 作者：Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, Kostas Tzoumas 英文原文 摘要 Apache Flink 是一个用于处理流数据和批处理数据的开源系统。 Flink 建立在这样的理念之上，即许多类别的数据处理应用程序，包括实时分析、连续的数据管道、历史数据处理(批处理)和迭代算法(机器学习、图分析) 都可以表示为可执行的并且容错的管道数据流。 在本文中，我们展示了 Flink 的架构，并扩展了如何在单个执行模型下统一一组(看似多样化的)用例。 1 引言 数据流处理(例如，以复杂事件处理系统为例)和静态(批处理)数据处理(例如，以 MPP 数据库和 Hadoop 为例)传统上被视为两种截然不同的应用程序类型。 它们使用不同的编程模型和 API 进行编程，并由不同的系统执行 (例如专用流系统有 Apache Storm、IBM Infosphere Streams、Microsoft StreamInsight 或 Streambase 与关系数据库或 Hadoop 的执行引擎， 包括 Apache Spark 和 Apache Drill) 传统上，批处理数据分析占用例、数据大小和市场的最大份额，而流数据分析主要服务于专门的应用程序。 然而，越来越明显的是，当今大量的大规模数据处理用例处理的数据实际上是随着时间的推移不断产生的。 例如，这些连续的数据流来自 Web 日志、应用程序日志、传感器，或数据库中应用程序状态的变化(事务日志记录)。 今天大多数的处理方式大多忽略了数据生产的连续性和及时性，针对数据流进行处理。 取而代之的是，数据(通常是人为地)被分批成静态数据集(例如，每小时、每天或每月的数据块)，然后以与时间无关的方式进行处理。 数据收集工具、工作流管理器和调度程序在实际上是连续数据处理管道中协调批次的创建和处理。 诸如“lambda 架构” [21] 之类的架构模式结合了批处理和流处理系统来实现多个计算路径: 用于及时近似结果的流式快速路径，以及用于后期精确结果的批处理离线路径。 由于使用了批处理的方式这些方法都存在较大的延时和较高的复杂性(连接和编排多个系统，并两次实现业务逻辑)以及不准确的问题， 因为时间维度没有由应用程序编码进行处理。 Apache Flink 遵循一种范式，即在编程模型和执行引擎中将数据流处理作为实时分析、连续流和批处理的统一模型。 结合允许准任意重放数据流的持久消息队列(如 Apache Kafka 或 Amazon Kinesis)， 流处理程序在实时处理最新事件、在大的时间窗口中定期连续聚合数据或处理 TB 级数据之间没有任何区别。 相反，这些不同类型的计算只是在持久流中的不同点开始处理，并在计算过程中保持不同形式的状态。 通过高度灵活的窗口机制，Flink 程序可以计算早期和近似，以及延迟和准确，结果相同，无需为两个用例组合不同的系统。 Flink 支持不同的时间概念(事件时间、摄取时间、处理时间)，以便让程序员在定义事件应该如何关联方面具有高度的灵活性。 同时，Flink 也认为将需要专门的数据进行批处理(处理静态数据集)。 对静态数据的复杂查询仍然非常适合批处理抽象。 此外，对于流用例的传统实现和分析应用程序，仍然需要批处理，在这些应用程序中，尚不知道对流数据执行此类处理的有效算法。 批处理程序是流程序的特例，其中流是有限的，记录的顺序和时间无关紧要(所有记录都隐含地属于一个全包窗口)。 然而，为了以具有竞争力的易用性和性能支持批处理用例，Flink 有一个专门的 API 来处理静态数据集， 使用专门的数据结构和算法来处理诸如 join 或 grouping 等运算符的批处理版本，并使用专用的调度策略。 结果是 Flink 将自己呈现为一个基于流运行时的成熟高效的批处理器，包括用于图形分析和机器学习的库。 Flink 起源于 Stratosphere 项目 [4]，是 Apache 软件基金会的顶级项目， 由一个庞大而活跃的社区(截至撰写本文时由 180 多个开源贡献者组成)开发和支持，并在几家公司的生产中使用。 本文的贡献如下： 建设了流和批处理数据处理的统一架构，包括仅与静态数据集相关的特定优化 我们展示了流、批处理、迭代和交互式分析如何表示为容错流数据流(第 3 节) 我们讨论如何在这些数据流之上构建具有灵活窗口机制(第 4 节)以及成熟的批处理器(第 4.1 节)的成熟的流分析系统， 通过展示流处理，批处理，迭代处理和交互式分析转换为数据流(streaming dataflows) 2 系统架构 在本节中，我们将 Flink 的架构布局分为软件堆栈和分布式系统进行展示。 虽然 Flink 的 API 堆栈不断增长，但我们可以区分四个主要层：部署、核心、API 和库。 Flink 的运行环境和 API 图 1 显示了 Flink 的软件堆栈。 Flink 的核心是分布式数据流引擎，它执行数据流程序。 Flink 运行时程序是连接的有状态算子的 DAG 与数据流。 Flink 中有两个核心 API：用于处理有限数据集(通常称为批处理)的 DataSet API，以及用于处理潜在无界数据流(通常称为流处理)的 DataStream API。 Flink 的核心运行时引擎可以看作是一个流式数据流引擎，DataSet 和 DataStream API 都创建了引擎可执行的运行时程序。 因此，它作为通用结构来抽象有界(批处理)和无界(流)处理。 在核心 API 之上，Flink 捆绑了特定领域的库和 API，用于生成 DataSet 和 DataStream API 程序。 目前，FlinkML 用于机器学习，Gelly 用于图形处理和 Table 用于类似 SQL 的操作。 如图 2 所示，一个 Flink 集群包含三种类型的进程：客户端(client)、作业管理器(JobManager)和至少一个任务管理器(TaskManager)。 客户端获取程序代码，将其转换为数据流图，然后将其提交给作业管理器。 此转换阶段还检查运算符之间交换的数据的数据类型(schema)，并创建序列化程序和其他类型/模式特定的代码。 DataSet 程序还经过一个基于成本的查询优化阶段，类似于物理优化，通过关系查询优化器进行(详情参见4.1节)。 作业管理器制定了数据流的分布式执行计划。 它跟踪每个操作和流的状态和进度，安排新的操作，并协调检查点和恢复。 在高可用性设置中，作业管理器将每个检查点的最小元数据集保存到容错存储，以便备用作业管理器可以重建检查点并从那里恢复数据流并重新开始执行。 实际的数据处理发生在任务管理器中。 任务管理器执行一个或多个产生流的操作，并将它们的状态报告给作业管理器。 任务管理器维护了缓冲池以缓冲或物化流，并维护网络连接以在操作之间交换数据流。 3 通用结构：数据流 尽管用户可以使用多种 API 编写 Flink 程序，但所有 Flink 程序最终都会编译为一个通用表示：数据流图。 数据流图由 Flink 的运行时引擎执行，这是批处理(DataSet)和流处理(DataStream) API 下的公共层。 3.1 数据流图 图 3 中描绘的数据流图是一个有向无环图(DAG)，它包括：(i)有状态运算操作(stateful operators) 和 (ii) 经由运算操作生成可被使用的数据流(data streams)。 由于数据流图以数据并行的方式执行，操作被并行化为一个或多个称为子任务的并行实例，并且流被分成一个或多个流分区（每个生产子任务一个分区）。 在特殊情况下有状态运算操作可以是无状态的，它实现了所有的逻辑处理(例如，过滤、哈希连接和流式窗口函数) 许多的运算操作是由众所周知算法的教科书级别的代码实现。 在第 4 节中，我们提供了有关窗口运算符实现的详细信息。 流以各种模式在生产者和消费者之间分配数据，例如点对点、广播、重新分区、扇出和合并。 注：扇出指的是将消息发送到所有的主题。 3.2 通过中间数据流进行数据交换 Flink 的中间数据流是操作之间数据交换的核心抽象。 中间数据流表示对由操作产生并且可由一个或多个操作使用的数据的逻辑句柄。 中间流是逻辑的，因为它们指向的数据可能会或可能不会在磁盘上进行物化。 数据流的特定行为由 Flink 中的高层进行参数化(例如，DataSet API 使用的程序优化器)。 管道和阻塞数据交换 管道的中间流在并发运行的生产者和消费者之间交换数据，从而执行管道。 因此，管道将背压从消费者传播到生产者，通过中间缓冲池进行缓冲，以补偿短期吞吐量波动。 Flink 对连续的流式程序以及批处理数据流中的许多部分使用管道的方式，以便在可能的情况下避免物化。 另一方面，阻塞流适用于有界数据流。 阻塞流缓冲所有生产操作的数据，然后使其可供消费，从而将生产运算符和消费操作分为不同的执行阶段。 阻塞流自然需要更多内存，经常溢出到二级存储，并且不会传播背压。 它们会在需要的地方将连续的操作彼此隔离，例如需要破坏当前管道的情况(例如排序合并连接)或可能导致分布式死锁的情况下。 平衡延迟和吞吐量 Flink 的数据交换机制是围绕缓冲区的交换实现的。 当一个数据记录在生产者端准备好时，它被序列化并分成一个或多个缓冲区(一个缓冲区也可以容纳多个记录)，可以转发给消费者。 缓冲区在 i) 满时或 ii) 达到超时条件时立即发送给使用者。 这使 Flink 能够通过将缓冲区的大小设置为高值(例如，几千字节)来实现高吞吐量，并通过将缓冲区超时设置为低值(例如，几毫秒)来实现低延迟。 图 4 显示了缓冲区超时对在 30 台机器(120 个内核)上的简单流式 grep 作业中传送记录的吞吐量和延迟的影响。 可以看到 Flink 能够在 99% 实现 20ms 的演示。 相应的吞吐量为每秒 150 万个事件。 随着我们增加缓冲区超时，我们看到延迟随着吞吐量的增加而增加，直到达到最大吞吐量(即缓冲区填满的速度快于超时到期)。 在 50ms 的缓冲区超时时，集群达到每秒超过 8000 万个事件的吞吐量，99% 的延迟为 50ms 控制事件 除了交换数据，Flink 中的流还传达了不同类型的控制事件。 这些是操作在数据流中注入的特殊事件，并与流分区内的所有其他数据记录和事件一起按顺序交付。 接收操作通过在它们到达时执行某些操作来对这些事件做出反应。 Flink 使用了很多特殊类型的控制事件，包括： 检查点屏障通过将流划分为检查点前和检查点后(在第 3.3 节中讨论)来协调检查点 表示流分区内事件时间进度的水印(在第 4.1 节中讨论) 在循环数据流之上的 Bulk/StaleSynchronous-Parallel 迭代算法(在第 5.3 节中讨论)中，迭代障碍表明流分区已到达超级步的末尾。 如上所述，控制事件假定流分区保留记录的顺序。 为 此，Flink 中使用单个流分区的一元运算符保证记录的 FIFO 顺序。 但是，收到多个流分区的操作符会按到达顺序合并流，以跟上流的速率并避免背压。 因此，Flink 中的流式数据流在任何形式的重新分区或广播后都不提供排序保证，处理乱序记录的责任留给了操作员实现。 我们发现这种安排提供了最有效的设计，因为大多数运算符不需要确定性顺序(例如，散列连接、映射)，并且需要补偿无序到达的运算， 例如时间窗口可以作为运算逻辑的一部分，可以更有效地做到这一点。 3.3 容错性 Flink 通过严格的一次性处理一致性保证提供可靠的执行，并通过检查点和部分重新执行来处理故障。 系统为有效提供这些保证而做出的一般假设是数据源是持久的和可重播的。 此类源的示例是文件和持久消息队列(例如，Apache Kafka)。 在实践中，也可以通过在源操作的状态内保留预写日志来合并非持久性源。 Apache Flink 的检查点机制建立在分布式一致快照的概念之上，以实现恰好一次处理的保证。 数据流可能无限的特性使得在恢复时重新计算变得不切实际，因为长时间运行的作业可能需要重放数月的计算。 为了限制恢复时间，Flink 会定期拍摄算子状态的快照，包括输入流的当前位置。 核心挑战在于在不停止拓扑执行的情况下对所有并行算子进行一致的快照。 本质上，所有算子的快照应该指的是计算中相同的逻辑时间。 Flink 中使用的机制称为 Asynchronous Barrier Snapshotting(ABS [7])。 屏障是从注入到与逻辑时间相对应的输入流中的控制记录，并且在逻辑上将流进行分离，插入之前的数据就会被快照保存。 操作从上游接收屏障并首先执行对齐阶段，确保已经接受到了输入的所有屏障。 然后，操作将其状态(例如，滑动窗口的内容或自定义数据结构)写入持久存储(例如，存储后端可以是外部系统，例如 HDFS)。 一旦状态被备份，操作将向下游转发屏障。 最终，所有操作都将注册其状态的快照，并且全局快照将完成。 例如在图 5 中，我们显示快照 t2 包含所有操作状态，这些状态是在 t2 屏障之前消耗所有记录的结果。 ABS 类似于用于异步分布式快照的 Chandy-Lamport 算法 [11]。 但是，由于 Flink 程序的 DAG 结构，ABS 不需要检查点飞行记录，而仅依靠对齐阶段将其所有影响应用于操作状态。 这保证了需要写入可靠存储的数据保持在理论上的最小值(即，仅操作的当前状态)。 从故障中恢复将所有操作状态恢复到从上次成功快照中获取的各自状态，并从有快照的最新屏障开始重新启动输入流。 恢复时所需的最大重新计算量限于两个连续障碍之间的输入记录量。 此外，通过额外重放在直接上游子任务中缓冲的未处理记录，可以部分恢复失败的子任务 [7]。 ABS 提供了几个好处： i) 它保证只更新一次状态而不会暂停计算 ii) 它与其他形式的控制消息完全分离(例如，通过触发窗口计算的事件，从而不限制窗口化机制到检查点间隔的倍数) iii) 它与用于可靠存储的机制完全解耦，允许将状态备份到文件系统、数据库等，这取决于使用 Flink 的外部环境。 3.4 迭代数据流 增量处理和迭代对于应用程序至关重要，例如图形处理和机器学习。 对数据并行处理平台中迭代的支持通常依赖于为每次迭代提交一个新作业，或者通过向正在运行的 DAG [6, 25] 或反馈边 [23] 添加额外的节点。 Flink 中的迭代是作为迭代步骤实现的，特殊的操作本身可以包含一个执行图(图 6)。 为了维护基于 DAG 的运行时和调度程序，Flink 允许迭代“头”和“尾”任务，这些任务与反馈边隐式连接。 这些任务的作用是为迭代步骤建立一个活跃的反馈通道，并为在这个反馈通道内处理传输中的数据记录提供协调。 实现任何类型的结构化并行迭代模型(例如批量同步并行 (BSP) 模型)都需要协调，并且使用控制事件来实现。 我们分别在第 4.4 节和第 5.3 节中解释了如何在 DataStream 和 DataSet API 中实现迭代。 4 基于数据流的流式分析 Flink 的 DataStream API 在 Flink 的运行时之上实现了一个完整的流分析框架，包括管理时间的机制，例如乱序事件处理、定义窗口以及维护和更新用户定义的状态。 流 API 基于 DataStream 的概念，DataStream 是给定类型元素的(可能是无界的)不可变集合。 由于 Flink 的运行时已经支持流水线数据传输、连续状态操作符和用于一致状态更新的容错机制，因此在其上叠加流处理器本质上归结为实现窗口系统和状态接口。 如前所述，这些对于运行时是不可见的，运行时将窗口视为只是有状态运算符的实现。 4.1 时间的概念 Flink 区分了两种时间概念： i) 事件时间，它表示事件发生的时间(例如，与传感器(例如移动设备)产生的信号相关联的时间戳) ii) 处理时间，它是正在处理数据的机器的挂钟时间。 在分布式系统中，事件时间和处理时间之间存在任意偏差 [3]。 这种偏斜可能意味着基于事件时间语义获得答案的任意延迟。 为了避免任意延迟，这些系统会定期插入称为低水印的特殊事件，用于标记全局进度度量。 例如，在时间进度的情况下，水印包括时间属性 t，指示所有低于 t 的事件都已经进入操作员。 水印有助于执行引擎以正确的事件顺序处理事件并序列化操作，例如通过统一的进度度量进行窗口计算。 水印起源于拓扑的源头，我们可以在其中确定未来元素的固有时间。 水印从源头通过数据流的其他操作符传播。 运营商决定他们如何对水印做出反应。 简单的操作，例如 map 或 filter 只是转发它们接收到的水印，而更复杂的基于水印(例如，事件时间窗口)进行计算的运算符首先计算由水印触发的结果，然后转发它。 如果一项操作有多个输入，系统只会将传入的最小水印转发给操作员，从而确保正确的结果。 基于处理时间的 Flink 程序依赖于本地机器时钟，因此具有不太可靠的时间概念，这可能导致恢复时不一致的重放。 但是，它们表现出较低的延迟。 基于事件时间的程序提供最可靠的语义，但由于事件时间处理时间滞后可能会出现延迟。 Flink 包含第三个时间概念，作为事件时间的一种特殊情况，称为摄取时间，即事件进入 Flink 的时间。 与事件时间相比，这实现了更低的处理延迟，并导致与处理时间相比更准确的结果。 4.2 有状态流处理 虽然 Flink 的 DataStream API 中的大多数算子看起来像函数式、无副作用的算子，但它们为高效的有状态计算提供支持。 状态对于许多应用程序至关重要，例如机器学习模型构建、图形分析、用户会话处理和窗口聚合。 根据用例，有大量不同类型的状态。 例如，状态可以是简单的计数器或总和或更复杂的东西，例如机器学习应用程序中经常使用的分类树或大型稀疏矩阵。 流窗口是有状态的运算符，它将记录分配给作为运算符状态一部分保存在内存中的不断更新的桶。 在 Flink 中，状态是显式的，并通过提供： i) 操作员接口或注释来静态注册在操作员范围内的显式局部变量， ii) 用于声明分区键值状态及其的操作员状态抽象相关操作。 用户还可以使用系统提供的 StateBackend 抽象来配置状态的存储和检查点，从而在流应用程序中实现高度灵活的自定义状态管理。 Flink 的检查点机制(在 3.3 节中讨论)保证任何注册状态都是持久的，并且具有一次性更新语义。 4.3 流窗口 无界流上的增量计算通常在不断发展的逻辑视图(称为窗口)上进行评估。 Apache Flink 将窗口合并到一个有状态操作符中，该操作符通过一个灵活的声明进行配置，该声明由三个核心函数组成：窗口分配器以及可选的触发器和驱逐器。 所有三个函数都可以从一组常见的预定义实现(例如，滑动时间窗口)中选择，或者可以由用户明确定义(即，用户定义的函数)。 更具体地说，分配器负责将每个记录分配给逻辑窗口。 例如，当涉及到事件时间窗口时，此决定可以基于记录的时间戳。 请注意，在滑动窗口的情况下，一个元素可以属于多个逻辑窗口。 一个可选的触发器定义何时执行与窗口定义关联的操作。 最后，一个可选的 evictor 决定在每个窗口中保留哪些记录。 Flink 的窗口分配过程能够覆盖所有已知的窗口类型，例如周期性时间窗口和计数窗口、标点符号、地标、会话和增量窗口。 请注意，Flink 的窗口功能无缝地整合了乱序处理，类似于 Google Cloud Dataflow [3]，并且原则上包含这些窗口模型。 例如，下面是一个窗口定义，范围为 6 秒，每 2 秒滑动一次(分配器)。 一旦水印通过窗口的末尾(触发器)，就会计算窗口结果。 123stream .window(SlidingTimeWindows.of(Time.of(6, SECONDS), Time.of(2, SECONDS)) .trigger(EventTimeTrigger.create()) 全局窗口创建单个逻辑组。 下面的示例定义了一个全局窗口(即分配器)，它对每 1000 个事件(即触发器)调用操作，同时保留最后 100 个元素(即驱逐器)。 1234stream .window(GlobalWindow.create()) .trigger(Count.of(1000)) .evict(Count.of(100)) 请注意，如果上面的流在窗口化之前在一个键上分区，上面的窗口操作是本地的，因此不需要工作节点之间的协调。 该机制可用于实现多种窗口功能 [3]。 4.4 异步流迭代 流中的循环对于多个应用程序至关重要，例如增量构建和训练机器学习模型、强化学习和图形近似 [9, 15]。 在大多数情况下，反馈循环不需要协调。 异步迭代涵盖了流应用程序的通信需求，并且不同于基于有限数据结构化迭代的并行优化问题。 如第 3.4 节和图 6 所示，当未启用迭代控制机制时，Apache Flink 的执行模型已经涵盖了异步迭代。 此外，为了遵守容错保证，反馈流被视为隐式迭代头中的操作状态运算符，并且是全局快照 [7] 的一部分。 DataStream API 允许对反馈流进行明确定义，并且可以简单地包含对流 [23] 上的结构化循环以及进度跟踪 [9] 的支持。 5 基于数据流的批量数据分析 有界数据集是无界数据流的特例。 因此，一个将所有输入数据插入窗口的流程序可以形成一个批处理程序，批处理应该完全被 Flink 上面介绍的特性所含盖。 然而，i)可以简化语法(即用于批量计算的 API，例如，不需要定义人工全局窗口)和 ii)处理有界数据集的程序可以进行额外的优化， 更高效的完成记录保持容错性完成分阶段调度。 Flink 批处理的方式如下： 批处理计算由与流计算相同的运行时执行。运行时可执行文件可以使用阻塞的数据流进行参数化，以将大型计算分解为连续调度的孤立阶段。 定期快照在其开销很高时会被关闭。相反，可以通过从最新的物化中间流(可能是源)重放丢失的流分区来实现故障恢复。 阻塞运算(例如，排序)只是运算的实现，直到它们消费了全部输入才结束阻塞状态。运行时不知道操作是否阻塞。 这些操作使用 Flink 提供的托管内存(无论是在 JVM 堆上还是在 JVM 堆外)，如果他们的输入超出其内存界限，则可能溢写到磁盘。 专用的数据集 API 为批量计算提供了熟悉的抽象，即有界容错数据集数据结构和数据集上的转换(例如，连接、聚合、迭代)。 查询优化层将 DataSet 程序转换为高效的可执行文件。 下面我们更详细地描述这些方面。 5.1 查询优化 Flink 的优化器建立在并行数据库系统的技术之上，例如计划等效(plan equivalence)、成本建模(cost modeling)和兴趣属性传播(interesting property propagation)。 然而，构成 Flink 数据流程序的任意 UDF-heavy DAG 不允许传统优化器使用开箱即用的数据库技术 [17]，因为操作符对优化器隐藏了它们的语义。 出于同样的原因，基数和成本估计方法同样难以使用。 Flink 的运行时支持各种执行策略，包括重新分区和广播数据传输，以及基于排序的分组和基于排序和哈希的方式实现连接。 Flink 的优化器基于有趣的属性传播概念枚举不同的物理计划 [26]，使用基于成本的方法在多个物理计划中进行选择。 成本包括网络和磁盘 I/O 以及 CPU 成本。 为了克服 UDF 存在时的基数估计问题，Flink 的优化器可以使用程序员提供的提示。 5.2 内存管理 基于数据库技术，Flink 将数据序列化到内存段中，而不是在 JVM 堆中分配对象来表示缓冲的动态数据记录。 排序和连接等操作尽可能直接对二进制数据进行操作，将序列化和反序列化开销保持在最低限度，并在需要时将部分数据溢出到磁盘。 为了处理任意对象，Flink 使用类型推断和自定义序列化机制。 通过将数据处理保持在二进制表示和堆外，Flink 设法减少了垃圾收集的开销，并使用缓存高效且健壮的算法在内存压力下优雅地扩展。 5.3 批量迭代 迭代图分析、并行梯度下降和优化技术过去已经在批量同步并行(BSP)和陈旧同步并行(SSP)模型等基础上实现。 Flink 的执行模型允许通过使用迭代控制事件在顶部实现任何类型的结构化迭代逻辑。 例如，在 BSP 执行的情况下，迭代控制事件标记了迭代计算中超步的开始和结束。 最后，Flink 引入了更多新颖的优化技术，例如 delta 迭代的概念 [14]，它可以利用稀疏的计算依赖关系 Delta 迭代已经被 Flink 的 Graph API Gelly 所利用。 6 相关工作 今天，有大量用于分布式批处理和流分析处理的引擎。我们将主要系统分类如下。 批处理 Apache Hadoop 是最流行的大规模数据分析开源系统之一，它基于 MapReduce 范式 [12]。 Dryad [18] 引入了嵌入式用户定义函数一般基于 DAG 的数据流，并由 SCOPE [26] 丰富，它是一种语言和基于它的 SQL 优化器。 Apache Tez [24] 可以看作是 Dryad 提出的想法的开源实现。 MPP 数据库 [13] 以及最近的开源实现(如 Apache Drill 和 Impala [19])将其 API 限制为 SQL 变体。 与 Flink 类似，Apache Spark [25] 是一个数据处理框架，它实现了基于 DAG 的执行引擎，提供了 SQL 优化器，执行基于驱动程序的迭代，并将无界计算视为微批次。 相比之下，Flink 是唯一一个包含 i) 分布式数据流运行时的系统，该运行时利用流水线流式执行来处理批处理和流工作负载， ii) 通过轻量级检查点实现一次状态一致性，iii) 本地迭代处理，iv) 复杂的窗口语义，支持乱序处理。 流处理 在学术和商业流处理系统(例如 SEEP、Naiad、Microsoft StreamInsight 和 IBM Streams)方面有大量的先前工作。 其中许多系统都基于数据库社区的研究 [1, 5, 8, 10, 16, 22, 23]。 上述大多数系统要么是 i) 学术原型，ii) 闭源商业产品，或 iii) 不在商品服务器集群上水平扩展计算。 最新的数据流方法支持水平可扩展性和组合数据流运算符，并具有较弱的状态一致性保证(例如，Apache Storm 和 Samza 中的至少一次处理)。 值得注意的是，诸如“乱序处理”(OOP) [20] 之类的概念获得了极大的吸引力，并被 MillWheel [2] 所采用， 它是后来提供的 Apache Beam/Google Dataflow [3] 商业执行器的 Google 内部版本。 Millwheel 充当了一次性低延迟流处理和 OOP 的概念证明，因此对 Flink 的演变非常有影响。 据我们所知，Flink 是唯一一个开源项目：i) 支持事件时间和无序事件处理 ii) 提供一致的托管状态，并保证只执行一次 iii) 实现高吞吐量和低延迟，提供批处理和流处理服务。 7 致谢 1234The development of the Apache Flink project is overseen by a self-selected team of active contributors to the project.A Project Management Committee (PMC) guides the project’s ongoing operations, including community development and product releases.At the current time of writing this, the list of Flink committers are : Marton Balassi, Paris Carbone, Ufuk Celebi, Stephan Ewen, Gyula F ´ ora, Alan Gates, Greg Hogan, ´Fabian Hueske, Vasia Kalavri, Aljoscha Krettek, ChengXiang Li, Andra Lungu, Robert Metzger, Maximilian Michels, Chiwan Park, Till Rohrmann, Henry Saputra, Matthias J. Sax, Sebastian Schelter, Kostas Tzoumas, Timo Walther and Daniel Warneke.In addition to these individuals, we want to acknowledge the broader Flink community of more than 180 contributors. 8 结论 在本文中，我们介绍了 Apache Flink，这是一个实现通用数据流引擎的平台，旨在执行流分析和批处理分析。 Flink 的数据流引擎将操作符状态和逻辑中间结果视为一等公民，并由具有不同参数的批处理和数据流 API 进行处理。 构建在 Flink 流数据流引擎之上的流 API 提供了保持可恢复状态以及分区、转换和聚合数据流窗口的方法。 虽然理论上批处理计算是流式计算的一个特例，但 Flink 对它们进行了特殊处理，通过使用查询优化器优化它们的执行并实现在没有内存的情况下优雅地溢出到磁盘的阻塞运算。 参考资料 [1] D. J. Abadi, Y. Ahmad, M. Balazinska, U. Cetintemel, M. Cherniack, J.-H. Hwang, W. Lindner, A. Maskey, A. Rasin, E. Ryvkina, et al. The design of the Borealis stream processing engine. CIDR, 2005. [2] T. Akidau, A. Balikov, K. Bekiroglu, S. Chernyak, J. Haberman, R. Lax, S. McVeety, D. Mills, P. Nordstrom, and ˘S. Whittle. Millwheel: fault-tolerant stream processing at Internet scale. PVLDB, 2013. [3] T. Akidau, R. Bradshaw, C. Chambers, S. Chernyak, R. J. Fernandez-Moctezuma, R. Lax, S. McVeety, D. Mills, ´F. Perry, E. Schmidt, et al. The dataflow model: a practical approach to balancing correctness, latency, and cost in massive-scale, unbounded, out-of-order data processing. PVLDB, 2015. [4] A. Alexandrov, R. Bergmann, S. Ewen, J.-C. Freytag, F. Hueske, A. Heise, O. Kao, M. Leich, U. Leser, V. Markl, F. Naumann, M. Peters, A. Rheinlaender, M. J. Sax, S. Schelter, M. Hoeger, K. Tzoumas, and D. Warneke. The stratosphere platform for big data analytics. VLDB Journal, 2014. [5] A. Arasu, B. Babcock, S. Babu, J. Cieslewicz, M. Datar, K. Ito, R. Motwani, U. Srivastava, and J. Widom. Stream: The stanford data stream management system. Technical Report, 2004. [6] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst. HaLoop: Efficient Iterative Data Processing on Large Clusters. PVLDB, 2010. [7] P. Carbone, G. Fora, S. Ewen, S. Haridi, and K. Tzoumas. Lightweight asynchronous snapshots for distributed ´ dataflows. arXiv:1506.08603, 2015. [8] B. Chandramouli, J. Goldstein, M. Barnett, R. DeLine, D. Fisher, J. C. Platt, J. F. Terwilliger, and J. Wernsing. Trill: a high-performance incremental query processor for diverse analytics. PVLDB, 2014. [9] B. Chandramouli, J. Goldstein, and D. Maier. On-the-fly progress detection in iterative stream queries. PVLDB, 2009. [10] S. Chandrasekaran and M. J. Franklin. Psoup: a system for streaming queries over streaming data. VLDB Journal, 2003. [11] K. M. Chandy and L. Lamport. Distributed snapshots: determining global states of distributed systems. ACM TOCS, 1985. [12] J. Dean et al. MapReduce: simplified data processing on large clusters. Communications of the ACM, 2008. [13] D. J. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I. Hsiao, R. Rasmussen, et al. The gamma database machine project. IEEE TKDE, 1990. [14] S. Ewen, K. Tzoumas, M. Kaufmann, and V. Markl. Spinning Fast Iterative Data Flows. PVLDB, 2012. [15] J. Feigenbaum, S. Kannan, A. McGregor, S. Suri, and J. Zhang. On graph problems in a semi-streaming model. Theoretical Computer Science, 2005. [16] B. Gedik, H. Andrade, K.-L. Wu, P. S. Yu, and M. Doo. Spade: the system s declarative stream processing engine. ACM SIGMOD, 2008. [17] F. Hueske, M. Peters, M. J. Sax, A. Rheinlander, R. Bergmann, A. Krettek, and K. Tzoumas. Opening the Black Boxes in Data Flow Optimization. PVLDB, 2012. [18] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. ACM SIGOPS, 2007. [19] M. Kornacker, A. Behm, V. Bittorf, T. Bobrovytsky, C. Ching, A. Choi, J. Erickson, M. Grund, D. Hecht, M. Jacobs, et al. Impala: A modern, open-source sql engine for hadoop. CIDR, 2015. [20] J. Li, K. Tufte, V. Shkapenyuk, V. Papadimos, T. Johnson, and D. Maier. Out-of-order processing: a new architecture for high-performance stream systems. PVLDB, 2008. [21] N. Marz and J. Warren. Big Data: Principles and best practices of scalable realtime data systems. Manning Publications Co., 2015. [22] M. Migliavacca, D. Eyers, J. Bacon, Y. Papagiannis, B. Shand, and P. Pietzuch. Seep: scalable and elastic event processing. ACM Middleware’10 Posters and Demos Track, 2010. [23] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, and M. Abadi. Naiad: a timely dataflow system. ACM SOSP, 2013. [24] B. Saha, H. Shah, S. Seth, G. Vijayaraghavan, A. Murthy, and C. Curino. Apache tez: A unifying framework for modeling and building data processing applications. ACM SIGMOD, 2015. [25] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: Cluster Computing with Working Sets. USENIX HotCloud, 2010. [26] J. Zhou, P.-A. Larson, and R. Chaiken. Incorporating partitioning and parallel plans into the scope optimizer. IEEE ICDE, 2010.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"ZooKeeper Wait-free coordination for Internet-scale systems 中文翻译版","slug":"treatise/zookeeper_wait_free_coordination_for_internet-scale_systems","date":"2021-07-05T14:26:13.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2021/zookeeper_wait_free_coordination_for_internet-scale_systems/","permalink":"https://wangqian0306.github.io/2021/zookeeper_wait_free_coordination_for_internet-scale_systems/","excerpt":"","text":"ZooKeeper: Wait-free coordination for Internet-scale systems 中文翻译版 作者： Patrick Hunt、Mahadev Konar、Flavio P. Junqueira、Benjamin Reed 摘要 在本文中，我们描述了 ZooKeeper，一种用于协调分布式应用程序进程的服务。 由于 ZooKeeper 是关键基础设施的一部分，ZooKeeper 旨在提供一个简单且高性能的内核，用于在客户端构建更复杂的协调原语。 它在一个复制的、集中的服务中整合了来自群消息传递、共享寄存器和分布式锁服务的元素。 ZooKeeper 公开的接口具有共享寄存器的免等待特性，具有类似于分布式文件系统的缓存失效机制的事件驱动机制，以提供简单而强大的协调服务。 ZooKeeper 接口支持高性能服务实现。 除了无等待属性之外，ZooKeeper 还为每个客户端保证请求的 FIFO 执行和所有更改 ZooKeeper 状态的请求的线性化。 这些设计决策可以实现高性能处理管道并使本地服务器可以满足读取请求。 对于目标工作负载，2:1 到 100:1 的读写比率，ZooKeeper 每秒可以处理数万到数十万个事务。 这种性能允许客户端应用程序广泛使用 ZooKeeper。 1 引言 大规模分布式应用需要不同形式的协调服务。 配置是最基本的协调形式之一。 在最简单的形式中，配置只是系统进程的操作参数列表，而更复杂的系统具有动态配置参数。 组成员和领导者选举在分布式系统中也很常见：进程通常需要知道哪些其他进程还活着以及这些进程负责什么。 锁构成了一个强大的协调原语，它实现了对关键资源的互斥访问。 一种协调方法是为每个不同的协调需求开发服务。 例如，Amazon Simple Queue Service [3] 专门关注队列。 其他服务是专门为领导选举 [25] 和配置 [27] 开发的。 实现更强大原语的服务可用于实现功能较弱的原语。 例如，Chubby [6] 是一个具有强同步保证的锁定服务。 然后可以使用锁来实现领导者选举、组成员资格等。 在设计我们的协调服务时，我们不再在服务器端实现特定的原语，而是选择公开一个 API，使应用程序开发人员能够实现他们自己的原语。 这样的选择导致了协调内核的实现，该内核支持新的原语，而无需更改服务核心。 这种方法支持适应应用程序要求的多种形式的协调，而不是将开发人员限制在一组固定的原语中。 在设计 ZooKeeper 的 API 时，我们远离了阻塞原语，例如锁。 协调服务的阻塞原语可能会导致慢速或故障客户端等问题，从而对更快客户端的性能产生负面影响。 如果处理请求依赖于其他客户端的响应和故障检测，则服务本身的实现会变得更加复杂。 因此，我们的系统 ZooKeeper 实现了一个 API，该 API 可以操作简单的无等待数据对象，就像在文件系统中一样分层组织。 实际上，ZooKeeper API 类似于任何其他文件系统，仅从 API 签名来看，ZooKeeper 似乎是没有锁定方法、打开和关闭的 Chubby。 然而，实现免等待数据对象将 ZooKeeper 与基于阻塞原语(例如锁)的系统区分开来。 尽管无等待属性对于性能和容错很重要，但对于协调来说还不够。 我们还必须为运营提供订单保证。 特别是，我们发现保证所有操作的 FIFO 客户端排序和线性化写入可以有效实现服务，并且足以实现我们的应用程序感兴趣的协调原语。 事实上，我们可以使用我们的 API 为任意数量的进程实现共识，并且根据 Herlihy 的层次结构，ZooKeeper 实现了一个通用对象 [14]。 ZooKeeper 服务包含一组使用复制来实现高可用性和性能的服务器。 其高性能使包含大量进程的应用程序能够使用这样的协调内核来管理协调的所有方面。 我们能够使用简单的流水线架构来实现 ZooKeeper，该架构允许我们处理成百上千个未完成的请求，同时仍然实现低延迟。 这样的管道自然能够以 FIFO 顺序从单个客户端执行操作。 保证 FIFO 客户端顺序使客户端能够异步提交操作。 通过异步操作，客户端一次可以有多个未完成的操作。 例如，当新客户端成为领导者并且必须操作元数据并相应地更新它时，此功能是可取的。 没有多个未完成操作的可能性，初始化时间可以是秒级而不是亚秒级。 为了保证更新操作满足线性化，我们实现了一个基于领导者的原子广播协议 [23]，称为 Zab [24]。 然而，ZooKeeper 应用程序的典型工作负载由读取操作主导，因此需要扩展读取吞吐量。 在 ZooKeeper 中，服务器在本地处理读操作，我们不使用 Zab 对它们进行完全排序。 在客户端缓存数据是提高读取性能的一项重要技术。 例如，一个进程缓存当前领导者的标识符而不是每次需要知道领导者时去检测 ZooKeeper。 ZooKeeper 使用监视机制使客户端能够缓存数据，而无需直接管理客户端缓存。 通过这种机制，客户端可以监视给定数据对象的更新，并在更新时收到通知。 Chubby 直接管理客户端缓存。 它阻止更新以使所有缓存正在更改的数据的客户端的缓存无效。 在这种设计下，如果这些客户端中的任何一个运行缓慢或出现故障，更新就会延迟。 Chubby 使用租约来防止有故障的客户端无限期地阻塞系统。 然而，租约只能限制慢速或故障客户端的影响，而 ZooKeeper watches 则完全避免了这个问题。 在本文中，我们将讨论 ZooKeeper 的设计和实现。 使用 ZooKeeper，我们能够实现应用程序所需的所有协调原语，即使只有写入是可线性化的。 为了验证我们的方法，我们展示了我们如何使用 ZooKeeper 实现一些协调原语。 总之，在本文中我们主要贡献有： 协调内核： 我们提出了一种在分布式系统中使用具有宽松一致性保证的无等待协调服务。 特别是，我们描述了协调内核的设计和实现，我们已在许多关键应用程序中使用它来实现各种协调技术。 协调清单： 我们展示了如何使用 ZooKeeper 来构建更高级别的协调原语，甚至是分布式应用程序中经常使用的阻塞和强一致性原语。 协调经验： 分享一些 ZooKeeper 的使用方法，并对其性能进行评估。 2 ZooKeeper 服务 客户端使用 ZooKeeper 库通过 API 向 ZooKeeper 提交请求。 除了通过 API 暴露 ZooKeeper 服务接口之外，客户端库还管理客户端和 ZooKeeper 服务器之间的网络连接。 在本节中，我们首先从架构来概览 ZooKeeper 服务。 然后我们讨论客户端用来与 ZooKeeper 交互的 API。 术语 在本文中，我们使用 client 表示 ZooKeeper 服务的用户，server 表示提供 ZooKeeper 服务的进程，znode 表示 ZooKeeper 数据中的内存数据节点， 它组织在一个分层的命名空间中，称作为数据树。 我们还使用术语更新和写入来指代任何修改数据树状态的操作。 客户端在连接到 ZooKeeper 并获取会话句柄时建立会话，并通过该句柄发出请求。 2.1 服务概览 ZooKeeper 向其客户端提供一组数据节点 (znodes) 的抽象，根据分层名称空间进行组织。 此层次结构中的 znode 是客户端通过 ZooKeeper API 操作的数据对象。 分层命名空间通常用于文件系统。 这是一种组织数据对象的理想方式，因为用户习惯于这种抽象，并且它可以更好地组织应用程序元数据。 为了引用给定的 znode，我们对文件系统路径使用标准的 UNIX 表示法。 例如，我们使用 /A/B/C 来表示 znode C 的路径，其中 C 有 B 作为其父级，B 有 A 作为其父级。 所有 znode 都存储数据，所有 znode，除了临时 znode，都可以有子节点。 客户端可以创建两种类型的 znode： 常规型(Regular)：客户端通过显式创建和删除它们来操作常规 znode。 临时型(Ephemeral)：客户端创建这样的 znode，他们要么明确删除它们，要么让系统在创建它们的会话终止时自动删除它们(故意或由于故障)。 此外，在创建新的 znode 时，客户端可以设置一个顺序标志。 使用顺序标志集(sequential flag)创建的节点具有附加到其名称的单调递增计数器的值。 如果 n 是新的 znode 并且 p 是父节点，则 n 的序列值永远不会小于在 p 下创建的任何其他顺序 znode 的名称中的值。 ZooKeeper 实现了 watches 以允许客户端及时接收更改通知而无需轮询。 当客户端发出设置了监视标志的读取操作时，该操作将正常完成，除非服务器承诺在返回的信息发生更改时通知客户端。 watches 是与会话相关的一次性触发器；一旦触发或会话关闭，它们将被取消注册。 watches 指示发生了更改，但不提供更改。 例如，如果客户端在 “/foo” 更改两次之前发出 getData(“/foo”, true)，客户端将收到一个监视事件，告诉客户端 “/foo” 的数据已更改。 例如连接丢失事件一样的会话事件，也被发送到 watches 回调，以便客户端知道 watches 事件可能会有延迟。 数据模型 ZooKeeper 的数据模型本质上是一个 API 简化的文件系统，只有完整的数据读写，或者是一个具有层次化的键/值表。 分层命名空间对于为不同应用程序的命名空间分配子树以及设置对这些子树的访问权限非常有用。 我们还利用客户端目录的概念来构建更高级别的原语，我们将在 2.4 节中看到。 与文件系统中的文件不同，znode 不是为一般数据存储而设计的。 相反，znode 映射到客户端应用程序的抽象，通常对应于用于协调目的的元数据。 为了说明这一点，在图 1 中，我们有两个子树，一个用于应用程序 1 (/app1)，另一个用于应用程序 2 (/app2)。 应用程序 1 的子树实现了一个简单的组成员协议：每个客户端进程 p_i 在 /app1 下创建一个 znode pi，只要进程正在运行，它就会一直存在。 尽管 znodes 不是为一般数据存储而设计的，但 ZooKeeper 确实允许客户端存储一些可用于分布式计算中的元数据或配置的信息。 例如，在基于领导者的应用程序中，这对于刚刚开始了解哪个其他服务器当前是领导者的应用程序服务器很有用。 为了实现这个目标，我们可以让当前的领导者将这些信息写在 znode 空间中的一个已知位置。 znode 还具有与时间戳和版本计数器相关联的元数据，这允许客户端跟踪对 znode 的更改并根据 znode 的版本执行条件更新。 会话 客户端连接到 ZooKeeper 并发起会话。 会话在到达配置的事件后会超时。 如果 ZooKeeper 在超过该超时时间而没有从其会话中收到任何内容，则认为客户端有故障。 当客户端明确关闭会话 handle 或 ZooKeeper 检测到客户端故障时，会话关闭。 在会话中，客户端观察反映其操作执行的一系列状态变化。 会话使客户端能透明地在 ZooKeeper 集合内从一台服务器移动到另一台服务器，而不中断服务。 2.2 客户端 API 我们在下面介绍 ZooKeeper API 的一个相关子集，并讨论每个请求的语义。 create(path, data, flags)： 创建一个 znode 将其路径名设置为 path 并存储 data[] 数据，然后返回新 znode 的名称。 flags 标识能让客户端选择 znode 的类型，具体是常规型还是临时型。 delete(path, version)： 如果 znode 处于预期版本，则删除 znode 路径。 exists(path, watch)： 如果具有路径名 path 的 znode 存在，则返回 true，否则返回 false。 watch 标志使客户端能够在 znode 上设置监视。 getData(path, watch)： 返回与 znode 关联的数据和元数据，例如版本信息。 watch 标志的工作方式与它对exists() 的工作方式相同，除了如果znode 不存在ZooKeeper 不会设置监视。 setData(path, data, version)： 如果版本号是 znode 的当前版本，则将 data[] 写入 znode 路径。 getChildren(path, watch)： 返回 znode 子节点的名称集。 sync(path): 等待所有挂起的更新操作开始传播向客户端所连接的服务器。 该路径当前被忽略。 注：即等待所有待完成的更新操作完成更新，但是忽略输入路径。 所有方法都在 API 中提供了同步和异步版本。 应用程序在需要执行单个 ZooKeeper 操作并且没有并发任务要执行时使用同步 API，因此它会进行必要的 ZooKeeper 调用并阻塞。 然而，异步 API 使应用程序能够同时执行多个未完成的 ZooKeeper 操作和其他任务。 ZooKeeper 客户端保证按顺序调用每个操作的相应回调。 值得注意的是，ZooKeeper 不使用 handle 来访问 znode。 每个请求都包含正在操作的 znode 的完整路径。 这种选择不仅简化了 API(没有 open() 或 close() 方法)，而且还消除了服务器需要维护的额外状态。 每个更新方法都采用一个预期的版本号，这使得有条件更新的实现成为可能。 如果 znode 的实际版本号与预期版本号不匹配，则更新失败并出现意外版本错误。 如果版本号为 -1，则不执行版本检查。 2.3 ZooKeeper 保证 ZooKeeper 有两个基本的排序保证： 可线性写入： 所有更新 ZooKeeper 状态的请求都是可序列化的并尊重优先级。 先进先出的客户端顺序： 来自给定客户端的所有请求都按照客户端发送的顺序执行。 请注意，我们对线性化的定义与 Herlihy [15] 最初提出的定义不同，我们称之为 A-线性化(异步线性化)。 在 Herlihy 对线性化的最初定义中，一个客户端一次只能有一个未完成的操作(一个客户端是一个线程)。 在我们的情况下，我们允许一个客户端有多个未完成的操作，因此我们可以选择让同一客户端的未完成操作无须的或按照 FIFO 顺序执行。 为了符合设计需求我们选择了后者。 重要的是要观察到所有适用于可线性化对象的结果也适用于 A 可线性化对象，因为满足 A-线性化的系统也同时满足了线性化。 因为只有更新请求是 A-线性化，所以 ZooKeeper 在每个副本本地处理读取请求。 这允许服务随着服务器添加到系统而线性的进行扩张。 要了解这两种保证如何相互作用，请考虑以下场景。 由多个进程组成的系统会选举一个领导者来指挥工作进程。 当新的领导者接管系统时，它必须更改大量配置参数并在完成后通知其他进程。 然后我们有两个重要的要求： 当新的领导开始修改内容时，我们不希望其它的进程使用正在修改的配置。 如果新的领导者在配置完全更新之前死亡，我们不希望其它进程使用这个部分配置。 请注意，分布式锁(例如 Chubby 提供的锁)将有助于满足第一个要求，但不足以满足第二个要求。 有了 ZooKeeper，新的领导者可以指定一条路径作为 ready znode；其他进程只会在该 znode 存在时使用该配置。 新的 leader 通过删除 ready、更新各种配置znode、重新创建 ready 来改变配置。 所有这些变动都可以通过管道进行异步发布，以快速更新配置状态。 尽管更改操作的延迟在 2 毫秒的数量级，但如果请求一个接一个地发出，更新 5000 个不同 znode 的新领导者将花费 10 秒； 如果通过异步发出请求，则可以在 1 秒钟之内完成。 由于顺序保证，如果一个进程看到 ready znode，它也必须看到新领导者所做的所有配置更改。 如果在创建 ready znode 之前新领导者死亡，则其他进程知道配置尚未最终确定并且不会使用它。 上述方案仍然有一个问题：如果一个进程在新的领导者开始进行更改之前看到 ready znode 存在，然后在更改正在进行时开始读取配置会发生什么。 这个问题是通过通知的顺序保证来解决的：如果客户端正在观察更改，客户端将在看到更改后系统的新状态之前看到通知事件。 因此，如果读取 ready znode 的进程请求收到有关该 znode 更改的通知，它将在读取任何新配置之前看到客户端更改的通知。 当客户端除了 ZooKeeper 之外还有自己的通信通道时，会出现另一个问题。 例如，考虑两个客户端 A 和 B，它们在 ZooKeeper 中具有共享配置并通过共享通信通道进行通信。 如果 A 更改 ZooKeeper 中的共享配置并通过共享通信通道告诉 B 更改，B 将期望在重新读取配置时看到更改。 如果 B 的 ZooKeeper 副本稍微落后于 A，则它可能看不到新配置。 使用上述保证 B 可以通过在重新读取配置之前发出写入来确保它看到最新的信息。 为了更有效地处理这种情况，ZooKeeper 提供了 sync 请求：跟随进行读取的操作被称为慢读。 sync 使得服务器先完成读取请求然后再完成所有等待的写入请求而没有产生完全写入的开销。 这个原语在思想上类似于 ISIS [5] 的 flush 。 ZooKeeper 还具有以下两个活动性和持久性保证： 如果大多数 ZooKeeper 服务器处于活动状态则通信服务将可用； 如果 ZooKeeper 服务成功响应更改请求，则只要仲裁服务器最终能够恢复，该更改就会在任何数量节点的故障中正常恢复。 2.4 原语的例子 在本节中，我们将展示如何使用 ZooKeeper API 来实现更强大的原语。 ZooKeeper 服务对这些更强大的原语一无所知，因为它们完全在客户端使用 ZooKeeper 客户端 API 实现。 一些常见的原语，如组成员资格和配置管理，也是免等待的。 对于其他的，比如集合点，客户端需要等待一个事件。 即使 ZooKeeper 是无等待的，我们也可以使用 ZooKeeper 实现高效的阻塞原语。 ZooKeeper 的排序保证允许对系统状态进行有效推理，而 watches 则允许有效等待。 配置管理 ZooKeeper 可用于在分布式应用程序中实现动态配置管理。 配置以最简单的形式存储在 znode Zc 中。 进程以 Zc 的完整路径名启动。 启动进程通过读取 zc 并将 watch 标志设置为 true 来获取它们的配置。 如果 zc 中的配置被更新，进程会收到通知并读取新配置，再次将 watch 标志设置为 true。 请注意，在此方案中，与大多数其他使用 watch 的方案一样，确保进程具有最新信息。 例如，如果一个正在观察 Zc 的进程收到 Zc 更改的通知，并且在它可以为 Zc 发出读取之前， 还有三个 Zc 更改，则该进程不会收到另外三个通知事件。 这不会影响进程的行为，因为这三个事件只会通知进程它已经知道的事情：它拥有的关于 Zc 的信息是陈旧的。 集合 有时在分布式系统中，最终系统配置的外观并不总是先验清楚的。 客户端可能想要启动一个主进程和几个工作进程，但是启动进程是由调度程序完成的，因此客户端不知道它可以给工作进程连接到主进程的地址和端口等信息。 我们通过 ZooKeeper 使用客户端创建的集合点 znode Zr 来处理这种情况。 客户端将 Zr 的完整路径名作为主进程和工作进程的启动参数进行传递。 当主进程启动时，它会在 Zr 中填入有关它正在使用的地址和端口的信息。 当工作进程启动时，它会在 watch 设置为 true 的情况下读取 Zr 。 如果 Zr 尚未填写，则工作进程会等待 Zr 更新时收到通知。 如果 Zr 是一个临时节点，主进程和工作进程可以监视 Zr 的删除并在客户端结束时清理自己。 组关系 我们利用临时节点来实现组成员身份。 具体来说，我们使用临时节点允许我们查看创建节点的会话状态这一事实。 我们首先指定一个 znode Zg 来表示该组。 当组的进程成员启动时，它会在 Zg 下创建一个临时子 znode。 如果每个进程都有唯一的名称或标识符，则该名称将用作子 znode 的名称；否则，该进程会使用 SEQUENTIAL 标志创建 znode 以获得唯一的名称分配。 例如，进程可以将进程信息放入子 znode 的数据中，例如进程使用的地址和端口。 在 Zg 下创建子 znode 后，进程正常启动。 它不需要做任何其他事情。 如果进程失败或结束，在 Zg 下代表它的 znode 将被自动删除。 进程可以通过简单地列出 Zg 的子进程来获取组信息。 如果进程想要监视组成员身份的更改，则该进程可以将监视标志设置为 true 并在收到更改通知时刷新组信息(始终将监视标志设置为 true)。 简单的锁 ZooKeeper 虽然不是锁服务，但是可以用来实现锁。 使用 ZooKeeper 的应用程序通常使用根据其需要定制的同步原语，如上所示。 这里我们展示了如何使用 ZooKeeper 实现锁，以表明它可以实现多种通用同步原语。 最简单的锁实现使用 “锁文件”。 锁由一个 znode 表示。 为了获取锁，客户端尝试使用 EPHEMERAL 标志创建指定的 znode。 如果创建成功，客户端持有锁。 否则，如果当前领导者死亡，客户端可以读取设置了监视标志的 znode 以得到通知。 客户端在死亡或显式删除 znode 时释放锁。 等待锁的其他客户端一旦观察到 znode 被删除，就会再次尝试获取锁。 虽然这个简单的锁定协议有效，但它确实存在一些问题。 首先，它受到羊群效应(herd effect)的影响。 如果有很多客户端在等待获取锁，即使只有一个客户端可以获取锁，他们也会在锁被释放时争夺锁。 其次，它只实现了排他锁。 以下两个原语展示如何克服这两个问题。 解决羊群效应简单锁 我们定义了一个锁 znode l 来实现这样的锁。 直观地，我们将所有请求锁定的客户端排列起来，每个客户端按照请求到达的顺序获得锁定。 加锁 123456n = create(l+“/lock-”, EPHEMERAL|SEQUENTIAL)C = getChildren(l, false)if n is lowest znode in C, exitp = znode in C ordered just before nif exists(p, true) wait for watch eventgoto 2 解锁 1delete(n) 在 Lock 部分的第 1 行中使用 SEQUENTIAL 标志命令使应用程序按顺序申请锁。 如果客户端的 znode 在第 3 行具有最低的序列号，则客户端持有锁。 否则，客户端会等待加锁程序删除 znode 或者从持有锁的客户端获得锁。 通过观测客户端 znode 之前获得锁的 znode，我们避免了羊群效应，当锁被释放或锁请求被放弃时，我们只唤醒一个进程。 一旦客户端正在监视的 znode 消失，客户端必须检查它现在是否持有锁。 (之前的锁请求可能已经被放弃，并且有一个序列号较低的 znode 仍在等待或持有锁。) 释放锁就像删除代表锁请求的 znode n 一样简单。 通过在创建时使用 EPHEMERAL 标志，崩溃的进程将自动清除任何锁请求或释放它们可能拥有的任何锁。 综上所述，这种加锁方案有以下优点： 删除一个 znode 只会导致一个客户端唤醒，因为每个 znode 正好被另一个客户端监视，所以我们不会受羊群效应影响。 没有轮询或超时。 由于我们实现了锁的方式，我们可以通过浏览 ZooKeeper 的数据看到锁争用的数量，断锁，以及调试锁。 读/写锁 为了实现读/写锁，我们稍微改变了锁程序，并有单独的读锁和写锁程序。 解锁过程与全局锁定情况相同。 写锁 123456n = create(l+“/write-”, EPHEMERAL|SEQUENTIAL)C = getChildren(l, false)if n is lowest znode in C, exitp = znode in C ordered just before nif exists(p, true) wait for eventgoto 2 读锁 123456n = create(l+“/read-”, EPHEMERAL|SEQUENTIAL)C = getChildren(l, false)if no write znodes lower than n in C, exitp = write znode in C ordered just before nif exists(p, true) wait for eventgoto 3 此锁定过程与以前的锁定略有不同。 写锁仅在命名上有所不同。 由于读锁可能是共享的，第 3 行和第 4 行略有不同，因为只有较早的写锁 znode 会阻止客户端获取读锁。 当有多个客户端等待读锁并在删除具有较低序列号的“写” znode 时得到通知，我们可能会遇到“羊群效应”； 事实上，这是一种理想的行为，所有这些读取客户端都应该被释放，因为它们现在可能拥有锁。 双重屏障 双屏障使客户端能够同步计算的开始和结束。 当由屏障阈值定义的足够进程加入屏障时，进程开始运行计算并在完成后离开屏障。 我们用 znode 表示 ZooKeeper 中的屏障，称为 b。 每个进程 p 在进入屏障时都向 b 注册(通过创建一个 znode 作为 b 的子节点)，在运算结束后进行注销(删除对应子节点)。 当 b 的子节点数超过屏障阈值时，进程可以进入屏障。 当所有进程都删除了它们的子进程时，进程可以离开屏障。 我们使用 watche 来有效地等待进入和退出条件得到满足。 在注册过程中，进程会观察 b 的就绪子进程的数量，然后进程会创建子进程导致子进程数量超过阈值。 在注销过程中，进程会观察运行完成的子进程，并且仅在该 znode 被删除后才检查退出条件。 3 ZooKeeper 应用程序 我们现在描述一些使用 ZooKeeper 的应用程序，并简要解释他们如何使用它。 我们以粗体显示每个示例的原语。 获取服务 数据爬取是搜索引擎的重要组成部分，而雅虎抓取了数十亿个 Web 文档。 获取服务(FS) 是雅虎爬取系统其中的一个服务。 本质上，它具有控制页面获取进程的主进程。 master 为 fetchers 提供配置，fetchers 写回通知他们的状态和健康状况。 使用 ZooKeeper for FS 的主要优点是从 master 的故障中恢复，即使出现故障也能保证可用性，以及将客户端与服务器解耦， 允许它们通过从 ZooKeeper 读取它们的状态来将它们的请求定向到健康的服务器。 因此，FS 主要使用 ZooKeeper 来管理 配置元数据(configuration metadata)，尽管它也使用 ZooKeeper 来选举 master (领导选举(leader election))。 注：每个点代表一个一秒的样本。 图 2 显示了 FS 使用的 ZooKeeper 服务器在三天内的读写流量。 为了生成这个图，我们计算了该时间段内每秒的操作次数，每个点对应于该秒的操作次数。 我们观察到，与写入流量相比，读取流量要高得多。 在速率高于每秒 1000 次操作的期间，读写比率在 10:1 和 100:1 之间变化。 此工作负载中的读取操作是 getData()、getChildren() 和 exists()，按流行程度递增。 Katta Katta [17] 是一个使用 ZooKeeper 进行协调的分布式索引器，它不是雅虎公司的应用。 Katta 使用分片来划分索引工作。 主服务器将分片分配给从服务器并跟踪进度。 主服务器也可能发生故障，因此其他服务器必须准备好在发生故障时接管。 Katta 使用 ZooKeeper 跟踪从服务器和主服务器(组成员身份)的状态，并处理主故障转移(领导选举)。 Katta 还使用 ZooKeeper 来跟踪和传播分片到从属设备的分配(配置管理)。 雅虎消息广播 雅虎消息广播服务(YMB)是一个分布式发布订阅系统。 系统管理数以千计的主题，客户端可以向这些主题发布消息和接收消息。 主题分布在一组服务器中以提供可伸缩性。 每个主题都使用主备份方案进行复制，以确保将消息复制到两台机器以确保可靠的消息传递。 组成 YMB 的服务器使用无共享分布式架构，这使得协调对于正确操作至关重要。 YMB 使用 ZooKeeper 管理主题的分发(配置元数据(configuration metadata))， 处理系统中机器的故障(故障检测和组成员身份(failure detection, group membership))，并控制系统运行。 图 3 显示了 YMB 的部分 znode 数据结构。 每个代理域都有一个称为节点的 znode，组成 YMB 服务的每个活动服务器都有一个临时 znode。 每个 YMB 服务器在节点下创建一个临时 znode，其负载和状态信息通过 ZooKeeper 提供组成员身份和状态信息。 禁止关闭和迁移等节点由构成该服务的所有服务器进行监控，并允许对 YMB 进行集中控制。 对于 YMB 管理的每个主题，主题目录都有一个子 znode。 这些主题 znode 具有子 znode，它们指示每个主题的主服务器和备份服务器以及该主题的订阅者。 主服务器和备用服务器 znode 不仅允许服务器发现负责主题的服务器，而且它们还管理 领导者选举(leader election) 和处理服务器崩溃。 4 ZooKeeper 实现 ZooKeeper 通过在组成服务的每个服务器上复制 ZooKeeper 数据来提供高可用性。 我们假设服务器因崩溃而失败，并且此类故障服务器稍后可能会恢复。 图 4 显示了 ZooKeeper 服务的高级组件。 收到请求后，服务器准备执行(请求处理器)。 如果这样的请求需要服务器之间的协调(写请求)，那么它们使用协议协议(原子广播的实现)，最后服务器提交对 ZooKeeper 数据库的更改， 并在集合的所有服务器之间完全复制。 在读取请求的情况下，快速读取本地数据库的状态并生成对请求的响应。 复制数据库是包含整个数据树的内存数据库。 树中的每个 znode 默认存储最大 1MB 的数据，但是这个最大值是一个配置参数，可以在具体案例。 为了可恢复性，我们有效地将更新记录到磁盘，并在将写入应用到内存数据库之前强制写入磁盘介质。 事实上，作为 Chubby [8]，我们保留已提交操作的重放日志(在我们的例子中是预写日志)并生成内存数据库的定期快照。 每个 ZooKeeper 服务器都为客户端提供服务。 客户端仅连接到一台服务器以提交其请求。 如前所述，读取请求由每个服务器数据库的本地副本提供服务。 改变服务状态的请求，写请求，由协议协议处理。 作为协议协议的一部分，写请求被转发到单个服务器，称为领导者(leader)。 其余的 ZooKeeper 服务器，称为追随者(follower)，从领导者接收由状态变化组成的消息提议，并就状态变化达成一致。 4.1 请求处理器 由于消息传递层是原子性的，我们保证本地副本永远不会发散，尽管在任何时间点某些服务器可能比其他服务器应用了更多的事务。 与客户端发送的请求不同，事务是幂等的。 当 leader 收到写入请求时，它会计算应用写入时系统的状态，并将其转换为捕获此新状态的事务。 必须计算未来状态，因为可能存在尚未应用到数据库的未完成事务。 例如，如果客户端执行条件 setData 并且请求中的版本号与正在更新的 znode 的未来版本号匹配，则服务生成一个 setDataTXN，其中包含新数据、新版本号和更新的时间戳。 如果出现错误，例如版本号不匹配或要更新的 znode 不存在，则生成一个 errorTXN。 4.2 原子性的广播 所有更新 ZooKeeper 状态的请求都会转发给领导者。 领导者执行请求并通过原子广播协议 Zab [24] 广播对 ZooKeeper 状态执行更改。 收到客户端请求的服务器在收到相应的状态变化的请求时响应客户端。 Zab 默认使用简单多数仲裁来决定提案，因此 Zab 和 ZooKeeper 只能在大多数服务器正确时才能工作 (即，集群中的总量为 2f+1 个服务器，我们可以容忍 f 个服务器故障) 为了实现高吞吐量，ZooKeeper 尝试保持完整的请求处理管道。 在处理管道的不同部分可能有数千个请求。 由于状态变化依赖于先前状态变化的应用，Zab 提供了比常规原子广播更强的顺序保证。 更具体地说，Zab 保证领导者广播的更改请求按照发送的顺序进行传递，并且在广播自己的更改之前，将先前领导者的所有更改传递给已建立的领导者。 有一些实现细节可以简化我们的实现并为我们提供出色的性能。 我们使用 TCP 进行传输，因此消息顺序由网络维护，这使我们能够简化我们的实现。 我们使用 Zab 选择的领导者作为 ZooKeeper 领导者，以便创建事务的相同过程也提出它们。 我们使用日志来跟踪建议作为内存数据库的预写日志，这样我们就不必将消息两次写入磁盘。 在正常操作期间，Zab 确实按顺序传递所有消息，并且只传递一次，但由于 Zab 不会持久记录每个传递的消息的 id，因此 Zab 可能会在恢复期间重新传递消息。 因为我们使用幂等交易，所以只要按顺序交付，多次交付是可以接受的。 事实上，ZooKeeper 要求 Zab 至少重新传递在上一个快照开始后传递的所有消息。 4.3 数据库副本 每个副本在 ZooKeeper 状态的内存中都有一个副本。 当 ZooKeeper 服务器从崩溃中恢复时，它需要恢复这个内部状态。 在服务器运行一段时间后，重放所有已传递的消息以恢复状态将花费非常长的时间，因此 ZooKeeper 使用定期快照，并且只需要从快照开始后重新传递消息。 我们称 ZooKeeper 快照为模糊快照，因为我们不锁定 ZooKeeper 状态来拍摄快照； 相反，我们对树进行深度优先扫描，以原子方式读取每个 znode 的数据和元数据并将它们写入磁盘。 由于生成的模糊快照可能应用了快照生成期间传递的状态更改的某些子集，因此结果可能与 ZooKeeper 在任何时间点的状态都不对应。 但是，由于状态更改是幂等的，只要我们按顺序应用状态更改两次就可以确定的恢复状态。 例如，假设在 ZooKeeper 数据树中，两个节点 /foo 和 /goo 分别具有值 f1 和 g1，并且在模糊快照开始时都处于版本 1， 状态变更流以如下形式到达：&lt;htransactionType, path, value, new-version&gt; 123&lt;SetDataTXN, /foo, f2, 2&gt;&lt;SetDataTXN, /goo, g2, 2&gt;&lt;SetDataTXN, /foo, f3, 3&gt; 处理这些状态更改后，/foo 和 /goo 分别具有版本 3 和 2 的值 f3 和 g2。 但是，模糊快照可能记录了 /foo 和 /foo 分别具有版本 3 和 1 的值 f3 和 g1，这不是 ZooKeeper 数据树的有效状态。 如果服务器崩溃并使用此快照恢复，并且 Zab 重新传递状态更改，则结果状态对应于崩溃前服务的状态。 4.4 客户端-服务器交互 当服务器处理写入请求时，它还会发送并清除与该更新对应的任何监视相关的通知。 服务器按顺序处理写入，不会同时处理其他写入或读取。 这确保了严格的通知连续性。 请注意，服务器在本地处理通知。 只有客户端连接到的服务器会跟踪和触发该客户端的通知。 读取请求在每个服务器本地处理。 每个读取请求都被处理并用一个 zxid 标记，该 zxid 对应于服务器看到的最后一个事务。 这个 zxid 定义了读请求相对于写请求的部分顺序。 通过在本地处理读取，我们获得了出色的读取性能，因为它只是本地服务器上的内存操作，没有磁盘活动或协议要运行。 这种设计选择是我们在读取占主导地位的工作负载下实现卓越性能目标的关键。 使用快速读取的一个缺点是不能保证读取操作的优先顺序。 也就是说，即使已提交对同一 znode 的更新操作，读取操作也可能返回陈旧值。 并非我们所有的应用程序都需要优先顺序，但对于确实需要它的应用程序，我们已经实现了 sync 源语。 该原语异步执行，并在所有未写入的操作写入本地副本后由领导者进行排序。 为了保证给定的读取操作返回最新更新的值，客户端调用 sync 源语，然后进行读取操作。 客户端操作的 FIFO 顺序保证以及 sync 源语的全局保证使读取操作可以反映出变更发布之前的任何更改。 在我们的实现中，我们不需要原子性的广播同步，因为我们使用基于领导者的算法，我们只需将同步操作放在领导者和执行同步调用的服务器之间的请求队列的末尾。 为了使其工作，追随者必须确保领导者仍然是领导者。 如果有待提交的事务进行提交，则服务器不会怀疑领导者。 如果待处理队列为空，则领导者需要发出并提交一个空事务并在该事务之后对 sync 源语进行排序。 这有一个很好的特性，即当领导者处于负载状态时，不会产生额外的广播流量。 在我们的实现中，超时设置为使领导者在追随者放弃他们之前意识到他们不是领导者，因此我们不会发出空事务。 ZooKeeper 服务器以 FIFO 顺序处理来自客户端的请求。 响应包括响应相关的 zxid。 甚至在没有活动的间隔期间的心跳消息也包括客户端连接到的服务器看到的最后一个 zxid。 如果客户端连接到新服务器，该新服务器通过检查客户端的最后一个 zxid 与其最后一个 zxid 来确保它的 ZooKeeper 数据视图至少与客户端的视图一样新。 如果客户端拥有比服务器更新的视图，则服务器不会与客户端重新建立会话，直到服务器的版本与客户端相同。 保证客户端能够找到另一个具有系统最新视图的服务器，因为客户端只能看到已复制到大多数 ZooKeeper 服务器的更改。 这种行为对于保证持久性很重要。 ZooKeeper 使用超时机制来检测客户端会话失败。 如果在会话超时内没有其他服务器从客户端会话接收任何内容，则领导者确定其存在故障。 如果客户端发送请求的频率足够高，则无需发送任何其他消息。 否则，客户端会在低活动期间发送心跳消息。 如果客户端无法与服务器通信以发送请求或心跳，它会连接到不同的 ZooKeeper 服务器以重新建立其会话。 为了防止会话超时，ZooKeeper 客户端库在会话空闲 s/3 ms 后发送心跳，如果 2s/3ms 没有收到服务器的消息，则切换到新服务器，其中 s 是会话超时参数以毫秒为单位。 5 评估 为了评估我们的系统，我们对系统饱和时的吞吐量以及各种注入故障的吞吐量变化进行了基准测试。 我们改变了组成 ZooKeeper 服务的服务器数量，但始终保持客户端数量不变。 为了模拟大量客户端，我们使用了 35 台机器来模拟 250 个并发客户端。 我们使用 Java 实现的 ZooKeeper 服务器，以及 Java 和 C 客户端。 对于这些实验，我们使用一个 Java 服务器将日志记录到一个磁盘上并在另一个磁盘上拍摄快照。 我们的基准测试客户端使用异步 Java 客户端 API，每个客户端至少有 100 个未完成的请求。 每个请求都包含对 1K 数据的读取或写入。 我们没有显示其他操作的基准，因为所有修改状态的操作的性能大致相同，非状态修改操作的性能(不包括同步)大致相同。 (同步的性能接近于轻量级写入，因为请求必须发送到领导者，但不会被广播。) 客户端每 300 毫秒发送一次已完成操作的计数，我们每 6 秒采样一次。 为了防止内存溢出，服务器会限制系统中并发请求的数量。 ZooKeeper 使用请求限制来防止服务器不堪重负。 对于这些实验，我们将 ZooKeeper 服务器配置为最多处理 2,000 个请求。 服务器数量 100% 读取 0% 读取 13 460k 8k 9 296k 12k 7 257k 14k 5 165k 18k 3 87k 21k 表 1：饱和系统的极端吞吐量性能 在图 5 中，我们显示了我们改变读写请求比率时的吞吐量，每条曲线对应于提供 ZooKeeper 服务的不同数量的服务器。 表 1 显示了读取负载极端情况下的数据。 读取吞吐量高于写入吞吐量，因为读取不使用原子广播。 该图还显示，服务器数量也对广播协议的性能产生负面影响。 从这些图中，我们观察到系统中的服务器数量不仅会影响服务可以处理的故障数量，还会影响服务可以处理的工作负载。 请注意，三台服务器的曲线与其他服务器的交叉率约为 60%。 这种情况不仅限于三服务器配置，并且由于启用了并行本地读取，所有配置都会发生这种情况。 然而，对于图中的其他配置无法观察到，因为我们已经限制了最大 y 轴吞吐量以提高可读性。 以下两个原因导致写请求比读请求花费更长的时间。 首先，写请求必须经过原子广播，这需要一些额外的处理并增加请求的延迟。 更长处理写入请求的另一个原因是服务器必须确保在将确认发送回领导者之前将事务记录到非易失性存储中。 原则上，这个要求是多余的，但对于我们的生产系统，我们用性能换取可靠性，因为 ZooKeeper 构成了应用程序的基本事实。 我们使用更多的服务器来容忍更多的故障。 我们通过将 ZooKeeper 数据划分为多个 ZooKeeper 集合来增加写入吞吐量。 Gray 等人先前已经观察到复制和分区之间的这种性能权衡 [12]。 ZooKeeper 能够通过在构成服务的服务器之间分配负载来实现如此高的吞吐量。 由于我们宽松的一致性保证，我们可以分配负载。 Chubby 客户端将所有请求发送给领导者。 图 6 显示了如果我们不利用这种放松并强制客户端只连接到领导者会发生什么。 正如预期的那样，读取主导的工作负载的吞吐量要低得多，但即使是写入主导的工作负载，吞吐量也更低。 由服务客户端引起的额外 CPU 和网络负载会影响领导者协调提案广播的能力，进而对整体写入性能产生不利影响。 注：误差线表示最小值和最大值 原子广播协议完成了系统的大部分工作，因此比任何其他组件都更能限制 ZooKeeper 的性能。 图 7 显示了原子广播组件的吞吐量。 为了对其性能进行基准测试，我们通过直接在领导者处生成交易来模拟客户端，因此没有客户端连接或客户端请求和回复。 在最大吞吐量下，原子广播组件变得受 CPU 限制。 理论上，图 7 的性能将与 ZooKeeper 100% 写入的性能相匹配。 CPU 的争用将 ZooKeeper 的吞吐量降低到远低于孤立的原子广播组件。 因为 ZooKeeper 是一个关键的生产组件，到目前为止，我们对 ZooKeeper 的开发重点一直是正确性和健壮性。 通过消除额外副本、同一对象的多个序列化、更高效的内部数据结构等，有很多机会可以显着提高性能。 为了显示系统在注入故障时随时间的行为，我们运行了一个由 5 台机器组成的 ZooKeeper 服务。 我们运行了与之前相同的饱和基准测试，但这次我们将写入百分比保持在 30% 不变，这是我们预期工作负载的保守比例。 我们定期杀死一些服务器进程。 图 8 显示了随时间变化的系统吞吐量。 图中标注的事件如下： 一个追随者的故障和恢复 不同追随者的故障和恢复 领导者故障 前两个标记(a,b)中的两个追随者故障，在第三个标记©恢复； 领导者故障 领导者恢复 从这张图中有一些重要的观察结果。 首先，如果跟随者失败并快速恢复，那么即使出现故障，ZooKeeper 也能够维持高吞吐量。 单个跟随者的失败并不会阻止服务器形成仲裁，并且只会大致降低服务器在失败前处理的读取请求份额的吞吐量。 其次，我们的领导者选举算法能够足够快地恢复以防止吞吐量大幅下降。 在我们的观察中，ZooKeeper 花费不到 200 毫秒的时间来选举一个新的领导者。 因此，尽管服务器在几分之一秒内停止服务请求，但由于我们的采样周期，我们没有观察到零吞吐量，这是几秒的数量级。 第三，即使追随者需要更多时间来恢复，一旦他们开始处理请求，ZooKeeper 也能够再次提高吞吐量。 在事件 1、2 和 4 之后我们没有恢复到完整吞吐量级别的一个原因是客户端仅在与跟随者的连接中断时才切换跟随者。 因此，在事件 4 之后，客户端不会重新分配自己，直到领导者在事件 3 和 5 中失败。 在实践中，随着客户的来来去去，这种不平衡会随着时间的推移而自行解决。 5.2 请求延迟 为了评估请求的延迟，我们创建了一个以 Chubby 基准 [6] 为模型的基准。 我们创建一个工作进程，它只是发送一个创建操作，然后等待它完成，之后发送一个新节点的异步删除请求，然后开始下一个创建。 我们相应地改变了进程的数量，对于每次运行，我们让每个进程创建 50,000 个节点。 我们通过将完成的创建请求数除以所有进程完成所需的总时间来计算吞吐量。 工作进程数量 3台服务器 5台服务器 7台服务器 9台服务器 1 776 748 758 711 10 2074 1832 1572 1540 20 2740 2336 1934 1890 表 2：创建每秒处理的请求 表 2 显示了我们的基准测试结果。 创建请求包括 1K 数据，而不是 Chubby 基准测试中的 5 个字节，以更好地符合我们的预期用途。 即使有这些更大的请求，ZooKeeper 的吞吐量也比 Chubby 公布的吞吐量高出 3 倍以上。 单个 ZooKeeper worker 基准测试的吞吐量表明，三台服务器的平均请求延迟为 1.2ms，9 台服务器为 1.4ms。 5.3 屏障的性能表现 在这个实验中，我们依次执行了许多屏障来评估使用 ZooKeeper 实现的原语的性能。 对于给定数量的障碍 b，每个客户端首先进入所有 b 个障碍，然后依次离开所有 b 个障碍。 当我们使用第 2.4 节的双屏障算法时，客户端首先等待所有其他客户端执行 enter() 过程，然后再进入下一个调用(类似于 leave())。 屏障数量 50 个客户端 100 个客户端 200 个客户端 200 9.4 19.8 41.0 400 16.4 34.1 62.0 800 28.9 55.9 112.1 1600 54.0 102.7 234.4 我们在表 3 中报告了我们的实验结果。 在这个实验中，我们有 50、100 和 200 个客户端连续进入 b 个障碍，b ∈ {200, 400, 800, 1600}。 尽管一个应用程序可以有数千个 ZooKeeper 客户端，但通常只有更小的子集参与每个协调操作，因为客户端通常根据应用程序的细节进行分组。 该实验的两个有趣观察结果是，处理所有屏障的时间与屏障的数量大致呈线性增长，这表明对数据树同一部分的并发访问不会产生任何意外延迟，并且延迟与增加的客户端数量成正比。 事实上，我们观察到，即使客户端以锁步方式进行，在所有情况下，屏障操作(进入和离开)的吞吐量在每秒 1,950 到 3,100 次操作之间。 在 ZooKeeper 操作中，这对应于每秒 10,700 到 17,000 次操作之间的吞吐量值。 由于在我们的实现中，读取与写入的比率为 4:1（读取操作的 80%），与 ZooKeeper 可以实现的原始吞吐量(根据图 5 超过 40,000)相比，我们的基准代码使用的吞吐量要低得多。 这是因为客户端在等待其他客户端。 6 相关的工作 ZooKeeper 的目标是提供一种服务，以缓解分布式应用程序中协调进程的问题。 为了实现这个目标，它的设计使用了以前的协调服务、容错系统、分布式算法和文件系统的思想。 我们并不是第一个提出分布式应用程序协调系统的人。 一些早期系统为事务应用程序 [13] 和在计算机集群中共享信息 [19] 提出了分布式锁服务。 最近，Chubby 提出了一个系统来管理分布式应用程序的咨询锁 [6]。 Chubby 分享了 ZooKeeper 的几个目标。 它还具有类似文件系统的接口，并使用协议协议来保证副本的一致性。 但是，ZooKeeper 不是锁服务。 客户端可以使用它来实现锁，但它的 API 中没有锁操作。 与 Chubby 不同，ZooKeeper 允许客户端连接到任何 ZooKeeper 服务器，而不仅仅是领导者。 ZooKeeper 客户端可以使用其本地副本来提供数据和管理 watches，因为它的一致性模型比 Chubby 宽松得多。 这使得 ZooKeeper 能够提供比 Chubby 更高的性能，允许应用程序更广泛地使用 ZooKeeper。 文献中提出了容错系统，目的是减轻构建容错分布式应用程序的问题。 一种早期系统是 ISIS [5]。 ISIS 系统将抽象类型规范转化为容错的分布式对象，从而使容错机制对用户透明。 Horus [30] 和 Ensemble [31] 是从 ISIS 演变而来的系统。 ZooKeeper 采用了 ISIS 虚拟同步的概念。 最后，Totem 在利用局域网硬件广播的体系结构中保证消息传递的总顺序 [22]。 ZooKeeper 与各种网络拓扑一起工作，这促使我们依赖服务器进程之间的 TCP 连接，而不假设任何特殊的拓扑或硬件功能。 我们也不公开 ZooKeeper 内部使用的任何集成通信。 构建容错服务的一项重要技术是状态机复制 [26]，而 Paxos [20] 是一种算法，可以为异步系统有效实现复制状态机。 我们使用的算法具有 Paxos 的一些特征，但它将共识所需的事务日志记录与数据树恢复所需的预写日志记录相结合，以实现高效的实现。 目前已经存在用于实现 Byzantine 灾难冗余复制状态机的协议 [7, 10, 18, 1, 28]。 ZooKeeper 不假设服务器可以是 Byzantine 式的，但我们确实采用了校验和和健全性检查等机制来捕获非恶意的 Byzantine 式故障。 克莱门特等人讨论了一种在不修改当前服务器代码库的情况下使 ZooKeeper 完全具有 Byzantine 灾难冗余的方法 [9]。 迄今为止，我们还没有观察到使用完全 Byzantine 灾难冗余协议可以防止的生产故障。 [29]。 Boxwood [21] 是一个使用分布式锁服务器的系统。 Boxwood 为应用程序提供了更高级别的抽象，它依赖于基于 Paxos 的分布式锁服务。 和 Boxwood 一样，ZooKeeper 也是一个用于构建分布式系统的组件。 但是，ZooKeeper 具有高性能要求，在客户端应用程序中使用更广泛。 ZooKeeper 公开应用程序用于实现更高级别原语的较低级别原语。 ZooKeeper 类似于一个小型文件系统，但它仅提供文件系统操作的一小部分，并添加了大多数文件系统中不存在的功能，例如排序保证和条件写入。 然而，ZooKeeper 监视在本质上类似于 AFS [16] 的缓存回调。 Sinfonia [2] 引入了迷你交易，这是一种构建可扩展分布式系统的新范例。 Sinfonia 旨在存储应用程序数据，而 ZooKeeper 存储应用程序元数据。 ZooKeeper 将其状态完全复制并保存在内存中，以实现高性能和一致的延迟。 我们使用像操作和排序这样的文件系统可以实现类似于小交易的功能。 znode 是一种方便的抽象，我们可以在其上添加监视，这是 Sinfonia 中缺少的功能。 Dynamo [11] 允许客户端在分布式键值存储中获取和放置相对少量(小于 1M)的数据。 与 ZooKeeper 不同，Dynamo 中的密钥空间不是分层的。 Dynamo 也不为写入提供强大的持久性和一致性保证，而是解决读取冲突。 DepSpace [4] 使用元组空间提供Byzantine 灾难冗余服务。 像 ZooKeeper 一样，DepSpace 使用简单的服务器接口在客户端实现强同步原语。 虽然 DepSpace 的性能远低于 ZooKeeper，但它提供了更强的容错和机密性保证。 7 结论 ZooKeeper 通过向客户端公开无等待对象，采用无等待方法来解决分布式系统中协调进程的问题。 我们发现 ZooKeeper 对无论是雅虎内部或外部的多个应用程序非常有用。 ZooKeeper 通过使用带 watches 的快速读取(两者都由本地副本提供服务)为工作负载主要为读取的应用实现每秒数十万次操作的吞吐量值。 尽管我们对读取和监视的一致性保证似乎很弱，但我们已经通过我们的用例表明，这种组合允许我们在客户端实现高效和复杂的协调协议， 即使读取不是优先顺序的并且使用免等待的数据结构进行实现。 事实证明，免等待特性对于高性能至关重要。 尽管我们只描述了几个应用程序，但还有许多其他应用程序使用 ZooKeeper。 我们相信这样的成功是由于其简单的接口以及可以通过该接口实现的强大抽象。 此外，由于 ZooKeeper 的高吞吐量，应用程序可以广泛使用它，而不仅仅是粗粒度的锁定。 致谢 1234We would like to thank Andrew Kornev and Runping Qi for their contributions to ZooKeeper;Zeke Huang and Mark Marchukov for valuable feedback;Brian Cooper and Laurence Ramontianu for their early contributions to ZooKeeper;Brian Bershad and Geoff Voelker made important comments on the presentation. 参考文献 [1] M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K. Reiter, and J. J. Wylie. Fault-scalable byzantine fault-tolerant services. In SOSP ’05: Proceedings of the twentieth ACM symposium on Operating systems principles, pages 59–74, New York, NY, USA, 2005. ACM. [2] M. Aguilera, A. Merchant, M. Shah, A. Veitch, and C. Karamanolis. Sinfonia: A new paradigm for building scalable distributed systems. In SOSP ’07: Proceedings of the 21st ACM symposium on Operating systems principles, New York, NY, 2007. [3] Amazon. Amazon simple queue service. http://aws.amazon.com/sqs/, 2008. [4] A. N. Bessani, E. P. Alchieri, M. Correia, and J. da Silva Fraga. Depspace: A byzantine fault-tolerant coordination service. In Proceedings of the 3rd ACM SIGOPS/EuroSys European Systems Conference - EuroSys 2008, Apr. 2008. [5] K. P. Birman. Replication and fault-tolerance in the ISIS system. In SOSP ’85: Proceedings of the 10th ACM symposium on Operating systems principles, New York, USA, 1985. ACM Press. [6] M. Burrows. The Chubby lock service for loosely-coupled distributed systems. In Proceedings of the 7th ACM/USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2006. [7] M. Castro and B. Liskov. Practical byzantine fault tolerance and proactive recovery. ACM Transactions on Computer Systems, 20(4), 2002. [8] T. Chandra, R. Griesemer, and J. Redstone. Paxos made live: An engineering perspective. In Proceedings of the 26th annual ACM symposium on Principles of distributed computing (PODC), Aug.2007. [9] A. Clement, M. Kapritsos, S. Lee, Y. Wang, L. Alvisi, M. Dahlin, and T. Riche. UpRight cluster services. In Proceedings of the 22 nd ACM Symposium on Operating Systems Principles (SOSP), Oct. 2009. [10] J. Cowling, D. Myers, B. Liskov, R. Rodrigues, and L. Shira. Hqreplication: A hybrid quorum protocol for byzantine fault tolerance. In SOSP ’07: Proceedings of the 21st ACM symposium on Operating systems principles, New York, NY, USA, 2007. [11] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: Amazons highly available key-value store. In SOSP ’07: Proceedings of the 21st ACM symposium on Operating systems principles, New York, NY, USA, 2007. ACM Press. [12] J. Gray, P. Helland, P. O’Neil, and D. Shasha. The dangers of replication and a solution. In Proceedings of SIGMOD ’96, pages 173–182, New York, NY, USA, 1996. ACM. [13] A. Hastings. Distributed lock management in a transaction processing environment. In Proceedings of IEEE 9th Symposium on Reliable Distributed Systems, Oct. 1990. [14] M. Herlihy. Wait-free synchronization. ACM Transactions on Programming Languages and Systems, 13(1), 1991. [15] M. Herlihy and J. Wing. Linearizability: A correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems, 12(3), July 1990. [16] J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West. Scale and performance in a distributed file system. ACM Trans. Comput. Syst., 6(1), 1988. [17] Katta. Katta - distribute lucene indexes in a grid. http://katta.wiki.sourceforge.net/, 2008. [18] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong. Zyzzyva: speculative byzantine fault tolerance. SIGOPS Oper. Syst. Rev., 41(6):45–58, 2007. [19] N. P. Kronenberg, H. M. Levy, and W. D. Strecker. Vaxclusters (extended abstract): a closely-coupled distributed system. SIGOPS Oper. Syst. Rev., 19(5), 1985. [20] L. Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2), May 1998. [21] J. MacCormick, N. Murphy, M. Najork, C. A. Thekkath, and L. Zhou. Boxwood: Abstractions as the foundation for storage infrastructure. In Proceedings of the 6th ACM/USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2004. [22] L. Moser, P. Melliar-Smith, D. Agarwal, R. Budhia, C. LingleyPapadopoulos, and T. Archambault. The totem system. In Proceedings of the 25th International Symposium on Fault-Tolerant Computing, June 1995. [23] S. Mullender, editor. Distributed Systems, 2nd edition. ACM Press, New York, NY, USA, 1993. [24] B. Reed and F. P. Junqueira. A simple totally ordered broadcast protocol. In LADIS ’08: Proceedings of the 2nd Workshop on Large-Scale Distributed Systems and Middleware, pages 1–6, New York, NY, USA, 2008. ACM. [25] N. Schiper and S. Toueg. A robust and lightweight stable leader election service for dynamic systems. In DSN, 2008. [26] F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys, 22(4), 1990. [27] A. Sherman, P. A. Lisiecki, A. Berkheimer, and J. Wein. ACMS:The Akamai configuration management system. In NSDI, 2005. [28] A. Singh, P. Fonseca, P. Kuznetsov, R. Rodrigues, and P. Maniatis. Zeno: eventually consistent byzantine-fault tolerance. In NSDI’09: Proceedings of the 6th USENIX symposium on Networked systems design and implementation, pages 169–184, Berkeley, CA, USA, 2009. USENIX Association. [29] Y. J. Song, F. Junqueira, and B. Reed. BFT for the skeptics. http://www.net.t-labs.tu-berlin.de/˜petr/BFTW3/abstracts/talk-abstract.pdf. [30] R. van Renesse and K. Birman. Horus, a flexible group communication systems. Communications of the ACM, 39(16), Apr.1996. [31] R. van Renesse, K. Birman, M. Hayden, A. Vaysburd, and D. Karr. Building adaptive systems using ensemble. Software- Practice and Experience, 28(5), July 1998.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://wangqian0306.github.io/tags/ZooKeeper/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"Spark SQL - Relational Data Processing in Spark 中文翻译","slug":"treatise/spark_sql_relational_data_processing_in_spark","date":"2021-06-29T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/spark_sql_relational_data_processing_in_spark/","permalink":"https://wangqian0306.github.io/2021/spark_sql_relational_data_processing_in_spark/","excerpt":"","text":"Spark SQL: Relational Data Processing in Spark 作者： Michael Armbrust, Reynold S. Xin , Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, Matei Zaharia 版权说明 1234567891011Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full cita-tion on the first page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia.Copyright is held by the owner/author(s). Publication rights licensed to ACM.ACM 978-1-4503-2758-9/15/05 ...$15.00.http://dx.doi.org/10.1145/2723372.2742797. 概要 Spark SQL 是 Apache Spark 中的一个新模块，它将关系处理与 Spark 的函数式编程 API 集成在一起。 基于我们使用 Shark 的经验，Spark SQL 使 Spark 程序员可以利用关系处理的优势(例如，声明式查询和优化存储)， 并且可以让 SQL 用户调用 Spark 中的完整分析库(例如，机器学习)。 与以前的系统相比，Spark SQL 增加了两个主要内容。 首先，它通过与过程 Spark 代码集成的声明性 DataFrame API，在关系处理和过程处理之间提供了更紧密的集成。 其次，它包括一个高度可扩展的优化器 Catalyst，它使用 Scala 编程语言的特性构建，可以轻松添加可组合规则、控制代码生成和定义扩展点。 使用 Catalyst，我们构建了各种功能(例如，JSON 的模式推断、机器学习类型和外部数据库的查询联合)，以满足现代数据分析的复杂需求。 我们将 Spark SQL 视为 SQL-on-Spark 和 Spark 本身的演变，提供更丰富的 API 和优化，同时保留 Spark 编程模型的优势。 类别和主题描述符 H.2 [Database Management]: Systems 关键词 数据库；数据仓库；机器学习；Spark；Hadoop 1 引言 大数据应用程序需要多种处理技术、数据源和存储格式。 最早为这些工作负载设计的系统，例如 MapReduce，为用户提供了强大但低级的程序编程接口。 对此类系统进行编程是一项繁重的工作，需要用户手动优化以实现高性能。 因此，多个新系统试图通过提供大数据的关系型接口来提供更高效的用户体验。 Pig、Hive、Dremel 和 Shark [29, 36, 25, 38]` 等系统都利用声明式查询来提供更丰富的自动优化。 虽然关系系统的流行表明用户通常更喜欢编写声明式查询，但关系方法对于许多大数据应用程序来说是不够的。 首先，用户希望在可能是半结构化或非结构化的各种数据源之间执行 ETL，这种需求得编写具体的代码。 其次，用户希望执行在关系系统中难以表达的高级分析，例如机器学习和图形处理。 在实践中，我们观察到大多数数据管道都可以理想地通过关系查询和复杂程序算法的组合来表达。 不幸的是，这两类系统——关系型系统和程序型系统——到目前为止仍然在很大程度上是不相交的，迫使用户选择一种范式或另一种范式。 本文描述了我们在 Spark SQL 中结合这两种模型的努力，Spark SQL 是 Apache Spark [39] 中的一个主要新组件。 Spark SQL 建立在我们早期的 SQL-on-Spark 成果之上，称为 Shark。 然而，Spark SQL 并没有强迫用户在关系型 API 或过程型 API 之间做出选择，而是让用户无缝地混合两者。 Spark SQL 通过两个贡献消除了两种模型之间的差距。 首先，Spark SQL 提供了一个 DataFrame API，可以对外部数据源和 Spark 内置的分布式集合执行关系操作。 此 API 类似于 R [32] 中广泛使用的数据框概念，但会延迟评估操作，以便执行关系优化。 其次，为了支持大数据中广泛的数据源和算法，Spark SQL 引入了一种名为 Catalyst 的新型可扩展优化器。 Catalyst 可以轻松为机器学习等领域添加数据源、优化规则和数据类型。 DataFrame API 提供了与 Spark 程序中的丰富的关系/过程方式集成。 DataFrames 是结构化记录的集合，可以使用 Spark 的过程 API 或使用允许更丰富优化的新关系 API 进行操作。 它们可以直接从 Spark 的内置分布式 Java/Python 对象集合中创建，从而在现有 Spark 程序中启用关系处理。 其他 Spark 组件，例如机器学习库，也会获取和生成 DataFrame。 在许多常见情况下，DataFrames 比 Spark 的过程式 API 更方便、更高效。 例如，它们使使用 SQL 语句在一次传递中计算多个聚合变得容易，这在传统的函数式 API 中很难表达。 它们还自动以比 Java/Python 对象更紧凑的列格式存储数据。 最后，与 R 和 Python 中现有的 DataFrame API 不同，Spark SQL 中的 DataFrame 操作通过关系优化器 Catalyst。 为了在 Spark SQL 中支持各种数据源和分析工作负载，我们设计了一个名为 Catalyst 的可扩展查询优化器。 Catalyst 使用 Scala 编程语言的特性(例如模式匹配)来表现图灵完备语言中可组合的规则。 它提供了一个用于转换树的通用框架，我们用它来执行分析、规划和运行时代码生成。 通过这个框架，Catalyst 还可以扩展新的数据源，包括半结构化数据，如 JSON 和可以推送过滤器的“智能”数据存储(例如 HBase)； 具有用户定义的功能；以及用于机器学习等领域的用户定义类型。 众所周知，函数式语言非常适合构建编译器 [37]，因此它们使构建可扩展优化器变得容易也就不足为奇了。 我们确实发现 Catalyst 可以有效地使我们能够快速向 Spark SQL 添加功能，并且自其发布以来，我们已经看到外部贡献者也可以轻松添加它们。 注：如果一个函数的值可以通过某种纯机械的过程找到，那么这个函数就可以有效地计算出来。 这样的函数叫图灵可计算函数。 如果一个计算系统可以计算每一个图灵可计算函数，那么这个系统就是图灵完备的。 具有图灵完备性的计算机语言，就被称为图灵完备语言。 Spark SQL 于 2014 年 5 月发布，现在是 Spark 中开发最活跃的组件之一。 在撰写本文时，Apache Spark 是最活跃的大数据处理开源项目，过去一年有超过 400 名贡献者。 Spark SQL 已经部署在非常大规模的环境中。 例如，一家大型互联网公司使用 Spark SQL 构建数据管道，并在 8000 节点的集群上运行超过 100 PB 数据的查询。 每个单独的查询通常要计算数十 TB 的内容。 此外，许多用户不仅将 Spark SQL 用于 SQL 查询，还用于将其与过程处理相结合的程序。 例如，托管服务 Databricks Cloud 的客户中有 2/3 运行 Spark，在其他编程语言中使用 Spark SQL。 在性能方面，我们发现 Spark SQL 与 Hadoop 上的 SQL-only 系统在关系查询方面具有竞争力。 在用 SQL 表达的计算中，它比简单的 Spark 代码快 10 倍，内存效率更高。 通常，我们将 Spark SQL 视为核心，它是 Spark API 的重要演变。 虽然 Spark 最初的函数式编程 API 非常通用，但它仅提供了有限的自动优化机会。 Spark SQL 同时使更多用户可以访问 Spark，并优化现有用户的体验。 在 Spark 中，社区现在正在将 Spark SQL 合并到更多 API 中：DataFrames 是用于机器学习的新“ML 管道” API 中的标准数据表示， 我们希望将其扩展到其他组件，例如 GraphX 和 Spark Streaming 。 我们从 Spark 的背景和 Spark SQL 的目标(第 2 章)开始这篇论文。 然后我们会描述 DataFrame API(第 3 章)，Catalyst 优化器(第 4 章)，以及我们在 Catalyst 上构建的高级功能(第 5 章)。 我们在第 6 章中评估 Spark SQL。 我们在第 7 章中描述了关于 Catalyst 的外部研究。 最后，第 8 章涵盖了相关的工作。 2 背景和目标 2.1 Spark 概述 Apache Spark 是一个通用的集群计算引擎，具有 Scala、Java 和 Python 中的 API 以及用于流、图形处理和机器学习的库 [6]。 据我们所知，它于 2010 年发布，是使用最广泛的系统之一，具有类似于 DryadLINQ [20] 的“语言集成” API，也是最活跃的大数据处理开源项目。 Spark 在 2014 年有 400 多个贡献者，并且被多个供应商打包。 Spark 提供了一个类似于其他最新系统 [20, 11] 的函数式编程 API，用户可以在其中操作称为弹性分布式数据集(RDD) [39] 的分布式集合。 每个 RDD 是跨集群分区的 Java 或 Python 对象的集合。 RDD 可以通过 map、filter 和 reduce 等操作进行操作，这些操作采用编程语言中的函数并将它们传送到集群上的节点。 例如，下面的 Scala 代码计算文本文件中以 “ERROR” 开头的行数： 123lines = spark.textFile(&quot; hdfs ://...&quot;)errors = lines.filter(s =&gt; s. contains (&quot; ERROR &quot;))println(errors.count()) 这段代码通过读取一个 HDFS 文件创建了一个名为 lines 的字符串 RDD，然后使用过滤器选择字符串中包含 ERROR 的行并生成另一个 RDD。 然后它对该数据执行计数。 RDD 是容错的，因为系统可以使用 RDD 的 lineage 图恢复丢失的数据(通过重新运行上面的过滤器等操作来重建丢失的分区)。 它们也可以明确地缓存在内存或磁盘上以支持迭代 [39]。 关于 API 的最后一个注意事项是 RDD 是惰性求值的。 每个 RDD 代表一个计算数据集的“逻辑计划”，但 Spark 会等到某些输出操作(例如计数)启动计算。 这允许引擎做一些简单的查询优化，例如管道操作。 例如，在上面的示例中，Spark 将通过应用过滤器并计算运行计数来管道从 HDFS 文件读取行，因此它永远不需要具体化中间行和错误结果。 虽然这种优化非常有用，但它也有局限性，因为引擎不理解 RDD 中数据的结构(任意 Java/Python 对象)或用户函数的语义(包含任意代码)。 2.2 Spark 之前的关系系统 我们在 Spark 上构建关系型接口的第一个努力是 Shark [38]，它“魔改“了 Apache Hive 以在 Spark 上运行， 并在 Spark 引擎上实现了传统的 RDBMS 优化，例如列式处理。 虽然 Shark 表现出良好的性能和与 Spark 程序集成度良好，但它面临三个重要挑战。 首先，Shark 只能用于查询存储在 Hive 目录中的外部数据，因此不适用于 Spark 程序内部数据的关系查询(例如，手动创建的错误 RDD 上)。 其次，从 Spark 程序调用 Shark 的唯一方法是将 SQL 字符串放在一起，这在模块化程序中使用起来不方便且容易出错。 最后，Hive 优化器是为 MapReduce 量身定制的，难以扩展，因此很难构建新功能，例如用于机器学习的数据类型或对新数据源的支持。 2.3 Spark SQL 的目标 凭借 Shark 的经验，我们希望扩展关系处理接口以涵盖 Spark 中的原生 RDD 和更广泛的数据源。 我们为 Spark SQL 设定了以下目标： 不仅是在 Spark 内部的程序(在原生 RDD 中)还有在对程序员友好的外部 API 中在都可以使用的关系处理。 使用成熟的 DBMS 技术提供高性能。 轻松支持新数据源，包括半结构化数据和适合联合查询的外部数据库。 使用高级分析算法(例如图形处理和机器学习)启用扩展。 3 编程接口 Spark SQL 作为一个基于 Spark 的库运行，如图 1 所示。 它公开了 SQL 接口，可以通过 JDBC/ODBC 或命令行控制台访问这些 SQL 接口，以及集成到 Spark 支持的编程语言中的 DataFrame API。 我们首先介绍 DataFrame API，它允许用户将程序和关系代码进行融合。 但是，高级函数也可以通过 UDF 在 SQL 中对外提供服务，例如，通过商业智能工具。 我们在 3.7 节讨论 UDF。 3.1 DataFrame API Spark SQL 的 API 中的主要抽象是一个 DataFrame，一个具有相同结构的分布式行集合。 DataFrame 相当于关系数据库中的表，也可以以类似于 Spark(RDD) 中“原生”分布式集合的方式进行操作。 与 RDD 不同，DataFrame 会跟踪其架构并支持各种关系操作，从而对执行流程进行优化。 DataFrames 可以从系统目录中的表(基于外部数据源)或从本机 Java/Python 对象现有的 RDD 进行构建(参见第 3.5 节)。 构建后，它们可以使用各种关系运算符进行操作，例如 where 和 groupBy，它们采用域特定语言(DSL)中的表达式，类似于 R 和 Python 中的 DataFrame [32, 30]。 每个 DataFrame 也可以看成是一个 Row 对象的 RDD，允许用户调用 map 等程序化的 Spark API。 注：这些 Row 对象是动态构建的，不一定代表数据的内部存储格式，通常将数据按列存储。 最后，与传统的 DataFrame API 不同，Spark DataFrames 是惰性的，因为每个 DataFrame 对象代表一个计算数据集的逻辑计划， 但在用户调用特殊的“输出操作”(例如保存)之前不会执行。 这可以对用于构建 DataFrame 的操作进行丰富优化。 为了说明这一点，下面的 Scala 代码从 Hive 中的表定义了一个 DataFrame，基于它进行了操作然后打印结果： 1234ctx = new HiveContext()users = ctx.table(&quot;users&quot;)young = users.where(users(&quot;age&quot;) &lt; 21)println(young.count()) 在这段代码中，users 和 young 是DataFrames。 代码段 users(&quot;age&quot;) &lt; 21 是 DataFrame DSL 中的一个表达式，它被捕获为抽象语法树，而不是像传统 Spark API 那样表示的 Scala 函数。 最后，每个 DataFrame 只代表一个逻辑计划(即，读取用户表并筛选年龄 &lt; 21 岁的记录)。 当用户调用 count 时(这是一个输出操作)，Spark SQL 会构建一个物理计划来计算最终结果。 物理计划可能包含优化，例如仅扫描数据的“age”列(如果其存储格式为列式)，或者甚至使用数据源中的索引来计算匹配的行。 接下来我们将介绍 DataFrame API 的详细信息。 3.2 数据模型 Spark SQL 使用基于 Hive [19] 的嵌套数据模型来处理表和 DataFrame。 它支持所有主要的 SQL 数据类型，包括 boolean, integer, double, decimal, string, date,timestamp 以及复杂(即非原子)数据类型： structs, arrays, maps, unions. 复杂数据类型也可以嵌套在一起以创建更强大的类型。 与许多传统 DBMS 不同，Spark SQL 在查询语言和 API 中为复杂数据类型提供一流的支持。 此外，Spark SQL 还支持用户自定义类型，如 4.4.2 节所述。 使用这种类型系统，我们能够准确地对来自各种来源和格式的数据进行建模，包括 Hive、关系数据库、JSON 和 Java/Scala/Python 中的本地对象。 3.3 DataFrame 的操作 用户可以使用类似于 R DataFrame [32] 和 Python Pandas [30] 的特定领域语言(DSL) 对 DataFrame 执行关系操作。 DataFrames 支持所有常见的关系运算符，包括投影(select)、过滤器(where)、连接(join)和聚合(groupBy)。 这些操作符都在一个有限的 DSL 中获取表达式对象，让 Spark 捕获表达式的结构。 例如，以下代码计算每个部门的女性员工人数。 12345employees .join(dept, employees(&quot;deptId&quot;) === dept(&quot;id&quot;)) .where(employees(&quot;gender&quot;) === &quot;female&quot;) .groupBy(dept(&quot;id&quot;), dept(&quot;name&quot;)) .agg(count(&quot;name&quot;)) 这里，employees 是一个 DataFrame，employees(“deptId”) 是一个表示 deptId 列的表达式。 表达式对象有许多返回新表达式的运算符，包括常用的比较运算符(例如，=== 用于相等性测试，&gt; 用于大于)和算术运算符(+、- 等)。 它们还支持聚合，例如 count(“name”)。 所有这些运算符都构建了表达式的抽象语法树(AST)，然后将其传递给 Catalyst 进行优化。 这与原生 Spark API 不同，后者采用包含任意 Scala/Java/Python 代码的函数，然后这些代码对运行时引擎是不透明的。 有关 API 的详细列表，我们建议读者参阅 Spark 的官方文档 [6]。 除了关系型 DSL，DataFrame 还可以为系统中的的内容建立临时表并使用 SQL 进行查询。 下面的代码显示了一个示例： 123users.where(users(&quot;age&quot;) &lt; 21) .registerTempTable(&quot;young&quot;)ctx.sql(&quot;SELECT count (*), avg(age) FROM young&quot;) SQL 有时便于简洁地计算多个聚合，并且还允许程序通过 JDBC/ODBC 将数据集对外输出。 在系统中注册的 DataFrame 仍然是尚未被具体化的视图，因此还可以跨越 SQL 和原始的 DataFrame 表达式对其进行优化。 然而，DataFrames 也可以具体化，正如我们在 3.6 节中讨论的那样。 3.4 DataFrames 与关系查询语言 虽然从表面上看，DataFrames 提供与 SQL 和 Pig [29] 等关系查询语言相同的操作，但我们发现，由于它们与完整的编程语言集成，用户可以更轻松地使用它们。 例如，用户可以将他们的代码分解成 Scala、Java 或 Python 函数，在它们之间传递 DataFrame 以构建逻辑计划，并且在运行输出操作时仍将受益于整个计划的优化。 同样，开发人员可以使用 if 语句和循环等控制结构来构建他们的工作。 一位用户说 DataFrame API “像 SQL 一样简洁和声明性，但我可以命名中间结果”，指的是如何更容易地构建计算和调试中间步骤。 为了简化 DataFrames 中的编程，我们还使 API 热切地分析逻辑计划(即识别表达式中使用的列名是否存在于底层表中，以及它们的数据类型是否合适)， 即使查询结果是惰性计算的。 因此，只要用户输入无效的代码行，Spark SQL 就会报告错误，而不是等待执行。 这再次比大型 SQL 语句更容易使用。 3.5 查询原生数据集 现实世界的数据管道通常从异构源中提取数据，并运行来自不同编程库的各种算法。 为了与过程 Spark 代码互操作，Spark SQL 允许用户直接针对编程语言本机对象的 RDD 构造 DataFrames。 Spark SQL 可以使用反射自动推断这些对象的模式。 在 Scala 和 Java 中，类型信息是从语言的类型系统(来自 JavaBeans 和 Scala case 类)中提取的。 在 Python 中，由于动态类型系统，Spark SQL 对数据集进行采样以执行模式推断。 例如，下面的 Scala 代码从用户对象的 RDD 中定义了一个 DataFrame。 Spark SQL 会自动检测列的名称(“name”和“age”)和数据类型(string 和 int)。 12345678case class User(name:String, age:Int)// Create an RDD of User objectsusersRDD = spark. parallelize( List(User(&quot;Alice&quot;, 22), User(&quot;Bob&quot;, 19)))// View the RDD as a DataFrameusersDF = usersRDD.toDF 在内部，Spark SQL 创建一个指向 RDD 的逻辑数据扫描操作符。 这被编译成访问本机对象字段的物理运算符。 需要注意的是，这与传统的对象关系映射(ORM)非常不同。 ORM 通常会导致将整个对象转换为不同格式的昂贵转换。 在内部，Spark SQL 创建一个指向 RDD 的逻辑数据扫描操作符。 这被编译成访问本机对象字段的物理运算符。 需要注意的是，这与传统的对象关系映射(ORM) 非常不同。 ORM 通常会导致将整个对象转换为不同格式性能消耗严重。 相比之下，Spark SQL 就地访问本机对象，仅提取每个查询中使用的字段。 查询本机数据集的能力让用户可以在现有 Spark 程序中运行优化的关系操作。 此外，它还可以轻松地将 RDD 与外部结构化数据相结合。 例如，我们可以将用户 RDD 与 Hive 中的一个表连接起来： 12views = ctx.table(&quot;pageviews&quot;)usersDF.join(views ,usersDF(&quot;name&quot;) === views(&quot;user&quot;) 3.6 缓存在内存中 与之前的 Shark 一样，Spark SQL 可以使用列式存储在内存中物化(通常称为“缓存”)热数据。 与 Spark 的原生缓存将数据简单地存储为 JVM 对象相比，列式缓存可以将内存占用减少一个数量级，因为它应用了列式压缩方案，例如字典编码和运行长度编码。 缓存对于交互式查询和机器学习中常见的迭代算法特别有用。 它可以通过在 DataFrame 上调用 cache() 方法来调用。 3.7 用户定义的函数(UDF) 用户定义函数(UDF) 已成为数据库系统的重要扩展点。 例如，MySQL 依靠 UDF 为 JSON 数据提供基本支持。 一个更高级的例子是 MADLib 使用 UDF 为 Postgres 和其他数据库系统实现机器学习算法 [12]。 但是，数据库系统通常需要在与主要查询接口不同的单独编程环境中定义 UDF。 Spark SQL 的 DataFrame API 支持 UDF 的内联定义，没有其他数据库系统复杂的打包和注册过程。 事实证明，此功能对于 API 的采用至关重要。 在 Spark SQL 中，UDF 可以通过传递 Scala、Java 或 Python 函数来内联注册，这些函数可能在内部使用完整的 Spark API。 例如，给定机器学习模型的模型对象，我们可以将其预测函数注册为 UDF： 123456val model: LogisticRegressionModel = ...ctx.udf.register (&quot; predict&quot;, (x: Float , y: Float) =&gt; model.predict(Vector(x, y)))ctx.sql(&quot;SELECT predict(age , weight) FROM users &quot;) 注册后，商业智能工具还可以通过 JDBC/ODBC 接口使用 UDF。 除了像这里这样对标量值进行操作的 UDF 之外，还可以通过取其名称来定义对整个表进行操作的 UDF， 如 MADLib [12]，就在其中使用分布式 Spark API，从而公开高级分析功能给 SQL 用户。 最后，由于 UDF 定义和查询执行使用相同的通用语言(例如 Scala 或 Python)表示，因此用户可以使用标准工具调试或分析整个程序。 注：在 Spark 权威指南中是这样描述 Python UDF 的 启动此 Python 进程代价很高，但主要代价是将数据序列化为 Python 可理解格式的过程。 造成代价高的原因有两个: 一个是计算昂贵，另一个是数据进入 Python 后 Spark 无法管理 worker 的内存。 这意味着，如果某个 worker 因资源受限而失败 (因为 JVM 和 Python 都在同一台计算机上争夺内存)，则可能会导致该worker出现故障。 所以建议使用 Scala 或 Java编写UDF，不仅编写程序的时间少，还能提高性能。 当然仍然可以使用 Python 编写函数。 4 Catalyst 优化器 为了实现 Spark SQL，我们设计了一个新的可扩展优化器 Catalyst，它基于 Scala 中的函数式编程结构。 Catalyst 的可扩展设计有两个目的。 首先，我们希望能够轻松地向 Spark SQL 添加新的优化技术和功能，尤其是解决我们在“大数据”(例如，半结构化数据和高级分析)方面遇到的各种问题。 其次，我们希望让外部开发人员能够扩展优化器——例如，添加特定于数据源的规则，这些规则可以将过滤或聚合推送到外部存储系统中，或者支持新的数据类型。 Catalyst 支持基于规则和基于成本的优化。 注：基于成本的优化是通过使用规则生成多个计划，然后计算它们的成本来执行的。 虽然过去已经提出了可扩展优化器，但它们通常需要一种复杂的领域特定语言来指定规则，并需要一个“优化器编译器”来将规则转换为可执行代码 [17, 16]。 这会导致显着的学习曲线和维护负担。 相比之下，Catalyst 使用 Scala 编程语言的标准特性，例如模式匹配 [14]，让开发人员使用完整的编程语言，同时仍然使规则易于指定。 函数式语言的部分设计目的是构建编译器，因此我们发现 Scala 非常适合这项任务。 尽管如此，据我们所知，Catalyst 是第一个基于这种语言构建的生产级别质量的查询优化器。 其核心中，Catalyst 包含一个通用库，用于表示树并应用规则来操作它们。 在此框架之上，我们构建了特定于关系查询处理(例如表达式、逻辑查询计划)的库，以及处理查询执行不同阶段的几组规则： 分析、逻辑优化、物理规划和代码生成以将部分查询编译为 Java 字节码。 对于后者，我们使用另一个 Scala 特性，quasiquotes [34]，它可以很容易地在运行时从可组合表达式代码生成。 最后，Catalyst 提供了几个公共扩展点，包括外部数据源和用户定义的类型。 4.1 树 Catalyst 中的主要数据类型是由节点对象组成的树。 每个节点都有一个节点类型和零个或多个子节点。 新的节点类型在 Scala 中定义为 TreeNode 类的子类。 这些对象是不可变的，可以使用函数转换来操作，如下一小节中所述。 作为一个简单的例子，假设对于一个非常简单的表达式语言，我们有以下三个节点类： Literal(value: Int): 一个常数值 Attribute(name: String): 来自输入行的属性，例如“x” Add(left: TreeNode, right: TreeNode): 获取两个表达式的合。 注：我们在这里对类使用 Scala 语法，其中每个类的字段都在括号中定义，它们的类型使用冒号标识。 这些类可用于构建树；例如，表达式 x+(1+2) 的树，如图 2 所示，将在 Scala 代码中表示如下： 1Add(Attribute (x), Add(Literal(1), Literal(2))) 4.2 规则 可以使用规则来操作树，规则是将一棵树转化为另一棵树的函数。 模式匹配是许多函数式语言的一个特性，它允许从代数数据类型的潜在嵌套结构中提取值。 在 Catalyst 中，树提供了一种转换方法，该方法在树的所有节点上递归地应用模式匹配函数，将与每个模式匹配的节点转换为结果。 例如，我们可以实现一个规则，在常量之间折叠 Add 操作，如下所示： 123tree. transform &#123; case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)&#125; 将此应用于图 2 中 x+(1+2) 的树将产生新树 x+3。 此处的 case 关键字是 Scala 的标准模式匹配语法 [14]，可用于匹配对象的类型以及为提取的值命名(此处为 c1 和 c2)。 传递给 transform 的模式匹配表达式是一个偏函数，这意味着它只需要匹配所有可能的输入树的一个子集。 Catalyst 将测试给定规则适用于树的哪些部分，自动跳过并下降到不匹配的子树。 这种能力意味着规则只需要推理应用给定优化的树，而不是那些不匹配的树。 因此，当新类型的运算符添加到系统中时，不需要修改规则。 规则(以及一般的 Scala 模式匹配)可以在同一个转换调用中匹配多个模式，使得一次实现多个转换变得非常简洁： 12345tree. transform &#123; case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2) case Add(left , Literal(0)) =&gt; left case Add(Literal(0), right) =&gt; right&#125; 在实践中，规则可能需要多次执行才能完全转换一棵树。 Catalyst 将规则分组，并执行每个批次，直到达到固定点，即直到应用其规则后树停止更改为止。 将规则运行到定点意味着每个规则都可以是简单的和自包含的，但最终仍会对树产生更大的全局影响。 在上面的例子中，重复应用会不断折叠更大的树，例如 (x+0)+(3+3)。 作为另一个示例，第一批可能会分析表达式以将类型分配给所有属性，而第二批可能会使用这些类型进行常量折叠。 在每批之后，开发人员还可以对新树运行健全性检查(例如，查看所有属性都分配了类型)，这些检查通常也通过递归匹配编写。 最后，规则条件及其主体可以包含任意 Scala 代码。 这使 Catalyst 比面向特定领域对应语言的优化器更强大，同时保持简单规则的简洁。 根据我们的经验，不可变树上的函数转换使整个优化器非常容易推理和调试。 它们还在优化器中启用并行化，尽管我们还没有进一步开发它。 4.3 在 Spark SQL 中使用 Catalyst 我们在四个阶段使用 Catalyst 的通用树转换框架，如图 3 所示： 分析逻辑计划以解析引用 逻辑计划优化 指定物理计划 生成并编译代码来查询部分 Java 字节码。 在物理规划阶段，Catalyst 可能会生成多个计划并根据成本进行比较。 所有其他阶段完全基于规则。 每个阶段使用不同类型的树节点；Catalyst 包括用于表达式、数据类型以及逻辑和物理运算符的节点库。 我们现在对这些内容进行描述。 4.3.1 分析 Spark SQL 从一个要计算的关系开始，可能是来自于 SQL 解析器返回的抽象语法树(AST)，也可能是来自 API 构造的 DataFrame 对象。 在这两种情况下，关系可能包含未解析的属性引用或关系：例如，在 SQL 查询 SELECT col FROM sales 中，col 的类型，甚至它是否是有效的列名， 直到我们查找 sales 表时才知道。 如果我们不知道其类型或未将其与输入表(或别名)匹配，则该属性被称为未解析。 Spark SQL 使用 Catalyst 规则和一个 Catalog 对象来跟踪所有数据源中的表来解析这些属性。 它首先构建具有未绑定属性和数据类型的“未解析逻辑计划”树，然后应用执行以下操作的规则。 从 Catalog 中按照名称查找关系。 将命名之后的属性(例如 col)作为输入映射到给定运算符的子项 确定哪些属性引用相同的值，并为它们提供唯一的 ID(稍后允许优化表达式，例如 col = col)。 表达式的传递和强制类型转换：例如，我们无法知道 1+col 的类型，直到我们解析 col 并可能将其子表达式转换为兼容类型。 总的来说，分析器的规则大约是 1000 行代码。 4.3.2 逻辑优化 在逻辑优化阶段会将标准的基于规则的优化方案应用于逻辑计划。 其中包括常量折叠(constant folding)、谓词下推(predicate pushdown)、投影修剪(projection pruning)、空值传导(null propagation)、 布尔表达式简化和其他规则。 总的来说，我们发现为各种情况添加规则非常简单。 注： 常量折叠——将常量渲染至代码中 谓词下推——将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。 投影修剪——? 推测是通过切分数据进行优化 空值传导——? 空值验证 例如，当我们将固定精度的 DECIMAL 类型添加到 Spark SQL 时，我们想优化小精度 DECIMAL 上的总和与平均值等聚合； 用 12 行代码编写了一个规则，在 SUM 和 AVG 表达式中找到这样的小数，并将它们转换为未缩放的 64 位 LONG，在其上进行聚合，然后将结果转换回。 仅优化 SUM 表达式的此规则的简化版本复制如下： 12345678910object DecimalAggregates extends Rule[LogicalPlan] &#123; /** Maximum number of decimal digits in a Long */ val MAX_LONG_DIGITS = 18 def apply(plan: LogicalPlan): LogicalPlan = &#123; plan transformAllExpressions &#123; case Sum(e @ DecimalType.Expression(prec , scale )) if prec + 10 &lt;= MAX_LONG_DIGITS =&gt; MakeDecimal(Sum(LongValue(e)), prec+10, scale) &#125;&#125; 再举一个例子，一个 12 行的规则使用简单的正则表达式将 LIKE 表达式优化为 String.startsWith 或 String.contains 方法。 在规则中自由使用任意 Scala 代码使得这些类型的优化变得容易表达，这些优化超越了子树结构的模式匹配。 逻辑优化规则总共有 800 行代码。 4.3.3 物理计划 在物理规划阶段，Spark SQL 使用与 Spark 执行引擎匹配的物理运算符，获取一个逻辑计划并生成一个或多个物理计划。 然后使用成本模型选择计划。 目前，基于成本的优化仅用于 select 和 join 算法：对于已知较小的关系，Spark SQL 会使用 Spark 中可用的点对点广播工具进行广播连接。 然而，该框架支持更广泛地使用基于成本的优化，因为可以使用规则递归地估计整个树的成本。 因此，我们打算在未来实施更丰富的基于成本的优化。 注：估计表大小的方式为：判断表是否缓存在内存中或来自外部文件，或者它是具有 LIMIT 的子查询的结果。 物理规划器还执行基于规则的物理优化，例如将投影或过滤器流水线化为一个 Spark 映射操作。 此外，它还可以将逻辑计划中的操作推送到支持谓词或投影下推的数据源中。 我们将在第 4.4.1 节中描述这些数据源的 API。 总的来说，物理规划规则大约有 500 行代码。 4.3.4 代码生成 查询优化的最后阶段涉及生成 Java 字节码以在每台机器上运行。 由于 Spark SQL 经常在内存数据集上运行，其中处理受 CPU 限制，我们希望支持代码生成以加快执行速度。 尽管如此，代码生成引擎的构建通常很复杂，本质上相当于一个编译器。 Catalyst 依靠 Scala 语言的一个特殊功能 quasiquotes [34] 来简化代码生成。 Quasiquotes 允许在 Scala 语言中以编程方式构建抽象语法树 (AST)，然后可以在运行时将其提供给 Scala 编译器以生成字节码。 我们使用 Catalyst 将表示 SQL 中表达式的树转换为 AST 以供 Scala 代码评估该表达式，然后编译并运行生成的代码。 作为一个简单的例子，参考第 4.2 节中介绍的 Add、Attribute 和 Literal 树节点，它们允许我们编写诸如 (x+y)+1 之类的表达式。 如果没有代码生成，则必须通过沿着 Add、Attribute 和 Literal 节点的树为每一行数据解释此类表达式。 这会引入大量分支和虚函数调用，从而减慢执行速度。 通过代码生成，我们可以编写一个函数来将特定的表达式树转换为 Scala AST，如下所示： 123456def compile(node: Node): AST = node match &#123; case Literal(value) =&gt; q&quot;$value&quot; case Attribute(name) =&gt; q&quot;row.get($name)&quot; case Add(left, right) =&gt; q&quot;$&#123;compile(left)&#125; + $&#123;compile(right)&#125;&quot;&#125; 以 q 开头的字符串是 quasiquotes，这意味着虽然它们看起来像字符串，但它们在编译时被 Scala 编译器解析并代表其中代码的 AST。 Quasiquotes 可以将变量或其他 AST 拼接到其中，使用 $ 符号表示。 例如，Literal(1) 将成为 1 的 Scala AST，而 Attribute(“x”) 将成为 row.get(“x”)。 最后，像 Add(Literal(1), Attribute(“x”)) 这样的树变成了像 1+row.get(“x”) 这样的 Scala 表达式的 AST。 Quasiquotes 在编译时进行类型检查，以确保只替换适当的 AST 或文字，使它们比字符串连接更有用， 并且它们直接生成 Scala AST，而不是在运行时依靠 Scala 解析器。 此外，它们是高度可组合的，因为每个节点的代码生成规则不需要知道其子节点返回的树是如何构建的。 最后，如果存在 Catalyst 遗漏的表达式级优化，Scala 编译器会进一步优化生成的代码。 图 4 显示 quasiquotes 让我们生成性能类似于手动调整程序的代码。 我们发现 quasiquotes 用于代码生成非常简单，并且我们观察到即使是 Spark SQL 的新贡献者也可以快速为新类型的表达式添加规则。 Quasiquotes 也适用于我们在原生 Java 对象上运行的目标：从这些对象访问字段时，我们可以通过代码生成对所需字段的直接访问， 而不必将对象复制到 Spark SQL Row 中并使用 Row 的访问方法。 最后，将代码生成的评估与我们尚未为其生成代码的表达式的解释评估结合起来很简单，因为我们编译的 Scala 代码可以直接调用我们的表达式解释器。 Catalyst 的代码生成器总共大约有 700 行代码。 4.4 执行点 Catalyst 围绕可组合规则的设计使用户和第三方库可以轻松扩展。 开发人员可以在运行时向查询优化的每个阶段添加批量规则，只要他们遵守每个阶段的约定(例如，确保分析解决所有属性)。 但是，为了在不了解 Catalyst 规则的情况下更简单地添加某些类型的扩展，我们还定义了两个更窄的公共扩展点：数据源和用户定义类型。 这些仍然依赖于核心引擎中的设施与优化器的其余部分进行交互。 4.4.1 数据源 开发人员可以使用多个 API 为 Spark SQL 定义新的数据源，这些 API 公开了不同程度的可能优化。 所有数据源都必须实现一个 createRelation 函数，该函数接受一组键值参数并返回该关系的 BaseRelation 对象(如果可以成功加载)。 每个 BaseRelation 都包含一个模式和一个可选的估计大小(以字节为单位)。 例如，代表 MySQL 的数据源可以将表名作为参数，并要求 MySQL 估计表大小。 注：非结构化数据源也可以将所需的模式作为参数； 例如，有一个 CSV 文件数据源可让用户指定列名称和类型。 为了让 Spark SQL 读取数据，BaseRelation 可以实现几个接口之一，让它们暴露不同程度的复杂性。 最简单的 TableScan 需要关系返回表中所有数据的 Row 对象的 RDD。 更高级的 PrunedScan 需要读取一组列名，并且应该返回仅包含这些列的行。 第三个接口 PrunedFilteredScan 接受所需的列名和 Filter 对象数组，它们是 Catalyst 表达式语法的子集，允许谓词下推。 过滤器是建议性的，即数据源应尝试仅返回通过每个过滤器的行，但在无法评估的过滤器的情况下允许返回误报。 最后，为 CatalystScan 接口提供了一个完整的 Catalyst 表达式树序列，用于谓词下推，尽管它们再次是建议性的。 注：目前，过滤器包括相等、与常量的比较、和 IN 子句，每个子句都在一个属性上。 这些接口允许数据源实现不同程度的优化，同时仍然使开发人员可以轻松添加几乎任何类型的简单数据源。 我们和其他人已经使用该接口实现了以下数据源： CSV 文件，它只是扫描整个文件，但允许用户指定结构。 Avro [4]，一种用于嵌套数据的自描述二进制格式。 Parquet [5]，一种柱状文件格式，我们支持列修剪和过滤器。 JDBC 数据源，它并行扫描来自 RDBMS 的表的范围并将过滤器推送到 RDBMS 以达到最小化通信。 为了使用这些数据源，程序员在 SQL 语句中指定他们的包名，为配置选项传递键值对。 例如，Avro 数据源采用文件的路径： 123CREATE TEMPORARY TABLE messagesUSING com. databricks .spark.avroOPTIONS (path &quot;messages.avro &quot;) 所有数据源也可以公开网络位置信息，即数据的每个分区从哪个机器读取最有效。 这是通过它们返回的 RDD 对象公开的，因为 RDD 具有用于数据局部性的内置 API [39]。 最后，可以使用将数据写入现有表或新表的类似接口。 这些更简单，因为 Spark SQL 只提供要写入的 Row 对象的 RDD。 4.4.2 用户定义的类型(UDTs) 我们希望在 Spark SQL 中允许高级分析的一项功能是支持用户定义的类型。 例如，机器学习应用程序可能需要向量类型，而图算法可能需要用于表示图的类型，这在关系表上是可能的 [15]。 然而，添加新类型可能具有挑战性，因为数据类型遍及执行引擎的所有方面。 例如，在 Spark SQL 中，内置数据类型以列式压缩格式存储，用于内存缓存(第 3.6 节)，而在上一节的数据源 API 中，我们需要公开所有可能的数据类型给数据源作者。 在 Catalyst 中，我们通过将用户定义的类型映射到由 Catalyst 的内置类型组成的结构来解决这个问题，如第 3.2 节所述。 要将 Scala 类型注册为 UDT，用户需要提供从其类的对象到内置类型的 Catalyst 行的映射，以及反向映射。 在用户代码中，他们现在可以在使用 Spark SQL 查询的对象中使用 Scala 类型，并且它会在幕后转换为内置类型。 同样，它们可以注册直接对其类型进行操作的 UDF(参见第 3.7 节)。 作为一个简短的例子，假设我们想要将二维点(x, y) 注册为 UDT。 我们可以将这些向量表示为两个 DOUBLE 值。 要注册 UDT，我们需要编写以下内容: 123456789class PointUDT extends UserDefinedType [Point] &#123; def dataType = StructType (Seq( // Our native structure StructField(&quot;x&quot;, DoubleType), StructField(&quot;y&quot;, DoubleType) )) def serialize (p: Point) = Row(p.x, p.y) def deserialize (r: Row) = Point(r.getDouble (0), r.getDouble (1))&#125; 注册此类型后，Spark SQL 转换为 DataFrames 时 Point 会在本机对象中识别，并将传递给 UDF。 此外，Spark SQL 将在缓存数据时以列格式存储 Points(将 x 和 y 压缩为单独的列)， 并且 Points 将可写入 Spark SQL 的所有数据源，这些数据源会将它们视为 DOUBLE 键值对。 我们在 Spark 的机器学习库中使用此功能，如第 5.2 节所述。 5 高级分析功能 在本节中，我们将描述我们添加到 Spark SQL 的三个特性，专门用于处理“大数据”环境中的挑战。 首先，在这些环境中，数据通常是非结构化或半结构化的。 虽然按程序解析此类数据是可能的，但它会导致冗长的样板代码。 为了让用户立即查询数据，Spark SQL 包含了一种用于 JSON 和其他半结构化数据的模式推理算法。 其次，大规模处理通常需要机器学习超过聚合和链接。 我们描述了 Spark SQL 如何被整合到 Spark 机器学习库的新高级 API 中 [26]。 最后，数据管道通常结合来自不同存储系统的数据。 基于第 4.4.1 节中的数据源 API，Spark SQL 支持查询联邦，允许单个程序高效地查询不同的数据源。 这些功能都建立在 Catalyst 框架之上。 5.1 半结构化数据的结构推断 12345678910111213141516&#123; &quot;text&quot;: &quot;This is a tweet about #Spark&quot;, &quot;tags&quot;: [&quot;#Spark&quot;], &quot;loc&quot;: &#123;&quot;lat&quot;: 45.1 , &quot;long&quot;: 90&#125;&#125;&#123; &quot;text&quot;: &quot;This is another tweet&quot;, &quot;tags&quot;: [], &quot;loc&quot;: &#123;&quot;lat&quot;: 39, &quot;long&quot;: 88.5&#125;&#125;&#123; &quot;text&quot;: &quot;A #tweet without #location&quot;, &quot;tags&quot;: [&quot;#tweet&quot;, &quot;#location&quot;]&#125; 图 5：一组示例 JSON 记录，表示推文。 123text STRING NOT NULL,tags ARRAY &lt;STRING NOT NULL &gt; NOT NULL,loc STRUCT &lt;lat FLOAT NOT NULL, long FLOAT NOT NULL&gt; 图 6：为图 5 中的推文推断的结构。 半结构化数据在大规模环境中很常见，因为随着时间的推移，它易于生成和添加字段。 在 Spark 用户中，我们看到 JSON 用于输入数据的使用率非常高。 不幸的是，在 Spark 或 MapReduce 等程序环境中使用 JSON 很麻烦： 大多数用户求助于类似 ORM 的库(例如，Jackson [21])将 JSON 结构映射到 Java 对象，或者一些尝试直接使用低级库。 在 Spark SQL 中，我们添加了一个 JSON 数据源，它可以从一组记录中自动推断出结构。 例如，图 5 中的 JSON 对象，经过库推断得出了图 6 中所示的结构。 用户可以简单地将 JSON 文件注册为表，并使用按路径访问字段的语法进行查询，例如： 12SELECT loc.lat ,loc.long FROM tweetsWHERE text LIKE ’%Spark%’ AND tags IS NOT NULL 我们的结构推断算法会一次性的处理数据，但也可以在数据样本上运行。 它与之前关于 XML 和对象数据库 [9, 18, 27] 结构推断的工作有关，但更简单，因为它只推断静态树结构，不允许在任意深度递归嵌套元素。 具体来说，该算法尝试推断 STRUCT 类型的树，每个类型可能包含原子、数组或其他 STRUCT。 Foreach 字段由根 JSON 对象(例如，tweet.loc.latitude)的不同路径定义，该算法找到与该字段的观察实例匹配的最具体的 Spark SQL 数据类型。 例如，如果该字段的所有出现都是适合 32 位的整数，它将推断为 INT；如果它们更大，它将使用 LONG(64 位)或 DECIMAL(任意精度) 如果还有小数值，它将使用 FLOAT。 具体来说，该算法尝试推断 STRUCT 类型的树，每个类型可能包含原子、数组或其他 STRUCT。 对于由 JSON 对象根(例如，tweet.loc.latitude)的不同路径定义的每个字段，该算法会找到与该字段的观察到的实例匹配的 Spark SQL 数据类型。 例如，如果该字段的所有出现都是适合 32 位的整数，它将推断为 INT；如果它们更大，它将使用 LONG(64 位)或 DECIMAL(任意精度)； 如果还有小数值，它将使用 FLOAT。 对于显示多种类型的字段，Spark SQL 使用 STRING 作为最通用的类型，保留原始 JSON 格式。 对于包含数组的字段，它使用相同的“最具体的超类型”逻辑从所有观察到的元素中确定元素类型。 我们使用对数据的单个归约操作来实现该算法，该操作从每个单独记录的模式(即类型树)开始，并使用关联的“最具体的超类型”函数合并它们，该函数概括了每个字段的类型。 这使得该算法既是单次传递又是高效通信，因为在每个节点上都发生了高度的减少。 作为一个简短的例子，请注意在图 5 和图 6 中，该算法如何概括 loc.lat 和 loc.long 的类型。 每个字段在一个记录中显示为整数，在另一个记录中显示为浮点数，因此算法返回 FLOAT。 另请注意，对于 tags 字段，算法如何推断出一个不能为 null 的字符串数组。 在实践中，我们发现该算法可以很好地处理实际的 JSON 数据集。 例如，它正确地为 Twitter 的 firehouse 中的 JSON tweets 标识了一个可用的模式，其中包含大约 100 个不同的字段和高度嵌套。 多个 Databricks 客户也成功地将其应用于其内部 JSON 格式。 在 Spark SQL 中，我们也使用相同的算法来推断 Python 对象的 RDD 模式(参见第 3 节)，因为 Python 不是静态类型的，因此 RDD 可以包含多种对象类型。 未来，我们计划为 CSV 文件和 XML 添加类似的推理。 开发人员发现能够将这些类型的数据集视为表格，并立即查询它们或将它们与其他对提高生产力非常有价值的数据结合起来。 5.2 与 Spark 机器学习库进行集成 注：此处机器学习相关专业名词可能有翻译错误。 作为 Spark SQL 在其他 Spark 模块中的实用程序的示例，Spark 的机器学习库 MLlib 引入了使用 DataFrames [26] 的新高级 API。 这个新 API 基于机器学习管道的概念，这是其他高级 ML 库(如 SciKit-Learn [33] )中的抽象。 管道是数据转换的图，例如特征提取、归一化、降维和模型训练，每一个都交换数据集。 管道是一种有用的抽象，因为 ML 工作流有很多步骤；将这些步骤表示为可组合元素，可以轻松更改管道的各个部分或在整个工作流程级别搜索调整参数。 注：我们从 (text, label) 组成的 DataFrame 开始，将文本标记为单词，运行词频特征化器(HashingTF)以获取特征向量，然后训练逻辑回归。 为了在管道阶段之间交换数据，MLlib 的开发人员需要一种紧凑(因为数据集可能很大)但又灵活的格式，允许为每条记录存储多种类型的字段。 例如，用户可能从包含文本字段和数字字段的记录开始，然后在文本上运行特征化算法(例如 TF-IDF)将其转换为向量， 对其他字段之一进行归一化，对文本执行降维整套功能等。 为了表示数据集，新 API 使用 DataFrames，其中每一列代表数据的一个特征。 可以在管道中调用的所有算法都为输入列和输出列命名，因此可以在字段的任何子集上调用并生成新的集合。 这使开发人员可以轻松构建复杂的管道，同时保留每条记录的原始数据。 为了说明 API，图 7 显示了一个简短的管道和创建的 DataFrame 的架构。 MLlib 使用 Spark SQL 必须做的主要工作是为向量创建用户定义的类型。 这个向量 UDT 可以存储稀疏向量和密集向量，并将它们表示为四个原始字段：类型(密集或解析)的布尔值、向量的大小、 索引数组(用于稀疏坐标)和数组 double 值(稀疏向量的非零坐标或其他所有坐标)。 除了 DataFrames 用于跟踪和操作列的实用程序之外，我们还发现它们还有另一个用处：它们使在 Spark 支持的所有编程语言中公开 MLlib 的新 API 变得更加容易。 以前，MLlib 中的每个算法都采用特定领域概念的对象(例如，用于分类的标记点或用于推荐的(用户、产品)评级)， 并且这些类中的每一个都必须以各种语言实现(例如，从 Scala 复制到 Python)。 在任何地方使用 DataFrames 使得以所有语言公开所有算法变得更加简单，因为它们已经存在所以我们只需要在 Spark SQL 中进行数据转换就可以了。 这一点尤其重要，因为 Spark 为新的编程语言添加了绑定的特性。 最后，在 MLlib 中使用 DataFrames 存储也可以很容易地在 SQL 中公开其所有算法。 我们可以简单地定义一个 MADlib 风格的 UDF，如 3.7 节所述，它将在内部调用表上的算法。 我们还在探索 API 来将管道构建相关内容在 SQL 中开放出来。 5.3 查询联合到外部数据库 数据管道通常结合来自异构源的数据。 例如，推荐管道可能会将流量日志与用户配置文件数据库和用户的社交媒体流结合起来。 由于这些数据源通常位于不同的机器或地理位置，因此单纯地查询它们可能会非常昂贵。 Spark SQL 数据源利用 Catalyst 尽可能将谓词下推到数据源中。 例如，下面使用 JDBC 数据源和 JSON 数据源将两个表连接在一起，以查找最近注册用户的流量日志。 方便的是，两个数据源都可以自动推断模式，而无需用户定义它。 JDBC 数据源还将过滤谓词下推到 MySQL 中以减少传输的数据量。 123456789CREATE TEMPORARY TABLE users USING jdbcOPTIONS(driver &quot;mysql&quot; url &quot;jdbc:mysql :// userDB/users&quot;)CREATE TEMPORARY TABLE logsUSING json OPTIONS (path &quot;logs.json&quot;)SELECT users.id , users.name, logs.messageFROM users JOIN logs WHERE users.id = logs.userIdAND users.registrationDate &gt; &quot;2015-01-01&quot; 在后台，JDBC 数据源使用第 4.4.1 节中的 PrunedFilteredScan 接口，该接口为其提供了请求列的名称和这些列上的简单谓词(相等、比较和 IN 子句)。 在这种情况下，JDBC 数据源将在 MySQL 上运行以下查询： 12SELECT users.id ,users.name FROM usersWHERE users.registrationDate &gt; &quot;2015-01-01&quot; 注：JDBC 数据源还支持按特定列“分片”源表并并行读取它的不同范围。 在未来的 Spark SQL 版本中，我们还希望为 HBase 和 Cassandra 等键值存储添加谓词下推，它们支持某些形式的过滤。 6 评估 我们从两个维度评估 Spark SQL 的性能：SQL 查询处理性能和 Spark 程序性能。 特别是，我们证明了 Spark SQL 的可扩展架构不仅支持更丰富的功能集，而且比以前基于 Spark 的 SQL 引擎带来了显着的性能改进。 此外，对于 Spark 应用程序开发人员而言，DataFrame API 可以比原生 Spark API 带来显着的加速，同时使 Spark 程序更加简洁和易于理解。 最后，与将 SQL 和过程代码作为单独的并行作业运行相比，将关系查询和过程查询相结合的应用程序在集成的 Spark SQL 引擎上运行得更快。 6.1 SQL 性能 我们使用 AMPLab 大数据基准测试 [3] 将 Spark SQL 与 Shark 和 Impala [23] 的性能进行了比较， 该基准测试使用 Pavlo 等人开发的 Web 分析工作负载。[31] 基准包含四种类型的查询，具有不同的参数，执行扫描、聚合、连接和基于 UDF 的 MapReduce 工作。 我们使用了六台 EC2 i2.xlarge 机器(一台主机，五台工作器)的集群，每台机器有 4 个内核、30 GB 内存和 800 GB SSD， 运行 HDFS 2.4、Spark 1.3、Shark 0.9.1 和 Impala 2.1.1。 使用柱状 Parquet 格式 [5] 压缩后的数据集是 110 GB 的数据。 注：参见 [31] 图 8 显示了每个查询的结果，按查询类型分组。 查询 1-3 具有不同的参数来改变它们的选择性，其中 1a、2a 等是最具选择性的，而 1c、2c 等是最不具有选择性并处理更多数据的。 查询 4 使用基于 Python 的 Hive UDF，它在 Impala 中不直接支持，但很大程度上受 UDF 的 CPU 性能限制。 我们看到，在所有查询中，Spark SQL 比 Shark 快得多，并且通常与 Impala 竞争。 与 Shark 不同的主要原因是 Catalyst 中的代码生成(第 4.3.4 节)，它减少了 CPU 开销。 此功能使 Spark SQL 在许多此类查询中与基于 C++ 和 LLVM 的 Impala 引擎竞争。 与 Impala 最大的差距是在查询 3a 中，Impala 选择了更好的连接计划，因为查询的选择性使得其中一个表非常小。 6.2 DataFrames 和原生 Spark 代码的对比 除了运行 SQL 查询之外，Spark SQL 还可以通过 DataFrame API 帮助非 SQL 开发人员编写更简单、更高效的 Spark 代码。 Catalyst 可以对手写代码难以完成的 DataFrame 操作执行优化，例如谓词下推、管道和自动连接选择。 即使没有这些优化，DataFrame API 也可以通过代码生成实现更高效的执行。 对于 Python 应用程序尤其如此，因为 Python 通常比 JVM 慢。 对于本次评估，我们比较了执行分布式聚合的 Spark 程序的两种实现。 数据集由 10 亿个整数对 (a, b) 组成，具有 100,000 个不同的 a 值，位于与上一节相同的五个 i2.xlarge 集群上。 我们测量为每个 a 值计算 b 的平均值所花费的时间。 首先，我们看一个使用 Python API for Spark 中的 map 和 reduce 函数计算平均值的版本： 12345sum_and_count = \\ data.map(lambda x: (x.a, (x.b, 1))) \\ .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\ .collect()[(x[0], x[1][0] / x[1][1]) for x in sum_and_count ] 相比之下，相同的程序可以使用 DataFrame API 编写为一个简单的操作： 1df.groupBy(&quot;a&quot;).avg(&quot;b&quot;) 图 9 显示代码的 DataFrame 版本比手写 Python 版本高 12 倍，而且更加简洁。 这是因为在 DataFrame API 中，Python 只构建了逻辑计划，所有物理执行都被编译成原生 Spark 代码作为 JVM 字节码，从而提高执行效率。 事实上，DataFrame 版本的性能也比上述 Spark 代码的 Scala 版本高 2 倍。 这主要是由于代码生成：DataFrame 版本中的代码避免了在手写 Scala 代码中发生的键值对的昂贵分配。 6.3 管道性能 DataFrame API 还可以让开发人员在单个程序中跨越关系和逻辑代码来编写业务逻辑并优化性能。 作为一个简单的例子，我们考虑一个两阶段管道，它从语料库中选择文本消息的子集并计算最常用的词。 虽然非常简单，但这可以模拟一些现实世界的管道，例如，计算特定人群在推文中使用的最流行的词。 在这个实验中，我们在 HDFS 中生成了一个包含 100 亿条消息的合成数据集。 每条消息平均包含 10 个从英语词典中提取的单词。 管道的第一阶段使用关系过滤器来选择大约 90% 的消息。 第二阶段计算字数。 首先，我们使用单独的 SQL 查询和基于 Scala 的 Spark 作业来实现管道，这可能发生在运行单独的关系和过程引擎(例如，Hive 和 Spark)的环境中。 然后我们使用 DataFrame API 实现了一个组合管道，即使用 DataFrame 的关系运算符执行过滤器，并使用 RDD API 对结果执行字数统计。 与第一个管道相比，第二个管道避免了在将 SQL 查询的整个结果传递到 Spark 作业之前将其保存到 HDFS 文件作为中间数据集的成本， 因为 SparkSQL 使用关系运算符进行过滤并管道化了统计单词数的 map 。 图 10 比较了两种方法的运行时性能。 除了更容易理解和操作，基于 DataFrame 的管道还将性能提升了 2 倍。 7 研究应用 除了 Spark SQL 的即时实际生产用例之外，我们还看到了从事更多实验项目的研究人员的浓厚兴趣。 我们概述了两个利用 Catalyst 可扩展性的研究项目：一个是近似查询处理，一个是基因组学。 7.1 广义在线聚合 曾和他的团队，已经在工作中使用了 Catalyst，以提高在线聚合的通用性 [40]。 这项工作概括了在线聚合的执行，以支持任意嵌套的聚合查询。 它允许用户通过查看在总数据的一小部分上计算的结果来查看执行查询的进度。 这些部分结果还包括准确性度量，让用户在达到足够的准确性时停止查询。 为了在 Spark SQL 内部实现这个系统，作者添加了一个新的操作符来表示一个被分解为采样批次的关系。 在查询规划期间，调用转换用于将原始完整查询替换为多个查询，每个查询对数据的连续样本进行操作。 然而，简单地用样本替换完整数据集不足以让在线方式计算出准确的答案。 标准聚合等操作必须替换为同时考虑当前样本和先前批次结果的有状态对应项。 此外，可能会根据近似答案过滤掉元组的操作必须替换为可以考虑当前估计错误的版本。 这些转换中的每一个都可以表示为 Catalyst 规则，这些规则修改算子树，直到它产生正确的在线答案。 不基于采样数据的树片段会被这些规则忽略，并且可以使用标准代码路径执行。 通过使用 Spark SQL 作为基础，作者能够用大约 2000 行代码实现一个相当完整的原型。 7.2 计算基因组学 计算基因组学中的一个常见操作涉及基于数值偏移检查重叠区域。 这个问题可以表示为不等式谓词的连接。 考虑两个数据集 a 和 b，其结构为 (start LONG, end LONG)。 范围连接操作可以用如下 SQL 表示： 12345SELECT * FROM a JOIN bWHERE a.start &lt; a.end AND b.start &lt; b.end AND a.start &lt; b.start AND b.start &lt; a.end 如果没有特殊优化，前面的查询将被许多系统使用低效算法(例如嵌套循环连接)执行。 相比之下，专门的系统可以使用区间树计算这个连接的答案。 ADAM 项目 [28] 中的研究人员能够在 Spark SQL 版本中构建一个特殊的规划规则，以有效地执行此类计算， 从而使他们能够利用标准数据操作能力以及专门的处理代码。 所需的更改大约是 100 行代码。 8 相关工作 编程模型 一些系统试图将关系处理与最初用于大型集群的程序处理引擎相结合。 其中，Shark [38] 最接近 Spark SQL，运行在相同的引擎上并提供相同的关系查询和高级分析组合。 Spark SQL 通过更丰富且对程序员更友好的 API DataFrames 改进了 Shark，其中可以使用宿主编程语言中的构造以模块化方式组合查询(参见第 3.4 节)。 它还允许直接在本机 RDD 上运行关系查询，并支持 Hive 之外的广泛数据源。 启发 Spark SQL 设计的一个系统是 DryadLINQ [20]，它将 C# 中的语言集成查询编译为分布式 DAG 执行引擎。 LINQ 查询也是关系查询，但可以直接对 C# 对象进行操作。 Spark SQL 超越了 DryadLINQ，还提供类似于常见数据科学库 [32, 30] 的 DataFrame 接口、数据源和类型的 API，以及通过在 Spark 上执行来支持迭代算法。 其他系统仅在内部使用关系数据模型，并将过程代码委托给 UDF。 例如，Hive 和 Pig [36, 29] 提供关系查询语言，但已广泛使用 UDF 接口。 ASTERIX [8] 内部有一个半结构化数据模型。 Stratosphere [2] 也有一个半结构化模型，但提供了 Scala 和 Java 的 API，让用户可以轻松调用 UDF。 PIQL [7] 同样提供了 Scala DSL。 与这些系统相比，Spark SQL 通过能够直接查询用户定义类(原生 Java/Python 对象)，并让开发人员可以在同一语言中混合使用过程 API 和关系 API。 此外，通过 Catalyst 优化器，Spark SQL 实现了大多数大型计算框架中不存在的优化(例如，代码生成)和其他功能(例如，JSON 和机器学习数据类型的模式推断)。 我们相信，这些功能对于为大数据提供集成的、易于使用的环境至关重要。 最后 DataFrame API 已经适用于单机 [32,30] 和集群 [13,10] 与以前的 API 不同，Spark SQL 优化器使用关系优化器进行 DataFrame 计算。 高级分析 Spark SQL 以最近的工作为基础，在大型集群上运行高级分析算法，包括迭代算法平台 [39] 和图分析 [15, 24]。 MADlib [12] 也希望公开分析功能，尽管方法不同，因为 MADlib 必须使用 Postgres UDF 的有限接口，而 Spark SQL 的 UDF 可以是成熟的 Spark 程序。 最后，包括 Sinew 和 Invisible Loading [35, 1] 在内的技术试图提供和优化对 JSON 等半结构化数据的查询。 我们希望在我们的 JSON 数据源中应用其中一些技术。 9 结论 我们展示了 Spark SQL，这是 Apache Spark 中的一个新模块，提供与关系处理的丰富集成。 Spark SQL 使用声明性 DataFrame API 扩展了 Spark，以允许进行关系处理，提供自动优化等优势，并让用户编写混合关系分析和复杂分析的复杂管道。 它支持为大规模数据分析量身定制的各种功能，包括半结构化数据、联邦查询和用于机器学习的数据类型。 为了启用这些功能，Spark SQL 基于名为 Catalyst 的可扩展优化器，通过嵌入 Scala 编程语言，可以轻松添加优化规则、数据源和数据类型。 用户反馈和基准测试表明，Spark SQL 使编写混合关系和过程处理的数据管道变得更加简单和高效，同时与以前的 SQL-on-Spark 引擎相比提供了显着的加速。 Spark SQL 在如下地址开源: http://spark.apache.org 10 致谢 We would like to thank Cheng Hao, Tayuka Ueshin, Tor Myklebust, Daoyuan Wang, and the rest of the Spark SQL contributors so far. We would also like to thank John Cieslewicz and the other members of the F1 team at Google for early discussions on the Catalyst optimizer. The work of authors Franklin and Kaftan was supported in part by: NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Guavus, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Schlumberger, Splunk, Virdata and VMware. 11 参考资料 [1] A. Abouzied, D. J. Abadi, and A. Silberschatz. Invisible loading: Access-driven data transfer from raw files into database systems. In EDBT, 2013. [2] A. Alexandrov et al. The Stratosphere platform for big data analytics. The VLDB Journal, 23(6):939–964, Dec. 2014. [3] AMPLab big data benchmark. https://amplab.cs.berkeley.edu/benchmark. [4] Apache Avro project. http://avro.apache.org. [5] Apache Parquet project. http://parquet.incubator.apache.org. [6] Apache Spark project. http://spark.apache.org. [7] M. Armbrust, N. Lanham, S. Tu, A. Fox, M. J. Franklin, and D. A. Patterson. The case for PIQL: a performance insightful query language. In SOCC, 2010. [8] A. Behm et al. Asterix: towards a scalable, semistructured data platform for evolving-world models. Distributed and Parallel Databases, 29(3):185–216, 2011. [9] G. J. Bex, F. Neven, and S. Vansummeren. Inferring XML schema definitions from XML data. In VLDB, 2007. [10] BigDF project. https://github.com/AyasdiOpenSource/bigdf. [11] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. FlumeJava: Easy, efficient data-parallel pipelines. In PLDI, 2010. [12] J. Cohen, B. Dolan, M. Dunlap, J. Hellerstein, and C. Welton. MAD skills: new analysis practices for big data. VLDB, 2009. [13] DDF project. http://ddf.io. [14] B. Emir, M. Odersky, and J. Williams. Matching objects with patterns. In ECOOP 2007 – Object-Oriented Programming, volume 4609 of LNCS, pages 273–298. Springer, 2007. [15] J. E. Gonzalez, R. S. Xin, A. Dave, D. Crankshaw, M. J. Franklin, and I. Stoica. GraphX: Graph processing in a distributed dataflow framework. In OSDI, 2014. [16] G. Graefe. The Cascades framework for query optimization. IEEE Data Engineering Bulletin, 18(3), 1995. [17] G. Graefe and D. DeWitt. The EXODUS optimizer generator. In SIGMOD, 1987. [18] J. Hegewald, F. Naumann, and M. Weis. XStruct: efficient schema extraction from multiple and large XML documents. In ICDE Workshops, 2006. [19] Hive data definition language. https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL. [20] M. Isard and Y. Yu. Distributed data-parallel computing using a high-level programming language. In SIGMOD, 2009. [21] Jackson JSON processor. http://jackson.codehaus.org. [22] Y. Klonatos, C. Koch, T. Rompf, and H. Chafi. Building efficient query engines in a high-level language. PVLDB, 7(10):853–864, 2014. [23] M. Kornacker et al. Impala: A modern, open-source SQL engine for Hadoop. In CIDR, 2015. [24] Y. Low et al. Distributed GraphLab: a framework for machine learning and data mining in the cloud. VLDB, 2012. [25] S. Melnik et al. Dremel: interactive analysis of web-scale datasets. Proc. VLDB Endow., 3:330–339, Sept 2010. [26] X. Meng, J. Bradley, E. Sparks, and S. Venkataraman. ML pipelines: a new high-level API for MLlib. https://databricks.com/blog/2015/01/07/ml-pipelines-a-newhigh-level-api-for-mllib.html. [27] S. Nestorov, S. Abiteboul, and R. Motwani. Extracting schema from semistructured data. In ICDM, 1998. [28] F. A. Nothaft, M. Massie, T. Danford, Z. Zhang, U. Laserson, C. Yeksigian, J. Kottalam, A. Ahuja, J. Hammerbacher, M. Linderman, M. J. Franklin, A. D. Joseph, and D. A. Patterson. Rethinking data-intensive science using scalable analytics systems. In SIGMOD, 2015. [29] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig Latin: a not-so-foreign language for data processing. In SIGMOD, 2008. [30] pandas Python data analysis library. http://pandas.pydata.org. [31] A. Pavlo et al. A comparison of approaches to large-scale data analysis. In SIGMOD, 2009. [32] R project for statistical computing. http://www.r-project.org. [33] scikit-learn: machine learning in Python. http://scikit-learn.org. [34] D. Shabalin, E. Burmako, and M. Odersky. Quasiquotes for Scala, a technical report. Technical Report 185242, École Polytechnique Fédérale de Lausanne, 2013. [35] D. Tahara, T. Diamond, and D. J. Abadi. Sinew: A SQL system for multi-structured data. In SIGMOD, 2014. [36] A. Thusoo et al. Hive–a petabyte scale data warehouse using Hadoop. In ICDE, 2010. [37] P. Wadler. Monads for functional programming. In Advanced Functional Programming, pages 24–52. Springer, 1995. [38] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and I. Stoica. Shark: SQL and rich analytics at scale. In SIGMOD, 2013. [39] M. Zaharia et al. Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. In NSDI, 2012. [40] K. Zeng et al. G-OLA: Generalized online aggregation for interactive analysis on big data. In SIGMOD, 2015.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译","slug":"treatise/an_architecture_for_fast_and_general_data_processing_on_large_clusters","date":"2021-06-24T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/architecture_of_next_generation_apache_hadoop_mapreduce_framework/","permalink":"https://wangqian0306.github.io/2021/architecture_of_next_generation_apache_hadoop_mapreduce_framework/","excerpt":"","text":"An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译 作者：Matei Zaharia 英文原文 注：由于原文内容过于庞大，此处仅仅针对于目前自己不太理解的部分进行翻译和整理。 1 引言 我们可以设计一个统一的编程抽象，不仅可以处理这些不同的计算任务，而且能使新的应用更好的编程。 特别的是，我们将展示 MapReduce 的一个简单扩展，称为弹性分布式数据集（RDDs）,它增加了高效的数据共享元语，以及大大增加了它的通用性。 由此产生的架构比当前系统有几个关键优势： 在相同的运行环境下，它支持批处理、交互式、迭代和流计算，结合这些模式提供丰富的应用编程，并且相对于单一模式的系统能更好的发挥其性能。 它以很小的代价在这些计算模式上提供结点故障和 straggler 的容忍功能。 事实上，在一些地方(如流和 SQL)，基于 RDD 产生的新系统比现有的系统有更强的容错性。 它实现的性能往往比 MapReduce 高 100 倍，并可媲美各个应用领域的专业系统。 这很适合多组织用户管理，允许应用程序弹性地扩缩容和响应式地共享资源。 1.1 专业系统相关问题 重复工作：许多专业系统仍然需要解决同样的潜在问题，如分布式执行和容错性。 举个例子，分布式 SQL 引擎或机器学习引擎都需要执行并行聚合。 对于独立的系统，针对每个领域也是需要解决这些问题。 组成：不同系统的组合进行计算的方式即昂贵又笨重。 尤其是对于“大数据”应用，中间处理过程的数据集是庞大的且难以移动的。 为了使得在各个计算引擎之间共享数据，当前的环境需要将数据导出到稳定且多备份的存储系统中，通常这比实际计算要消耗更多的资源。 因此，相比于一栈式的系统，由多个系统组成的管道常常效率很低。 范围限制：如果应用程序不符合专业系统的编程模型，用户只能修改程序以适应当前的系统，否则就针对该程序写一个新的运行系统。 资源共享：在计算引擎之间动态共享资源是很困难的，因为大多数引擎在应用程序运行期间都假定独自拥有一组机器。 管理和管理员：相对单一的系统，独立的系统需要在管理和部署上处理更多的事务。 对于用户来说，它们需要学习多种 API 和执行模型。 1.2 弹性分布式数据集(RDD) 为了解决这个问题，我们引入一个新的概念，弹性分布式数据集(RDDs)，它是 MapReduce 模型一种简单的扩展和延伸。 进一步说，虽然乍一看那些不适合 MapReduce 的计算任务(例如，迭代，交互性和流查询)之间存在着明显的不同，但他们却都有一个功能特性， 也是 MapReduce 模型的缺陷：在并行计算阶段之间能够高效地数据共享，这正是 RDD 具有真知灼见的地方。 运用高效的数据共享概念和类似于 MapReduce 的操作方式，使得所有这些计算工作都可以有效地执行，并可以在当前特定的系统中获得关键性的优化。 RDDs 以一种既高效有能容错的方式为广泛的并行计算提出这样一个抽象。 特别提出的是，以前的这些集群容错处理模型，像 MapReduce、Dryad，将计算转换为一个有向无环图(DAG)的任务集合。 这使得它们能够高效地重复执行 DAG 里的其中一部分任务来完成容错恢复。 但对于一个独立的计算，(例如在一个迭代过程中)，这些模型除了可复制的文件系统外没有提供其他存储的概念， 这就导致因为在网络上进行数据复制而增加了大量的消耗。 RDDs 是一个可以避免复制的容错分布式存储概念。 取而代之，每一个 RDD 都会记住由构建它的那些操作所构成的一个图，类似于批处理计算模型，可以有效地重新计算因故障丢失的数据。 由于创建 RDD 的操作是相对粗粒度的，即单一的操作应用于许多数据元素，该技巧比通过网络复制数据更高效。 RDDs 很好地运用于当前广泛的数据并行算法和处理模型中，所有的这些对多个任务使用同一种操作。 1.3 基于 RDD 机制实现的模型 我们使用 RDD 机制实现了多类模型，包括多个现有的集群编程模型和之前模型所没有支持的新应用。 在这些模型中，RDD 机制不仅在性能方面能够和之前系统相匹配，在其他方面，他们也能加 入现有的系统所缺少的新特性，比如容错性，straggler 容忍和弹性。我们讨论以下四类模型。 迭代式算法 一种目前已经开发的针对特定系统最常见的的工作模式是迭代算法，比如应用于图处理，数值优化，以及机器学习中的算法。 RDD 可以支持广泛类型的各种模型，包括 Pregel，像 HaLoop 和 Twister 这类的迭代式 MapReduce 模型， 以及确定版本的 GraphLab 和 PowerGraph 模型。 关系查询 在 MapReduce 集群中的首要需求中的一类是执行 SQL 查询，长期运行或多个小时的批量计算任务和即时查询。 这促进了很多在商业集群中应用的并行数据库系统的发展。 MapReduce 相比并行数据库在交互式查询上有非常大的缺陷，例如 MapReduce 的容错机制模型， 而我们发现通过在 RDD 操作中实现很多常用的数据库引擎的特性(比如，列处理)，这样能够达到相当可观的性能。 由上述方式所构建的系统 Shark，提供完整的容错机制，能够在短查询和长查询中很好的扩展，同时也能在 RDD 之上提供复杂分析函数的调用(例如, 机器学习)。 注：Shark 是基于 Hive 进行实现的，通过更换了 Hive 的物理执行引擎进行了性能改进。但是它仍然限制于 MapReduce 的遗留问题。 之后被 Spark SQL 模块替换，本文中不会再去关注这些内容了。 MapReduce RDD 通过提供 MapReduce 的一个超集，能够高效地执行 MapReduce 程序， 同样也可以指向比如 DryadLINQ 这样常见的机遇 DAG 数据流的应用。 流式数据处理 我们的系统与定制化系统最大的区别是我们也使用 RDD 实现了流式处理。 流式数据处理已经在数据库和系统领域进行了很长时间研究，但是实现大规模流式数据处理仍然是一项挑战。 当前的模型并没有处理在大规模集群中频繁出现的 straggler 的问题，同时对故障恢复的方式也非常有限，需要大量的复制或浪费很长的恢复时间。 特别是，当前的系统是基于一种持续操作的模型，这就需要长时间的有状态的操作处理每一个到达的记录。 为了恢复一个丢失的节点，当前的系统需要保存每一个操作符的两个副本，或通过一系列耗费大量开销的串行处理来对上游的数据进行重放。 我们提出了一个新的模型，离散数据流(D-Streams),来解决这样的问题。 对使用长期状态处理的过程进行替换，D-Streams 把流式计算的执行当做一系列短而确定性的批量计算的序列，将状态保存在 RDD 里。 D-Stream 模型通过根据相关 RDD 的依赖关系图进行并行化恢复，就能达到快速的故障恢复，这样不需要通过复制。 另外，它通过推测(Speculative)来支持对 straggler 执行迁移，例如，对那些慢任务运行经过推测的备份副本。 尽管 D-Stream 将计算转换为许多不相关联的 jobs 来运行从而增加了部分延迟，然而我们证明了 D-Stream 能够被达到次秒级延时的实现， 这样能够达到以前系统单个节点的性能，并能线性扩展到 100 个节点。 D-Stream 的强恢复特性让他们成为了第一个处理大规模集群特性的流式处理模型，并且他们基于 RDD 的实现使得应用能够有效的整合批处理和交互式查询。 1.4 总结 略 1.5 论文计划 本文组织结构如下。 第 2 章介绍了 RDD 抽象并涵盖了一些简单的编程模型的应用。 第 3 章介绍了 Shark SQL 系统基于 RDDs 实现的更高级的存储和处理模型的技术。 第 4 章介绍了如何使用 RDDs 开发离散的流，这是一种新的流式处理模型。 第 5 章则介绍了为什么 RDD 模型在这些应用中如此通用，同时介绍它的限制和扩展性。 最后，在第 6 章，我们总结和讨论一些未来工作的可能方向。 2 弹性分布式数据集 2.1 简介 我们所提出的弹性分布式数据集(RDDs)，这种全新的抽象模式令用户可以直接控制数据的共享。 RDD 具有可容错和并行数据结构特征，这使得用户可以指定数据存储到硬盘还是内存、控制数据的分区方法并在数据集上进行种类丰富的操作。 他们提供了一个简单高效的编程接口，可以同时满足现有的特定模型和全新的应用场景。 RDD 设计时的最大挑战在于定义一个能提供高效容错能力的编程接口。 现有的基于集群的内存存储抽象，比如分布式共享内存，键-值存储，数据库，以及 Piccolo,提供了一个对内部状态基于细粒度更新的接口(例如,表格里面的 cell)。 在这样的设计之下，提供容错性的方法就要么是在主机之间复制数据，要么对各主机的更新情况做日志记录。 这两种方法对于数据密集型的任务来说代价很高，因为它们需要在带宽远低于内存的集群网络间拷贝大量的数据，同时还将产生大量的存储开销。 与上述系统不同的是，RDD 提供一种基于粗粒度变换(如， map, filter, join)的接口，该接口会将相同的操作应用到多个数据集上。 这使得他们可以通过记录用来创建数据集的变换(lineage)，而不需存储真正的数据，进而达到高效的容错性。 当一个 RDD 的某个分区丢失的时候，RDD 记录有足够的信息记录其如何通过其他的 RDD 进行计算，且只需重新计算该分区。 因此，丢失的数据可以被很快的恢复，而不需要昂贵的复制代价。 2.2 RDD 概述 2.2.1 概念 从形式上看，RDD是一个分区的只读记录的集合。 RDD只能通过在(1)稳定的存储器或(2)其他RDD的数据上的确定性操作来创建。 我们把这些操作称作变换以区别其他类型的操作。例如 map, filter, 和 join。 注：尽管单个的 RDDs 是不可变的，但可以通过多个 RDDs 来表示一个数据集的多个版本来实现可变。 这种性质(可变)使得描述其 lineage(获取 RDD 所需要经过的变换)变得容易。 可以这样理解，RDD 是版本化的数据集，并且可以通过变换记录追踪版本。 RDD 在任何时候都不需要被&quot;物化&quot;(进行实际的变换并最终写入稳定的存储器上)。 实际上，一个 RDD 有足够的信息描述着其如何从其他稳定的存储器上的数据生成。 它有一个强大的特性：从本质上说，若 RDD 失效且不能重建，程序将不能引用该 RDD。 最后，用户可以控制 RDD 的其他两个方面：持久化和分区。 用户可以选择重用哪个 RDD，并为其制定存储策略(比如， 内存存储)。 也可以让 RDD 中的数据根据记录的 key 分布到集群的多个机器。 这对位置优化来说是有用的，比如可用来保证两个要 join 的数据集都使用了相同的哈希分区方式。 2.2.2 Spark 编程接口 具体来说，每一个数据集都会表示为一个对象，而各种变换则通过该对象相应方法的调用而实现。 在最开始，编程人员通过对稳定存储上的数据进行变换操作(例如, map 和 filter) 来得到一个或多个 RDD。 之后，他们可以调用这些 RDD 的 actions(动作)类的操作。 这类操作的目的或是返回一个值，或是将数据导入到存储系统中。 动作类的操作如 count(返回数据集的元素数), collect(返回元素本身的集合)和 save(输出数据集到存储系统)。 与 DryadLINQ 一样，Spark 直到 RDD 第一次调用一个动作时才真正计算 RDD。 这也就使得 Spark 可以按序缓存多个变换。 此外，编程人员还可以调用 RDD 的 persist(持久化)方法来表明该 RDD 在后续操作中还会用到。 默认情况下，Spark 会将调用过 persist 的 RDD 存在内存中。 但若内存不足，也可以将其写入到硬盘上。 通过指定 persist 函数中的参数，用户也可以请求其他持久化策略并通过标记来进行 persist，比如仅存储到硬盘上，又或是在各机器之间复制一份。 最后，用户可以在每个 RDD 上设定一个持久化的优先级来指定内存中的哪些数据应该被优先写入到磁盘。 2.2.3 优点 略 2.2.4 不适合 RDD 的应用 正如在引言中讨论的，RDDs 最适合对数据集中所有的元素进行相同的操作的批处理类应用。 在这些情况下，作为整个 lineage 图中的其中一步，RDD 高效地记住每一次变换，从而不需要对大量数据做日志记录便可恢复失效分区。 RDDs 不太适用于通过异步细粒度更新来共享状态的应用，比如针对 Web 应用或增量网络爬虫的存储系统。 对于这些应用，那些传统的更新日志和数据检查点的系统会更有效。 2.3 Spark 编程接口 为了使用 Spark, 开发者需要写一个 driver program 来连接到 workers 集群。 driver program 定义一个或多个 RDDs 以及相关的一些 action 操作。 driver 上的 spark 代码也跟踪记录 RDDs 的继承关系，即 lineage。 Worker 是一直运行着的进程，它将经过一系列操作后的 RDD 分区数据保存在内存中。 用户通过传递闭包的方式将参数传递给 Map 等操作。 在 Scala 中每个闭包都代表一个 Java 对象，这些对象可以被序列化，也可以通过网络将闭包传递给其他节点并加载。 Scala 会将闭包中的所有变量转义成 Java 对象的属性域。 2.4 抽象 RDDs 抽象 RDDs 的一个挑战是如何在经过一系列 transform 操作后追踪其继承关系。 理想情况下，一个实现了 RDD 的系统必须尽可能多地提供各种变换操作，并允许用户随意进行组合。 我们提出了一种基于图的方式来抽象 RDD，它可以实现上述目标。 我们已经在 Spark 中使用了这种表现形式来提供各种 transform 操作，而无需为每个 transform 操作的调度增加额外的逻辑。 这极大度简化了系统的设计。 简而言之，我们提供了一个通用接口来抽象每个 RDD，并提供 5 种信息： 一种是分区信息，这些信息是数据集最小的分片； 一种是依赖关系信息，指向其父 RDD； 一种是函数，基于父 RDD 进行计算； 一种是分区方案； 最后一种是数据存放位置。 例如：一个表现 HDFS 文件的 RDD 将文件的每个文件块表示为一个分区，并且知道每个文件块的位置信息。 同时，对 RDD 进行 map 操作后具有相同的划分。 当计算其元素时，将 map 函数应用于父 RDD 的数据。 在设计接口的过程中，最有趣的问题在于如何表示 RDD 之间的依赖关系。 我们发现，比较合理的方式是将依赖关系分成两类： 窄依赖，每个父 RDD 的分区都至多被一个子 RDD 的分区使用； 宽依赖：多个子 RDD 的分区依赖一个父 RDD 的分区。 这两种依赖的的区别从两个方面来说比较有用。 首先，窄依赖允许在单个集群节点上流水线式执行，这个节点可以计算所有父级分区。 例如，可以逐个元素地依次执行 filter 操作和 map 操作。 相反，宽依赖需要所有的父 RDD 数据可用并且数据已经通过类 MapReduce 的操作 shuffle 完成。 其次，在窄依赖中，节点失败后的恢复更加高效。 因为只有丢失的父级分区需要重新计算，并且这些丢失的父级分区可以并行地在不同节点上重新计算。 与此相反，在宽依赖的继承关系中，单个失败的节点可能导致一个 RDD 的所有先祖 RDD 中的一些分区丢失，导致计算的重新执行。 RDD 的这种通用接口使得在 Spark 中使用不到 20 行的代码来实现大多数 transform 操作。 事实上，即使是 Spark 的新用户也能实现新的 transform 操作(如：抽样和各种类型的 join)而不必了解调度细节。 下面是一些 RDD 实现的概略说明： HDFS 文件：在我们的例子中，HDFS 文件作为输入 RDD 的元素。 对于这些 RDD，partitions 代表文件中每个文件块的分区(包含文件块在每个分区对象中的偏移量)， preferredLocations 表示文件块所在的节点，而 iterator 读取这些文件块。 map：在任何一个 RDD 上调用 map 操作将返回一个 MappedRDD 对象。 这个对象与其父对象具有相同的分区以及首选地点(preferredLocations)，但在其迭代方法(iterator）中， 传递给 map 的函数会应用到父对象记录。 union：在两个 RDD 上调用 union 操作将返回一个 RDD，这个 RDD 的分区为原始两个 RDD 的父 RDD 的分区进行 union 后的结果。 每个子分区都是通过窄依赖于同一个父级分区计算出来的。 sample：抽样类似于映射。不同之处在于，RDD 会为每一个分区保存一个生成随机数的种子，来对确定如何对父级记录进行抽样。 join：连接两个 RDD 可能会产生两个窄依赖，或两个宽依赖，或一个窄依赖和一个宽依赖。 如果两个 RDD 都是基于相同的 Hash/范围划分策略，那么就会产生窄依赖； 如果一个父 RDD 具有某种划分策略而另一个不具有，则会同时产生窄依赖和宽依赖。 无论哪种情况，结果 RDD 都具有一个划分策略(要么继承自父 RDD，要么是一个默认的 Hash 划分策略)。 2.5 实现 每一个 Spark 程序都以一个独立的应用在集群上运行，它有它自己的驱动节点(主节点,Master)和工作节点(Workers)。 各个应用之间的资源共享则通过集群管理器来控制。 Spark 可以从任何 Hadoop 的输入源(例如使用 Hadoop 的 HDFS 和 HBase)中使用 Hadoop 的现有输入插件 APIs 读取数据， 并且在未更改的 Scala 版本上运行。 现在我们描述了几种在系统中有趣的技术：我们的作业调度程序，多用户支持，Spark 解析器的交互式使用，内存管理，并且检查点支持。 2.5.1 作业调度 注：实线圆角方框标识的是 RDD。阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。 阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。 RDD G 上一个 Action 的执行将会以宽依赖为分区来构建各个 stage，对各 stage 内部的窄依赖则前后连接构成流水线。 在本例中，stage 1 的输出已经存在 RAM 中，所以直接执行 stage 2 ，然后 stage 3。 总的来说，我们的调度器与 Dryad [61] 类似，但它额外会考虑被持久化(persist)的 RDD 的哪个分区保存在内存中并可供使用。 当用户对一个 RDD 执行 Action(如 count 或 save)操作时，调度器会根据该 RDD 的 lineage， 来构建一个由若干阶段(stage)组成的一个 DAG(有向无环图)以执行程序，正如图 2.5 所示。 每个 stage 都包含尽可能多的连续的窄依赖型转换。 各个阶段之间的分界则是宽依赖所需的 shuffle 操作，或者是 DAG 中一个经由该分区能更快到达父 RDD 的已计算分区。 之后，调度器运行多个任务来计算各个阶段所缺失的分区，直到最终得出目标 RDD。 调度器向各机器的任务分配采用延时调度机制并根据数据存储位置(本地性)来确定。 若一个任务需要处理的某个分区刚好存储在某个节点的内存中，则该任务会分配给那个节点。 否则，如果一个任务处理的某个分区，该分区含有的 RDD 提供较佳的位置(例如，一个 HDFS 文件)，我们把该任务分配到这些位置。 对应宽依赖类的操作(比如 shuffle 依赖)，我们会将中间记录物理化到保存父分区的节点上。 这和 MapReduce 临时存储 Map 的输出类似，能简化数据的故障恢复过程。 对于执行失败的任务，只要它对应 stage 的父类信息仍然可用，它便会在其他节点上重新执行。 如果某些 stage 变为不可用(例如，因为 shuffle 在 map 阶段的某个输出丢失了)，则重新提交相应的任务以并行计算丢失的分区。 我们还不能接受调度程序的失败，尽管复制相应 RDD 的 lineage 是比较直接的解决之道。 若某个任务执行缓慢(即&quot;落后者&quot;straggler)，系统则会在其他节点上执行该任务的拷贝这与 MapReduce 做法类似，并取最先得到的结果作为最终的结果。 最后，虽然目前在 Spark 中所有的计算都是为了对驱动程序中调用动作的响应而执行，我们也试验让集群上的任务（如映射）调用查找操作， 它根据键值能够随机访问散列分区的 RDDs 的元素。 在这种设计下，如果任务所需要的分区丢失了，则该任务需要告知调用器去重新计算该分区。 2.5.2 多用户管理 RDD 模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群上能支持多种资源共享算法。 特别地是，每个 RDD 应用可以在执行过程中动态增长，并且可以轮询访问每台设备，或者可以被高优先级的应用占用。 Spark 应用中大多数的任务的执行周期在 50 毫秒到数秒之间，这使得共享请求能得到快速响应。 虽然多用户共享算法并非本论文的主题，但如下我们列出了所支持的那些具体算法，以给读者一个感性的认识： 在每个应用程序中，Spark 允许多线程同时提交作业，并通过一种等级公平调用器来实现多个作业对集群资源的共享。 这种调用器和 Hadoop Fair Scheduler 类似。 此特性主要用于创建基于针对相同内存数据的多用户应用，例如：Shark SQL 引擎有一个服务模式支持多用户并行运行查询。 公平共享确保作业彼此分离，同时短的作业能在即使长作业占满集群资源的情况下也可尽早完成。 Spark 的公平调度也使用延迟调度 [117]，通过轮询每台机器的数据，在保持公平的情况下给予作业高的数据本地性。 在本章几乎所有的试验中，内存本地化访问(Memory Locality)为 100%。 Spark 支持多级本地化访问策略(本地性)，包括内存、磁盘和机架，以降低在一个集群里不同方式下的数据访问的代价。 由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。 纵观 Spark 的应用，Spark 仍然使用 Mesos 中资源提供的概念来支持细粒度共享，它让不同的应用使用相同的 API 发起细粒度的任务请求。 这使得 Spark 应用能相互之间或在不同的计算框架(例如 Hadoop)之间实现资源的动态共享。 延迟调度仍然能够在资源提供模型中提供数据本地性。 最后，Spark 使用 Sparrow 系统扩展支持分布式调度。 该系统允许多个 Spark 应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。 通过以去主节点方式来进行多任务提交时，分布式调度可极大地提升系统的可扩展性。 2.5.3 解析器集成 与 Ruby 和 Python 类似，Scala 也提供了一个交互式 Shell(解析器)。 借助内存数据所带来的低延迟特性，我们希望让用户也能通过解析器来运行 Spark 并对大数据集进行交互式查询。 Scala 解析器通常会为用户输入的每一行生成一个类，把它导入 JVM ，调用上面的一个函数。 Scala 解析器的解析通常有如下组成： 将用户输入的每一行编译出其所对应的一个类； 将该类载入到 JVM 中； 调用该类的某个函数。 这个类包含一个单例对象，对象中包含当前行的变量或函数，在初始化方法中包含运行该行的代码。 例如，如果用户输入 var x = 5，换一行再输入 println(x)，那解析器会定义一个叫 Line1 的类，该类包含 x。 第二行编译成 println(Line1.getInstance().x)。 Spark 中我们做了两个改变： 类传输：为了让工作节点能够从各行生成的类中获取到字节码，我们让解析器通过 HTTP 来为类提供服务。 为能让 Worker 节点能获取到各行对应类的字节码，我们让解析器通过 HTTP 来提供这些类。 代码生成器的改动：通常，各种代码生成的单例对象是经由其相应类的一个静态方法来访问的。 也就是说，当我们序列化一个引用了上一行中定义的变量的闭包(例如上面例子中的 Line1.x)时，Java 不会通过检索对象树的方式去传输包含 x 的 Line1 实例。 因此，工作节点不能得到 x。 我们修改了代码生成器的逻辑，让各行对象的实例可以被直接引用。 2.5.4 内存管理 Spark 提供了三种对持久化 RDD 的存储策略： 未序列化 Java 对象存于内存中、序列化后的数据存于内存及磁盘存储。 第一个选项的性能表现是最优秀的，因为可以直接访问在 JAVA 虚拟机内存里的 RDD 对象。 在空间有限的情况下，第二种方式可以让用户采用比 JAVA 对象图更有效的内存组织方式，代价是降低了性能。 第三种策略适用于 RDD 太大难以存储在内存的情形，但每次重新计算该 RDD 会带来额外的资源开销。 对于有限可用内存，我们使用以 RDD 为对象的 LRU 回收算法来进行管理。 当计算得到一个新的 RDD 分区，但却没有足够空间来存储它时，系统会从最近最少使用的 RDD 中回收其一个分区的空间。 除非该 RDD 便是新分区对应的 RDD，这种情况下，Spark 会将旧的分区继续保留在内存，防止同一个 RDD 的分区被循环调入调出。 这点很关键–因为大部分的操作会在一个 RDD 的所有分区上进行，那么很有可能已经存在内存中的分区将会被再次使用。 到目前为止，这种默认的策略在我们所有的应用中都运行很好，当然我们也为用户提供了“持久化优先级”选项来控制 RDD 的存储。 最后，Spark 集群中的每一个实例都有其自己独立的内存空间。 在后续的工作中，我们计划通过一个统一的内存管理器来实现多个 Spark 实例之间的 RDD 共享。 Berkeley 正在进行的 Tachyon 项目便是朝着这个目标。 2.5.5 检查点相关支持 虽然 lineage 可用于错误后 RDD 的恢复，但对于很长的 lineage 的 RDD 来说，这样的恢复耗时较长。 由此，将某些 RDD 进行检查点操作(Checkpoint)保存到稳定存储上，是有帮助的。 通常情况下，对于包含宽依赖的长血统的 RDD 设置检查点操作是非常有用的，比如 PageRank 例子 (§2.3.2)中的排名数据集； 在这种情况下，集群中某个节点的故障会使得从各个父 RDD 得出某些数据丢失，这时就需要完全重算。 相反，对于那些窄依赖于稳定存储上数据的 RDD 来说，对其进行检查点操作就不是有必要的。 这样的 RDD 如 logistic 回归的例子(第 2.3.2 节)和 PageRank 中的链接列表。 如果一个节点发生故障，RDD 在该节点中丢失的分区数据可以通过并行的方式从其他节点中重新计算出来，计算成本只是复制整个 RDD 的很小一部分。 Spark 当前提供了为 RDD 设置检查点(用一个 REPLICATE 标志来持久化)操作的 API,让用户自行决定需要为哪些数据设置检查点操作。 但是，我们也正在对检查点操作自动化进行研究。 因为调度器知道每个数据集的大小以及计算它的消耗的时间，那它应该可以选出所需 Checkpoint 的那些 RDD 以最小化系统恢复所需时间。 最后，由于 RDD 的只读特性使得比常用的共享内存更容易做 checkpoint。 由于不需要关心一致性的问题，RDD 的写出可在后台进行，而不需要程序暂停或进行分布式快照。 2.6 性能评估 略 2.7 讨论 虽然 RDD 的不可变性质和粗粒度变换特质，使得其编程接口看上去能力有限，但实际中我们发现它们能适应的应用种类广泛。 具体来说，RDD 可以表达大量的集群编程模型，而这些模型之前都是针对独立框架而提出。 从而，RDD 使得用户可以在一个程序中组合这些模型 (比如, 先运行一个 MapReduce 操作来构建一个图，而后对该图调用 Pregel)，并在它们之间共享数据。 在本节中，我们将讨论 RDD 可以表达哪些编程模型，以及为什么它们被广泛应用(第 2.7.1 节)。 另外，我们还将讨论 RDD 中 lineage 信息的另一个好处–它使得在这些模型之间的调试变得容易。 2.7.1 对现有编程模型的表达 RDD 可以高效的表现一些此前相对独立的集群编程模型。 所谓“高效”，是指 RDD 不仅能得到与它们相同的输出结果， 而且还囊括了对这些框架所进行的优化，比如将特定的数据保持在内存中、数据分区优化以降低网络通讯和高效率的故障恢复。 可以用 RDD 表达的模型包括： MapReduce：这个模型可以由 Spark 中的 flatMap 与 groupByKey 操作进行表达，而如果用到 combiner 时则可引入 reduceByKey 操作。 DryadLINQ：DryadLINQ 系统基于 Dryad 更通用的运行机制上，提供了比 MapReduce 更为丰富的操作。 但这些操作都是批处理操作，且 Spark 中都有相应的 RDD 变换操作与之对应(如 map，groupByKey，join 等等)。 Pregel：谷歌的 Pregel 是一个专门针对迭代图型应用的模型。 这种模型初看之下与其他系统面向集合的编程模型有很大的不同。 在 Pregel 中，一个程序以一系列协调好的 superstep 运行。 在每一个 superstep 里，图内的各节点都通过执行一个用户定义的函数来实现对自身状态的更新和对图的拓扑结构的改变， 并向其他节点发送包含它们在下一超步所需要的信息的消息。 该模型可以表达许多图形算法，包括最短路径，二分匹配，和 PageRank。 在 Pregel 的关键是每次迭代中，它是将相同的用户自定义函数运用到所有节点上。 这是 RDD 可以表达 Pregel 模型的关键。 具体来说，我们可以将各次迭代时的节点状态保存为一个 RDD，然后调用一个变换(flatMap)来执行用户自定义的函数，并生成上述消息所对应的 RDD。 之后，通过将该 RDD 和节点状态的 RDD 进行 join 操作，便可实现消息的交换。 同样值得注意的是，RDD 同时也能提供如 Pregel 那样将节点状态保存在内存中、控制节点分区策略来减少网络通讯以及出现故障时的部分恢复功能。 我们在 Spark 上实现了一个 200 行的 Pregel 库。 迭代式 MapReduce：近期所提出的系统，包括 HaLoop 和 Twister ， 提供了一种迭代式的 MapReduce 模型。 在该模型下用户可以指定系统运行一系列的 MapReduce 任务。 这些系统可以保持每次迭代的数据分区一致性，其中 Twister 还可将讲数据保持在内存中。 这些优化都可轻松地用 RDD 来表达，HaLoop 模型的实现对应一个 200 行左右的库。 2.7.2 RDD 表达能力的相关说明 为什么 RDD 能够表达这些不同的编程模型？ 原因就是 RDD 上的限制在许多并行应用程序中影响非常小。 其原因在于，虽然 RDD 仅能通过批量变换来创建，但众多的并行程序本质上都是对多条记录执行相同的操作，而这点便使得它们易于表达。 另外，RDD 的不变性也不会影响其表达，因为相同数据集的各个不同版本可以通过多个对应的 RDD 来表示。 事实上，大多数当前的 MapReduce 应用所基于的文件系统，比如 HDFS，并不允许更新文件。 在后续章节(3 和 5)中，我们会对 RDD 表达进行更为详细的阐述。 2.7.3 利用 RDD 进行调试 RDD 的最初始设计时能为容错进行确切的重算特性，该特性也方便了对其的调试。 具体来说，通过记录在作业中创建的 RDD 的 lineage，借助重算所依赖的的RDD分区，人们可以： 重建这些 RDD，同时对其进行交互式查询 在一个单进程调试器中从该作业里运行任意一个任务。 不同于传统的针对一般分布式系统的重放(replay)调试器, 需要对多个节点记录并推断出其事件的先后顺序，而 RDD 只需要记录 lineage 图即可。 因此基本上不会引入任何记录开销。 我们现在就是基于这些概念进行 Spark 调试器的开发工作。 2.8 相关工作 略 2.9 总结 略 3 基于 RDD 的模型 3.1 简介 尽管在之前的章节中已经介绍了一些简单的基于RDD的编程模型，例如 Pregel 和 MapReduce 的迭代计算。 不过 RDD 的抽象模型可以用来实现更复杂的工作，包括专用引擎中关键的优化(例如，列式存储的处理和索引)自 Spark 发布以来，已经实现了如下的一些模型： SQL 引擎 图计算系统 机器学习库 注：此处没有说明 Spark Streaming。 简单回顾一下之前的章节，RDD 可以提供如下特性： 在一个集群中对于任意记录具有不变性的存储（在 Spark 中以 Java 对象的方式来表示） 通过每一条记录的 key 字段来控制数据分区 将粗粒度的操作用于分区的操作 利用内存存储的低延迟特性 接下来，我们将展示如何利用这些特性来实现更复杂的数据处理和存储。 3.2 一些在 RDDs 上实现其他模型的技术 在特定的引擎上，不仅仅优化了数据上的运算符，也优化了数据的存储格式和数据的访问方式。 例如，像 Shark 这样的 SQL 引擎可能会按列来处理数据，但是像 GraphX 这样的图引擎按照索引来处理数据，使得效率表现很出色。 下面我们讨论几个已经在 RDDs 上实现了的这些优化的常见方法，这些方法使得可以在享受 RDD 模型带来的容错等好处的同时，还能保持特定系统的性能。 3.2.1 RDD 里的数据格式 虽然 RDD 存储的是简单、扁平的数据记录，但我们发现一个实现更丰富的存储格式的有效策略是通过在同一条 RDD 记录中存储多个数据项， 并对每一条记录实施更加复杂的存储。 用这样的方式批处理即使几千条记录所带来的效率就足以非常接近使用专门的数据结构，同时仍然保持了每个 RDD 的大小为几兆字节。 RDD模型有两方面的因素使得这个方法非常有效。 首先，RDD通常在内存中，因此对每个操作可以用指针来只读取整个“组合”记录中相关的部分。 例如，一组用列表示的(整型，浮点型)记录可以用一个整型数组和浮点数组来实现。 如果我们只想读取整数字段，我们可以根据指针找到第一个数组而不用在内存中扫描第二个数组。 其次，一个很自然的问题便是，如果每个计算模型都有自己的批处理记录的表示方式，那么如何有效地把要处理的类型结合起来？ 幸运的是，RDD 底层接口是基于迭代器的(见 2.4 节中的计算方法)，这可以实现数据在不同格式中快速和流式地转换。 含有复合记录的 RDD 可以通过 flatMap 批量操作有效地在解压的记录上返回一个迭代器， 并且这个迭代器可以被进一步地在解压后的记录上进行管道化的窄变换，或者被重新打包成另一种格式进行转换，从而使未提交的且未解压的数据量最小化。 迭代器由于通常进行扫描操作，一般情况下是用于内存数据中的一个高效的接口。 只要每个批记录能放在 CPU 缓存中，这使得数据能够快速地转换和转化。 3.2.2 数据分区 在特定模型中的第二个常见优化是在一个集群中用特定领域的方式对数据进行划分来提高应用程序的性能。 例如，Pregel 和 HaLoop 使用了一个可能的用户自定义函数来划分数据，从而加快针对一个数据集的连接操作。并行数据库通常也提供了多种数据划分形式。 在 RDD 里，对于每条记录都可以通过记录里的 key 值很容易地进行数据划分。 (事实上，这是在 RDD 拥有的一条记录元素的唯一标识。) 值得注意的是，即使 key 是针对整个记录的，但含有多个潜在数据项的“复合”记录仍然可以有一个有意义的 key 值。 例如，在 GraphX 中，每一个分区中的记录都有相同的散列码来取分区数的模，但是为了能够高效的查找仍然需要在内部进行散列。 当系统使用复合记录来进行 shuffle 操作时，如 groupBy，它们可以将每个突出的记录散列为目标组合记录的 key 值。 最后一个有趣的划分用例是复合数据结构，数据结构的一些字段会随着时间被更新而另外一些字段则不被更新。 一般情况下，用户可以认为 RDDs 是作为一个在集群环境里更为具体的内存抽象。 当在单台机器上使用内存时，程序员主要为了优化查找和最大化提高常访问信息的集中式放置，而需要考虑数据的分布。 RDDs 通过让用户选择一个划分函数和划分的数据集，来提供对分布式内存的控制，但 RDDs 避免了要求用户精确的指定每一个分区的位置。 因此，运行时系统可以基于可用资源对分区数据进行有效地放置，或在出现故障时对分区数据移动，而程序员仍然可以控制访问的性能。 3.2.3 关于不可变性 RDD 模型与大多数特定系统的第三个区别是 RDDs 是不可变的。 不可变性对于 lineage 和错误恢复来说是很重要的，尽管它与为这些目的而具有可变的数据集和记录版本号没有本质上的不同。 但是，可能有人会问这是否会导致性能低下。 虽然不可变性和容错性肯定会导致一些开销，但是我们发现这两项技术在很多情况下都能够表现出良好的性能。 我们用多个 co-partitioned RDDs 来表示复合数据结构，只允许程序修改需要修改部分的状态。 在很多算法中，尽管记录的其他字段在每次迭代中都会改变，但是有些字段是永远不变的，所以这种方式就可以取得了很好的性能。 当内部的数据结构是不可变的时候，即使在一条记录中，我们也可以用指针来重复使用记录之前&quot;版本&quot;的状态。 例如，在 Java 中字符串是不可变的，所以一个(Int, String)记录上的 Map 如果保持 String 不变，只改变 Int 值的话， 我们只需要使用一个指向之前 String 对象指针，而不是去复制它。 更笼统地说，在函数式编程中的持久化数据结构可以用其他形式的数据上的增量更新(例如，散列表) 来表示之前版本的增量。 令人很愉快的是，许多函数式编程中的想法可以直接帮助 RDD。 在今后的工作中，我们将继续寻求其他方式来跟踪多个版本的状态，从而接近可变状态系统的性能。 3.2.4 实现自定义转换 最后，我们发现在一些应用中，使用低级的 RDD 接口实现自定义的依赖模式和转换是有用的(参见 2.4 节)。 该接口非常简单，实现他们仅仅需要依赖于父 RDDs 的列表和为 RDD 的分区从父 RDDs 给定的迭代器里进行迭代计算的一个函数。 在 Shark 和 GraphX 的第一版中，我们实现了一些这样的自定义运算符，这导致了很多新的运算符也被加入到 Spark 中去。 比如说，mapPartitions，是我们实现的一个有用的运算符， 在给定一个 RDD[T] 和一个计算迭代器函数(给定 Iterator[T]，计算出 Iterator[U])的情况下，通过将这个函数作用到每一个分区上， 最后能返回一个 RDD[U]。 这是非常接近于 RDDs 最低级的接口，允许在每个分区里执行非功能性的操作(例如，使用可变状态)。 在 Shark 的实现中同样包含 join 和 groupBy 的自定义版本，这是为了取代内置的相应运算符的工作。 但是，请注意即使是实现了自定义转换的应用，这些应用依然能够自动地享受到 RDD 模型的容错、多租户和组合所带来的好处， 并且使得开发将会比独立系统更加简单。 3.3 Shark：RDDs 上的 SQL 略 3.4 实现 略 注：此处暂时略过，若 Spark SQL 部分的论文进行了详细阐述则不会重新更新此部分。 3.5 性能 略 3.6 与 SQL 相结合的复杂分析 略 3.7 总结 略 4 离散化的流数据处理 本章讲述第二章里的 RDD 模型在一个有可能是最令人心动领域里的应用：大规模流处理。 虽然其设计与传统流系统不同，但它提供丰富的故障恢复，以及强大的与其它处理类型融合的能力。 大规模流处理的动机很简单：大部分“大数据”都是实时获取的，并且到达之时最有价值。 例如，社交网络或想检测出近几分钟内的热点话题，搜索网站会想对哪些用户会访问新网页进行建模，又或是服务运营商对程序日志进行秒级监控以实现实时故障侦测。 要实现这些应用，就需要能扩展到大型集群的流处理模型。 然而，设计这样的模型并不容易，因为应用(比如实时日志处理或机器学习)所需的规模可达数百个节点。 在这种规模下，系统故障和慢节点（straggler）问题会变得很严重，并且流式应用尤其需要快速恢复。 事实上，相比在批处理类应用中，快速恢复在流应用中更显重要：在批处理下，用 30 秒钟从系统故障或慢节点里恢复或许可以接受，而在流处理中， 这 30 秒便可错过一个重要的决策。 不幸的是，现有的流系统对系统故障或慢任务（straggler）的应对能力有限。 大多数分布式流式系统，包括 Storm，TimeStream，MapReduce Online，和流数据库，都基于连续操作模型。 在这种模型中，长期运行的带有状态的操作会接收每条记录，更新内部状态，并且发送新的记录。 这样的模型设计很自然，但也让它难以应对系统故障和慢任务问题。 具体来说，给定连续运算符模型，系统通过两种方法执行恢复：复制，每个节点存储两个副本， 或者 upstream backup，使节点缓存发送信息并在故障节点的新副本里重新执行。 两种途径都不太适合大规模集群：复制要占用 2 倍的硬件，而 upstream backup 需要长时间的恢复， 因为整个系统必须等待一个新的节点通过重新运行操作数据串行重建故障节点的状态。 此外，这两种途经都不能处理慢任务问题： 在 upstream backup 方案下，慢任务需要当作系统故障来处理，而这样的恢复代价高昂； 在复制方案的系统里，采用如 Flux 的同步协议来进行副本之间的协调，恢复速度将受限于慢任务。 这里提出一种名为 D-Stream 的新式流数据处理模型来克服上述问题。 与管理长时间存在的操作不同，D-Streams 结构将各运算流化成为一系列短时间间隔的无状态、确定性的批计算。 例如，我们可以将每秒钟(或每 100 毫秒)所接受的数据按照较短的时间间隔来分段，然后对每一段数据进行 MapReduce 操作来实现计数。 同样的，可将各间隔求出的新的计数加到旧的结果上而实现滚动计数。 通过以这样的方式构建计算，D-Streams 可以确保： 对于给定输入数据，每个时间间隔内的状态完全确定，而不需要同步协议； 当前状态和旧数据的依赖关系细粒度可见。 这样的设计能提供如批处理系统那样的强大的恢复机制，比复制和 upstream backup 方案更好。 实现 D-Stream 模型存在两方面的挑战。首先是要降低延时（间隔粒度）。 传统批处理，如 Hadoop，在这方面有明显缺陷，因为它们任务间使用复制、磁盘储存的方式保存状态。 相反，我们建立在第二章提到的 RDD 数据结构上，可以在内存中保存数据，并且使用操作的 lineage 进行恢复，从而避免了复制。 通过 RDD，我们证明可以达到低于秒级的端对端延迟。 相信这足以满足许多实际大数据应用的需要，一般来说这些应用的时间尺度(例如：社会媒体的倾向)要高得多。 第二个挑战是从故障和慢任务中快速恢复。 这里我们通过 D-Streams 的确定性提供一种新的恢复机制，这种机制在以往的流系统中均未使用过。 丢失节点状态的并行恢复，当某个节点失效时，集群中的各个节点都分担并计算出丢失节点 RDD 的一部分， 从而使得恢复速度远快于 upstream backup，且没有复制开销。 由于需要复杂的状态同步协议，即使是简单的复制操作(例如，Flux)，在连续处理系统中并行恢复也难以实现，但这对完全确定性的 D-stream 模型却变得简单。 与前一条类似，D-Stream 可通过推测性执行(speculative execution)来从慢任务中恢复，而之前的系统均不处理该类任务。 基于 Spark，我们已经在 Spark Streaming 中实现了 D-Streams。 这个系统在 100 个节点上能够处理超过 6000 万记录/秒，延迟在亚秒级，并且可以亚秒时间内从故障和慢任务中恢复。 Spark Streaming 的单节点吞吐量可与商用流数据库相当，但同时提供百级节点上线性扩展的能力。 最后，因为 D-Streams 使用与批任务相同的处理模型和数据结构（RDD），该流处理模型能实现流查询和交互式计算以及批计算无缝结合。 这是一个明显的优势。在 Spark Streaming 中，我们利用这个特性让用户使用 Spark 在流上进行即时(Ad-hoc)查询， 或把流和已计算出的历史数据连接成一个 RDD。 在实际使用中这种特性很有价值，它使得用户通过单一 API 来整合以前彼此不同的计算。 下文将阐述 D-Stream 是如何被用来桥接在线处理和离线处理处理的。 4.2 目标与背景 许多重要的应用需要对实时到达的大规模数据流进行处理。 我们的工作目标是应用需要在几十到数百台机器上执行，并且可以容忍几秒钟的延迟。 一些示例如下： 网站活动的统计数据：Facebook 建立了一个分布式聚合系统 Puma，来让广告者统计用户在 10-30 秒内点击他们网页的次数和处理时间。 集群监控：数据中心运营商往往使用由数百个节点组成的如 Flume 这样的系统，来对程序日志进行收集和挖掘以发现问题。 垃圾邮件检测：社交网络如 Twitter 可能希望利用统计学习算法来实时识别新的垃圾邮件。 对这些应用，我们认为，D-Streams 的 0.5-2 秒的延迟是足够的，因为该延迟远高于所监控的趋势的时间响应需求。 我们特意不针对那些延迟要求低于几百毫秒的应用程序，如高频交易。 4.2.1 目标 为使得这类应用可大规模运行，理想的系统设计需满足如下四个目标： 成百上千的可伸缩节点数目。 基本计算之外的开销最小–例如，不希望付出 2 倍的备份开销。 二级延迟。 从系统故障和慢节点恢复所带来的二级恢复。 据我们所知，之前的系统无法满足这些目标： 基于复制的系统开销很大，而基于 upstream backup 的系统则需数十秒来恢复丢失的状态信息，另外两者均不处理慢节点问题。 4.2.2 以往的处理模型 a (左图)：连续操作处理模型。每个节点连续地接收数据、更新内部状态并且发出新的记录。 容错一般来说是通过复制数据来实现的，用类似于 Flux 或 DPC 的同步协议来确保副本数据在每个节点看来都是相同的顺序 (例如, 当它们有多个父节点时)。 b (右图)：D-Stream 处理模型。 在每个时间间隔，到达的记录被可靠的存储在集群中，形成一个不可变的分区的数据集。 之后，这个数据集通过确定性的并行操作，计算其他表示程序输出或状态的分布式数据集。 这些会作为下一个间隔里的输入。 一个系列里的各个数据集构成一个 D-Stream。 虽然人们已经对分布式流处理进行了广泛的研究，但大部分现有系统均使用连续操作处理模型。 在该模型下，流计算被分隔为由多个有状态的算子(运算)的集合，而各算子以新到的记录为输入来更新自身状态(比如一个统计某个时间段内页面浏览次数的表)， 从而完成对其的计算，并发出新的记录来作为回应，如图 4.1(a)。 尽管连续处理最小化了延迟，但是操作的状态化的特征和由于网络传输记录导致的不确定性，很难有效提供容错机制。 具体来说，恢复的最大挑战在于操作状态在丢失节点或慢节点上的重建。 之前的系统使用两种方案中的一种：复制或 upstream backup。 这并不能在恢复开销和恢复时间之间进行良好平衡。 在复制模式下，这种模型是数据库系统中常用的模型，处理流程会有一个备份，而输入数据会都发给它们。 然而，只是对节点做备份并不够，系统还需如 Flux 或者 Borealis’s DPC 那样运行一个同步协议，来保证每个操作(含备份的)会以相同的顺序来对待上游发来的消息。 比如说：一个输出联合（union）两个父运算流的操作需要确保父运算流顺序相同，才能得出相同的输出流，所以操作与其拷贝之间需要协调。 因此，复制方案虽然可以很快恢复，但是耗费大量资源。 upstream backup 模式下：每个节点在检查点时保存其所发出数据的副本。 当一个节点失败之后，备用节点马上接管，父运算流会重新发送信息给备用节点来重建。 这种方式需要花费大量恢复时间，因为通过运行一系列带状态的操作的代码来重新计算出丢失的状态只能在同一个节点上进行。 TimeStream 和 MapReduce Online 使用的是这个模型。 主流的消息队列系统，比如 Storm，也是使用的这种模式，而且通常只保证“至少一次”发送消息，这依赖用户代码来实现处理状态恢复。 更重要的是，复制和 upstream backup 模式都不能应对慢任务问题。 如果在复制模式下，如果一个节点运算很慢，为了确保复制能够保持同序，整个系统都会很慢。 在 upstream backup 模式下，处理慢任务的唯一方法就是标记为失败，这就需要经历前面所提到的缓慢的状态恢复进程，对于偶尔发生的问题，这种规模是太过笨重。 因此，传统的流方法在小规模环境中工作良好，但是在大规模集群中会面对大量问题。 4.3 D-Streams D-Streams 通过将计算构造为一组短的，无状态的，确定性的任务代替连续的，有状态的操作来避免传统流处理的问题。 然后，它们将状态存放在内存中，再通过容错的数据结构(RDDs)可以重新计算出该状态。 将计算分解成短任务并暴露其细粒度的依赖性，并允许像并行恢复和推测(speculate)这样强大的恢复技术。 除了容错，D-Stream 模型提供了其他好处，比如与批处理相结合。 4.3.1 计算模型 我们把流计算看作在一小段时间周期上进行的一系列确定性的批处理计算。 对于每个时间周期收到的数据，通过集群可靠地存储成一个输入数据集。 一旦时间周期完成时，该数据集便通过确定的并行操作来处理， 例如通过 map,reduce 和 groupBy 等操作来产生新的数据集，该数据集可以表示程序输出或中间状态。 对于前面的情况，结果可以以分布式的方式推送到一个外部系统。 在后面的例子中，中间状态可以通过弹性分布式数据集（RDDs）的高效抽象的存储方式保存，这样可以避免为了恢复使用 lineage 而产生冗余。 该状态数据集可以随同下一批输入数据一起处理，以产生一个新的数据集来更新中间状态。 图 4.1(b) 显示了我们的模型。 基于这个模型，我们用 Spark Streaming 实现了这个系统。 我们对每一批数据使用 Spark 作为批处理引擎。 图 4.2 大致描绘了 Spark Streaming 上下文中的计算模型，后面我们会作更详细的解释。 在我们的 API 中，用户通过操纵对象来定义流程，我们称之为 D-Stream。 D-Stream 是一系列具有不可变性的分区数据集（RDDs），我们可以通过确定的转换对它们进行操作。 这些转换生成新的 D-Streams，并且可以通过 RDDs 的形式创建中间状态。 我们通过 Spark Steaming 流式计算运行的程序实现了这个想法。 通过URL计算访问事件。类似于 LINQ，Spark Streaming 通过 Scala 语言的可编程的API 使用 D-Streams。 我们的程序代码如下： 123pageViews = readStream(&quot;http://...&quot;, &quot;1s&quot;)ones = pageViews.map(event =&gt; (event.url, 1))counts = ones.runningReduce((a, b) =&gt; a + b) 这段代码创建了一个名叫 pageViews 的 D-Stream，通过从 HTTP 读取事件流将他们用 1 秒的时间周期来分组页面访问。 然后将这个事件流通过建立（URL，1）这样的键值的变化对来形成新的 D-Stream，再通过一个状态相关的 runningReduce 转换来对他们进行计数操作。 传入 map 和 runningReduce 的参数是 Scala 的函数字面量。 为了执行这个程序，Spark Streaming 接收数据流，然后将其划分成秒级的批处理任务，并将其存储在 Spark 的 RDDs 内存中(见图 4.2)。 同时，他也会调用 RDD 的转换操作如同 map 和 reduce 来对 RDD 进行处理。 为了执行这些转换，首先 Spark 会启动 map 任务来对这些事件进行处理，同时生成 (url,1) 这样的键值对。 然后，会对 map 的结果和之前 reduce 得到的结果启动 reduce 任务，最后将结果存储在 RDD 里。 这些任务会产生一个更新计数的新 RDD。 程序中的每个 D-Stream 因此变成了一系列的 RDD。 最后，为了错误恢复和慢任务，D-Streams 和 RDDs 要跟踪他们的 lineage,，即用于生成他们的确定性的操作图。 在每一个分布的数据集中，Spark 会在分区的层面跟踪这些信息，如图 4.3 所示。 如果一个节点任务失败，通过重新运行从集群中可靠存储的输入数据构建的任务来重新计算相应的RDD分区。 这个系统还周期性的检查 RDD 的状态(例如通过异步的方式对每十个RDD进行复制)以避免过度重算。 但是不需要对所有数据都进行那样的操作，因为恢复总是很快的：丢失的分区可以在不同的节点上并行计算。 类似的，当一个节点运行缓慢时，因为总会产生同样的结果，我们可以在其他节点上对任务的副本进行推测执行。 我们发现在 D-Streams 中并行恢复比在上游备份具有更高的可用性，即使每个节点上执行了多个操作。 D-Stream 从操作分区和时间两个方面展现了并行化： 如同每个节点执行多个任务的批处理系统，每个节点在每个转换操作的时间片会产生多个 RDD 分区(例如 100 核的集群产生 1000 个 RDD 分区)。 当节点出现故障时，我们可以在其它节点以并行方式重新计算其分区。 lineage 图通常可以使数据从不同的时间片并行地进行重建。 如图 4.3 所示，如果一个节点出错，我们可能丢失一些时间片的 map 的输出，不同时间片的 map 任务可以并行的重新执行。 假设需要执行一系列的操作，这在一个连续处理的系统中是无法实现那样的功能的。 依赖这些特性，当每 30 秒建立一次检查点时(参见 4.6.2 节)，D-Streams 仅用 1-2 秒就可以在数百个核心上并行恢复。 我们将在本节的剩余部分更详细地介绍 D-Streams 的可靠性和编程接口。 并在第 4.4 节中讨论如何实现。 4.3.2 时序方面的考虑 D-Streams 将每个记录按其到达系统的时间存入输入数据集。 这样做可以确保系统总是可以及时开始一个新的批次，尤其是在那些记录从相同的地方里产生的应用中，例如，同一数据中心的服务产生的数据。 这样分割处理的方式，在语义上不会产生错误。 而在其他应用中，开发者可能希望基于事件发生的外部时间戳将记录分组，例如，基于用户点击某一个链接的时间。 这样一来记录的到达可能是无序的。 D-Streams 提供了两种方法来处理这种情况： 系统可以在开始每个批次之前等待一个有限的“空闲时间” 用户程序可以在应用级别上对晚到的记录进行纠正。 例如，假设一个应用想要在 t 时刻 与 t+1 时刻间对某广告的点击数进行统计。 一旦 t+1 时刻过去，应用就可以使用以一秒为周期的 D-Streams，对 t 时刻与 t+1 时刻间接收的点击数进行统计。 然后，在后面的时间周期里系统可以进一步收集 t 与 t+1 时刻间的其他带有外部时间戳的事件并计算更新统计结果。 例如，它可能将基于从 t 到 t+5 时间段内收到的记录，在 t+5 时刻产生一个关于 [t, t + 1) 时间区间的新的计数。 这种计算可以应用一种高效的增量 reduce 操作，即在 t+1 时刻的老计数基础上加上对之后新记录的计数，以避免重复计算。 此方法类似于处理和顺序无关的程序。 这些时序性的考虑是流式处理系统所必须面对的，因为任何系统都会有外部延时。 数据库领域对此已经进行了详细的研究。 一般来说，这些技术都可以通过D-Streams 来实现，即将计算”离散化“到小批次数据的计算(相同批次的处理逻辑相同)。 因此我们将不在本文中对这些方法做进一步的探讨。 4.3.3 D-Stream API 在 Spark Streaming 中，用户使用函数 API 来注册一个或多个数据流。 程序可以将输入数据流定义为从外部系统中读取数据，该系统通过从对节点端口监听或周期性地从一个存储系统(例如,HDFS)加载来获取数据。 它可以适用于两种类型对这些数据流的操作： 转换操作：从一个或多个父数据流创建一个新的 D-Stream。 这些操作可能是无状态的，在每个时间周期内对 RDD 分别进行处理，或跨越不同周期来生成状态。 输出操作：使得程序将数据写入外部系统。 例如，save 操作将 D-Stream 中的每一个 RDD 输出到数据库。 D-Streams 支持在典型批处理框架中所拥有的无状态的转换操作，包括 Map，Reduce，GroupBy 和 Join。 此外，为了支持跨越多个周期的计算，D-Streams 提供了多个有状态的转换操作，这些操作是基于类似于滑动窗口的数据流处理技术基础进行设计的。 这些操作包括： 窗口：窗口操作将每一个过去的时间周期的滑动窗口里的所有记录组合到一个 RDD。 增量式聚合：对于常用的聚合计算的用例，就像在一个滑动窗口上进行 count 或 max 操作，D-Streams 有增量 reduceByWindow 操作的几个变种操作。 最简单的一个是仅仅用一个关联的合并函数来对值进行合并。 状态跟踪：通常，应用程序为了对表示状态变化的事件流进行响应，需要对各类对象进行状态跟踪。 例如，一个监控在线视频传输的程序可能会希望对活跃连接的数量进行追踪，一个连接表示从系统收到一个新客户端的“join”事件和当它收到“exit”的时间。 然后，它能够解决这样的问题：“有多少个传输比特率大于 X 的会话”。 D-Streams 提供了一个转换数据流的操作 updateStateByKey。 基于三个参数记录(Key, Event)到(Key, State)记录的数据流中。 一个从第一个事件中为新的键值创建一个 State 值的 initialize 函数。 一个从给定的一个旧状态和一个事件里为它的键值返回一个新的 State 值的 update 函数。 一个用于删除旧的状态的 timeout 函数。 例如，用户可以从一个(ClientID, Event) 对的数据流中计算活跃的会话数，如下所示： 12345sessions = events.track((key, ev) =&gt;1， // initialize function(key, st, ev) =&gt; (ev == Exit ? null:1)， // update 函数&quot;30s&quot;) // 超时counts = sessions.count() // 一个整型的数据流 这段代码给每一个活跃客户的状态设为 1，并且在它退出时通过从 update 中返回 null 来将它删掉。 因此，会话对于每一个活跃客户含有一个(ClientID, 1) 元素，同时 counts 用来计算会话的总数。 最后，用户调用输出操作符 将 Spark Streaming 的结果发送到外部系统(例如，展示在 dashboard 上)。 我们提供了两个这样的操作：save 操作， 将 D-Stream 中的每一个 RDD 写入到一个存储系统(例如, HDFS 或 HBase)， 和 foreachRDD 在每一个 RDD 上执行一段用户代码段(任意的 Spark 代码)。 4.3.4 一致性语义 D-Streams 的一个好处是，它们具有真正的一致性语义。 跨节点的状态一致性在以记录为基础的流式系统中是一个迫切的问题。 例如，有这样一个系统，按国家来计算其页面访问量，每个页面的浏览事件被发送给负责汇总其国家统计数据的不同节点上。 如果负责英国的节点落后于负责法国的节点，例如，由于加载的原因，那么它们的状态快照将会出现不一致： 英国的计数与法国的相比将会反映流的一个较老的状态，而且计数值通常会较低，从而混淆有关事件的推论。 有些系统，像 Borealis , 其同步节点会避免这个问题。 而其他的系统，像 Storm，却是忽略它。 在 D-Streams 中，一致性语义是非常明确的，因为时间会自然离散为时间周期，每个时间周期的输出 RDDs 反映当前时间周期内以及以前的时间周期收到的所有输入。 这是真实的，无论输出 RDDs 和状态 RDDs 是否分布在集群中，用户无需担心是否有节点在执行上落后。 具体来讲，由于计算的确定性和不同时间周期的数据集的单独命名， 每个输出 RDD 的计算效果相当于以前的时间周期上所有批量作业已经步调一致地运行，并且没有落后的和失败的。 因此，D-Streams 提供一致并“恰好一次”的处理。 4.3.5 批处理与交互式处理的统一 因为 D-Streams 遵循与比处理系统相同的处理模型，数据结构(RDDs)，和类似批处理系统的容错机制，因此两者可以无缝结合。 Spark Streaming 提供了多种强大的功能来统一流式计算和批处理计算。 首先，D-Streams 能够使用标准的 Spark 作业与静态的 RDDs 结合进行计算。 例如，我们可以将消息事件流和预先计算的垃圾过滤器进行连接操作，或者与历史数据进行比较。 其次，用户可以使用“批处理”的模式对历史数据运行一个 D-Stream 程序。 这可以非常方便为历史数据计算一个新的数据流报告。 第三，附加一个 Scala 控制台到 Spark Streaming 程序里，用户可以在 D-Streams 上进行交互式 询，并且在 RDDs 上运行任意的 Spark 操作。 与流式系统和批处理系统拥有各自单独 API 的系统相比，共享同一份代码库可以节省许多开发时间。 同时在流系统中交互式查询状态的能力则更加吸引人；它使得调试一个运行程序，或者在聚类操作的流式作业中查询未定义状态变得更加容易。 如果没有这种特性，用户通常需要等待数十分钟来将数据导入到集群里，即使流式系统处理节点的内存具有所有相关的状态信息。 4.3.6 总结 方面 D-Streams 持续处理系统 延时 0.5-2s 1-100ms 除非对记录进行批处理以保持一致性 一致性 记录以到达的时间间隔进行原子处理 有些系统可以等待短暂的时间再继续执行同步操作 记录延迟 宽松的时间延迟或者应用程序级别的校正 宽松的时间延迟但是无序的进行处理 故障恢复 快速并行恢复 在单节点上复制或串行恢复 慢任务恢复 推测执行的可能 通常情况下没有处理 批处理混合操作 通过简单统一的 RDD API 在某些数据库中存在，但在消息队列系统中不存在 D-Streams 将任务划分成小的且确定性的批量操作。 这会导致最小的延迟时间变长，但是可以让系统采取更高效的可恢复技术。 这导致了延迟时间由过去的毫秒级变为 D-Streams 里面的秒级。 4.4 系统架构 我们已经在 Spark Streaming 上实现了 D-Stream，它是基于 Spark 处理引擎的一个修改版本。 Spark Streaming 由三部分组成，如图 4.6 所示 master 跟踪 D-Stream lineage，并调度任务来计算新的 RDD 分区。 工作节点接收数据，保存输入分区和已计算的 RDD，并执行任务。 客户端库将数据发送到系统中 如图中所示，Spark Streaming 重用了 Spark 的许多组件，但仍然需要修改和添加多个组件来支持流处理。 这些变化将在第 4.4.2 节讨论。 从架构角度来看，Spark Streaming 和传统的流系统之间区别在于，Spark Streaming 将计算过程分解为小的、无状态的、确定的任务。 每个任务都可以在集群中的任何节点或同时在多个节点运行。 在传统系统的固定拓扑结构中，将部分计算过程转移到另一台机器是一个很大的动作。 Spark Streaming 的做法，可以非常直接地在集群上进行负载均衡，应对故障或启动慢节点恢复。 同理也能用于批处理系统——如 MapReduce。 然而，由于 RDD 运行于内存中，Spark Streaming 的任务执行时间会短得多，一般只有 50-200 毫秒。 不同于以前系统将状态存储在长时间运行的处理过程中，Spark Streaming 中的所有状态都以容错数据结构(RDD)来保存。 由于 RDD 分区被确定性地计算出来，它可以驻留在任何节点上，甚至可以在多个节点上进行计算。 这套系统试图最大限度地提高数据局部性，同时这种底层的灵活性使得推测执行和并行恢复成为可能。 这些优势在批处理平台(Spark)上运行时可以很自然地获得。 但依然需要进行显著的修改来支持流处理。 在介绍这些修改之前，先讨论任务执行的更多细节。 4.4.1 应用程序执行 Spark Streaming 的应用从一个或多个输入流开始执行。 系统加载数据流的方式，要么是通过直接从客户端接收记录数据，要么是通过周期性的从外部存储系统中加载数据，如 HDFS，外部的存储系统也可以被日志收集系统所代替。 在前一种方式下，由于 D-Streams 需要输入的数据被可靠地进行存储来重新计算结果， 因此我们需要确保新的数据在向客户端程序发送确认之前，在两个工作节点间复制数据。 如果一个工作节点发生故障，客户端程序向另一个工作节点发送未经确认的数据。 所有的数据在每一个工作节点上被一个块存储进行管理，同时利用主服务器上的跟踪器来让各个节点找到数据块的位置。 由于我们的输入数据块和我们从数据块计算得到的 RDD 的分区是不可变的，因此对块存储的跟踪是相对简单的： 每一个数据块只是简单的给定一个唯一 ID，并所有拥有这个 ID 的节点都能够对其进行操作(例如,多个节点同时计算它)。 块存储将新的数据块存储在内存中，但会以 LRU 策略将这些数据块丢弃，这在后面会进行描述。 为了确定何时开始一个新的时间周期，我们假设各个节点通过 NTP 进行了时钟同步，并且在每一个周期结束时每一个节点都会向主服务器报告它所接收到的数据块 ID。 主服务器之后会启动任务来计算这个周期内的输出 RDDs，不需要其他任何同步。 和其他的批处理调度器一样，一旦完成上个周期任务，它就简单地开始每个后续任务。 Spark Streaming 依赖于每一个时间间隔内 Spark 现有的批处理调度器，并加入了像 DryadLINQ 系统中的大量优化： 它对一个单独任务中的多个操作进行了管道式执行，如一个 map 操作后紧跟着另一个 map 操作。 它根据数据的本地性对各个任务进行调度。 它对 RDD 的各个划分进行了控制，以避免在网络中数据的 shuffle。 例如，在一个 reduceByWindow 的操作中，每一个周期内的任务需要从当前的周期内“增加”新的部分结果(例如，每一个页面的点击数)， 和“删除”多个周期以前的结果。 调度器使用相同的方式对不同周期内的状态 RDD 进行切分，以使在同一个节点的每一个 key 的数据(例如,一个页面) 在各时间分片间保持一致。 更多的细节见 2.5.1 节。 4.4.2 流处理优化 尽管 Spark Streaming 建立在 Spark 之上，我们仍然必须优化这个批处理引擎以使其支持流处理。 这些优化包括以下几个方面： 网络通信：我们重写了 Spark 的数据层，通过使用异步 I/O 使得带有远程输入的任务，比如说 reduce 任务，能够更快地获取它们。 时间间隔流水线化：因为每一个时间间隔内的任务都可能没有充分地使用集群的资源， 所以，我们修改了 Spark 的调度器，使它允许在当前的时间间隔还没有结束的时候调用下一个时间间隔的任务。 任务调度：我们对 Spark 的任务调度器做了大量的优化，比如说手工调整控制消息的大小，使得每隔几百毫秒就可以启动上百个任务的并行作业。 存储层：为了支持 RDDs 的异步检查点和性能提升，我们重写了 Spark 的存储层。 因为 RDDs 是不可变的，所以可以在不阻塞计算和减慢作业的情况下通过网络对 RDDs 设置检查点。 在可能的情况下，新的数据层还会使用零拷贝特性。 lineage 截断：因为在 D-Streams 中 RDDs 之间的 lineage 可以无限增长， 我们修改了调度器使之在一个 RDD 被设置检查点之后删除自己的 lineage，修改之后 RDDs 之间的 lineage 不能任意生长。 类似地，对于 Spark 中的其他无限增长的数据结构来说，将会定期调用一个清理进程来清理它们。 Master 的恢复：因为流应用需要不间断运行，我们给 Spark 加入对 master 状态恢复的支持(见 4.5.3 节)。 4.4.3 内存管理 在我们当前的 Spark Streaming 实现中，每个结点的块存储管理 RDD 的分片是以 LRU(最近最少使用)的方式，如果内存不够会依 LRU 算法将数据调换到磁盘。 另外，用户可以设置最大的超时时间，当达到这个时间之后系统会直接将旧的数据块丢弃而不进行磁盘 I/O 操作(这个超时时间必须大于检查点间隔的时间)。 我们发现在很多应用中，Spark Streaming 需要的内存并不是很多，这是因为一个计算中的状态通常比输入数据少很多(很多应用是计算聚合统计)， 并且任何可靠的流式处理系统都需要像我们这样通过网络来复制数据到多个结点。 但是，我们还是会计划探索优化内存使用的方式。 4.5 故障和慢节点恢复 D-Streams 的确定性使得可以使用两种有效却不适合常规流式系统的恢复技术来恢复工作 节点状态：并行恢复和推测执行。此外，它也简化了主节点的恢复。 4.5.1 并行恢复 当一个节点失败，D-Streams 允许节点上 RDD 分片的状态以及运行中的所有任务能够在其它节点并行地重新计算。 通过异步地复制 RDD 状态到其它的工作节点，系统可以周期性地设置 RDDs 状态的检查点。 例如，在运行时统计页面浏览数的程序中，系统可能对于该计算每分钟选择一个检查点。 然后，如果一个节点失败了，系统会检查所有丢失的 RDD 分片，然后启动一个任务从上次的检查点开始重新计算。 多个任务可以同时启动去计算不同的 RDD 分片，使得整个集群参与恢复。 如 4.3 节所述，D-Stream 在每个时间片中并行地计算 RDDs 的分区以及并行处理每个时间片中相互独立的操作(例如开始的map操作)， 因为可以从 lineage 中细粒度地获得依赖关系。 4.5.2 减缓慢节点 除了节点故障，在大型集群中另一个值得关注的问题是运行较慢的节点。 幸运的是, D-Streams 同样也可以让我们像批处理系统那样减少较慢节点的影响，这是通过推测性(speculative)地运行较慢任务的备份副本实现的。 这种推测执行在连续的处理系统中可能很难实现，因为它需要启动一个结点的新副本，填充新副本的状态，并追赶上较慢的副本。 事实上, 流式处理中的复制算法, 比如 Flux 和 DPC，主要在研究两个副本之间的同步。 在我们的实现中，我们使用了一个简单的阈值来检测较慢的节点：如果一个任务的运行时长比它所处的工作阶段中的平均值高 1.4 倍以上，那么我们标记它为慢节点。 我们将来也可能会采用更精细的算法，但是我们看到目前的方法仍然工作的很好，它能够在 1 秒内从较慢节点中恢复过来。 4.5.3 Master 恢复 持续不断运行 Spark Streaming 的一个最终要求是能够容忍 Spark master 的故障。 我们通过两个步骤来做到这些，第一步是当开始每个时序时可靠地记录计算的状态，第二步是当旧的 master 失败时， 让计算节点连接到一个新的 master 并且报告他们的 RDD 分区。 D-Streams 简化恢复的一个关键方面是如果一个给定的 RDD 被计算两次是没有问题的。 因为操作是确定的，这一结果与从故障中进行恢复类似。 因为任务可以重新计算，这意味着当 master 重新连接时丢掉一些运行中的任务也是可以的。 我们目前的实现方式是将 D-Stream 元数据存储在 HDFS 中， 用户 D-Streams 的图以及表明用户代码的 Scala 的函数对象 最后的检查点的时间 自检查点开始的 RDD 的 ID 号，其中检查点通过在每个时序进行重命名(原子操作)来更新 HDFS 文件。 恢复后，新 master 会读取这个文件找到它断开的地方，并重新连接到计算节点，以便确定哪些 RDD 分区是在内存中。 然后再继续处理每一个漏掉的时序。 虽然我们还没有优化恢复处理，但它是相当快了，100 个节点的集群可以在 12 秒内恢复。 4.6 评估 略 4.7 讨论 虽然我们给出了 D-Streams 的一个基本实现，但是未来还有几个方面需要完善。 表现力：一般来说，既然 D-Streams 主要是一个执行策略，通过简单地把算法的执行划分成批处理步骤然后在这些步骤中间发送状态，就应该能够运行大多数的流数据算法。 在 D-Streams 上调用流 SQL 和复杂事件处理模型将会是一件很有意思的事情。 设置批处理间隔：给定任何一个应用，设置一个合适的批次间隔是非常重要的，因为这个批处理间隔直接决定了端对端的延迟与整个流负载吞吐量之间的权衡。 目前来说，开发人员必须自己探索这个权衡并且手动确定批处理间隔。 未来有可能实现系统自动调整。 内存使用：每处理完一批数据，我们的状态流处理模型就会生成一个新的 RDD 来存放每个运算的状态。 在我们目前的实现中，相比于使用可变状态的连续运算，它会使用更多的内存。 为了让系统能够执行基于 lineage 的错误恢复，必须要存储状态 RDDs 的不同版本。 但是，可以通过存储不同状态 RDDs 之间的变化量来达到减少内存使用的目的。 检查点和容错策略：因为检查点的成本很高，所以选择一个合适的频率对每一个 D-Stream 自动设置检查点是很有价值的。 另外，除了检查点之外，D-Streams 还允许使用大量的其他容错策略，比如说计算的部分复制，在部分复制中，任务的一个子集被复制 (例如， 我们在 4.6.2 节中复制的 reduce 任务)。自 动应用这些策略也是一件有趣的事。 近似的结果：除了重新计算丢失的工作，另外一种处理失败的方式就是返回近似的部分结果。 通过在父节点全部结束之前启动一个任务，并且提供用来推断哪些父节点丢失了的 lineage 数据，D-Streams 提供了一个计算部分结果的机会。 4.8 相关工作 略 4.9 总结 略 5 RDD 的通用性 5.1 简介 首先，从表述的观点看，RDDs 可以模拟任何分布式系统，并且在大多数情况下这样做都是高效的，除非系统对网络延迟非常敏感。 特别是，增加了数据共享的 MapReduce 使得这个模拟更高效。 第二，从系统的角度来看，在集群环境中 RDDs 能给应用程序对常见的资源瓶颈加以控制(特别是网络和存储 I/O)， 这使得应用程序能够表达那些特定系统所具有的资源优化，并因此达到相似的性能。 最后，RDDs 通用性的探索也决定了模型的一些局限性，即它可能不能有效地模仿其他分布式系统并导致一些扩展性方面的问题。 5.2 观点描述 为了从理论角度描述 RDDs，我们首先拿 RDDs 和从其派生和借鉴所得的 MapReduce 模型进行比较。 5.2.1 MapReduce 所能涵盖的计算范围 MapReduce 提供了 Map 操作用来执行本地计算和 Reduce 操作用来所有节点间相互通信。 这样，通过将计算拆分成多个时间步长的方式，任何分布式系统都能够被模拟(或许有点低效率)， 通过运行 Map 任务来执行每个时间步长上的本地计算任务，并在每个时间步长的最后进行消息的打包和交换。 一系列的 MapReduce 步骤足以获得整个结果。 注：MapReduce 为本地计算和 all-to-all 通信提供原语。 (a) 通过将这些步骤链接在一起，我们可以模拟任何分布式系统。 (b) 这种模拟的主要成本将是轮次的延迟和跨步骤传递状态的开销。 两方面因素导致了这种模拟的效率低下。 第一，就如我们在论文的其他部分讨论的那样，MapReduce 在时间步长间的 共享数据的方式是低效的，因为这种共享是基于可复制的外部存储系统。 因此，由于每个时间步长都要输出自己的状态，我们模拟的分布式系统可能变得比较缓慢。 第二，MapReduce 步骤的延迟决定了我们的模拟如何匹配一个真实的网络，并且大多数 MapReduce 的实现是为耗时几分钟到几小时的批量环境设计的。 RDD 架构和 Spark 系统解决了这两方面的限制。在数据共享方面，RDDs 的架构是通过避免复制的方式，使得数据快速共享。 并且能够比较贴切地模拟跨越时间的 “内存数据共享”，这是由多个长驻进程组成的分布式系统所实现的。 在延迟方面上，Spark 展示了在 100 多个节点组成的商业集群中执行 MapReduce 计算任务，有 100ms 的延迟——没有固有的 MapReduce 模型能够避免这种情况。 然而，一些应用程序可能需要更细粒度的时间步长和通信，这样 100ms 的延迟已经足够实现许多数据密集型的计算，而在通信密集的情况下，大量的计算可以被批量执行。 5.2.2 lineage 和故障恢复 上面基于 RDD 模拟的一个有趣特性是它也提供了故障容错。 特别的，每个步骤的 RDD 计算仅仅比前面的步骤多一个常数大小的继承结构，这意味着存储 lineage 和执行故障恢复的代价很小。 每个步骤的本地消息处理在“map”函数中循环进行(它的旧状态作为输入，新状态作为输出)，然后在时间步长间使用“reduce”函数来进行消息的交换。 只要将每个过程中的程序计数存储在它的状态里，这些 map 和 reduce 函数在每个时间步骤中是一样的： 它们仅仅读取程序的计数，接受消息和状态，然后模拟执行过程。 因此它们能够在常量空间内编码。由于在状态中增加了程序计数，这可能引起一些开销，这通常只是状态的一小部分，并且这些状态只是在节点本地共享。 默认情况下，上面模拟过程的 lineage 可能使得状态恢复代价很高，这是因为每一步增加了一个新的所有节点相互间的 shuffle 依赖。 但是，如果应用程序将计算语义表达的更精确(比如 ：说明某些步骤仅仅产生窄依赖)，或者将每个节点的工作切分到多个任务，我们就可以在集群间来并行恢复。 基于 RDD 模拟分布式系统的最后一个问题是在本地维护多个版本状态的代价，这些状态会被转换操作使用，以及为了便于故障恢复而维护对外发送消息的拷贝。 这个代价不小，但是在很多应用中，我们可以通过一段时间执行异步的状态检查点 (比如, 如果检查点的可用带宽比内存带宽低 10 倍, 我们可以在每 10 个步骤执行检查点)或者通过保存多个版本的差异(如 3.2.3 节所述)来限制它。 只要每台机器上的“快速”存储足够储存对外发送的消息以及一些版本的状态，我们就可以达到原来系统的性能。 5.2.3 与 BSP 的比较 作为关于 MapReduce 与 RDDs 的通用性的第二个例子，我们注意到上述“本地计算和所有结点相互间通讯”模式与 Valian 的批量同步并行模型(BSP)非常吻合。 BSP 是一个桥接模型，旨在捕捉真实的硬件上简单却最显着的特性(即通信具有延迟且同步是昂贵的)并对其进行简单的数据分析。 因此，它不仅被直接用于设计一些并行算法，而且其成本(即通信的步骤数，每一步中的本地计算量以及每一步骤中各处理器之间通信的数据量)也是大多数并行 应用中用来优化的自然因素。 因此，我们可以预期与 BSP 吻合的算法都可以用 RDDs 进行有效的评估。 请注意，这个 RDDs 的仿真参数因此也可应用于基于 BSP 的分布式运行时，如 Pregel。 RDD 相比与 Pregel 增加了两个好处。 首先，Google 论文中描述的 Pregel 只支持‘检查点回滚’的系统错误恢复机制。 随着系统规模的扩大，这使得系统扩展的效率降低并且节点失效会变得越来越频繁。 该论文中确实介绍了一种还在开发中的‘限制性恢复’模式，它记录下传出的信息并且并行地恢复丢失的系统状态。 这与 RDDs 的并行恢复机制类似。第二，因为 RDDs 有一个基于遍历器的接口，它们可以更有效地对不同类库编写的计算流水线化，这对于编写程序是非常有用的。 更一般地，从编程接口的角度来看，我们发现 RDD 允许用户使用更高层次的抽象(例如，将状态分割为多个分区数据集或允许用户建立可窄可宽的的依赖模式而不需要在 每一步都进行所有结点相互间的通信)，同时还提供一个简单通用的接口使数据可以按上述讨论进行共享。 5.3 系统角度 完全不同于仿真的方法来表征 RDD 的特性，我们可以采取一种系统方法：在大多数集群计算中资源的瓶颈是什么，能否用 RDD 来有效的解决这些问题？ 从这个角度来看，大多数集群应用最明显的瓶颈是通信和存储。 RDD 的分区和本地特性使得应用有足够的控制力来对这些资源进行优化，从而使得在许多应用中达到类似的性能。 5.3.1 瓶颈资源 虽然集群应用是多种多样的，但是它们都受到相同的底层硬件的限制。 目前的数据中心有一个非常不合理的存储层次结构，这将会因相同的原因限制大多数应用。 例如，现在一个典型的数据中心可能有以下硬件特性： 每个节点的本地内存大约有 50 GB/s 的内存带宽以及多个磁盘(通常在 Hadoop 集群中为 12-24 个)。 也就是说，假设有 20 个磁盘，每个磁盘带宽 100 MB/s，那么将意味着本地存储带宽约为 2 GB/s。 每个节点都有一个 10 Gbps (1.3 GB/s) 的网络输出带宽，大约比内存带宽小 40 倍，比它的磁盘总带宽小 2 倍。 20-40 台机器节点组成机架，机架间的带宽为 20-40 Gbps，这比机架内部的网络性能要低 10 倍。 鉴于这些特性，许多应用所关心的最重要的性能指标就是控制网络布局和通信。 幸运的是，RDDs 提供了这样的条件：其接口在运行时将计算调度在离数据最近的节点，就像 MapReduce 的 Map 任务 (其实在 2.4 章节的定义中，RRDs 有一个“优先位置”的 API)，并且 RDDs 还提供了数据分区和共存。 不像 MapReduce 中的数据共享，总是隐式地需要经过网络传输。 而 RRDs 是不会造成网络流量的，除非用户明确调用了一个跨节点操作，或是对数据集设置检查点。 从这个角度看，如果大部分的应用都有网络带宽限制，那么一个节点(例如,数据结构或 CPU 开销)的本地效率的影响将小于网络通信的效率。 以我们的经验，很多 Spark 应用都有带宽限制的，特别是如果数据可以放进内存中。 当数据无法放进内存中时，应用受 I/O 限制，并且数据本地性将是最重要的一个因素。 CPU 密集型应用通常更容易执行，(例如,许多应用在MapReduce 上也都做得很好)。 就像在上一章节的讨论中，RDDs 明显增加成本的地方就是网络延迟，但是 Spark 的工作表明，这种延迟对很多应用来说可能会足够小，甚至小到足够支持数据流。 5.3.2 容错的开销 最后从系统的角度要说明的一点是，由于其容错性，基于 RDD 的系统产生了一些额外的开销。 例如，在 Spark 中，每个 shuffle 操作中的“map”任务将他们的输出保存到本地文件系统中，所以之后 “reduce” 任务可以重复获取。 另外，Spark(就像原生的MapReduce)在 shuffle 阶段执行了一个 “barrier”，所以 “reduce” 任务不会启动，直到所有的map任务完成。 相比于直接从 map 任务以管道的方式直接推送到 reduce 任务，这简化了容错的复杂性。 尽管移除一些低效率之处会加快系统运行速度，并且我们也打算在以后的工作中这样做，但是即便如此 Spark 依旧性能突出。 最主要的原因是前一节中的一个说法：许多应用程序都受 I/O 所限制， 例如，通过网络传输大量数据，或从磁盘中读取数据，除此之外，如流水线技术仅增加了一个少量的改进。 例如，可以考虑从 map 任务直接推送数据到 reduce 任务，而不是等待所有的 map 任务执行完再调度 reduce 任务： 最理想情况下，如果 map 任务的 CPU 计算时间刚好与网络传输时间重叠，那么这将使速度加快 2 倍。 当然，这是一个非常有用的优化，但不像将 map 任务调度到数据所在节点或者避免中间状态的复制那么重要。 此外，如果运行过程中其他部分占主要开销 (例如,map 任务花费很长时间从磁盘中读取，或 shuffle 过程比计算时间慢很多)，那么效益就会降低。 最后我们注意到，即使故障并 不经常发生，Spark 和类 MapReduce 的设计将任务划分成细粒度的独立的任务会有其他的好处。 首先，它可以缓解慢节点(straggler)问题，这在传统的基于推送的数据流设计中显得异常复杂。 (类似于我们第 4 章中比较 D-streams 和连续操作)。 甚至在较小的集群上，慢节点问题也比故障更常见，尤其是在虚拟化环境。 其次，独立的任务模式有利于多用户管理：来自不同用户的多个应用程序可以动态共享资源，从而实现多用户的交互执行。 我们大部分并行编程模式的工作，已经使动态资源在集群用户之间进行共享，大型群集必然有很多用户，并且在这些集群中所有的应用都需要实现快速定位和数据本地化 基于这些原因，一个细粒度的任务设计可能会让大多数用户在多用户环境中有更好的性能体验，即使单一应用的性能还比较差。 鉴于这些容错成本和其他独立任务模型的优点，我们认为，集群系统设计者应该考虑容错性和弹性因素，即使仅仅是针对短期工作。 提供这些特性的系统将会更容易得扩展到大规模查询以及多用户环境。 5.4 限制与扩展 虽然先前的章节讲述了 RDD 能够有效模拟分布式系统，但它也有无法做到的情况。 现在，我们研究一些关键限制，并讨论几种可能绕过它们的模型扩展。 5.4.1 延迟 正如前面几章说明的那样，基于 RDD 分布式系统的仿真与实际系统之间差距的主要系能指标就是延迟。 因为 RDD 操作在整个集群中是确定且同步的，又由于启动每个&quot;时步(timestep)&quot;计算存在固有的延迟，所以大量的时步计算导致系统更慢了。 这类应用有两种设计方式，低延迟流式系统(如毫秒级请求)和细粒度时间片模拟(如，科学模型)。 而我们发现，在实践中 RDD 操作可以低至 100ms 的延迟，这对一些应用程序还不够。 从“人”的时间尺度来看，延迟已经足够低来跟踪事件(如 Web 点击率和社交媒体的趋势)，并且符合互联网大范围的延迟。 在仿真应用中，对抖动容忍的最新研究工作非常适用于 RDD。 这项研究工作在每台机器的本地网络区域上模拟多个时步。 在大多数模拟中，信息需要花费几个时间片在整个网络中转移，并需要更多的时间来与其他节点进行同步。 它被专门用来处理慢任务倾向(straggler-prone)的云环境。 5.4.2 通信模式 5.2.1 章节表明 MapReduce 和 RDD 可以仿真一个分布式系统。 当使用 reduce 操作时，系统的节点之间通过点对点通信，虽然这是一个常见的场景，但实际网络也有其他有效的原语，如广播或者网络内聚合。 一些数据密集型应用能够从这些中显著受益(如：在机器学习算法将当前模型广播出去，或者收集反馈结果)这些原语仅仅通过点对点的消息传输是非常难以效仿的， 因此在 RDDs 之上直接支持这些原语是有帮助的。 比如，spark 已经包含了一个有效的操作，“广播”，它通过 BitTorrent 来实现。 5.4.3 异步 RDD 操作，例如多对多的 reduce 操作，都是通过同步来提供确定性。 当节点间的工作不均衡或者某些节点是慢节点时，这可能会减缓计算速度。 最近，一些集群计算系统提出了让节点异步发送消息，这使得即使存在慢节点，计算块也可以继续。 尽管仍然保留故障恢复，一个类似 RDD 的模型是否能够支持这样的操作，这个话题值得探讨。 例如，节点可能可靠地记录每次迭代计算的信息 ID，这可能仍然比记录信息更节省。 请注意，对一般的计算，从丢失故障中重建不同的状态通常没有用，因为在丢失节点上后续计算可能已经使用了丢失状态。 一些算法，例如统计优化，能够从没有完全丢失所有过程的损坏状态继续执行。 这种情况下，一个基于 RDD 的系统可以在一个特殊模式下运行这些算法，它不执行恢复，但对结果执行检查点， 允许在其上进行后续计算(例如, 交互查询)以得到一个一致的结果。 5.4.4 细粒度更新 由于记录每个操作的 lineage 代价很高，用户对 RDDs 进行许多细粒度更新时是不高效的。 例如，Spark 并不适合实现一个分布式的 key-value 存储系统。 在某些情况下，我们或许可以将多个操作聚合起来一次处理以及将多个更新操作合为一个粗粒度操作。 就如同在 D-streams 中的“离散化”流处理。 当然在目前的 Spark 系统中，对于一个 key-value 存储系统来说还算不上低延迟，如果能在一个低延迟的系统中实现这个模型还是很有趣的。 这个方法类似于 Calvin 分布式数据库，通过批处理事务以及确定性执行来获得更好的扩展性和可靠性。 5.4.5 不变形和版本追踪 正如本章之前讨论的，不变性可能会增加开销，因为更多的数据需要在基于 RDD 的系统中进行复制。 RDDS 被设计为不可变的主要原因是为了跟踪不同版本的数据集的依赖关系，并恢复依赖于旧版本数据集的状态。 但是，在任务执行时仍可通过别的途径来跟踪这些依赖关系，以及使用基于 RDD 抽象的可变状态。 当处理流数据或者细粒度更新时，这将会是个很有趣的补充。 正如 3.2.3 节所讨论的，用户可以使用其他的方法来手动把状态分割为多个 RDD，或是借助存储的(持久化)数据来对旧数据进行重用。 5.5 相关工作 略 5.6 小结 略 6 总结 略 6.1 经验总结 略 6.2 更深远的影响 略 6.3 未来的工作 如 5.4 节所述，我们对 RDDs 的分析也表现出了模型的局限性，这是我们未来研究以进一步泛化该模型的兴趣点。 这里重述下这部分内容，未来扩展的主要领域包括： 通信延迟：基于 RDD 模拟任意分布式系统的一个主要缺点是，它需要额外的延迟去同步每一个步骤以使得计算是确定性的。 未来在这方面有两个有趣的方向。 第一个是系统方面的挑战，我们要研究一个基于内存集群计算系统其延迟时间能够达到的水平-新的数据中心网络可能达到微秒级的延迟， 另外，对于一个优化好的代码库，每一步的延迟可能只需要几毫秒。 (在当前的基于 JVM 的 Spark 系统中，java 程序运行时间会使得延迟更高)第二个工作是在 RDDs 中使用延迟隐藏技术用以执行需要紧密同步的应用， 它是通过将工作划分为不同的区块或是推测其它机器的响应时间来实现的。 新的通信模式：RDDs 目前只在通信节点之间提供了点对点的 shuffle 模式，但也有些并行计算采用其他的通信模式会带来更好的结果， 比如广播或者多对一的聚和。 研究发掘这些模式也许能提高应用程序的性能，以及创造新的运行时优化和故障恢复方法的机会。 异步：虽然基于 RDD 的计算是同步和确定性的，但它可能也适用于在模型内执行异步计算步骤，同时在这些步骤之间提供故障恢复保障。 细粒度的更新：RDDs 擅长于粗粒度和数据并行的操作，但是，如第 5.4.4 节所述，使用细粒度的操作来模拟这样一个系统也是可行的， 如读写一个键值对数据时，通过将这些操作分组来批量执行。 特别是在更低的延迟运行时间下，可能非常有趣的是这种方法对比传统数据库设计究竟能有多快，以及通过运行事务和分析工作共存的情况下能带来什么好处。 版本跟踪：RDDs 定义为不可变的数据集,以允许依赖关系执行具体的版本，但是在这个抽象框架中通过使用可变的存储和更高效的版本追踪方法还有很大的提升空间。 除了以上这些方面来扩展 RDD 的编程模型外，我们在 Spark 方面的经验还指出了用户面临的实际系统相关的问题，这些可能也是未来研究工作的兴趣点。 一些重要的领域有： 正确性调试：分布式应用的调试和正确性测试是复杂的，尤其是操作大量的，没有对比的数据集。 在 Spark 中我们已经探索过的一个想法是使用 RDDs 的依赖关系信息高效地重现调试应用程序的一部分 (例如，异常引起任务的崩溃，或产生某个特定输出的执行图的一部分)。 该工具还可以在第二次运行时修改 lineage 图，比如，在用户的函数中添加日志记录或增加错误跟踪记录。 性能调试：在 Spark 的邮件列表中，最常咨询的调试问题是关于性能的而不是程序正确性的。 分布式应用程序的调优是非常困难的，一部分原因是用户对于什么是好的性能缺少直觉。 如果一个 PageRank 的实现在有 5 个节点的集群上，处理 100 GB 的数据需要 30 分钟，这个性能好吗？ 这个应用能够只使用 2 分钟，或 10 秒吗？ 诸多因素如通信成本，各种数据表示的存储开销和数据倾斜等，都可能明显的影响并行应用程序的性能。 开发 一些可以自动检测这些低效率因素的工具，或者甚至是能够给用户提供足够的关于应用程序性能信息以辨别问题的监控工具，都是有趣且具有挑战性的工作。 调度策略：虽然 RDD 模型非常灵活的支持运行时细粒度任务的调度，并且 RDD 已被用于 实现了一些调度机制，如公平共享调度， 但是在用这种模型编写的应用中找到一个正确的调度策略仍然是一个有挑战的问题。 例如，在 Spark 的流式应用中，在有多个数据流的情况下，我们该如何调度计算以满足任务的执行期限或优先级？ 如果同样的应用同时在数据流上执行交互式查询呢？ 同样的，给定一个 RDDs 或 D-Streams 的图结构，我们能够自动确定检查点以减少预期的执行时间吗？ 随着应用变的越来越复杂以及用户要求更多的响应接口，这些策略对于维持良好的性能是很重要的。 内存管理：在大多数集群中分配有限的内存是一个有趣的挑战，同时也取决于应用程序定义的优先级和使用模式。 问题之所以特别有趣，是因为有不同的“层次”的存储，需要权衡内存大小与访问速度。 例如，在内存中的数据能够被压缩，这可能使它需要更大的计算开销，但所需的内存更少；或者数据也可以被换出到 SSD 中，或者是磁盘上。 有一些 RDDs 可能没有任何持久化会更高效，它是通过在运行过程中对前一个 RDD 运行 map 函数来重新计算 (对于大多数用户而言计算可能足够快了，这对于节省空间是值得的)。 一些 RDDs 的计算可能要很大开销，因此 RDDs 一直需要被复制。 特别的，因为 RDDs 总是能够可以从头开始计算丢失的数据，所以为存储管理策略留下了很大的优化空间。 我们希望在开源系统 Spark 上的持续经验将有助于我们应对这些挑战，并设计出适用于 Spark 和其他集群计算系统的解决方案。 参考文献 略","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"In Search of an Understandable Consensus Algorithm 中文翻译","slug":"treatise/in_search_of_an_understandable_consensus_algorithm","date":"2021-06-24T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/in_search_of_an_understandable_consensus_algorithm/","permalink":"https://wangqian0306.github.io/2021/in_search_of_an_understandable_consensus_algorithm/","excerpt":"","text":"In Search of an Understandable Consensus Algorithm 中文翻译 作者：Diego Ongaro and John Ousterhout, Stanford University 英文原文 引言 Raft 是一种用于管理复制日志的共识算法。 它产生的结果等价于 (multi-)Paxos，和 Paxos 一样高效，但它的结构设计与 Paxos 不同；这使得 Raft 更易于理解，也为构建实用系统提供了更好的基础。 为了增强可理解性，Raft 将共识的关键要素(例如领导节点选举、日志复制和安全性)进行分离，并强制执行更强的一致性以减少必须判断的状态数量。 用户研究的结果表明，Raft 比 Paxos 更容易让学生学习。 Raft 还包括一种用于更改集群成员的新机制，该机制使用重叠多数来保证安全。 1 简介 共识算法允许一组机器作为一个连贯的组进行工作，可以在其某些成员的失效的情况下继续工作。 因此，它们在构建可靠的大型软件系统方面发挥着关键作用。 Paxos [13, 14] 在过去十年中主导了共识算法的讨论：大多数共识的实现都是基于 Paxos 或受其影响，而 Paxos 已成为用于教授学生共识的主要工具。 不幸的是，Paxos 非常难以理解，尽管作者多次尝试使其更易于理解。 此外，其架构需要进行复杂的更改才能支持实际系统。 这样做结果是使的系统构建者和学生都在为 Paxos 苦苦挣扎。 在自己研读 Paxos 之后，我们着手寻找一种新的共识算法，可以为系统构建和教育提供更好的基石。 我们的方法非常独特，因为我们的主要目标是可理解性：我们能否为实际系统定义一个共识算法，并以比 Paxos 更容易学习的方式来描述它？ 此外，我们希望该算法能够促进对系统构建者至关重要的直觉的成长。 重要的不仅是算法要起作用，而且要清楚它为什么起作用。 这项工作的结果是一种称为 Raft 的共识算法。 在设计 Raft 时，我们应用了特定的技术来提高可理解性，包括分解(Raft 将领导选举、日志复制和安全性分开)和状态空间缩减 (相对于 Paxos，Raft 降低了不确定性的程度以及服务器之间可能不一致的方式)。 对两所大学的 43 名学生进行的用户研究表明，Raft 比 Paxos 更容易理解： 在学习了这两种算法后，其中 33 名学生能够比关于 Paxos 的问题更好地回答有关 Raft 的问题。 Raft 在许多方面类似于现有的共识算法(最值得注意的是，Oki 和 Liskov 的 Viewstamped Replication [27, 20])，但它有几个新颖的特点： 强领导：Raft 使用比其他共识算法更强大的领导形式。例如，日志条目仅从领导节点流向其他服务器。这简化了复制日志的管理，让 Raft 更容易理解。 领导选举：Raft 使用随机计时器来选举领导节点。这仅为任何共识算法已经需要的心跳增加了少量机制，同时简单快速地解决了冲突。 成员变更：Raft 用于更改集群中服务器集的机制使用了一种新的联合共识方法，其中两种不同配置的大多数在转换期间重叠。这允许集群在配置更改期间继续正常运行。 我们相信 Raft 优于 Paxos 和其他共识算法，无论是出于教育目的还是作为实现的基础。 它比其他算法更简单易懂；描述完整，足以满足实际系统的需要；它有几个开源实现，被多家公司使用；其安全特性已得到正式规定和证明；其效率可与其他算法相媲美。 论文的其余部分介绍了复制状态机问题(第 2 节)，讨论了 Paxos 的优缺点(第 3 节)，描述了我们实现可理解性的一般方法(第 4 节)， 介绍了 Raft 共识算法(第 5-7 节)，评估 Raft 性能(第 8 节)，并讨论相关工作(第 9 节)。 2 复制状态机 共识算法通常出现在复制状态机的背景下 [33]。 在这种方法中，一组服务器上的状态机计算相同状态的相同副本，并且即使某些服务器关闭也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。 例如，具有单个集群领导节点的大型系统，如 GFS [7]、HDFS [34] 和 RAMCloud [30] 通常使用单独的复制状态机来管理领导节点选举并存储必须生存的配置信息防止领导崩溃。 复制状态机的例子包括 Chubby [2] 和 ZooKeeper [9]。 注：共识算法管理包含来自客户端的状态机命令的复制日志。状态机处理来自日志的相同命令序列，因此它们产生相同的输出。 复制状态机通常使用复制日志来实现，如图 1 所示。 每个服务器存储一个包含一系列命令的日志，它的状态机按顺序执行这些命令。 每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。 由于状态机是确定性的，每个状态机都计算相同的状态和相同的输出序列。 保持复制日志的一致性是一致性算法的工作。 服务器上的共识模块接收来自客户端的命令并将它们添加到其日志中。 它与其他服务器上的共识模块通信，以确保每个日志最终包含相同顺序的相同请求，即使某些服务器出现故障。 一旦命令被正确复制，每个服务器的状态机就会按日志顺序处理它们，并将输出返回给客户端。 结果，服务器似乎形成了一个单一的、高度可靠的状态机。 实际系统的共识算法通常具有以下特性： 它们在所有非拜占庭(non-Byzantine)条件下确保安全(从不返回错误结果)，包括网络延迟、分区和丢包、重复和重新排序。 只要任何大多数服务器都可以运行并且可以相互通信以及与客户端通信，它们就具有完整的功能(可用)。因此，一个典型的五台服务器集群可以容忍任意两台服务器的故障。假设服务器因停止而失效：他们稍后可能会从稳定存储的状态中恢复并重新加入集群。 它们不依赖于时间来确保日志的一致性：错误的时钟和极端的消息延迟在最坏的情况下会导致可用性问题。 在一般情况下，只要集群的大部分响应了一轮远程过程调用，命令就可以完成；少数慢速服务器不会影响整体系统性能。 3 Paxos 有什么问题？ 在过去的十年中，Leslie Lamport 的 Paxos 协议 [13] 几乎成为共识的同义词：它是课程中最常教授的协议，大多数共识的实现都以它为起点。 Paxos 首先定义了一个能够就单个决策达成一致的协议，例如单个复制的日志条目。 我们将这个子集称为单法令 Paxos。 Paxos 然后将这个协议的多个实例组合起来，以促进一系列决策，例如日志(multi-Paxos)。 Paxos 确保安全性和生命监测，并且支持集群成员的更改。 其正确性已被证明，在正常情况下是有效的。 不幸的是，Paxos 有两个明显的缺点。 第一个缺点是 Paxos 异常难以理解。 众所周知，完整的解释 [13] 是不透明的。很少有人能够成功地理解它，而且只有付出巨大的努力的情况下才有可能。 因此，有几次尝试用更简单的术语来解释 Paxos [14, 18, 19]。 这些解释侧重于单法令子集，但它们仍然具有挑战性。 在 NSDI 2012 对与会者的非正式调查中，我们发现很少有人对 Paxos 感到满意，即使是经验丰富的研究人员也是如此。 我们自己也在与 Paxos 斗争；直到阅读了几个简化的解释并设计了我们自己的替代方案之后，我们才能够理解完整的方案，这个过程花了将近一年的时间。 我们假设 Paxos 的不透明性源于它选择单法令子集作为其基础。 单法令 Paxos 密集而微妙：它分为两个阶段，没有简单直观的解释，不能独立理解。 因此，很难对单法令协议的工作原理产生直觉。 multi-Paxos 的组合规则显着增加了复杂性和微妙性。 我们认为，就多个决策(即日志而不是单个条目)达成共识的整体问题可以用其他更直接、更明显的方式进行分解。 Paxos 的第二个问题是它没有为构建实际实现提供良好的基础。 原因之一是对于 multi-Paxos 没有广泛认可的算法。 Lamport 的描述主要是关于单法令 Paxos；他勾画了多 Paxos 的可能方法，但缺少许多细节。 已经有几次尝试充实和优化 Paxos，例如 [24]、[35] 和 [11]，但这些尝试彼此不同，也与 Lamport 的初始设计不同。 Chubby [4] 等系统已经实现了类似 Paxos 的算法，但在大多数情况下，它们的细节尚未公开。 此外，Paxos 架构对于构建实用系统来说是一种糟糕的架构；这是单法令分解的另一个结果。 例如，独立选择一组日志条目，然后将它们融合到一个顺序日志中几乎没有什么好处；这只会增加复杂性。 围绕日志设计一个系统会更简单、更有效，其中新条目以受约束的顺序进行追加。 另一个问题是 Paxos 在其核心使用对称的点对点方法(尽管它最终暗示了一种弱领导形式作为性能优化)。 这在一个只做出一个决定的简化世界中是有意义的，但很少有实际系统使用这种方法。 如果必须做出一系列决策，首先选举一个领导节点，然后让领导节点协调决策会更简单、更快捷。 因此，实际系统与 Paxos 几乎没有相似之处。 每个实现都从 Paxos 开始，发现实现它的困难，然后开发出截然不同的架构。 这既费时又容易出错，理解 Paxos 的困难加剧了这个问题。 Paxos 的公式可能是证明其正确性定理的好方法，但实际实现与 Paxos 如此不同，以至于证明没有什么价值。 以下来自 Chubby 实施者的评论是非常典型的： 1Paxos 算法的描述与现实世界系统的需求之间存在重大差距。......最终系统将基于未经证实的协议 [4]。 由于这些问题，我们得出结论，Paxos 没有为系统构建或教育提供良好的基础。 考虑到共识在大型软件系统中的重要性，我们决定看看是否可以设计一种替代的共识算法，其属性比 Paxos 更好。 Raft 是那个实验的结果。 4 设计可理解性 我们在设计 Raft 时有几个目标： 它必须为系统构建提供完整且实用的基础，从而显着减少开发人员所需的设计工作量； 它必须在所有条件下都是安全的，并且在典型的操作条件下可用； 并且它必须对常见操作有效。 但我们最重要的目标——也是最困难的挑战——是可理解性。 这种算法必须能被大部分的受众理解。 此外，必须让受众能够对算法产生直觉，以便系统构建者可以进行在现实世界实现中进行扩展(往往是不可避免的)。 在 Raft 的设计中有很多地方我们不得不在替代方法中进行选择。 在这些情况下，我们根据可理解性评估了备选方案：解释每个备选方案有多难(例如，它的状态空间有多复杂，是否有微妙的含义？)，以及读者完全理解的难易程度，用户了解该方法及其含义吗？ 我们认识到这种分析具有高度的主观性；尽管如此，我们还是使用了两种普遍适用的技术。 第一种技术是众所周知的问题分解方法：在可能的情况下，我们将问题分成可以相对独立地解决、解释和理解的单独部分。 例如，在 Raft 中，我们将领导节点选举、日志复制、安全性和成员资格更改进行分离。 我们的第二种方法是通过减少要判断的状态数量来简化状态空间，使系统更加连贯并尽可能消除不确定性。 具体来说，日志是不允许有漏洞的，Raft 限制了日志彼此不一致的方式。 尽管在大多数情况下我们试图消除不确定性，但在某些情况下，不确定性实际上提高了可理解性。 特别是，随机方法引入了不确定性，但它们倾向于通过以类似的方式处理所有可能的选择来减少状态空间(“选择任何一个；无关紧要”)。 我们使用随机化来简化 Raft 领导节点选举算法。 5 Raft 共识算法 注：左上框中的服务器行为被描述为一组独立且重复触发的规则。诸如第 5.2 节之类的部分编号指示讨论特定功能的位置。正式的规范 [28] 更准确地描述了算法。 Raft 是一种用于管理第 2 节中描述的形式的复制日志的算法。 图 2 以简明的形式总结了算法以供参考，图 3 列出了算法的关键特性；这些图的元素将在本节的其余部分逐个讨论。 特性 描述 安全选举 在给定的任期中至多会有一个节点被选举成为领导节点。(第 5.2 节) 只做追加的领导节点 领导节点不会重写或者删除条目日志，只会做追加操作。(第 5.3 节) 日志匹配 如果两个日志包含具有相同索引和任期号，则日志在给定索引之前的所有条目中都是相同的。(第 5.3 节) 领导节点完备 如果在给定的任期内提交了一个日志条目，那么该条目将出现在所有更高编号任期的领导节点的日志中。(第 5.4 节) 状态机安全 如果服务器已经通过了一项日志条目到索引对应的状态机上，则其他服务器将永远不会为同一索引通过不同的日志条目。(第 5.4 节) 图 3：Raft 保证这些属性中的每一个在任何时候都是正确的。章节编号指示了每个属性对应描述的所处位置。 Raft 通过首先选举一个杰出的领导节点来实现共识，然后让领导节点完全负责管理复制日志。 领导节点接受来自客户端的日志条目，将它们复制到其他服务器上，并告诉服务器何时将日志条目应用到它们的状态机是安全的。 拥有领导节点简化了复制日志的管理。 例如，领导节点可以在不咨询其他服务器的情况下决定在日志中放置新条目的位置，并且数据以简单的方式从领导节点流向其他服务器。 领导节点可能会失败或与其他服务器断开连接，在这种情况下会选出新的领导节点 鉴于领导节点的实现思路，Raft 将共识问题分解为三个相对独立的子问题，这些子问题将在以下小节中讨论： 领导选举：在旧的领导节点故障或下线时，一个新的领导节点必须被选举出来(第 5.2 节)。 日志副本：领导节点必须从客户端接收日志条目，并且将它们进行复制并分发到集群中，使得其他节点的日志与领导节点的日志保持一致。 安全性：Raft 的关键安全属性是图 3 中的状态机的安全属性：如果任何服务器已将特定日志条目应用于其状态机，则其他服务器不能为相同的日志索引应用不同的命令。5.4 节描述了 Raft 如何保证这个属性；该解决方案涉及对第 5.2 节中描述的选举机制的额外限制。 在介绍了共识算法之后，本节将讨论可用性问题和时序在系统中的作用。 5.1 Raft 基础知识 一个 Raft 集群包含多个服务器；五是一个典型的设置，它允许系统容忍两次故障。 在任何给定时间，每个服务器都处于以下三种状态之一：领导节点、从属节点或候选节点。 在正常操作中，只有一个领导节点，所有其他服务器都是从属节点。 从属节点是被动的：他们不会自己发出请求，而只是回应领导节点和候选节点的请求。 领导节点处理所有客户端请求(如果客户端联系从属节点，则将其重定向到领导节点)。 第三个状态，候选节点，用于选举一个新的领导者，如第 5.2 节所述。 图 4 显示了状态及其转换；下面将讨论这些转换。 注：从属节点只相应来自其他服务的请求，如果从属节点没有收到任何通信，它就会成为候选节点并发起选举。 从整个集群的大多数节点中获得选票的候选节点会成为新的领导者，领导者通常会运行到失败为止。 注：选举成功后，由一个领导节点管理集群直到任期结束。若选举失败，在这种情况下，任期结束但是没有新的领导者。 我们可以在不同的时间和服务器上观测到任期的转换。 Raft 将时间划分为任意长度的任期，如图 5 所示。 任期使用连续整数进行编号。 每个任期都从选举开始，其中一个或多个候选节点尝试成为领导节点，如第 5.2 节所述。 如果一个候选节点赢得了选举，他就会在剩余的任期中持续作为领导节点。 在某些情况下选举会导致分裂投票。 在这种情况下，任期将在没有领导节点的情况下结束；新的任期(新的选举)将很快开始。 Raft 确保在给定的任期内最多有一个领导节点。 不同的服务器可能会在不同的时间观察任期之间的转换，在某些情况下，服务器可能不会观察到选举甚至整个任期。 任期在 Raft 中充当逻辑时钟 [12]，它们允许服务器检测过时的信息，例如过时的领导节点。 每个服务器存储一个当前的任期编号，随着时间的推移单调增加。 每当服务器通信时都会交换当前任期；如果一台服务器的当任期小于另一台服务器的当前任期，则它将其当前任期更新为较大的值。 如果候选节点或领导节点发现其任期已过时，它会立即恢复到从属节点状态。 如果服务器收到带有过期任期号的请求，它会拒绝该请求。 Raft 服务器使用远程过程调用(RPC) 进行通信，共识算法只需要两种类型的 RPC。 RequestVote RPC 由候选节点在选举期间发起(第 5.2 节)，而 AppendEntries RPC 由领导节点发起以复制日志条目并提供一种心跳监测的形式(第 5.3 节)。 如果服务器没有及时收到响应，它们会重试 RPC，并且它们并行发出 RPC 以获得最佳性能。 5.2 领导选举 Raft 使用心跳机制来触发领导者选举。 当服务器启动时，它们从跟从属节点开始。 只要服务器从领导节点或候选节点那里接收到有效的 RPC，它就会保持从属节点状态。 领导节点定期向所有从属节点发送心跳(不携带日志条目的 AppendEntries RPC)以维护他们的权限。 如果从属节点在称为选举超时的一段时间内没有收到任何通信，则它假定没有可行的领导节点并开始选举以选择新的领导节点。 要开始选举，从属节点会增加其当前任期并转换到候选状态。 然后它为自己投票并并行地向集群中的每个其他服务器发出 RequestVote RPC。 候选人继续保持这种状态，直到发生以下三件事之一：(a) 赢得选举, (b) 另一个服务器将自己确立为领导节点, © 一段时间没有选中的领导节点。 这些结果将在以下段落中单独讨论。 候选节点会在获得集群内众多节点的同样任期的投票之后赢得选举。 每个服务器将在给定的任期内以先到先得的方式投票给至多一个候选节点(注：第 5.4 节增加了对投票的附加限制)。 多数规则确保至多一名候选节点可以赢得特定任期的选举(图 3 中的选举安全属性)。 一旦候选节点赢得选举，它就会变成领导节点。 然后它向所有其他服务器发送心跳消息以建立权限并防止产生新的选举。 在等待投票时，候选节点可能会收到来自另一台声称是领导节点服务器的 AppendEntries RPC。 如果领导节点的任期(包含在其 RPC 中)至少与候选节点的当前任期一样大，则候选节点将领导者视为合法并返回从属节点状态。 如果 RPC 中的任期小于候选节点的当前任期，则候选节点拒绝 RPC 并继续处于候选节点状态。 第三种可能的结果是候选节点既不赢也不输选举：如果许多从属节点同时成为候选节点，可能会分裂选票，从而没有候选节点获得多数票。 发生这种情况时，每个候选节点将超时并通过增加其任期并启动另一轮 RequestVote RPC 来开始新的选举。 然而，如果没有额外的措施，分裂选票可能会无限重复。 Raft 使用随机选举超时来确保分裂选票很少发生并且使它们能被快速解决。 为了首先防止分裂投票，选举超时是从固定间隔(例如，150-300 毫秒)中随机选择的。 这会分散服务器，以便在大多数情况下只有一个服务器会超时；它赢得了选举并在任何其他服务器超时之前发送心跳。 相同的机制用于处理分裂投票。 每个候选节点在选举开始时重新开始其随机选举超时，并在开始下一次选举之前等待该超时过去；这降低了在新选举中再次出现分裂投票的可能性。 8.3 节表明这种方法可以快速选举出一个领导节点。 选举过程只是一个样例，用于使我们更便于理解在设计思路上的取舍。 最初我们计划采用一个排名系统：每个候选节点都会分配一个唯一的排名，便于进行竞选。 如果一个候选节点发现另一个排名更高的候选节点那它会返回从属节点状态，这样排名更高的候选节点可以更轻松的赢得下一次选举。 我们发现这种方法在可用性方面产生了一些微妙的问题(如果排名较高的服务器出现故障，排名较低的服务器可以需要超时并再次成为候选节点，但如果这样做过早，它可以重置选举领导者的进度)。 我们对算法进行了多次调整，但每次调整后都会出现新的极端情况。 最终我们得出结论，随机重试方法更加明显和易于理解。 5.3 日志副本 一旦领导节点被选举出来，它就会开始向客户端提供服务。 每个客户端发送的请求中都会包含一个命令，这个命令会在由复制状态机上执行。 领导节点会将命令追加到日志中并新增一个条目，然后并行触发 AppendEntries RPC 到其他节点，从而新增日志条目副本。 当日志条目被复制写入完成后(如同下面描述的一样)，领导节点就会将日志条目应用于其状态机并将该执行的结果返回给客户端。 如果从属节点崩溃或运行缓慢，或者网络数据包丢失，领导节点会无限期地重试 AppendEntries RPC (即使它已经响应了客户端)，直到所有从属节点最终存储所有日志条目。 注：每个条目包含创建它的任期(每个框中的数字)和状态机的命令。如果该条目可以安全地应用于状态机，则该条目被视为已提交。 日志的组织方式如图 6 所示。 每个日志条目存储一个状态机命令以及领导者收到条目时的任期号。 日志条目中的术语编号用于检测日志之间的不一致并确保图 3 中的某些属性。 每个日志条目还有一个整数索引，用于标识其在日志中的位置。 领导节点决定何时将日志条目应用到状态机是安全的；这样的条目称为已提交。 Raft 保证提交的日志条目是持久的，并且最终会被所有可用的状态机执行。 一旦创建条目的领导者在大多数服务器上复制了它（例如，图 6 中的条目 7），就会提交一个日志条目。 这也会提交领导节点日志中的所有先前条目，包括由以前的领导者创建的条目。 第 5.4 节讨论了在领导节点变更后应用此规则时的一些微妙之处，并且还表明这种承诺的定义是安全的。 领导节点跟踪它知道要提交的最高索引，并将该索引包含在未来的 AppendEntries RPC(包括心跳)中，以便其他服务器最终发现。 一旦从属节点获悉日志条目已提交，它就会将该条目应用于其本地状态机(按日志顺序)。 我们设计了 Raft 日志机制来保持不同服务器上的日志之间的高度一致性。 这不仅简化了系统的行为并使其更具可预测性，而且还是确保安全的重要组成部分。 Raft 维护了以下属性，它们共同构成了图 3 中的日志匹配属性： 如果不同日志中的两个条目具有相同的索引和任期，则它们存储相同的命令。 如果不同日志中的两个条目具有相同的索引和任期，则日志在所有前面的条目中都是相同的。 第一个属性来自这样一个事实，即领导节点在给定期限内最多创建一个具有给定日志索引的条目，并且日志条目永远不会改变它们在日志中的位置。 第二个属性由 AppendEntries 执行的简单一致性检查保证。 在发送 AppendEntries RPC 时，领导节点在其日志中包含条目的索引和任期，该条目紧接在新条目之前。 如果从属节点在其日志中没有找到具有相同索引和任期的条目，则它拒绝新条目。 一致性检查作为一个归纳步骤：日志的初始空状态满足日志匹配属性，并且一致性检查在日志扩展时保留日志匹配属性。 结果，每当 AppendEntries 成功返回时，领导节点就知道从属节点的日志通过新条目与自己的日志保持同步。 注：每个框代表一个日志条目；框中的数字是它的任期。 从属节点可能缺少日志条目(a-b)，含有额外未提交的条目(c-d)，或两者都有(e-f) 例如：如果该服务器是第 2 任期的领导节点，在其日志中添加了几个条目，然后在提交任何条目之前崩溃，则可能会发生的场景如下(f) 它迅速重启，成为第 3 任期的领导节点，并在其日志中添加了更多的条目；在提交第 2 任期或第 3 任期中的任何条目之前，服务器再次崩溃并保持停机数个任期。 正常运行时，领导节点和从属节点的日志保持一致，因此 AppendEntries 进行的一致性检查永远不会失败。 然而，领导节点崩溃可能会造成日志不一致(旧的领导节点可能没有完全复制它日志中的所有条目)。 这些不一致可能会导致一系列领导节点和从属节点崩溃。 图 7 说明了从属节点日志可能与新领导节点日志不同的方式。 从属节点可能缺失领导节点上存在的日志条目，或可能有领导节点上不存在的额外条目又或者兼而有之。 日志中缺失和无关的条目可能跨越多个任期。 为了使从属节点的日志与自己的一致，领导节点必须找到两个日志一致的最新日志条目，删除该点之后从属节点日志中的任何条目，并将该点之后领导节点的所有条目发送给从属节点。 所有这些操作都是为了响应 AppendEntries RPC 执行的一致性检查而发生的。 领导节点为每个从属节点维护一个 nextIndex，这是领导节点将发送给该从属节点的下一个日志条目的索引。 当领导领导节点第一次选举成功时，它会将所有 nextIndex 值初始化为紧随其后的索引其日志中的最后一个(图 7 中的 11)。 如果一个从属节点的日志与领导节点不一致，则 AppendEntries 一致性检查将在下一次 AppendEntries RPC 中失败。 拒绝后，领导领导节点递减 nextIndex 并重试 AppendEntries RPC。 最终，nextIndex 将达到领导节点和从属节点日志匹配的点。 当这种情况发生时，AppendEntries 将成功，它会删除从属节点日志中的任何冲突条目，并从领导节点的日志中附加条目(如果有的话)。 一旦 AppendEntries 成功，从属节点的日志就会与领导节点的日志一致，并且在接下来的任期内都会保持这种状态。 可以优化协议以减少被拒绝的 AppendEntries RPC 的数量；详情见 [29]。 有了这种机制，领导节点在选举成功后不需要采取任何特殊的行动来恢复日志的一致性。 领导节点只需要正常启动，日志会自动进行收敛以响应 AppendEntries 一致性检查的失败。 领导节点永远不会覆盖或删除自己日志中的条目(图 3 中的领导节点仅附加属性)。 这种日志复制机制展示了第 2 节中描述的理想共识属性：只要大多数服务器都启动，Raft 就可以接受、复制和应用新的日志条目；在正常情况下，可以通过一轮 RPC 将新条目复制到集群中的大多数节点；并且单个慢从属节点不会影响整体性能。 5.4 安全性 前面的部分描述了 Raft 如何选举领导节点和复制日志条目。 然而，到目前为止描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。 例如，当领导节点提交多个日志条目时，追随节点可能不可用，然后它可以被选为领导节点并用新的条目覆盖这些条目；因此，不同的状态机可能会执行不同的命令序列。 本节通过添加对哪些服务器可以被选为领导节点的限制来完成 Raft 算法。 该限制确保任何给定任期的领导节点都包含之前任期中提交的所有条目(图 3 中的领导者完整性属性)。 考虑到选举限制，我们然后使承诺规则更加精确。 最后，我们展示了领导节点完整性属性的证明草图，并展示了它如何导致复制状态机的正确行为。 5.4.1 选举限制 在任何基于领导节点的共识算法中，领导节点最终必须存储所有提交的日志条目。 在一些共识算法中，例如 ViewSamped 复制算法 [20]，即使它不最初包含所有承载的条目，也可以选择领导节点。 这些算法包含额外的机制来识别丢失的条目并将它们传输给新的领导节点，无论是在选举过程中还是之后不久。 不幸的是，这会导致相当多的额外机制和复杂性。 Raft 使用一种更简单的方法，它保证从选举的那一刻起，每个新领导节点都存在以前任期的所有提交条目，而无需将这些条目传输给领导节点。 这意味着日志条目仅在一个方向上流动，从领导节点到从属节点，领导节点永远不会覆盖其日志中的现有条目。 Raft 使用投票过程来防止候选节点赢得选举，除非其日志包含所有承诺的条目。 候选节点必须联系集群的大多数成员才能当选，这意味着每个提交的条目必须至少出现在其中一个服务器中。 如果候选人的日志至少与该多数中的任何其他日志一样最新(“最新”定义如下)，那么它将保存所有提交的条目。 RequestVote RPC 实现了这个限制：RPC 包含有关候选节点日志的信息，如果从属节点自己的日志比候选节点的日志更新，则从属节点拒绝投票。 Raft 通过比较日志中最后一个条目的索引和任期来确定两个日志中的哪一个是最新的。 如果日志的最后条目具有不同的任期，则具有较晚任期的日志是最新的。 如果日志以相同的任期结束，则以更长的日志为准。 5.4.2 提交以前任期的日志条目 注：在(a)中，S1 是领导节点，部分复制了索引 2 处的日志条目。 在(b)中 S1 崩溃；S5 被选为第 3 任期的领导节点，其投票来自 S3、S4 及其本身，并在日志索引 2 处接受不同的条目。 在©中 S5 崩溃；S1 重新启动，被选举成领导节点，并继续进行日志复制。此时，第 2 项的日志条目已在大多数服务器上复制，但尚未提交。 如果 S1 崩溃如 (d) 中，则 S5 可以是选举领导节点(来自 S2，S3 和 S4 的投票)，并从任期 3 中覆盖自己的日志条目。 然而，如果 S1 在崩溃之前在大多数服务器上复制了当前任期的条目，如(e)所示，则该条目已提交(S5 无法赢得选举)。 此时，日志中的所有先前条目也都已提交。 如第 5.3 节所述，一旦该日志条目存储在大多数服务器上，领导节点就知道其当前任期中的日志条目已提交。 如果领导节点在提交条目之前崩溃，未来的领导节点将尝试完成复制该条目。 然而，领导节点不能立即断定前一任期的条目一旦存储在大多数服务器上就已提交。 图 8 说明了一种情况，旧日志条目存储在大多数服务器上，但仍然可以被未来的领导者覆盖。 为了消除图 8 中的问题，Raft 从不通过计算副本数来提交前几项的日志条目。 通过计算副本数，仅提交来自领导者当前任期的日志条目；一旦以这种方式提交了当前术语中的条目，那么由于日志匹配属性，所有先前的条目都将被间接提交。 在某些情况下，领导节点可以安全地得出一个较旧的日志条目已提交的结论(例如，如果该条目存储在每个服务器上)，但 Raft 为简单起见采取了更保守的方法。 Raft 在提交规则中会产生这种额外的复杂性，因为当领导者从以前的任期复制条目时，日志条目会保留其原始任期号。 在其他共识算法中，如果一个新的领导节点从之前的“任期”中重新复制条目，它必须使用新的“任期号”这样做。 Raft 的方法使推理日志条目变得更容易，因为它们随着时间的推移和跨日志保持相同的术语编号。 此外，与其他算法相比，Raft 中的新领导节点发送的先前任期中的日志条目更少(其他算法必须发送冗余日志条目以重新编号，然后才能提交)。 5.4.3 安全论证 在给出了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导节点的完整性特性(Leader Completeness Property, 这一讨论基于 9.2 节的安全性证明)。 我们假设领导节点的完全性特性是不满足的，然后我们推出矛盾来。 假设任期 T 的领导节点(leaderT)在任期内提交了一个日志条目，但是该日志条目没有被存储到未来某些任期的领导节点中。 假设 U 是大于 T 的没有存储该日志条目的最小任期号。 注：如果 S1 (任期 T 的领导节点)在它的任期里提交了一个新的日志条目，然后 S5 在之后的任期 U 里被选举为领导节点，那么肯定至少会有一个节点，如 S3，既接收了来自 S1 的日志条目，也给 S5 投票了。 在选举时，leaderU 的日志中必须没有待提交的条目(领导节点永远不会删除或覆盖条目)。 leaderT 复制了在集群的大多数成员上的条目，leaderU 收到了集群大多数成员的投票。因此，至少有一个服务器(“投票者(the voter)”)同时接受了来自 leaderT 的条目并投票给了 leaderU，如图 9 所示。投票者是达成矛盾的关键。 在投票给 leaderU 之前，投票者必须已经接受了 leaderT 提交的条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求(其当前任期将高于 T)。 投票者在投票给 leaderU 时仍然存储该条目，因为每个干预的领导节点都包含该条目(假设)，领导者永远不会删除条目，而从属节点只会在与领导节点冲突时删除条目。 选民投票给了 leaderU，所以 leaderU 的日志必须和选民的一样最新。这导致了两个矛盾之一。 首先，如果投票者和 leaderU 共享相同的最后一个日志任期，那么 leaderU 的日志必须至少和投票者一样长，所以它的日志包含了投票者日志中的每一个条目。这是一个矛盾，因为选民包含提交的条目，而 leaderU 被认为没有。 否则，leaderU 的最后一个日志任期必须大于投票者的。此外，它大于 T，因为投票者的最后一个日志任期至少是 T(它包含来自期限 T 的提交条目)。创建 leaderU 的最后一个日志条目的较早领导者必须在其日志中包含已提交的条目 (假设)。那么，根据日志匹配属性，leaderU 的日志也必须包含提交的条目，这是一个矛盾。 因此，所有比 T 大的任期的 leader 一定都包含了任期 T 中提交的所有日志条目。 日志匹配特性保证了未来的 leader 也会包含被间接提交的日志条目，例如图 8 (d) 中的索引 2。 通过领导节点的完整性特性，很容易证明图 3 中的状态机安全属性，并且所有状态机都以相同的顺序应用相同的日志条目(参见 [29] )。 5.5 从属节点和候选节点崩溃 到目前为止，我们一直专注于讨论领导节点的故障。 从属节点和候选节点崩溃比领导者崩溃更容易处理，并且它们都以相同的方式处理。 如果从属节点或候选节点崩溃，那么未来发送给它的 RequestVote 和 AppendEntries RPC 将失败。 Raft 通过无限重试来处理这些失败；如果崩溃的服务器重新启动，则 RPC 将成功完成。 如果服务器在完成 RPC 之后但在响应之前崩溃，那么它会在重新启动后再次收到相同的 RPC。 Raft RPC 是幂等的，所以这不会造成伤害。 例如，如果一个从属节点收到一个 AppendEntries 请求，其中包括其日志中已经存在的日志条目，它会忽略新请求中的这些条目。 5.6 时间和可用性 我们对 Raft 的要求之一是安全不能依赖于时间：系统不能仅仅因为某些事件发生得比预期的快或慢而产生错误的结果。 然而，可用性(系统及时响应客户的能力)必须不可避免地取决于时间。 例如，当有服务器崩溃时，消息交换的时间就会比正常情况下长，候选节点将不会等待太长的时间来赢得选举；没有一个稳定的领导节点，Raft 将无法工作。 领导人选举是选举的一个方面，时机是最关键的。只要系统满足以下时间要求，筏板将能够选择并保持稳定的领导者： 1broadcastTime &lt; electionTimeout &lt; MTBF 1广播时间 &lt; 选举超时周期 &lt; MTBF 在这个不等式中，broadcastTime 是服务器向集群中的每个服务器并行发送 RPC 并接收它们的响应所花费的平均时间；electionTimeout 是第 5.2 节中描述的选举超时；MTBF 是单个故障的平均间隔时间。 广播时间应该比选举超时时间少一个数量级，以便领导者可以可靠地发送阻止追随者开始选举所需的心跳消息； 鉴于用于选举超时的随机方法，这种不平等也使得分裂选票不太可能。 选举超时应该比 MTBF 小几个数量级，以便系统稳步前进。 当领导节点崩溃时，系统将在大约选举超时时间内不可用；我们希望这仅占总时间的一小部分。 广播时间和 MTBF 是底层系统的属性，而选举超时是我们必须选择的。 Raft 的 RPC 通常需要接收方将信息持久化到稳定的存储中，因此广播时间可能在 0.5 毫秒到 20 毫秒之间，具体取决于存储技术。 因此，选举超时很可能在 10 毫秒到 500 毫秒之间。 典型的服务器 MTBF 为几个月或更长时间，很容易满足时序要求。 6 集群成员变更 到目前为止，我们假设集群配置（参与共识算法的服务器集）是固定的。 在实践中，有时需要更改配置，例如在服务器出现故障时更换服务器或更改复制程度。 虽然这可以通过使整个集群脱机、更新配置文件，然后重新启动集群来完成，但这会使集群在转换期间不可用。 此外，如果有任何手动步骤，则存在操作员错误的风险。 为了避免这些问题，我们决定自动化配置更改并将它们合并到 Raft 共识算法中。 注：在本例中，集群从三台服务器增加到五台。不幸的是，有一个时间点可以选举两个不同的领导者担任同一任期，一个拥有大多数旧配置(C old )，另一个拥有大多数新配置(C new )。 为了使配置更改机制安全，在过渡期间不能有可能在同一任期内选举两个领导者的情况。 不幸的是，服务器直接从旧配置切换到新配置的任何方法都是不安全的。 一次原子地切换所有服务器是不可能的，因此在转换过程中，集群可能会分裂为两个独立的大多数(参见图 10)。 为了确保安全，配置更改必须使用两阶段方法。 有多种方法可以实现这两个阶段。 例如，一些系统(例如，`[20] )使用第一阶段禁用旧配置，使其无法处理客户端请求；然后第二阶段启用新配置。 在 Raft 中，集群首先切换到我们称之为联合共识的过渡配置； 一旦达成联合共识，系统就会转换到新的配置。 联合共识结合了新旧配置： 日志条目被复制到两种配置中的所有服务器。 任一配置中的任何服务器都可以充当领导者。 协议(用于选举和进入承诺)需要与新旧配置不同的多数。 联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。 此外，联合共识允许集群在整个配置更改期间继续为客户端请求提供服务。 注：虚线显示已创建但未提交的配置条目，实线显示最新提交的配置条目。 领导节点首先在其日志中创建 C old,new 配置条目并将其提交到 C old,new（大多数 C old 和大多数 C new）。 然后它创建 C new 条目并将其提交给大多数 C new。 在时间线中没有任何的节点可以使 C old 和 C new 独立做出决定。 集群配置使用复制日志中的特殊条目进行存储和通信；图 11 说明了配置更改过程。 当领导节点收到将配置从 C old 更改为 C new 的请求时，它将联合共识的配置(图中的 C old,new)存储为日志条目，并使用前面描述的机制复制该条目。 一旦给定的服务器将新的配置条目添加到其日志中，它将使用该配置进行所有未来决策(服务器始终使用其日志中的最新配置，无论该条目是否已提交)。 这意味着领导者将使用 C old,new 的规则来确定 C old,new 的日志条目何时被提交。 如果领导者崩溃，则可能会在 C old 或 C old,new 下选择新的领导者，这取决于获胜候选人是否收到了 C old,new。 无论如何，C new 在此期间不能单方面做出决定。 一旦 C old,new 被提交，C old 和 C new 都不能在未经对方同意的情况下做出决定，并且领导节点完备性(Leader Completeness Property)确保只有具有 C old,new 日志条目的服务器才能被选举为领导节点。 现在，领导节点可以安全地创建一个描述 C new 的日志条目并将其复制到集群中。 同样，此配置将在每个服务器上看到后立即生效。 当新配置在 C new 规则下被提交时，旧配置是无关紧要的，不在新配置中的服务器可以被关闭。 如图 11 所示，C old 和 C new 没有时间可以同时做出单边决策；这样就保证了安全。 关于重新配置还有三个问题需要解决。 第一个问题是新服务器最初可能不会存储任何日志条目。 如果在这种状态下将它们添加到集群中，它们可能需要很长时间才能赶上进度，在此期间可能无法提交新的日志条目。 为了避免可用性差距，Raft 在配置更改之前引入了一个额外的阶段，在这个阶段，新服务器作为非投票成员加入集群(领导节点将日志条目复制给他们，但他们不作为集群中的投票成员)。 一旦新服务器的配置赶上集群的其余部分，重新配置就可以如上所述进行 第二个问题是集群领导节点可能没有新配置。 在这种情况下，一旦提交了 C new 日志条目，领导节点就会下台(返回从属节点状态)。 这意味着在领导节点在管理一个不包含自己的集群时会有一段时间(在它正在提交 C new 的时候)；它会复制日志条目，但不作为集群中的投票成员。 当 C new 被提交时会发生领导节点变更，因为这是新配置可以独立运行的第一个点(总是可以从 C new 中选择领导节点)。 在这一点之前，可能是只有来自 C old 的服务器可以选择领导节点。 第三个问题是移除的服务器(那些不在 C new 中的)可能会破坏集群。 这些服务器不会收到心跳，因此它们将超时并开始新的选举。 然后他们将发送带有新任期号的 RequestVote RPC，这将导致当前领导节点恢复到从属节点状态。 最终会选出一个新的领导节点，但是被移除的服务器会再次超时，这个过程会重复，导致可用性较差。 为防止出现此问题，服务器在认为当前领导节点存在时会忽略 RequestVote RPC。 具体来说，如果服务器在听取当前领导节点的最小选举超时内收到 RequestVote RPC，则不会更新其任期或授予其投票权。 这不会影响正常选举，其中每个服务器在开始选举之前至少等待最小选举超时。 然而，它有助于避免被移除的服务器造成的中断：如果领导节点能够获得其集群的心跳，那么它就不会被更大的任期号废黜。 7 客户端和日志压缩 由于篇幅限制，本节已被省略，但该材料可在本文的扩展版本中获得 [29]。 它描述了客户端如何与 Raft 交互，包括客户端如何找到集群领导者以及 Raft 如何支持线性化语义 [8]。 扩展版本还描述了如何使用快照方法回收复制日志中的空间。 这些问题适用于所有基于共识的系统，Raft 的解决方案与其他系统类似。 8 实施和评估 我们已经将 Raft 实现为复制状态机的一部分，该状态机存储 RAMCloud [30] 的配置信息并协助 RAMCloud 协调器的故障转移。 实现 Raft 使用了大约 2000 行 C++ 代码，不包括测试、注释或空行。 源代码可免费获得 [21]。 根据本文的草稿，还有大约 25 个独立的第三方开源实现 [31] Raft 处于不同的开发阶段。 此外，各种公司正在部署基于 Raft 的系统 [31]。 本节的其余部分使用三个标准评估 Raft：可理解性、正确性和性能。 8.1 可理解性 为了衡量 Raft 相对于 Paxos 的可理解性，我们对斯坦福大学的高级操作系统课程和伯克利大学的分布式计算课程的高年级毕业生和普通学生进行了实验研究。 我们录制了一个 Raft 和另一个 Paxos 的视频讲座，并创建了相应的测验。 Raft 讲座涵盖了本文的内容； Paxos 讲座涵盖了足够的材料来创建等效的复制状态机，包括单法令 Paxos、多法令 Paxos、重新配置和一些实践中需要的优化(例如领导节点选举)。 测验测试了对算法的基本理解，还要求学生对极端情况进行推理。 每个学生观看一个视频，参加相应的测验，观看第二个视频，并参加第二个测验。 大约一半的参与者先做 Paxos 部分，另一半先做 Raft 部分，以考虑到从研究的第一部分中获得的表现和经验的个体差异。 我们比较了参与者在每个测验中的分数，以确定参与者是否对 Raft 表现出更好的理解。 关注点 为减轻偏见而采取的措施 审查材料 同等的授课质量 两者的讲师相同。Paxos 讲座基于并改进了几所大学使用的现有材料。Paxos 讲座的时间延长了 14%。 视频 相同的测验难度 问题按难度分组并在考试中配对。 测验 公平的评分 使用说明的方式，以随机顺序评分，在测验之间交替进行。 说明文档 表 1：对研究中可能对 Paxos 存在偏见的担忧、针对每种偏见采取的措施以及可用的其他材料。 我们试图让 Paxos 和 Raft 之间的比较尽可能公平。 该实验在两个方面对 Paxos 有利：43 名参与者中有 15 人报告说之前有使用 Paxos 的经验，Paxos 视频比 Raft 视频长 14%。 如表 1 所述，我们已采取措施减轻潜在的偏见来源。 我们所有的材料都可供审查 [26, 28]。 注：对角线 (33) 以上的点代表 Raft 得分更高的参与者。 平均而言，参与者在 Raft 测验中的得分比 Paxos 测验高 4.9 分(在可能的 60 分中，平均 Raft 得分为 25.7，平均 Paxos 得分为 20.8)； 图 12 显示了他们的个人得分。 配对 t 检验表明，在 95% 的置信度下，Raft 分数的真实分布的平均值至少比 Paxos 分数的真实分布大 2.5 个百分点。 我们还创建了一个线性回归模型，该模型根据三个因素预测新生的测验分数：他们参加了哪个测验、他们先前的 Paxos 经验程度以及他们学习算法的顺序。 该模型预测测验的选择会产生 12.5 分的差异，有利于 Raft。 这明显高于观察到的 4.9 分的差异，因为许多实际的学生之前都有 Paxos 经验，这对 Paxos 有很大帮助，而对 Raft 的帮助略小。 奇怪的是，该模型还预测已经参加 Paxos 测验的人在 Raft 上的分数降低了 6.3 分； 虽然我们不知道为什么，但这似乎在统计上是显着的。 注：使用 5 分制，参与者被问到(左)他们认为哪种算法在功能正常、正确和高效的系统中更容易实现，(右)哪种算法更容易向计算机专业的毕业生解释。 我们还在测验后对参与者进行了调查，以了解他们认为哪种算法更容易实现或解释；这些结果如图 13 所示。 绝大多数参与者报告说，Raft 更容易实现和解释（每个问题 41 个中的 33 个）。 然而，这些自我报告的感觉可能不如参与者的测验分数可靠，而且参与者可能因为我们对 Raft 更容易理解的假设的了解而产生偏见。 Raft 用户研究的详细讨论可在 [28] 中找到。 8.2 准确性 我们已经为第 5 节中描述的共识机制制定了正式的规范和安全性证明。 正式规范 [28] 使用 TLA+ 规范语言 [15] 使图 2 中总结的信息完全准确。 它大约有 400 行长，作为证明的主题。 对于任何实现 Raft 的人来说，它本身也很有用。 我们已经使用 TLA 证明系统 [6] 实际证明了日志完整性属性。 然而，这个证明依赖于没有经过机械检查的不变量(例如，我们没有证明规范的类型安全)。 此外，我们编写了状态机安全属性的非正式证明 [28]，该证明是完整的(仅依赖于规范)且相对精确(大约 3500 字长)。 8.3 性能 Raft 的性能类似于 Paxos 等其他共识算法。 性能最重要的情况是当已建立的领导者正在复制新的日志条目时。 Raft 使用最少数量的消息（从领导者到一半集群的单次往返）实现了这一点。 还可以进一步提高 Raft 的性能。 例如，它可以轻松支持批处理和流式请求，以实现更高的吞吐量和更低的延迟。 在其他算法的文献中已经提出了各种优化；其中许多可以应用于 Raft，我们将在未来进行实现。 我们使用“官方”的 Raft 实现方式来衡量 Raft 的领导节点选举算法的性能并回答两个问题。 第一，选举过程收敛很快吗？ 其次，领导节点崩溃后可以达到的最少停机时间是多少？ 注：上图改变了选举超时的随机性，下图缩放了最小选举超时。 每行代表 1000 次试验(除了“150-150ms”的 100 次试验)并且对应于选举超时的特定选择；例如，“150-155ms”表示选举超时时间是随机选择的，并且在 150ms 和 155ms 之间统一。 测量是在五台服务器的集群上进行的，广播时间大约为 15 毫秒。 九台服务器集群的结果是相似的。 为了衡量领导选举，我们反复使由五台服务器组成的集群的领导崩溃，并对检测到崩溃和选举新领导所需的时间进行计时(见图 14)。 为了产生最坏的情况，每次试验中的服务器都有不同的日志长度，因此一些候选节点没有资格成为领导节点。 此外，为了鼓励分裂投票，我们的测试脚本在终止进程之前触发了来自领导节点的心跳 RPC 的同步广播(这近似于领导者在崩溃之前复制新日志条目的行为)。 领导节点在其心跳间隔内均匀随机崩溃，这是所有测试的最小选举超时时间的一半。 因此，最小可能的停机时间大约是最小选举超时时间的一半。 图 14 中的上方图表显示，选举超时中的少量随机化足以避免选举中的分裂投票。 在缺乏随机性的情况下，由于许多分裂选票，在我们的测试中，领导节点选举的时间始终超过 10 秒。 仅添加 5 毫秒的随机性有很大帮助，导致平均停机时间为 287 毫秒。 使用更多的随机性可以改善最坏情况的行为：随机性为 50 毫秒时，最坏情况的完成时间(超过 1000 次试验)为 513 毫秒。 图 14 中的下方图表显示可以通过减少选举超时来减少停机时间。 选举超时时间为 12-24 毫秒，平均只需要 35 毫秒就可以选举出一个领导节点(最长的试验需要 152 毫秒)。 然而，将超时时间降低到这一点之后违反了 Raft 的时间要求：在其他服务器开始新的选举之前，领导节点很难广播心跳。 这可能会导致不必要的领导节点变更并降低整体系统可用性。 我们建议使用保守的选举超时，例如 150-300 毫秒；此类超时不太可能导致不必要的领导节点更改，并且仍将提供良好的可用性。 9 相关工作 有许多与共识算法相关的出版物，其中许多属于以下类别之一： Lamport 对 Paxos 的原始描述 [13]，并试图更清楚地解释它 [14, 18, 19]。 Paxos 的详细说明，填补缺失的细节并修改算法，为实现提供更好的基础 [24, 35, 11]。 实现共识算法的系统，例如 Chubby [2, 4]、ZooKeeper [9, 10] 和 Spanner [5]。Chubby 和 Spanner 的算法尚未详细发布，但都声称基于 Paxos。ZooKeeper 的算法已经更详细的公布了，但是和 Paxos 有很大的不同。 可应用于 Paxos [16, 17, 3, 23, 1, 25] 的性能优化。 Oki 和 Liskov 的 Viewstamped Replication (VR)，一种与 Paxos 大约同时开发的共识替代方法。最初的描述 [27] 与分布式交易协议交织在一起，但在最近的更新 [20] 中，核心共识协议已被分离。VR 使用基于领导者的方法，与 Raft 有许多相似之处。 Raft 和 Paxos 最大的区别在于 Raft 的强大领导力：Raft 将领导选举作为共识协议的重要组成部分，并将尽可能多的功能集中在领导身上。 这种方法产生了更容易理解的更简单的算法。 例如，在 Paxos 中，leader 选举与基本共识协议是正交的：它仅用作性能优化，而不是达成共识所必需的。 然而，这导致了额外的机制：Paxos 包括用于基本共识的两阶段协议和用于领导者选举的单独机制。 相比之下，Raft 将领导节点选举直接纳入共识算法，并将其用作共识的两个阶段中的初始阶段。 这样就减少了很多机制。 与 Raft 一样，VR 和 ZooKeeper 也是基于领导者的，因此与 Paxos 相比，有许多 Raft 的优势。 然而，Raft 的机制比 VR 或 ZooKeeper 少，因为它最大限度地减少了非领导节点的功能。 例如，Raft 中的日志条目仅向一个方向流动：从 AppendEntries RPC 中的领导节点向外流动。 在 VR 中，日志条目是双向流动的(领导节点可以在选举过程中收到日志条目)；这会导致额外的机制和复杂性。 已发布的 ZooKeeper 描述也将日志条目传输到领导节点和从领导节点传输日志条目，但实现显然更像 Raft [32]。 Raft 的消息类型比我们所知道的任何其他基于共识的日志复制算法都要少。 例如，VR 和 ZooKeeper 各自定义了 10 种不同的消息类型，而 Raft 只有 4 种消息类型(两个 RPC 请求及其响应)。 Raft 的消息比其他算法更密集一些，但它们总体上更简单。 另外，VR 和 ZooKeeper 是按照领导节点变化时传输整个日志的方式来描述的；将需要额外的消息类型来优化这些机制，以便它们实用。 在其他工作中已经提出或实施了几种不同的集群成员更改方法，包括 Lamport 的原始提案 [13]、VR [20] 和 SMART [22]。 我们为 Raft 选择了联合共识方法，因为它利用了共识协议的其余部分，因此很少有额外的机制需要更改成员资格。 Lamport 的基于 α 的方法不是 Raft 的选择，因为它假设可以在没有领导者的情况下达成共识。 相比 VR 和 SMART，Raft 的重配置算法的优势在于可以在不限制正常请求处理的情况下发生成员变化；相比之下，VR 在配置更改期间停止所有正常处理，而 SMART 对未完成请求的数量施加了类似 α 的限制。 Raft 的方法也比 VR 或 SMART 添加更少的机制。 10 结论 算法的设计通常以正确性、效率和/或简洁性为主要目标。 尽管这些都是有价值的目标，但我们认为可理解性同样重要。 在开发人员将算法转化为实际实现之前，其他任何目标都无法实现，这将不可避免地偏离和扩展已发布的形式。 除非开发人员对算法有深刻的理解并且可以对它产生直觉，否则他们将很难在他们的实现中保留其理想的属性。 在本文中，我们解决了分布式共识的问题，其中一种被广泛接受但难以理解的算法 Paxos 多年来一直在挑战学生和开发人员。 我们开发了一种新算法 Raft，我们已经证明它比 Paxos 更容易理解。 我们也相信 Raft 为系统构建提供了更好的基础。 使用可理解性作为主要设计目标改变了我们处理 Raft 设计的方式；随着设计的进展，我们发现自己重复使用了一些技术，例如分解问题和简化状态空间。 这些技术不仅提高了 Raft 的可理解性，而且更容易让我们相信它的正确性。 11 致谢 The user study would not have been possible without the support of Ali Ghodsi, David Mazieres, and the students of CS 294-91 at Berkeley and CS 240 at Stanford. Scott Klemmer helped us design the user study, and Nelson Ray advised us on statistical analysis. The Paxos slides for the user study borrowed heavily from a slide deck originally created by Lorenzo Alvisi. Special thanks go to David Mazi`eres and Ezra Hoch for finding subtle bugs in Raft. Many people provided helpful feedback on the paper and user study materials, including Ed Bugnion, Michael Chan, Hugues Evrard, Daniel Giffin, Arjun Gopalan, Jon Howell, Vimal kumar Jeyakumar, Ankita Kejriwal, Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos, Robbert van Renesse, Mendel Rosenblum, Nicolas Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman, David Terei, Stephen Yang, Matei Zaharia, 24 anonymous conference reviewers (with duplicates), and especially our shepherd Eddie Kohler. Werner Vogels tweeted a link to an earlier draft, which gave Raft significant exposure. This work was supported by the Gigascale Systems Research Center and the Multiscale Systems Center, two of six research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program, by STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA, by the National Science Foundation under Grant No. 0963859, and by grants from Facebook, Google, Mellanox, NEC, NetApp, SAP, and Samsung. Diego Ongaro is supported by The Junglee Corporation Stanford Graduate Fellowship. 附录 图 2 中文翻译 状态 (State) 所有服务器上的持久状态 (在响应 RPC 之前更新稳定存储) 特性 描述 currentTerm 服务器最后知道的任期号(从0开始递增) votedFor 在当前任期内收到选票的候选人ID (如果没有就为 null) log[] 日志条目；每个条目包含状态机的要执行命令和从领导节点处收到时的任期号 所有服务器上的易失性状态 特性 描述 commitIndex 已知的被提交的最大日志条目的索引值(从0开始递增) lastApplied 被状态机执行的最大日志条目的索引值(从0开始递增) 领导节点上的易失性状态 (选举后重新初始化) 特性 描述 nextIndex[] 对于每一个服务器，记录需要发给它的下一个日志条目的索引(初始化为领导节点上一条日志的索引值 + 1) matchIndex[] 对于每一个服务器，记录已经复制到该服务器的日志的最高索引值(从 0 开始递增) 附加内容 RPC (AppendEntries RPC) 此方法由领导节点调用并复制日志条目(第 5.3 节)；也用作心跳信号的传输(第 5.2 节)。 参数 特性 描述 term 领导节点任期号 leaderId 领导节点 ID，为了其他服务器能重定向到客户端 prevLogIndex 紧接在新条目之前的日志条目的索引 prevLogTerm 最新日志之前的日志的 Leader 任期号 entries[] 将要存储的日志条目(表示心跳信息时为空，有时会为了效率发送超过一条) leaderCommit 领导节点提交的日志条目索引 结果 特性 描述 term 目前的任期号，用于领导节点更新自己的任期号 success 如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 接受者需要实现： 如果 term &lt; currentTerm返回 false(第 5.1 节) 如果在 prevLogIndex 处的日志的任期号与 prevLogTerm 不匹配时，返回 false(第 5.3 节) 如果一条已经存在的日志与新的冲突(index 相同但是任期号 term 不同)，则删除已经存在的日志和它之后所有的日志(第 5.3 节) 添加任何在已有的日志中不存在的条目 如果 leaderCommit &gt; commitIndex，将 commitIndex 设置为 leaderCommit 和最新日志条目索引号中较小的一个 投票请求 RPC (RequestVote RPC) 由候选人调用来收集选票(第 5.2 节)。 参数 特性 描述 term 候选人任期 candidateId 要求投票的候选人 lastLogIndex 最后存储的候选人日志条目索引(第 5.4 节) lastLogTerm 最后存储的候选人日志条目任期(第 5.4 节) 结果 特性 描述 term 目前的任期，让候选人可以更新自己 voteGranted 若候选人获取到了选票则为 true 接收者要实现的内容 如果任期 &lt; 当前任期则返回 false(第 5.1 节) 如果 votedFor 为 null 或是 CandidateId，并且候选人的日志至少与接收者的日志一样都处于最新，则批准投票(第 5.2 和 5.4 节) 服务器要遵守的规则 所有服务器部分： 如果 commitIndex &gt; lastApplied，就提升 lastApplied 然后接受 log[lastApplied] 至状态机(第 5.3 节) 如果 RPC 请求或相应内容中的 term T &gt; currentTerm：就将 currentTerm = T，并转化为从属节点(第 5.1 节) 从属节点部分： 回应来自候选人和领导节点的 RPC 如果选举超时之前没有收到来自当前领导节点的 Append Entries RPC 或候选人的投票请求：转化为候选人 候选人部分： 转变为候选人之后开始选举 增加 currentTerm 为自己投票 重置选举定时器 发送 RequestVote RPC 至其他节点 如果接收到了大多数节点的投票：转化为领导节点 如果接收到了新的领导节点的 AppendEntries RPC：转化为从属节点 如果选举超时：重新开始选举 领导节点部分： 一旦成为领导节点：向其他所有服务器发送空的 AppendEntries RPC(heartbeat)；在空闲时间重复发送以防止选举超时(第 5.2节) 如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端(第 5.3节) 如果从属节点最新一次的日志索引(lon index) &gt;= nextIndex：通过 AppendEntries RPC 将 nextIndex 之后的所有日志条目发送出去 如果发送成功：更新从属节点的 nextIndex 和 matchIndex 如果因为日志不一致导致 AppendEntries 发送失败：递减 nextIndex 然后进行重试 如果存在 N 使得 N &gt; commitIndex，大部分 matchIndex[i] ≥ N，并且 log[N].term == currentTerm: 将 commitIndex 设置为 N(第 5.3 和 5.4 节) 参考资料 [1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154. [2] BURROWS, M. The Chubby lock service for looselycoupled distributed systems. In Proc. OSDI’06, Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350. [3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 316–317. [4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J.Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407. [5] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KANTHAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implementation (2012), USENIX, pp. 251–264. [6] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. M´ery, Eds., vol. 7436 of Lecture Notes in Computer Science, Springer, pp. 147–154. [7] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43. [8] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems 12 (July 1990), 463–492. [9] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Conference (2010), USENIX, pp. 145–158. [10] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup systems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Dependable Systems &amp; Networks (2011), IEEE Computer Society, pp. 245–256. [11] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University,2008. [12] LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7(July 1978), 558–565. [13] LAMPORT, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (May 1998), 133–169. [14] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [15] LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. AddisonWesley, 2002. [16] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005. [17] LAMPORT, L. Fast paxos. Distributed Computing 19, 2(2006), 79–103. [18] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17. [19] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13. [20] LISKOV, B., AND COWLING, J. Viewstamped replication revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012. [21] LogCabin source code. http://github.com/logcabin/logcabin. [22] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. EuroSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115. [23] MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384. [24] MAZIERES , D. Paxos made practical. http://www.scs.stanford.edu/˜dm/home/papers/paxos.pdf, Jan. 2007. [25] MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM. [26] Raft user study. http://ramcloud.stanford.edu/˜ongaro/userstudy/. [27] OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing(1988), ACM, pp. 8–17. [28] ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress). http://ramcloud.stanford.edu/˜ongaro/thesis.pdf. [29] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm (extended version). http://ramcloud.stanford.edu/raft.pdf. [30] OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D.,KOZYRAKIS, C., LEVERICH, J., MAZIERES ` , D., MITRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Communications of the ACM 54 (July 2011), 121–130. [31] Raft consensus algorithm website. http://raftconsensus.github.io. [32] REED, B. Personal communications, May 17, 2013. [33] SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Computing Surveys 22, 4 (Dec. 1990), 299–319. [34] SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Systems and Technologies (2010), IEEE Computer Society, pp. 1–10. [35] VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Raft","slug":"Raft","permalink":"https://wangqian0306.github.io/tags/Raft/"}]},{"title":"Architecture of Next Generation Apache Hadoop MapReduce Framework 中文翻译版","slug":"treatise/architecture_of_next_generation_apache_hadoop_mapreduce_framework","date":"2021-06-22T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/architecture_of_next_generation_apache_hadoop_mapreduce_framework/","permalink":"https://wangqian0306.github.io/2021/architecture_of_next_generation_apache_hadoop_mapreduce_framework/","excerpt":"","text":"Architecture of Next Generation Apache Hadoop MapReduce Framework 中文翻译版 作者： Arun C. Murthy, Chris Douglas, Mahadev Konar, Owen O’Malley, Sanjay Radia, Sharad Agarwal, Vinod K V 背景 Apache Hadoop Map-Reduce 框架显然已经过时了。 特别是依据观察到的集群大小和工作负载的相关趋势，Map-Reduce JobTracker 需要彻底改革以解决其内存消耗、优化线程模型， 解决可扩展性/可靠性/性能方面的几个技术缺陷。 虽然我们会定期进行维护。 但是在最近的一年我们发现，这些问题已经变成了基础性的问题并且随着规模的增长变得尤为严重。 即便是在 2007 年，这种架构性的缺陷和纠正措施都很古老而且很容易理解： https://issues.apache.org/jira/browse/MAPREDUCE-278. 需求 当我们考虑改进 Hadoop Map-Reduce 框架的方法时，重要的是不要偏离原先的设计需求。 展望 2011 年及以后，Hadoop 客户对下一代 Map-Reduce 框架的最紧迫要求是： 可靠性 可用性 扩展性(由 10000 个节点和 200,000 个核心组成的集群) 向后兼容(确保用户的 Map-Reduce 应用程序可以在框架的下一版本中保持不变地运行。同时也意味着向前兼容。) 进化(给用户进行软件栈的全面升级) 可预测的延迟(用户的主要关注点) 集群利用率 次级的需求是： 支持 Map-Reduce 的替代编程范式 支持有限的、短期的服务 鉴于上述要求，很明显我们需要重新思考用于系统中的数据处理基础设施。 新一代的 MapReduce (MRv2/YARN) MRv2 的基本思想是将 JobTracker 的两个主要功能(资源管理和作业调度/监控)拆分为单独的守护程序。 这个想法是通过拥有一个全局的 ResourceManager (RM) 和每个应用程序的 ApplicationMaster (AM) 来实现的。 一个应用程序要么是传统意义上的 MapReduce 作业中的单个作业，要么是作业的 DAG。 ResourceManager 和每个从属节点的 NodeManager (NM) 构成了数据计算框架。 ResourceManager 是在系统中的所有应用程序之间仲裁资源的最高机关。 每个应用程序的 ApplicationMaster 实际上是一个特定于框架的库，其任务是协商来自 ResourceManager 的资源并与 NodeManager 一起执行并监视任务。 ResourceManager 有两个主要组件： Scheduler (S) ApplicationsManager (ASM) Scheduler 负责将资源分配给各种正在运行的应用程序，这些应用程序受到容量、队列等常见的限制。 Scheduler 只负责调度，它不去监视或跟踪应用程序的状态。 此外，它也不能保证由于应用程序故障或硬件故障而重新启动失败的任务。 Scheduler 根据应用程序的资源需求执行其调度功能；它是基于资源容器的抽象概念来实现的，它包含了内存、cpu、磁盘、网络等元素。 Scheduler 有一个可配置的策略插件，负责在不同的队列、应用程序等之间划分集群资源。 当前的 MapReduce Scheduler (如 CapacityScheduler 和 FairScheduler)就是例子。 ApplicationManager 负责接受作业提交，协商用于执行特定于应用程序的 ApplicationMaster 的第一个容器， 并提供用于在出现故障时重新启动 ApplicationMaster 容器的服务。 NodeManager 是每台机器的框架代理，负责启动应用程序的容器，监视它们的资源使用情况(cpu、内存、磁盘、网络)并向调度器报告。 每个应用程序的 ApplicationMaster 负责与 Scheduler 协商适当的资源容器，跟踪它们的状态并监视进度。 YARN v1.0 在这一节中我们会描述第一版 YARN 的设计目标。 需求 可靠性 可用性 扩展性(由 10000 个节点和 200,000 个核心组成的集群) 向后兼容(确保用户的 Map-Reduce 应用程序可以在框架的下一版本中保持不变地运行。同时也意味着向前兼容。) 进化(给用户进行软件栈的全面升级) 集群利用率 ResourceManager ResourceManager 是在系统中的所有应用程序之间仲裁资源的最高机关。 资源模型 YARN 1.0版中的 Scheduler 只对内存建模。 系统中的每个节点都被认为是由多个最小内存大小(比如 512 MB 或 1 GB)的容器组成的。 ApplicationMaster可 以请求任何容器作为最小内存大小的倍数。 最后，我们希望转向更通用的资源模型，但是，对于Yarn v1，我们提出了一个相当简单的模型： 资源模型完全基于内存(RAM)，每个节点都由离散的内存块组成。 与 Hadoop MapReduce 不同，集群没有切分到 Map slot 和 Reduce slot 。 每一块内存都是可替换的，这对集群的利用有着巨大的好处。 目前 Hadoop MapReduce 的一个众所周知的问题是，Reduce slot 受到限制，缺少可替换资源是一个严重的限制因素。 在应用程序中(通过 ApplicationMaster)可以请求跨越任意数量内存块的容器，也可以请求不同数量不同类型的容器。 ApplicationMaster 通常要求特定的主机/机架具有特定的容器功能。 资源定位 ApplicationMaster 可以请求具有适当资源需求的容器，包括特定的机器。 它们还可以在每台机器上请求多个容器。 所有资源请求都受到应用程序、其队列等的容量限制。 ApplicationMaster 负责计算应用程序的资源需求，例如拆分 MapReduce 应用程序的输入，并将其转换为 Scheduler 可以理解的协议。 调度器理解的协议将是: &lt;priority, (host, rack, *), memory, #containers&gt;。 注释：&lt;优先级，(主机，机架，*)，内存，#容器&gt; 在 MapReduce 中，ApplicationMaster 接受拆分后的输入， 并向 ResourceManager 的 Scheduler 呈现一个在主机上设置了键的反转表，并限制其生命周期中所需的容器总数，但这一限制可能会发生更改。 下面是一个典型的 ApplicationMaster 中单个应用程序的资源获取请求： 优先级 主机名 获取资源(内存/GB) 容器数量 1 h1001.company.com 1 GB 5 1 h1010.company.com 1 GB 3 1 h2031.company.com 1 GB 6 1 rack11 1 GB 8 1 rack45 1 GB 6 1 * 1 GB 14 2 * 2 GB 3 Scheduler 将尝试为应用程序匹配适当的机器；如果特定机器不可用，它还可以在同一机架或不同机架上提供资源。 有时，由于集群的异常繁忙，ApplicationMaster 可能会收到不太合适的资源；然后，它可以通过将它们返回给 Scheduler 来拒绝它们，而不必占用资源。 与 Hadoop MapReduce 相比，一个非常重要的改进是不再将集群资源拆分为 Map slot 和 Reduce slot。 这对集群利用率有着巨大的影响，因为应用程序在这两种类型的 slot 上都不再是瓶颈。 注：异常的 ApplicationMaster 可能去请求超出需要的容器，Scheduler 使用应用程序限额、用户限额、队列限额等来保护集群不被滥用。 利弊 该模型的主要优点是，对于每个应用程序来说， 在 ResourceManager 上进行调度所需的状态以及 ApplicationMaster 和 ResourceManager 之间传递的信息量来说，都是非常紧凑的。 这对于扩展 ResourceManager 至关重要。 在这个模型中，每个应用程序的信息量总是 O(集群大小)，在当前的 Hadoop MapReduce JobTracker 中，它是O(任务数)，而任务数可以是成百上千个。 计划资源模型的主要问题是，如上所述，在从拆分到主机/机架的转换过程中会丢失信息。 这种转换是单向的、不可撤销的，ResourceManager 对资源请求之间的关系没有概念，例如，如果将 h43 和 h32 分配给应用程序，则不需要 h46。 为了克服上述缺点并证明 Scheduler 未来的兼容性，我们提出了一个简单的扩展—— 其思想是向 ApplicationMaster 添加一个互补的调度器组件，该组件在 ApplicationMaster 没有帮助/知识的情况下执行翻译， ApplicationMaster 在没有翻译的情况下继续按照任务/拆分进行思考。 ApplicationMaster 中的这个调度程序组件与 Scheduler 协同工作，可以想象的是在未来移除这个组件，而不影响 ApplicationMaster 的任何实现。 Scheduler Scheduler 聚合来自所有正在运行的应用程序的资源请求，以构建分配资源的全局计划。 然后，调度器基于特定于应用程序的约束(如适当的机器)和全局约束(如应用程序、队列、用户等的配额)来分配资源。 Scheduler 使用熟悉的容量和容量保证概念作为在竞争应用程序之间分配资源的策略。 调度算法是很简单的： 选择系统中最空闲的队列 选择队列中最高优先级的任务 给这个任务它需要的资源 Scheduler API YARN 调度程序和 ApplicationMaster 之间只有一个 API： 1Response allocate (List&lt;ResourceRequest&gt; ask, List&lt;Container&gt; release) AM 通过 ResourceRequests 列表(ask)请求特定的资源，并释放 Scheduler 分配的不必要的容器。 该响应包含新分配的容器的列表、自 AM 和 RM 之间的上一次交互以来完成的特定于应用程序的容器的状态，以及向应用程序指示可用的集群资源。 AM 可以使用容器状态来收集有关已完成容器的信息，并对失败等做出反应。 AM 可以使用空余资源来调整其未来的请求，例如，在 MapReduce 中 AM 可以使用此信息来安排 Map 和 Reduce，以避免死锁， 例如将其所有空间用于 Reduce 等。 资源监控 Scheduler 从 NodeManagers 接收有关已分配资源的资源使用情况的定期信息。 Scheduler 还向 ApplicationMaster 提供已完成容器的状态。 程序提交 应用程序遵循下面的提交流程： 用户(通常来自网关)向 ApplicationsManager 提交作业。 客户端首先得到一个新的 Application ID 打包应用相关内容，上传至 HDFS 中的 $&#123;user&#125;/.staging/$&#123;application_id&#125; 提交程序至 ApplicationsManager ApplicationsManager 接受用户发出的程序申请 ApplicationsManager 与 Scheduler 协商，申请所需的 slot 并启动 ApplicationMaster ApplicationsManager 还负责向客户端提供正在运行的 ApplicationMaster 的详细信息，用于监控应用程序进度等。 ApplicationMaster 的生命周期 ApplicationsManager 负责管理系统中所有应用程序的 ApplicationMaster 的生命周期。 如应用程序提交部分所述，ApplicationsManager 负责启动 ApplicationMaster。 之后 ApplicationsManager 会监控 ApplicationMaster 发送周期性的心跳，并负责确保其运行状态，在失败时重新启动 ApplicationMaster 等。 ApplicationsManager 的组件 如前几节所述，ApplicationsManager 的主要职责是管理 ApplicationMaster 的生命周期。 为了实现这一需求，ApplicationsManager 具有以下组件： SchedulerNegotiator ——负责与 Scheduler 协调用于 AM 的容器 AMContainerManager ——通过 NodeManager 启动和停止 AM 的容器 AMMonitor ——负责管理 AM 的运行状态并负责在必要时重新启动 AM 可用性 ResourceManager 将其状态存储在 ZooKeeper 中以确保其高可用。 依靠 ZooKeeper 中保存的状态可以快速重启。 有关更多详细信息，请参阅 YARN 可用性部分。 NodeManager 一旦 Scheduler 将容器分配给应用程序，每台机器的 NodeManager 负责启动应用程序的容器。 NodeManager 还负责确保分配的容器不超额使用其在机器上分配的资源。 NodeManager 还负责为任务设置容器的环境。 这包括二进制文件、jar 文件等。 NodeManager 还提供了一个简单的服务来管理节点上的本地存储。 即使应用程序在节点上没有分配到活动，也可以使用本地存储。 例如 MapReduce 应用程序使用此服务来存储临时的 Map 任务输出并将它们 shuffle 到 Reduce 任务。 ApplicationMaster ApplicationMaster 是每个应用程序的 master，负责协商来自 Scheduler 的资源，与 NodeManager 一起在适当的容器中运行字任务并监控任务。 它通过向 Scheduler 请求备用资源来对容器故障作出反应。 ApplicationMaster 负责计算应用程序的资源需求，例如拆分 MapReduce 应用程序的输入，并将其转换为 Scheduler 可以理解的协议。 ApplicationsManager 仅在失败时重新启动 AM；AM 负责从保存的持久状态中恢复应用程序。 当然，AM 也可以从一开始就运行应用程序。 在 YARN 上运行 MapReduce 任务 本节介绍用于通过 YARN 运行 Hadoop MapReduce 作业的各种组件。 MapReduce ApplicationMaster Map-Reduce ApplicationMaster 是 MapReduce 特定类型的 ApplicationMaster，负责管理 MR 作业。 MR AM 的主要职责是从 ResourceManager 获取适当的资源，在分配的容器上调度任务，监控正在运行的任务并管理 MR 作业的生命周期直至完成。 MR AM 的另一个职责是将作业的状态保存到持久存储中，以便在 AM 本身发生故障时可以恢复 MR 作业。 该提议是在 MR AM 中使用基于事件的有限状态机(FSM)来管理 MR 任务和作业的生命周期和恢复。 这允许将 MR 作业的状态自然表达为 FSM，并允许可维护性和可观察性。 下图显示了 MR 作业、任务和任务尝试的 FSM： 注：此处原文截图本来就不清晰。。。基本没法看 在 YARN 中运行的 MapReduce 作业的生命周期应当是这样的： Hadoop MR JobClient 将作业提交给 YARN ResourceManager (ApplicationsManager) 而不是 Hadoop MapReduce JobTracker。 YARN ASM 与 Scheduler 协商 MR AM 的容器，然后为作业启动 MR AM。 MR AM 启动并注册到 ASM。 Hadoop MapReduce JobClient 轮询 ASM 以获取有关 MR AM 的信息，然后直接与 AM 通信获取状态、计数器等。 MR AM 将输入数据进行拆分并将所有映射构建成 YARN Scheduler 的资源请求。 MR AM 为作业运行 Hadoop MR OutputCommitter 的必要作业设置 API。 MR AM 将 Map/Reduce 任务的资源请求提交给 YARN Scheduler，从 RM 中获取容器，并通过与每个容器的 NodeManager 协同工作， 在对应容器上调度适当的任务。 MR AM 监控单个任务的完成情况，如果任何任务失败或停止响应，则请求备用资源。 MR AM 还会使用应用程序清理 Hadoop MR OutputCommitter 已完成的任务。 一旦整个一次 Map 和 Reduce 任务完成，MR AM 就会提交相应任务或者终止相应任务的 Hadoop MR OutputCommitter 的API 作业完成只有 MR AM 也会终止。 MapReduce ApplicationMaster 具有以下组件： Event Dispatcher ——作为协调器的中央组件，为下面的其他组件生成事件。 ContainerAllocator ——该组件负责将任务的资源需求转换为 YARN Scheduler 可以理解的资源请求，并与 RM 协商资源。 ClientService ——负责使用作业状态、计数器、进度等响应 Hadoop MapReduce JobClient 的组件。 TaskListener ——从 Map/Reduce 任务接收心跳。 TaskUmbilical ——负责接收心跳和更新 Map Reduce 任务的组件 ContainerLauncher ——负责通过适当的 NodeManager 来启动容器的组件。 JobHistoryEventHandler ——将作业历史事件写入 HDFS。 Job ——负责维护作业和组件任务状态的组件。 可用性 ApplicationMaster 通常将其状态存储在 HDFS 中，以确保其高可用。 有关更多的详细信息，请参阅 YARN 可用性部分。 YARN 可用性 本节描述了 YARN 高用性相关的设计。 运行失败的场景 本节列举了 YARN 中实体(如 ResourceManager、ApplicationMasters、Containers 等)的各种异常和失败场景，并重点介绍了负责处理这些情况的实体。 场景和处理实体： 容器(任务)活着，但被卡住了 ——AM 容器运行超时并杀死了它。 容器(任务)死了 ——NM 发现容器已经退出，并通知了 RM(Scheduler)；AM 在下一次与 YARN 调度器交互期间获取容器的状态(参见 Scheduler API 部分) NM 活着，但被卡住了 ——RM 发现 NM 已经超时了，在下一次交互时通知所有在该节点上有容器的 AM。 NM 死了 ——RM 发现 NM 已经超时了，在下一次交互时通知所有在该节点上有容器的 AM。 AM 活着，但被卡住了 ——ApplicationsManager 使 AM 超时，释放 RM 的容器并通过协商另一个容器重新启动 AM。 AM 死了 ——ApplicationsManager 使 AM 超时，释放 RM 的容器并通过协商另一个容器重新启动 AM。 RM 死了 ——参见 RM 重启的部分 MapReduce 应用程序和 ApplicationMaster 的可用性 本节介绍 MR ApplicationMaster 的故障转移以及如何恢复 MR 作业。 如 YARN ResourceManager 中的 ApplicationsManager(ASM) 部分所述， ASM 负责监控 MR ApplicationMaster 或任何与此相关的 ApplicationMaster 并为其提供高可用性。 MR ApplicationMaster 自己负责恢复具体的 MR 作业，ASM 只重启 MR AM。 当 MR AM 重新启动时，我们有多种选择来恢复 MR 作业： 从头开始重新启动作业。 仅重启未完整的 Map 和 Reduce 任务。 为 Map 和 Reduce 任务标识正在运行的 MR AM 来完成它们。 仅针对单个容器(即运行 AM 的容器)失败，而从头开始重新启动 MR 作业的成本太高因此是不可行的选择。 仅重新启动不完整的 Map 和 Reduce 任务的方式很吸引我们，因为它确保我们不会运行已完成的任务，但是它仍然会伤害 Reduce， 因为它们可能(几乎)完成了 shuffle 并会因重新启动它们而受到影响。 从工作本身的角度来看，为 Map 和 Reduce 任务标识正在运行的 MR AM 是最有吸引力的选择，但从技术上讲，这是一项更具挑战性的任务。 鉴于上述权衡，我们建议对 YARN-v1.0 使用选项 2。 是一个更简单的实现，但有显著的好处。 我们很可能会在未来的版本中实现选项 3。 实现选项 2 的建议是让 MR AM 将事务日志写出到 HDFS，以跟踪所有已完成的任务。 在重新启动时，我们可以从事务日志中重放任务完成事件，并使用 MR AM 中的状态来跟踪已完成的任务。 因此，让 MR AM 运行剩余的任务是相当简单的。 YARN ResourceManager 的可用性 这里的主要目标是能够在遇到灾难性故障时快速重启 ResourceManager 和所有正在运行/挂起的应用程序。 因此 YARN 的可用性有两个方面： ResourceManager 的可用性 单个应用程序及其 ApplicationMaster 的可用性。 YARN 的高可用性设计在概念上非常简单 ——ResourceManager 负责恢复它自己的状态，例如运行 AM、分配 NM 和资源。 AM 本身负责恢复它自己的应用程序的状态。 有关恢复 MR 作业的详细讨论，请参阅 MR ApplicationMaster 可用性部分。 本节介绍了确保 YARN ResourceManager 高可用的设计。 为了提供高可用性，YARN ResourceManager 具有以下需要的特性： Queue definitions ——这些定义已经存在于持久存储中，例如文件系统或 LDAP。 正在运行和待处理的应用程序的定义 各种应用程序和队列的资源分配。 已经运行并分配给应用程序的容器。 单个 NodeManager 上的资源分配。 该提议是使用 ZooKeeper 来存储 YARN ResourceManager 的状态，该状态尚未存在于持久存储中，即 AM 的状态和单个应用程序的资源分配。 通过扫描对应用程序的分配，可以快速重建对各种 NM 和队列的资源分配。 ZooKeeper 允许通过主选举快速恢复 RM 和故障转移。 一个简单的方案是为每个 NodeManager 创建 ZooKeeper 临时节点，并为每个分配的容器创建一个节点，并使用以下最少信息： 应用 ID 容器资源 样例如下： 12345678910111213141516171819|+ app_1| + &lt;container_for_AM&gt;| || + &lt;container_1, host_0, 2G&gt;| || + &lt;container_11, host_10, 1G&gt;| || + &lt;container_34, host_9, 2G&gt;|+ app2| + &lt;container_for_AM&gt;| || + &lt;container_5, host_87, 1G&gt;| || + &lt;container_67, host_14, 4G&gt;| || + &lt;container_87, host_9, 2G&gt;| | 有了这些信息，ResourceManager 在重启时可以通过 ZooKeeper 快速重建状态。 ZooKeeper 是所有分配的真实来源。 这是容器分配的流程。 ResourceManager 获取了 ApplicationMaster 容器的请求 ResourceManager 在 /app$i 下的 ZooKeeper 中创建一个 znode，其中包含 containerid、node id 和资源能力信息。 ResourceManager 响应 ApplicationMaster 获取对应资源的请求。 在释放容器时，遵循以下步骤： ApplicationMaster 向 NodeManager 发送 stopContainer() 请求以停止容器，如果遇到失败它会自动重试。 ApplicationMaster 然后使用 deallocateContainer(containerid) 向 ResourceManager 发送请求。 ResourceManager 然后从 ZooKeeper 中删除容器的 znode。 为了能够在 ApplicationMaster、ResourceManager 和 NodeManager 之间保持所有容器分配同步，我们需要一些 API： ApplicationMaster 使用 getAllocatedContainers(appid) 在分配给 ApplicationMaster 的容器集合上与 ResourceManager 保持同步。 NodeManager 保持自己与 ResourceManager 的心跳同步，ResourceManager 则使用 NodeManager 应当清理和删除的容器作为响应。 重新启动时，ResourceManager 会遍历 HDFS/ZK 中的所有应用程序定义，即 /systemdir 并假设所有 RUNNING 状态的 ApplicationMaster 都处于活动状态。 ApplicationsManager 负责重新启动任何不更新 ApplicationsManager 的失败 AM，如前所述。 启动时的 ResourceManager 在 $&#123;yarn-root&#125;/yarn/rm 下的 ZooKeeper 上发布其主机和端口。 1234|+ yarn| || + &lt;rm&gt; 这个 ZooKeeper 节点是一个临时节点，如果 ResourceManager 死亡，它会自动删除。 在这种情况下，备份 ResourceManager 可以通过来自 ZooKeeper 的通知来接管。 由于 ResourceManager 状态在 ZooKeeper 中，因此备份 ResourceManager 可以启动。 所有的 NodeManager 都会监视 $&#123;yarn-root&#125;/yarn/rm 以便在 znode 被删除或更新时得到通知。 ApplicationMaster 也会监视 $&#123;yarn-root&#125;/yarn/rm 上的通知。 删除 znode 时，NodeManagers 和 ApplicationMasters 会收到更改通知。 ResourceManager 中的这种变化可以通过 ZooKeeper 通知 YARN 组件。 YARN 的安全性 本节介绍 YARN 安全方面的内容。 一个硬性要求是 YARN 至少与当前的 Hadoop MapReduce 框架一样安全。 这意味着几个方面，例如： 全面基于 Kerberos 的强身份验证。 YARN 守护程序（例如 RM 和 NM）应该以安全的非 root Unix 用户身份运行。 单个应用程序作为提交应用程序的实际用户运行，并且他们可以安全地访问 HDFS 等上的数据。 支持 Oozie 等框架的超级用户。 队列、管理工具等的类似授权。 该提议是使用 Hadoop 现有的安全机制和组件来实现 YARN 的安全性。 YARN 的唯一额外要求是，NodeManager 可以通过确保 ResourceManager 实际分配容器来安全地验证传入请求以分配和启动容器。 该提议是在 RM 和 NM 之间使用共享密钥。 RM 使用密钥来签署授权的 ContainerToken，其中包括 ContainerID、ApplicationID 和分配的资源能力。 NM 可以使用共享密钥来解码 ContainerToken 并验证请求。 ZooKeeper 的安全性 ZooKeeper 具有可配置的安全插件，可以使用 YCA 或共享秘密身份验证。它具有用于授权的 ACL。 ZooKeeper 中用于发现 RM 的 RM 节点和 YARN RM 可用性一节中提到的应用程序容器分配可由 ResourceManager 写入，并且只能由其他人读取。 YCA 用于认证系统。 请注意，由于 ZooKeeper 尚不支持 Kerberos，因此不允许单个 ApplicationMaster 和任务写入 ZooKeeper，所有写入均由 RM 自己完成。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://wangqian0306.github.io/tags/MapReduce/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"YARN","slug":"YARN","permalink":"https://wangqian0306.github.io/tags/YARN/"}]},{"title":"Kafka a Distributed Messaging System for Log Processing 中文翻译版","slug":"treatise/kafka_a_distibuted_messaging_system_for_log_processing","date":"2021-06-17T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/kafka_a_distibuted_messaging_system_for_log_processing/","permalink":"https://wangqian0306.github.io/2021/kafka_a_distibuted_messaging_system_for_log_processing/","excerpt":"","text":"Kafka: a Distributed Messaging System for Log Processing 作者： Jay Kreps, Neha Narkhede, Jun Rao 原版的版权说明 123456Permission to make digital or hard copies of all or part of this work for personal or classroom useis granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. NetDB&#x27;11, Jun. 12, 2011, Athens, Greece.Copyright 2011 ACM 978-1-4503-0652-2/11/06…$10.00. 摘要 日志处理已经是消费互联网公司数据管道的关键组成部分。 我们将介绍 Kafka，这是一个分布式消息系统，我们开发该系统是为了以低延迟收集和传递大量日志数据。 我们的系统融合了现有日志聚合和消息传递系统的思想，适合离线和在线消息消费。 我们在 Kafka 中做了很多非传统但实用的设计选择，使我们的系统高效且可扩展。 实验结果表明与两种流行的消息系统相比，Kafka 具有更好的性能。 我们在生产环境中使用 Kafka 已经有一段时间了，它每天要处理数百 GB 的新数据。 General Terms Management, Performance, Design, Experimentation. 关键词 消息、分布式、日志处理、吞吐量、在线。 1 引言 任何一家大型互联网公司都会产生大量的“日志”数据。 这些数据通常包括: 与登录、页面浏览、点击、“喜欢”、共享、评论和搜索查询相对应的用户活动事件 操作度量，如服务调用堆栈、调用延迟、错误，以及系统度量，如每台计算机上的CPU、内存、网络或磁盘利用率。 日志数据长期以来一直是用于跟踪用户参与度、系统利用率和其他指标的分析的一个组成部分。 然而，最近互联网应用的趋势使得活动数据成为生产数据管道的一部分并将其直接用于站点上的特定功能。 这些用途包括： 搜索相关性 在活动的数据流中获取流行的或是共性的推荐 广告定位和数据报告 防止垃圾邮件或未经授权的数据抓取等滥用行为的安全应用程序 用户的“朋友”或“联系人”获取用户状态的更新或新闻的功能 这种日志数据的产生、实时使用给数据系统带来了新的挑战，因为它的容量比“真实”数据大几个数量级。 例如，搜索、推荐和广告通常需要计算精确的点击率，这不仅会为每个用户点击生成日志记录，而且还会为每个页面上未点击的几十个项目生成日志记录。 中国移动每天收集 5-8 TB 的通话记录 [11]，而 Facebook 则收集了近 6 TB 的各种用户活动事件 [12]。 许多用于处理此类数据的早期系统依赖于从生产服务器上物理抓取日志文件进行分析。 近年来，已经构建了几个专门的分布式日志聚合器，包括 Facebook 的 Scribe [6]、雅虎的 Data Highway [4] 和 Cloudera 的 Flume [3]。 这些系统主要设计用于收集日志数据并将其加载到数据仓库或 Hadoop [8] 中以供离线使用。 我们为日志处理构建了一个新的消息系统，称为 Kafka [18]，它结合了传统日志聚合和消息系统的优点。 一方面，Kafka 是分布式和可扩展的，以此来提供高吞吐量。 另一方面，Kafka 提供了一个类似于消息系统的 API，允许应用程序实时消费日志事件。 Kafka 已经开源并在 LinkedIn 的生产环境中成功使用了 6 个多月。 它极大地简化了我们的基础设施，使我们可以利用一个软件来在线和离线消费所有类型的日志数据。 本文的其余部分安排如下。 我们在第 2 节中会重新审视传统的日志聚合和消息系统。 我们在第 3 节中会描述 Kafka 的架构及其关键设计原则。 我们在第 4 节中描述了我们在 LinkedIn 上部署的 Kafka，在第 5 节中描述了 Kafka 的性能。 我们在第 6 节中进行总结并讨论未来的工作。 2 相关内容 传统的企业消息系统 [1][7][15][17] 已经存在了很长时间，并且经常作为处理异步数据流的事件总线发挥着关键作用。 但是，有几个原因导致它们往往不适合处理日志。 首先，企业系统提供的功能与目的不匹配。 这些系统通常专注于提供丰富的交付保证。 例如，IBM Websphere MQ [7] 具有事务支持，允许应用程序以原子方式将消息插入多个队列。 JMS [14] 规范允许在消费后确认每个单独的消息，然而这些消息可能是无序的。 这种交付保证对于收集日志数据来说通常是多余的。 例如，偶尔丢失一些网页浏览事件当然不会造成世界末日。 这些不需要的功能往往会增加 API 和这些系统的底层实现的复杂性。 其次，许多系统并没有像它们的主要设计约束那样强烈关注吞吐量。 例如，JMS 没有在单个请求中批处理多个消息的 API。 这意味着每条消息都需要完整的 TCP/IP 往返，对于我们来说这样的吞吐量无法满足需求。 再次，这些系统在分布式支持方面很弱。 没有简单的方法可以在多台机器上分区和存储消息。 最后，许多消息系统假设消息几乎立即被消费，因此未消费的消息队列总是相当小。 如果允许消息累积，它们的性能会显着下降，对于离线消费者(例如数据仓库应用程序)执行定期大负载而不是连续消费的情况就是如此。 在过去的几年中，已经有了许多专门的日志聚合器。 Facebook 使用名为 Scribe 的系统。 每个前端机器都可以通过 Socket 将日志数据发送到一组 Scribe 机器。 每台 Scribe 机器都会聚合日志条目并定期将它们转储到 HDFS [9] 或 NFS 设备。 Yahoo 的 data highway 也有类似的数据流。 一组机器聚合来自客户端的事件并推出“分钟”级别的文件，然后将这些文件添加到 HDFS。 Flume 是 Cloudera 开发的一个相对较新的日志聚合器。 它支持可扩展的“管道”和“接收器”，可以非常灵活的处理流式日志数据。 它还具有更多集成的分布式支持。 然而，这些系统中的大多数都是为离线使用日志数据而构建的，并且经常向消费者不必要地公开实现细节(例如“分钟”级别的文件)。 此外，它们中的大多数使用“推送”模型，其中代理将数据转发给消费者。 在 LinkedIn，我们发现“拉取”模型更适合我们的应用程序，因为每个消费者都可以以它可以维持的最大速率检索消息，并避免被推送速度超过其处理速度的消息淹没。 拉动模型还可以轻松地回滚消费者，我们将在第 3.2 节末尾讨论这一益处。 最近 Yahoo 研究开发了一种新的分布式发布/订阅系统，称为 HedWig [13]。 HedWig 具有高度可扩展性和可用性，并提供强大的耐用性保证。 但是，它主要用于传输数据存储的提交日志。 3 Kafka 的架构和设计原则 由于现有系统的限制，我们开发了一个新的基于消息的日志聚合器 Kafka。 我们先介绍 Kafka 的基本概念。 特定类型的消息流由主题定义。 生产者可以向主题发布消息。 然后将发布的消息存储在一组称为 Broker 的服务器中。 消费者可以从 Broker 订阅一个或多个主题，并通过从 Broker 拉取数据来消费订阅的消息。 消息传递在概念上很简单，我们试图让 Kafka API 同样简单来反映这一点。 我们没有展示确切的 API，而是展示了一些示例代码来展示如何使用 API。 下面给出了生产者的示例代码。 消息载荷被定义为仅仅可以包含字节。 用户可以选择她最喜欢的序列化方法来对消息进行编码。 为了提高效率，生产者可以在单个发布请求中发送一组消息。 生产者代码样例： 1234producer = new Producer(…);message = new Message(“test message str”.getBytes());set = new MessageSet(message);producer.send(“topic1”, set); 要订阅一个主题，消费者首先要为该主题创建一个或多个消息流。 发布到该主题的消息将均匀分布到这些子流中。 Kafka 如何分发消息的细节将在 3.2 节中描述。 每个消息流都为正在生成的连续消息流提供了一个迭代器接口。 然后，消费者遍历流中的每条消息并处理消息的有效负载。 与传统迭代器不同，消息流迭代器永远不会终止。 如果当前没有更多消息要消费，迭代器会阻塞，直到新消息发布到主题。 我们支持多个消费者共同消费一个主题中所有消息的单个副本的点对点交付模型，以及多个消费者各自检索自己的主题副本的发布/订阅模型。 生产者代码样例： 12345streams[] = Consumer.createMessageStreams(“topic1”, 1)for (message : streams[0]) &#123; bytes = message.payload(); // do something with the bytes&#125; Kafka 的整体架构如 图 1 所示。 由于 Kafka 本质上是分布式的，一个 Kafka 集群通常由多个 Broker 组成。 为了平衡负载，一个主题被分成多个分区，每个 Broker 存储一个或多个分区。 多个生产者和消费者可以同时发布和检索消息。 在第 3.1 节中，我们描述了 Broker 上单个分区的布局以及我们选择的一些设计选择，以提高访问分区的效率。 在第 3.2 节中，我们描述了生产者和消费者如何在分布式环境中与多个 Broker 交互。 在第 3.2 节中，我们描述了 Kafka 的交付保证。 3.1 单个分区的效率 我们在 Kafka 中做出了一些决定，以使系统高效。 简单的存储： Kafka 有一个非常简单的存储布局。 一个主题的每个分区对应一个逻辑日志。 在物理上，日志被实现为一组大小大致相同(例如，1 GB)的分段文件。 每次生产者向分区发布消息时，Broker 只需将消息附加到最后一个分段文件。 为了获得更好的性能，我们仅在发布了可配置数量的消息或经过一定时间后才将分段文件刷新到磁盘。 消息只有在刷新后才会暴露给消费者。 与典型的消息系统不同，存储在 Kafka 中的消息没有明确的消息 ID。 相反，每条消息都由其在日志中的逻辑偏移量(offset)寻址。 这避免了维护将消息 ID 映射到实际消息位置的辅助、搜索密集型随机访问索引结构的开销。 请注意，我们的消息 ID 是正在增加但不是连续的。 要计算下一条消息的 ID，我们必须将当前消息的长度加到它的 ID 上。 从现在开始，我们将交替使用消息 ID 和偏移量。 消费者总是按顺序消费来自特定分区的消息。 如果消费者确认特定的消息偏移量，则意味着消费者已收到分区中该偏移量之前的所有消息。 在后台，消费者向 Broker 发出异步拉取请求，以准备好数据缓冲区供应用程序使用。 每个拉取请求都包含开始消费的消息的偏移量和可接受的要获取的字节数。 每个 Broker 在内存中保存一个排序的偏移列表，包括每个分段文件中第一条消息的偏移。 Broker 通过查找偏移量列表来定位请求消息所在的分段文件，并将数据发送回消费者。 消费者收到一条消息后，计算下一条要消费的消息的偏移量，并在下一个拉取请求中使用它。 Kafka 日志和内存中索引的布局如图 2 所示。 其中每个框显示消息的偏移量。 传输效率： 我们非常小心地将数据传入和传出 Kafka。 早些时候，我们已经说明了生产者可以在单个发送请求中提交一组消息。 尽管最终消费者 API 一次迭代一条消息，但在后台，来自消费者的每个拉取请求也会检索到特定大小(通常为数百 KB)的多条消息。 我们做出的另一个非常规选择是避免在 Kafka 层的内存中显式缓存消息。 相反，我们依赖底层文件系统页面缓存。 这具有避免双缓冲的主要好处——消息仅缓存在页面缓存中。 即使在 Broker 进程重新启动时，这也具有保留热缓存的额外好处。 由于 Kafka 根本不缓存进程中的消息，因此它在垃圾收集内存方面的开销很小，这使得在基于 VM 的语言中的高效实现变得可行。 最后，由于生产者和消费者都按顺序访问分段文件，消费者通常会落后于生产者少量，因此正常的操作系统缓存启发式方法非常有效 (特别是直写缓存和预读)。 我们发现生产和消费都具有与数据大小成线性关系的一致性能，最高可达数 TB 的数据。 此外，我们优化了消费者的网络访问。 Kafka 是一个多订阅者系统，一条消息可能会被不同的消费者应用程序多次消费。 将字节从本地文件发送到远程 Socket 的典型方法包括以下步骤： 从存储介质中读取数据到 OS 中的页面缓存 将页缓存中的数据复制到应用程序缓冲区中 将应用程序缓冲区复制到另一个内核缓冲区 将内核缓冲区发送到 Socket 这包括 4 个数据复制和 2 个系统调用。 在 Linux 和其他 Unix 操作系统上，存在一个发送文件 API [5]，可以直接将字节从文件通道传输到 Socket 通道。 这通常避免了在步骤(2)和(3)中引入的 2 个副本和 1 个系统调用。 Kafka 利用发送文件 API 有效地将日志分段文件中的字节从 Broker 传送到消费者。 无状态 Broker： 与大多数其他消息系统不同，在 Kafka 中，每个消费者消费了多少的信息不是由 Broker 维护，而是由消费者自己维护。 这样的设计降低了 Broker 的很多复杂性和开销。 然而，这使得删除消息变得棘手，因为代理不知道是否所有订阅者都消费了该消息。 Kafka 通过为保留策略使用简单的基于时间的 SLA 解决了这个问题。 如果消息在代理中保留的时间超过一定时间(通常为 7 天)，则会自动删除。 该解决方案在实践中运行良好。 大多数消费者，包括线下消费者，每天、每小时或实时完成消费。 Kafka 的性能不会随着数据量的增加而降低，这一事实使得这种长时间的保留是可行的。 这种设计有一个重要的附带好处。 消费者可以故意倒回到旧的偏移量并重新消费数据。 这违反了队列的共同契约，但被证明是许多消费者的基本特征。 例如，当消费者的应用程序逻辑出现错误时，应用程序可以在错误修复后重新播放某些消息。 这对于将 ETL 数据加载到我们的数据仓库或 Hadoop 系统中尤为重要。 作为另一个例子，消耗的数据可以仅定期刷新到持久存储(例如，全文索引器)。 如果消费者崩溃，则未刷新的数据将丢失。 在这种情况下，消费者可以检查未刷新消息的最小偏移量，并在重新启动时从该偏移量重新消费。 我们注意到，在拉模型中比推模型更容易支持回退消费者。 3.2 分布式协调 我们现在描述生产者和消费者在分布式环境中的行为。 每个生产者可以将消息发布到随机选择的分区或由分区键和分区函数语义确定分区。 我们将关注消费者如何与 Broker 互动。 Kafka 有消费组的概念。 每个消费组由一个或多个共同消费一组订阅主题的消费者组成，即每条消息仅传递给组内的一个消费者。 不同的消费者组各自独立地使用完整的订阅消息集，并且不需要跨消费组进行协调。 同一组内的消费者可以在不同的进程或不同的机器上。 我们的目标是将存储在 Broker 中的消息平均分配给消费者，而不会引入过多的协调开销。 我们的第一个决定是使主题内的分区成为并行度的最小单位。 这意味着在任何给定时间，来自一个分区的所有消息仅由每个消费者组中的一个消费者消费。 如果我们允许多个消费者同时消费一个分区，他们将不得不协调谁消费什么消息，这需要锁定和状态维护开销。 相比之下，在我们的设计中，消费流程只需要在消费者重新平衡负载时进行协调，这是一种罕见的事件。 为了真正平衡负载，我们需要一个主题中的分区比每个组中的消费者多得多。 我们可以通过对主题进行过度分区来轻松实现这一点。 我们做出的第二个决定是没有中央 “master” 节点，而是让消费者以去中心化的方式相互协调。 添加 master 会使系统复杂化，因为我们必须进一步担心 master 故障。 为了促进协调，我们采用了高度可用的共识服务 ZooKeeper [10]。 ZooKeeper 有一个非常简单的文件系统，类似于 API。 它可以创建路径、设置路径的值、读取路径的值、删除路径以及列出路径的子项。 此外 ZooKeeper 还做了一些更有趣的事情： 可以在路径上注册一个观察者，并在路径的子路径或路径的值发生变化时得到通知 可以将路径创建为 ephemeral (与持久性相反,临时路径)，这意味着如果创建的客户端消失了，ZooKeeper 服务器会自动删除该路径 ZooKeeper 将其数据复制到多个服务器，这使得数据高度可靠和可用 Kafka 使用 ZooKeeper 完成以下任务： 检测 Broker 和消费者的添加和删除 当上述事件发生时，在每个消费者中触发重新平衡过程 维护消费关系并跟踪每个分区的消耗偏移量 具体来说，当每个 Broker 或消费者启动时，它会将其信息存储在 ZooKeeper 中的 Broker 或消费者注册表中。 Broker 注册表包含 Broker 的主机名和端口，以及存储在其上的一组主题和分区。 消费者注册表包括消费者所属的消费组及其订阅的主题集。 每个消费组都与 ZooKeeper 中的一个所有权注册表和一个偏移注册表相关联。 所有权注册表对于每个订阅的分区都有一个路径，路径值是当前从该分区消费的消费者的 ID (我们使用消费者拥有该分区的术语)。 偏移注册表为每个订阅的分区存储分区中最后消费的消息的偏移量。 ZooKeeper 中创建的路径对于 Broker 注册表、消费者注册表和所有权注册表是短暂的，对于偏移注册表是持久的。 如果 Broker 故障，其上的所有分区都会自动从 Broker 注册表中删除。 消费者的失败会导致它丢失其在消费者注册表中的条目以及它在所有权注册表中拥有的所有分区。 每个消费者都会在 Broker 注册中心和消费者注册中心注册一个 ZooKeeper 观察者，并且会在 Broker 集或消费者组发生变化时收到通知。 算法 1: 在 G 消费组中重新平衡 Ci 的过程 for (每个 Ci 订阅的话题 T) { 在所有权注册表中移除被 Ci 拥有的分片(partition) 从 ZooKeeper 读取代理和消费者注册表 计算 PT = 主题 T 下所有 Broker 中可用的分区 计算 CT = G 消费组中订阅主题 T 的所有消费者 对 PT 和 CT 进行排序 令 j 为 Ci 在 CT 中的索引位置，令 N = | PT | / | C T | 将 PT 中从 j*N 到 (j+1)*N-1 的分区分配给消费者 Ci for (每个分配的分片 p){ 在所有权注册表中将 p 的所有权设定为 Ci 令 Op = 存储在偏移注册表中的分区 p 的偏移量 调用一个线程从偏移量 Op 中拉取分区 p 中的数据 } } 在消费者的初始启动期间，或者当消费者通过观察者收到有关 Broker/消费者更改的通知时，消费者会启动重新平衡过程以确定它应该从中消费的新分区子集。 该过程在算法 1 中描述。 通过从 ZooKeeper 读取 Broker 和消费者注册表， 消费者首先计算每个订阅主题 T 可用的分区集( PT )和订阅 T 的消费者集( CT )。 然后将 PT 范围划分为 |CT| 块并确定性地选择一个块来拥有。 对于消费者选择的每个分区，它在所有权注册表中将自己写入为分区的新所有者。 最后，消费者开始一个线程从每个拥有的分区中提取数据，从存储在偏移注册表中的偏移开始。 当消息从分区中提取时，消费者会定期更新偏移注册表中最新使用的偏移量。 注：在最新版本 2.8.0 中已经开始尝试逐步移除 ZooKeeper 当一个消费组中有多个消费者时，他们每个人都会收到 Broker 或消费者更改的通知。 但是，通知可能会在不同的事件送达给每个消费者。 因此，有可能一个消费者试图获得另一个消费者仍然拥有的分区的所有权。 发生这种情况时，第一个使用者只需释放其当前拥有的所有分区，稍等片刻并重试重新平衡过程。 在实践中，重新平衡过程通常只在几次重试后就稳定下来。 创建新的消费者组时，偏移注册表中没有可用的偏移。 在这种情况下，消费者将使用我们在 Broker 上提供的 API 从每个订阅分区上可用的最小或最大偏移量(取决于配置)开始消费。 3.3 送达保证 一般来说，Kafka 只保证至少一次交付。 确当一次交付通常需要两阶段提交，对于我们的应用程序来说不是必需的。 大多数情况下，一条消息只传递给每个消费者组一次。 然而，在消费者进程在没有干净关闭的情况下崩溃的情况下，接管失败消费者拥有的那些分区的消费者进程可能会收到一些重复的消息， 这些消息在成功提交给 ZooKeeper 的最后一个偏移之后。 如果应用程序关心重复，它必须添加自己的重复数据删除逻辑，要么使用我们返回给消费者的偏移量，要么使用消息中的某个唯一键。 这通常是比使用两阶段提交更具成本效益的方法。 Kafka 保证来自单个分区的消息按顺序传递给消费者。 但是，无法保证来自不同分区的消息的顺序。 为了避免日志损坏，Kafka 为日志中的每条消息存储了一个 CRC。 如果 Broker 上出现任何 I/O 错误，Kafka 会运行恢复过程以删除那些具有不一致 CRC 的消息。 在消息级别拥有 CRC 还允许我们在生成或使用消息后检查网络错误。 如果 Broker 宕机，存储在其上的任何尚未消费的消息将变得不可用。 如果 Broker 上的存储系统永久损坏，则任何未使用的消息将永远丢失。 未来，我们计划在 Kafka 中添加内置复制功能，将每条消息冗余存储在多个代理上。 注：副本功能已经实现，默认三个副本，而送达保证消费者部分描述详细内容参见 官方文档 4 LinkedIn 使用 Kafka 的方式 在本节中，我们将描述我们如何在 LinkedIn 上使用 Kafka。 图 3 显示了我们部署的简化版本。 我们有一个 Kafka 集群，与我们面向用户的服务运行的每个数据中心位于同一位置。 前端服务生成各种日志数据，批量发布到本地的 Kafka Broker。 我们依靠硬件负载平衡器将发布请求均匀地分发到一组 Kafka Broker。 Kafka 的在线消费者在同一数据中心内的服务中运行。 我们还在单独的数据中心部署了一个 Kafka 集群，用于离线分析，地理位置靠近我们的 Hadoop 集群和其他数据仓库基础设施。 这个 Kafka 实例运行一组嵌入式消费者，以从实时数据中心的 Kafka 实例中提取数据。 然后我们运行数据加载作业，将数据从这个 Kafka 副本集群中提取到 Hadoop 和我们的数据仓库中，在那里我们对数据运行各种报告作业和分析过程。 我们还使用这个 Kafka 集群进行原型设计，并能够针对原始事件流运行简单的脚本以进行临时查询。 无需过多调整，整个管道的端到端延迟平均约为 10 秒，足以满足我们的要求。 目前，Kafka 每天累积数百 GB 的数据和接近 10 亿条消息，随着我们完成对遗留系统的转换以利用 Kafka，我们预计这些数据会显着增长。 将来会添加更多类型的消息。 当操作人员启动或停止 Broker 进行软件或硬件维护时，重新平衡过程能够自动重定向消耗。 我们的跟踪还包括一个审计系统，以验证整个管道没有数据丢失。 为方便起见，每条消息在生成时都带有时间戳和服务器名称。 我们检测每个生产者，使其定期生成一个监控事件，该事件记录该生产者在固定时间窗口内为每个主题发布的消息数量。 生产者在单独的主题中将监控事件发布到 Kafka。 然后，消费者可以计算他们从给定主题收到的消息数量，并使用监控事件验证这些计数以验证数据的正确性。 加载到 Hadoop 集群是通过实现一种特殊的 Kafka 输入格式来完成的，该格式允许 MapReduce 作业直接从 Kafka 读取数据。 MapReduce 作业加载原始数据，然后对其进行分组和压缩，以便将来进行高效处理。 消息偏移的无状态 Broker 和客户端存储在这里再次发挥作用，允许 MapReduce 任务管理(允许任务失败并重新启动)以自然的方式处理数据负载， 而不会在发生任务重启的情况时复制或丢失消息。 只有在作业成功完成后，数据和偏移量才会存储在 HDFS 中。 我们选择使用 Avro [2] 作为我们的序列化协议，因为它高效且支持模式演化。 对于每条消息，我们将其 Avro 架构的 ID 和序列化字节存储在有效负载中。 这种模式允许我们强制执行契约以确保数据生产者和消费者之间的兼容性。 我们使用轻量级架构注册服务将架构 ID 映射到实际架构。 当消费者收到一条消息时，它会在模式注册表中查找以检索模式，该模式用于将字节解码为对象(每个模式只需进行一次此查找，因为值是不可变的)。 5 实验结果 我们进行了一项实验研究，比较了 Kafka 与 Apache ActiveMQ v5.4 [1](一种流行的 JMS 开源实现)和 RabbitMQ v2.4 [16] (一种以其性能而闻名的消息系统)的性能。 我们使用了 ActiveMQ 的默认持久消息存储 KahaDB。 虽然这里没有介绍，但我们也测试了一个替代的 AMQ 消息存储，发现它的性能与 KahaDB 非常相似。 只要有可能，我们都会尝试在所有系统中使用可比较的设置。 我们在 2 台 Linux 机器上进行了实验，每台机器都有 8 个 2GHz 内核、16GB 内存、6 个 RAID 10 磁盘。 两台机器通过 1Gb 网络链接连接。 其中一台机器用作 Broker，另一台机器用作生产者或消费者。 生产者测试： 我们将所有系统中的 Broker 配置为将消息异步刷新到其持久性存储。 对于每个系统，我们运行一个生产者来发布总共 1000 万条消息，每条消息 200 字节。 我们将 Kafka 生产者配置为分批发送大小为 1 和 50 的消息。 ActiveMQ 和 RabbitMQ 似乎没有一种简单的方法来批处理消息，我们假设它使用的批处理大小为 1。 结果如图 4 所示。x 轴表示随着时间的推移发送到代理的数据量(以 MB 为单位)，y 轴对应于每秒消息的生产者吞吐量。 平均而言，Kafka 可以分别以每秒 50,000 和 400,000 条消息的速率发布消息，批量大小分别为 1 和 50。 这些数字比 ActiveMQ 高出几个数量级，至少比 RabbitMQ 高出2倍。 Kafka 表现更好的原因有几个。 首先，Kafka 生产者目前不会等待来自 Broker 的确认，而是以代理可以处理的速度发送消息。 这显着增加了发布者的吞吐量。 批量处理大小设置为 50 时，单个 Kafka 生产者几乎饱和了生产者和代理之间的 1Gb 链接。 这是对日志聚合情况的有效优化，因为数据必须异步发送以避免在实时流量服务中引入任何延迟。 我们注意到，生产者不进行确认监测，就不能保证每条发布的消息实际上都被 Broker 收到了。 对于许多类型的日志数据，只要丢弃的消息数量相对较少，就需要用持久性来换取吞吐量。 但是，我们确实计划在未来解决更关键数据的持久性问题。 注：确认监测参数可以进行配置，详情参见 官方文档 消费者测试： 在第二个实验中，我们测试了消费者的表现。 同样，对于所有系统，我们使用一个消费者来检索总共 1000 万条消息。 我们配置了所有系统，以便每个拉取请求应该预取大约相同数量的数据——最多 1000 条消息或大约 200 KB。 对于 ActiveMQ 和 RabbitMQ，我们将消费者确认模式设置为自动。 由于所有消息都能暂时存放在内存中，因此所有系统都从底层文件系统的页面缓存或一些内存缓冲区中提供数据。 结果如图 5 所示。 Kafka 平均每秒消费 22,000 条消息，是 ActiveMQ 和 RabbitMQ 的 4 倍以上。 我们可以想到几个原因。 首先，由于 Kafka 具有更高效的存储格式，因此在 Kafka 中从 Broker 传输到消费者的字节更少。 其次，ActiveMQ 和 RabbitMQ 中的 broker 都必须维护每条消息的传递状态。 我们观察到，在此测试期间，一个 ActiveMQ 线程正忙于将 KahaDB 页面写入磁盘。 相比之下，Kafka 代理上没有磁盘写入活动。 最后，通过使用文件发送 API，Kafka 减少了传输开销。 我们通过指出实验的目的不是表明其他消息传递系统不如 Kafka 来结束本节。 毕竟，ActiveMQ 和 RabbitMQ 的功能都比 Kafka 多。 重点是说明专用系统可以实现的潜在性能增益。 6 结论与未来的工作 我们提出了一个名为 Kafka 的新系统，用于处理大量日志数据流。 与消息传递系统一样，Kafka 采用基于拉取的消费模型，该模型允许应用程序以自己的速率消费数据，并在需要时回滚消费。 通过专注于日志处理应用程序，Kafka 实现了比传统消息传递系统更高的吞吐量。 它还提供集成的分布式支持并且可以横向扩展。 我们一直在 LinkedIn 上成功地将 Kafka 用于离线和在线应用程序。 未来有很多方向是我们想要的。 首先，我们计划添加跨多个 Broker 的内置消息复制，即使在不可恢复的机器故障的情况下也能保证持久性和数据可用性。 我们希望同时支持异步和同步复制模型，以允许在生产者延迟和提供的保证强度之间进行一些权衡。 应用程序可以根据其对持久性、可用性和吞吐量的要求选择正确的冗余级别。 其次，我们想在 Kafka 中添加一些流处理能力。 从 Kafka 检索消息后，实时应用程序通常会执行类似的操作，例如基于窗口的计数并将每条消息与二级存储中的记录或另一个流中的消息连接起来。 在最低级别，这是通过在发布期间在连接键上对消息进行语义分区来支持的，以便使用特定键发送的所有消息都转到同一分区，从而到达单个消费者进程。 这为跨消费者机器集群处理分布式流提供了基础。 最重要的是，我们认为一个有用的流处理程序库，提供例如不同的窗口函数或连接技术，将对此类应用程序有很大益处。 7 参考资料 [1] http://activemq.apache.org/ [2] http://avro.apache.org/ [3] Cloudera’s Flume, https://github.com/cloudera/flume [4] http://developer.yahoo.com/blogs/hadoop/posts/2010/06/enabling_hadoop_batch_processi_1/ [5] Efficient data transfer through zero copy: https://www.ibm.com/developerworks/linux/library/jzerocopy/ [6] Facebook’s Scribe, http://www.facebook.com/note.php?note_id=32008268919 [7] IBM Websphere MQ: http://www01.ibm.com/software/integration/wmq/ [8] http://hadoop.apache.org/ [9] http://hadoop.apache.org/hdfs/ [10] http://hadoop.apache.org/zookeeper/ [11] http://www.slideshare.net/cloudera/hw09-hadoop-baseddata-mining-platform-for-the-telecom-industry [12] http://www.slideshare.net/prasadc/hive-percona-2009 [13] https://issues.apache.org/jira/browse/ZOOKEEPER-775 [14] JAVA Message Service:http://download.oracle.com/javaee/1.3/jms/tutorial/1_3_1-fcs/doc/jms_tutorialTOC.html. [15] Oracle Enterprise Messaging Service: http://www.oracle.com/technetwork/middleware/ias/index093455.html [16] http://www.rabbitmq.com/ [17] TIBCO Enterprise Message Service: http://www.tibco.com/products/soa/messaging/ [18] Kafka, http://sna-projects.com/kafka/","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"Bigtable A Distributed Storage System for Structured Data 中文翻译版","slug":"treatise/bigtable_a_distributed_storage_system_for_structured_data","date":"2021-06-10T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/bigtable_a_distributed_storage_system_for_structured_data/","permalink":"https://wangqian0306.github.io/2021/bigtable_a_distributed_storage_system_for_structured_data/","excerpt":"","text":"Bigtable: A Distributed Storage System for Structured Data 中文翻译版 作者： Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber 摘要 Bigtable 是一个分布式的结构化数据存储系统，它被设计用来处理海量数据：通常是分布在数千台普通服务器上的 PB 级的数据。 Google 的很多项目使用 Bigtable 存储数据，包括 Web 索引、Google Earth、Google Finance。 这些应用对 Bigtable 提出的要求差异非常大，无论是在数据量上(从 URL 到网页到卫星图像)还是在响应速度上(从后端的批量处理到实时数据服务)。 尽管应用需求差异很大，但是，针对 Google 的这些产品，Bigtable 还是成功的提供了一个灵活的、高性能的解决方案。 本论文描述了 Bigtable 提供的简单的数据模型，利用这个模型，用户可以动态的控制数据的分布和格式；我们还将描述 Bigtable 的设计和实现。 1 引言 在过去的两年半中我们在 Google 设计实现并部署了一个分布式系统来管理结构数据，这个系统被称为 Bigtable。 Bigtable 被设计用来可靠并可扩展的在数千台设备上存储 PB 级别的数据。 Bigtable 完成了一下几项目标：广泛的适用性、可伸缩性、高性能和高可用性。 Bigtable 被 16 项 Google 的产品和项目使用，其中包含 Google Analytics, Google Finance, Orkut, Personalized Search, Writely, 和 Google Earth。 这些产品将 Bigtable 用于各种苛刻的工作负载，例如面向吞吐量的批处理作业和对延迟敏感的向最终用户提供数据服务。 这些产品使用的 Bigtable 集群的配置范围很广，从少数服务器到数千台服务器，最多可存储几百 TB 的数据。 在许多方面，Bigtable 类似于一个数据库：它与数据库共享许多实现策略。 并行数据库 [14] 和内存数据库 [13] 已经实现了可伸缩性和高性能，但是 Bigtable 提供了与这类系统不同的接口。 Bigtable 不支持完整的关系数据模型；相反，它为客户机提供了一个简单的数据模型，支持对数据布局和格式的动态控制， 并允许客户机对底层存储中表示的数据的局部性属性进行推理。 数据的索引使用可以是任意的行和列的字符串。 Bigtable 还将数据视为字符串，尽管客户机通常将各种形式的结构化和半结构化数据序列化到这些字符串中。 通过仔细选择数据的模式，客户可以控制数据的位置相关性。 最后，可以通过 BigTable 的模式参数来控制数据是存放在内存中、还是硬盘上。 第 2 节更详细地描述了数据模型，第 3 节概述了客户端 API。 第 4 节简要描述了 Bigtable 所依赖的 Google 基础设施。 第 5 节描述了 Bigtable 实现的基本原理， 第 6 节描述了我们为提高 Bigtable 性能所做的一些改进。 第 7 节提供了 Bigtable 性能的数据。 我们在第 8 节中描述了 Bigtable 在 Google 内部使用的例子， 并在第 9 节中讨论了我们在设计和支持 Bigtable 时学到的一些经验教训。 最后，第 10 节描述了相关工作，第 11 节给出了我们的结论。 2 数据模型 Bigtable是一个稀疏的、分布式的、持久的多维排序的 Map。 这个 Map 使用 row key, column key, timestamp 作为索引；每个值在 Map 中都是一段未解释的字节数组。 1(row:string, column:string, time:int64) → string 注：行的名称是逆序的 URL。contents column family 包含了整个页面的内容，anchor column family 包含了引用(anchor HTML &lt;a&gt;)本页的所有文本。 CNN 的主页被 Sports Illustrated 和 MY-look主页引用，所以这一行就会有两个列分别是 anchor:cnnsi.com 和 anchor:my.look.ca 。 每个 anchor 单元含有一个版本，content 列含有三个版本，时间戳分别为 t3, t5 和 t6 。 在研究了 Bigtable 系统的各种潜在用途之后，我们确定了这个数据模型。 作为一个具体的例子，驱动我们的一些设计决策，假设我们想保留一个网页和相关信息的大集合的副本， 可以由许多不同的项目使用；让我们把这个特殊的表称为 Webtable 。 在 Webtable 中，我们将使用 url 作为行键，将 Web 页面的各个方面作为列名， 并将 Web 页面的内容存储在 contents:column 中，在获取这些内容时将其存储在时间戳下，如图 1 所示。 行 表中的 row key 是任意字符串(当前大小高达 64 KB, 但对于大多数用户来说 10-100 bytes 应该是一个典型的大小) 在同一个 row key 下读取或者写入的每个数据都是具有原子性的(不管在该行中读取或写入不同列的数量)， 这是一个设计决策，可以让客户很容易的理解程序在对同一个行进行并发更新操作时的行为。 Bigtable 按 row key 的字典顺序维护数据。 表的行范围是动态分区的。 每行范围称为 Tablet，它是分布和负载平衡的单位。 因此，短行范围的读取是有效的，并且通常只需要与少量机器进行通信。 客户机可以通过选择其行键来利用此属性，以便获得数据访问的良好位置。 例如，在 Webtable 中，通过反转 URL 的主机名，可以将同一域中的页面分组到相邻的行中。 例如，我们将 maps.google.com/index.html 的数据存储在 com.google.maps/index.html 键下方。 将来自同一个域的页面彼此相邻地存储在一起会使某些主机和领域分析更有效。 Column Families column key 的集合被称为 column families，这些集合构成了访问控制的基本单元。 存储在同一 column family 中的所有数据通常都是相同类型的(我们将同一 column family 中的数据压缩在一起)。 必须先创建 column family 才能在其下存储数据，创建 column family 后可以使用其下的所有 column key。 我们的目的是表中不同 column family 的数量要少 (最多几百个)，并且其在操作过程中很少改变。 与之相对应的，一张表可以有无限多个列。 column key 使用以下语法命名：family:qualifier。 column family 的名称必须是可以打印的，但 qualifier 可以是任意字符串。 Webtable 的一个示例 column family 是 language，它存储编写网页时使用的语言。 我们在 language column family 中只使用一个 column key，它存储每个网页的语言 ID。 这个表的另一个有用的 column family 是 anchor；这个 column family 中的每个 column key 表示一个anchor，如图 1 所示。 访问控制以及磁盘和内存统计都是在 column family 级别执行的。 在我们的 Webtable 示例中，这些控件允许我们管理几种不同类型的应用程序： 一些添加新的基础数据，一些读取基础数据并创建派生 column family， 还有一些只允许查看现有数据(出于隐私原因，甚至可能不允许查看所有现有的 column family)。 时间戳 Bigtable 中的每个单元格可以包含相同数据的多个版本；这些版本按时间戳进行索引。 Bigtable 的时间戳是 64 位整型。 它们可以由 Bigtable 分配，在这种情况下，它们以微秒表示“实时”，也可以由客户端应用程序显式分配。 需要避免冲突的应用程序必须自己生成唯一的时间戳。 单元格的不同版本按时间戳降序存储，以便可以先读取最新版本。 为了使版本化数据的管理不那么繁重，我们支持每个 column family 有两个设置参数，触发 Bigtable 自动垃圾收集机制。 客户端可以指定只保留单元格的最后 n 个版本，或者只保留足够新的版本 (例如，只保留在过去7天内写入的值)。 在我们的 Webtable 示例中，我们将 contents 列中存储的已爬网页面的时间戳设置为实际爬网这些页面版本的时间。 上面描述的垃圾收集机制允许我们只保留每个页面的最新三个版本。 3 API Bigtable API 提供了创建和删除表和 column family 的函数。 它还提供了更改集群、表和 column family 元数据的功能，例如访问控制权限。 123456789// Open the tableTable *T = OpenOrDie(&quot;/bigtable/web/webtable&quot;);// Write a new anchor and delete an old anchorRowMutation r1(T, &quot;com.cnn.www&quot;);r1.Set(&quot;anchor:www.c-span.org&quot;, &quot;CNN&quot;);r1.Delete(&quot;anchor:www.abc.com&quot;);Operation op;Apply(&amp;op, &amp;r1); 图 2：写入数据至 Bigtable 客户机应用程序可以在 Bigtable 中写入或删除值，从单个行中查找值，或者迭代表中的数据子集。 图 2 显示了使用 RowMutation 抽象来执行一系列更新的 C++ 代码。 (为了简短起见，不相关的细节被省略了。) 调用 Apply 函数对 Webtable 进行了一个原子修改操作：它为 www.cnn.com 增加了一个 anchor，同时删除了另外一个 anchor。 123456789101112Scanner scanner(T);ScanStream *stream;stream = scanner.FetchColumnFamily(&quot;anchor&quot;);stream-&gt;SetReturnAllVersions();scanner.Lookup(&quot;com.cnn.www&quot;);for (; !stream-&gt;Done(); stream-&gt;Next()) &#123; printf(&quot;%s %s %lld %s\\n&quot;, scanner.RowName(), stream-&gt;ColumnName(), stream-&gt;MicroTimestamp(), stream-&gt;Value());&#125; 图 3：从 Bigtable 读取数据 图 3 为 C++ 代码，它使用扫描仪抽象来迭代特定行中的所有 anchor。 客户机可以迭代多个 column family，并且有几种机制限制扫描产生的行、列和时间戳。 例如，我们可以将上面的扫描限制为只生成列与正则表达式 anchor:*.cnn.com 匹配的 anchor，或者只生成时间戳在当前时间 10 天内的 anchor。 Bigtable 支持了一系列特性让用户可以对数据进行复杂的处理。 首先，Bigtable 支持单行的事务，这样一来就支持了对单个 row key 原子性的读取修改写入序列操作。 Bigtable 目前并不支持跨 row key 的事务，不过它提供了一个接口，用于在客户端进行批处理写操作。 其次，Bigtable 允许将单元格用作整数计数器。 最后，Bigtable 支持在服务器的地址空间中执行客户端提供的脚本。 这些脚本是用 Google 为处理数据而开发的一种语言 Sawzall 编写的 [28]。 目前，我们基于 Sawzall 的 API 不允许客户端脚本写回 Bigtable，但是它允许各种形式的数据转换、基于任意表达式的过滤以及通过各种操作符进行汇总。 Bigtable 可以和 MapReduce [12] 一起使用，MapReduce 是 Google 开发的大规模并行计算框架。 我们已经开发了一些 Wrapper 类，通过使用这些 Wrapper 类，Bigtable 可以作为 MapReduce 框架的输入和输出。 4 构件 Bigtable 是建立在其它的几个 Google 基础设施上的。 Bigtable 使用分布式的 Google File System (GFS) [17] 来存储日志和数据文件。 BigTable 集群通常运行在一个共享的机器池中，池中的机器还会运行其它的各种各样的分布式应用程序，BigTable 的进程经常要和其它应用的进程共享机器。 BigTable 依赖集群管理系统来调度任务、管理共享的机器上的资源、处理机器的故障、以及监视机器的状态。 BigTable 内部存储数据的文件是 GoogleSSTable 格式的。 SSTable 是一个持久化的、排序的、不可更改的 Map 结构，而 Map 是一个键/值映射的数据结构，键和值都是任意的 Byte 串。 操作用于查找与指定键关联的值，并在指定键范围内迭代所有键/值对。 在内部，每个 SSTable 包含一个块序列(通常每个块的大小为 64 KB，但这是可配置的)。 块索引(存储在SSTable的末尾)用于定位块；当 SSTable 打开时，索引被加载到内存中。 Bigtable 依赖于一个名为 Chubby [8] 的高可用性和持久性分布式锁服务。 Chubby 服务由五个活动副本组成，其中一个被选为主副本并主动服务请求。 当大多数副本都在运行并且可以相互通信时，该服务是实时的。 Chubby 使用 Paxos 算法 [9，23] 在失败时保持其副本的一致性。 Chubby 提供了一个由目录和小文件组成的命名空间。 每个目录或文件都可以用作锁，对文件的读写是原子的。 Chubby 客户端库提供了对 Chubby 文件的一致缓存。 每个 Chubby 客户机都维护一个与 Chubby 服务的会话。 如果客户端的会话在租约到期时间内无法续订，则会话将到期。 当客户端会话过期时，它将丢失所有锁和打开的句柄。 Chubby 客户端还可以注册对 Chubby 文件和目录的回调，以通知更改或会话过期。 Bigtable 使用 Chubby 执行以下任务： 确保在任何时候最多有一个活动的主控器； 存储 Bigtable 数据的引导位置(参见第 5.1 节)； 发现 Tablet 服务器并终止 Tablet 服务器(见第 5.2 节)； 存储 Bigtable 结构数据(每个表的 column family 信息)； 以及存储访问控制列表。 如果 Chubby 长时间不可用，Bigtable 将不可用。 我们最近在 14 个集群中测量了这种效应，这些集群跨越 11 个 Chubby 实例。 由于 Chubby 不可用而导致 BigTable 中的部分数据不能访问的平均比率是 0.0047% (Chubby 不能访问的原因可能是 Chubby 本身失效或者网络问题)。 单个集群受 Chubby 的不可用性影响最大的百分比为 0.0326%。 5 实现方式 Bigtable 由以下 3 个主要组件构成：链接到每个客户机的库，一个 Master 服务器和许多 Tablet 服务器。 Tablet 服务器可以从集群中动态添加(或删除)，以适应工作负载的变化。 Master 服务器负责将 Tablet 分发给 Tablet 服务器，并对 Tablet 服务器进行添加和过期，平衡负载并在 GFS 中对文件进行垃圾收集。 此外，它会还处理结构更改，例如创建表和 column family。 每个 Tablet 服务器管理一组 Tablet (通常一个服务器会管理 10-1000个 Tablet)。 Tablet 服务器处理对已加载的 Tablet 的读写请求，还可以拆分过大的 Tablet。 与许多单主机分布式存储系统一样 [17，21] ，客户机数据不会通过 Master 移动：客户机直接与 Tablet 服务器进行读写通信。 由于 Bigtable 客户端不依赖 Maser 获取 Tablet 服务器的位置信息，因此大多数客户端从不与 Master 通信。 因此，Master 在实即使用中是轻负载的。 一个 BigTable 集群存储了很多表，每个表包含了一个 Tablet 的集合，而每个 Tablet 包含了某个范围内的行的所有相关数据。 初始状态下，一个表只有一个 Tablet。 随着表中数据的增长，它被自动分割成多个 Tablet ，默认情况下，每个 Tablet 的尺寸大约是 100 MB到 200 MB。 5.1 Tablet 存储路径 我们使用类似于 B+ [10] 树的三层结构来存储 Tablet 的位置信息(图 4)。 第一级是存储在 Chubby 中的文件，其中包含根 Tablet 的位置。 根 Tablet 在 METADATA 表中存储了所有 Tablet 的地址。 每个 METADATA 表中存储了用户的 Tablet 的地址。 根 Tablet 只是在 METADATA 表中的第一个 Tablet， 但是经过了特殊的处理，它永远不会被拆分，以确保 Tablet 的层次体系永远不超过三层。 METADATA 表将 Tablet 的位置存储在 row key 下，row key 是由 Tablet 所在的表的标识符和 Tablet 的最后一行编码而成的。 每个 METADATA 行在内存中大约需要 1 KB的空间。 每个 METADATA 表的限制大小一般为 128 MB，我们三层的层次体系足以支持 2^34 个 Tablet 的地址(或者说 261 bytes 在 128 MB 的 Tablet)。 链接到每个客户机的库中缓存了 Tablet 的位置。 如果客户端不知道 Tablet 的位置，或者发现缓存的位置信息不正确，则会递归地向上移动查找 Tablet 层次体系。 如果客户机的缓存是空的，那么定位算法需要三次网络请求，包括一次从 Chubby 读取。 如果客户机的缓存是过时的，那么定位算法可能需要多达六次请求，因为过时的缓存项只在未命中时才能被发现(假设 METADATA 表不经常移动)。 尽管 Tablet 的位置存储在内存中，因此不需要访问 GFS，但是我们通过让链接到客户机的库预取 Tablet 位置来进一步降低这种成本： 每当它读取 METADATA 表时，它都会读取多个 Tablet 的元数据。 在 METADATA 表中还存储了次级信息，包括每个 Tablet 的事件日志 (例如，什么时候一个服务器开始为该 Tablet 提供服务)。 这些信息有助于排查错误和性能分析。 5.2 Tablet 分配 每个 Tablet 一次只会分配给一个 Tablet 服务器。 Master 会追踪一系列活动的 Tablet 服务器，以及当前分配给这些服务器上的 Tablet，还有哪些 Tablet 未分配。 如果 Tablet 没有被分配，并且某个 Tablet 服务器有空间容纳此 Tablet，则 Master 会向 Tablet 服务器发送加载请求来分配此 Tablet。 Bigtable 使用 Chubby 来追踪 Tablet 服务器。 当 Tablet 服务器启动时，它会在特定的 Chubby 目录中创建一个唯一命名的文件，并获取该文件的独占锁。 Master 监视此目录（服务器目录）以发现 Tablet 服务器。 Tablet 服务器在获取不到锁的情况下会终止对 Tablet 提供服务，例如由于网络分区异常导致服务器失去 Chubby 连接。 (Chubby 提供了一种有效的机制，允许 Tablet 服务器检查它是否仍然持有锁，而不会增加网络负载。) 只要文件仍然存在，Tablet 服务器就会尝试重新获取其文件上的独占锁。 如果该文件不再存在，那么 Tablet 服务器将无法再次提供服务，因此它会自杀。 每当 Tablet 服务器终止时(例如，由于群集管理系统正在将 Tablet 服务器所在的计算机从群集中删除)，它都会尝试释放其锁， 以便 Master 更快地重新分配其 Tablet。 Master 会负责检测当 Tablet 服务器不在对 Tablet 提供服务时尽快重新分配 Tablet。 为了检测 Tablet 服务器何时不再为其 Tablet 提供服务，主机会定期向每个 Tablet 服务器询问其锁的状态。 如果 Tablet 服务器报告它失去了锁，或者如果 Master 在最近几次尝试中无法访问服务器，则 Master 将尝试获取服务器文件的独占锁。 如果 Master 能够获得锁，那么 Chubby 是活动的，Tablet 服务器要么已经死了， 要么无法访问 Chubby，因此 Master 通过删除其服务器文件来确保 Tablet 服务器不能再次服务。 一旦服务器的文件被删除，Master 可以将以前分配给该服务器的所有 Tablet 移动到未分配的 Tablet 集合中。 为了确保 Bigtable 集群不易受到 Master 和 Chubby 之间的网络问题的影响， 如果其 Chubby 会话过期，主机将自杀。所述，Master 故障不会更改 Tablet 到 Tablet 但是，如上所述，Master 故障不会更改 Tablet 到 Tablet 服务器的分配。 当群集管理系统启动 Master 时，它需要先发现当前的 Tablet 分配，然后才能更改它们。 Master 在启动时执行以下步骤。 Master 在 Chubby 中获取一个唯一的锁，这防止了并发的 Master 实例化。 Master 扫描 Chubby 中的服务器目录以找到活动服务器。 Master 与每个活动的 Tablet 服务器通信，获取目前 Tablet 的分配情况。 主机扫描 METADATA 表获取所有 Tablet。 当主机扫描到未分配的 Tablet 时 Master 会将其放在未分配集合中，从而使该 Tablet 符合分配条件。 一个复杂的问题是，在分配 METADATA 表之前，无法对其进行扫描。 因此，在开始此扫描(步骤 4)之前，如果在步骤 3 期间发现根 Tablet 未分配，则 Master 会将根 Tablet 添加到未分配的集合中。 这样就可以确保根 Tablet 的分配。 因为根 Tablet 包含METADATA 表中所有 Tablet 的名称，所以在扫描根 Tablet 后，Master 会知道所有 Tablet 的名称。 只有在创建或删除表、合并两个现有 Tablet 以形成一个较大的 Tablet 或将一个现有 Tablet 拆分为两个较小的 Tablet 时，现有 Tablet 集才会更改。 Master 能够跟踪这些更改，因为除了最后一个事件外的两个事件都是由它启动的。 由于 Tablet 拆分是由 Tablet 服务器启动的，因此需要特别处理。 Tablet 服务器通过在 METADATA 表中记录新 Tablet 的信息来提交拆分。 当拆分提交时，它会通知 Master。 如果拆分通知丢失(可能是因为 Tablet 服务器或 Master 死亡)，则 Master 会在请求 Tablet 服务器加载已拆分的 Tablet 时检测到新的 Tablet。 Tablet 服务器将通知 Master 拆分，因为它在 METADATA 表中找到的 Tablet 条目将只指定 Master 要求它加载的 Tablet 的一部分 5.3 Tablet 服务 Tablet 的持久化存储在 GFS 中，如图 5 所示。 更新操作提交到 REDO 日志中。 在这些更新中，最近提交的更新被存储在内存中一个称为 memtable 的排序缓冲区中; 旧的更新存储在一系列 SSTables 中。 要恢复 Tablet ， Tablet 服务器从 METADATA 表中读取其元数据。 此元数据包含 SSTables 的列表，SSTables 中包含一个 Tablet 和一组 REDO 点， 它们是指向可能包含 Tablet 数据的任何提交日志的指针。 服务器将 SSTables 的索引读入内存，并通过应用自 REDO 点以来提交的所有更新来重建 memtable。 当写入操作到达 Tablet 服务器时，服务器会检查其格式是否正确，以及发送方是否有权执行该操作。 授权是通过从一个 Chubby 文件(客户机几乎总是命中 Chubby 缓存中的文件)中读取允许写入程序的列表来执行的。 将有效的变更写入提交日志。 组提交操作用于提高许多小变更的吞吐量 [13，16]。 提交写操作后，其内容将插入 memtable。 当读取操作到达 Tablet 服务器时，同样会检查其格式是否良好以及授权是否正确。 有效的读取操作会对 SSTables 和 memtable 序列的合并视图进行检索。 由于 SSTables 和 memtable 是按字典顺序排序的数据结构，因此可以高效地形成合并视图。 当进行 Tablet 的合并和拆分时，正在进行的读写操作能够继续执行。 5.4 压缩 当执行写操作时，memtable 的大小会增加。 当 memtable 大小达到阈值时，memtable 被冻结，创建一个新的 memtable，冻结的 memtable 被转换成 SSTable 并写入 GFS。 这个小型压缩过程有两个目标：它会收缩 Tablet 服务器的内存使用量，并且在恢复过程中，如果该服务器死机，它可以减少必须从提交日志中读取的数据量。 当压缩发生时，传入的读写操作可以继续执行。 每一次轻微的压缩都会创建一个新的 SSTable。 轻微的压缩过程不停滞的持续进行，则读取操作可能需要合并任意数量的 SSTable 中的更新。 恰恰相反的是，我们通过在后台定期执行合并压缩来限制这样的文件的数量。 合并压缩读取几个 SSTable 和 memtable 的内容，并写出一个新的 SSTable。 压缩完成后，可以立即丢弃输入的 SSTables 和 memtable。 将所有 SSTable 重写为一个 SSTable 的合并压缩称为主压缩。 由非主要压缩生成的 SSTable 可以包含特殊的删除条目，这些条目抑制仍然有效的旧 SSTable 中删除的数据。 另一方面，主压缩会生成一个不包含删除信息或删除数据的 SSTable。 Bigtable 在其所有的 Tablet 中循环使用，并定期对它们进行主压缩。 这些主压缩允许 Bigtable 回收已删除数据使用的资源，还允许 Bigtable 确保已删除的数据及时从系统中消失，这对于存储敏感数据的服务非常重要。 6 改进 上一节中描述的实现需要许多改进，以实现用户所需的高性能、可用性和可靠性。 本节更详细地描述了实现的各个部分，以突出这些改进。 局部组 客户机可以将多个 column family 组合到一个局部组中。 将每个 Tablet 中的每个局部组生成一个单独的 SSTable。 将通常不能一起访问的 column family 分离到单独的局部组中可以实现更高效的读取。 例如，Webtable 中的页面元数据(如语言和校验和)可以位于一个局部组中，而页面的内容可以位于另一个组中：希望读取元数据的应用程序不需要读取所有页面内容。 此外，可以在每个局部组的基础上指定一些有效的调优参数。 例如，可以将局部组声明为在内存中。 内存中局部组的 SSTables 被延迟加载到 Tablet 服务器的内存中。 加载后，可以在不访问磁盘的情况下读取属于此类局部组的 column family。 此功能对于频繁访问的小块数据非常有用：我们在内部将其用于 METADATA 表中的存储地址 column family。 数据压缩 客户可以控制是否压缩局部组的 SSTable 以及使用哪种压缩格式。 用户指定的压缩格式应用于每个 SSTable 块(其大小可通过特定于局部组的调优参数控制)。 虽然我们通过单独压缩每个块而损失了一些空间，但我们的优势在于，可以读取 SSTable 的一小部分，而无需解压整个文件。 许多客户使用双通道自定义压缩方案。 第一个过程使用 Bentley-McIlroy 算法 [6] ，它通过一个大窗口压缩长的公共字符串。 第二步使用快速压缩算法，在一个小的 16 KB 的数据窗口中寻找重复数据。 这两种压缩过程都非常快，它们在现代机器上的编码速度为 100–200 MB/s，解码速度为 400–1000 MB/s。 尽管我们在选择压缩算法时强调的是速度而不是空间缩减，但这种两遍压缩方案的性能却出人意料地好。 例如，在 Webtable 中，我们使用这种压缩方案来存储网页内容。 在一次实验中，我们将大量文档存储在一个压缩的局部组中。 为了实验的目的，我们将每个文档的版本限制为一个，而不是存储所有可用的版本。 该方案实现了 10 比 1 的空间缩减。 这比 HTML 页面上典型的 Gzip 减少 3 比 1 或 4 比 1 要好得多，因为 Webtable 行的布局方式是：来自单个主机的所有页面都彼此靠近地存储。 这使得 Bentley-McIlroy 算法能够识别来自同一主机的页面中的大量共享样板文件。 许多应用程序(不仅仅是 Webtable)选择合适的 row 名称，这样类似的数据最终会聚集在一起，从而获得非常好的压缩比。 当我们在 Bigtable 中存储相同值的多个版本时，压缩比会更好。 缓存以提高读取性能 Tablet 服务器采用了两级缓存的方式来提高读取性能。 扫描缓存是第一级缓存，主要缓存 Tablet 服务器通过 SSTable 接口获取的键值对。 块缓存是第二级缓存，用于缓存从 GFS 读取的 SSTables 块。 扫描缓存对于倾向于重复读取相同数据的应用程序最为有用。 块缓存对于倾向于读取与其最近读取的数据接近的数据的应用程序非常有用(例如，连续读取，或对热点行中同一位置组中不同列的随机读取)。 布隆过滤器 如第 5.3 节所述，读取操作必须从构成 Tablet 状态的所有 SSTable 中读取。 如果这些 SSTable 不在内存中，我们可能会进行许多次磁盘访问。 我们允许客户机指定应该为特定位置组中的 SSTable 创建布隆过滤器 [7] ，从而减少访问次数。 布隆过滤器允许我们询问 SSTable 是否包含指定行/列对的任何数据。 对于某些特定应用程序，我们只付出了少量的、用于存储布隆过滤器的内存的代价，就换来了读操作显著减少的磁盘访问的次数。 我们对布隆过滤器的使用还意味着对不存在的行或列的大多数查找不需要接触磁盘。 提交日志的实现 如果我们将每个 Tablet 的提交日志保存在一个单独的日志文件中，那么在 GFS 中会同时写入大量的文件。 根据每个 GFS 服务器上的底层文件系统实现，这些写入操作可能会导致大量磁盘查找写入不同的物理日志文件。 此外，每个 Tablet 有单独的日志文件也会降低组提交优化的效率，因为组往往会更小。 为了解决这些问题，我们将变更附加到每个 Tablet 服务器的单个提交日志中，将不同 Tablet 的变更混合在同一个物理日志文件中 [18，20]。 使用一个日志文件可以在正常操作期间提供显著的性能优势，但会使恢复复杂化。 当 Tablet 服务器死机时，它所服务的 Tablet 将被移动到大量其他 Tablet 服务器上： 每台服务器通常装载少量原始服务器的 Tablet。 要恢复 Tablet 的状态，新的 Tablet 服务器需要从原始 Tablet 服务器编写的提交日志中重新应用该 Tablet 的变更。 然而，这些 Tablet 的变更被混合在同一个物理日志文件中。 一种方法是让每个新的 Tablet 服务器读取这个完整的提交日志文件，并只应用它需要恢复的 Tablet 所需的条目。 然而，在这样一种方案下，如果 100 台机器从一台发生故障的 Tablet 服务器上分别分配一个 Tablet，那么日志文件将被读取100次(每台服务器读取一次)。 我们首先按关键字(table、行名称、日志序列号)的顺序对提交日志条目进行排序，以避免重复日志读取。 在排序的输出中，特定 Tablet 的所有变更都是连续的，因此可以通过一次磁盘寻道和一次顺序读来有效地读取。 为了并行排序，我们将日志文件划分为 64MB 的段，并在不同的 Tablet 服务器上对每个段进行并行排序。 此排序过程由 Master 协调，并在 Tablet 服务器指示需要从某个提交日志文件中恢复变更时启动。 将提交日志写入 GFS 有时会由于各种原因导致性能波动 (例如，参与写入的 GFS 服务器崩溃，或为到达特定的一组三个 GFS 服务器而遍历的网络路径遇到网络拥塞，或负载过重)。 为了防止 GFS 延迟峰值的巨变，每个 Tablet 服务器实际上有两个日志写入线程，每个线程都写入自己的日志文件；并且在任何时刻，只有一个线程是工作的。 如果对活动日志文件的写入执行得不好，则将切换到另一个线程，并且提交日志队列中的变更将由新的线程写入。 日志条目包含序列号，以允许恢复过程删除此日志切换过程产生的重复条目。 加速 Tablet 的恢复 如果 Master 将 Tablet 从一个 Tablet 服务器移动到另一个服务器，则源 Tablet 服务器首先对该 Tablet 进行轻微的压缩。 这种压缩通过减少 Tablet 服务器提交日志中未压缩状态的数量来减少恢复时间。 完成此压缩后，Tablet 服务器将停止为对应 Tablet 提供服务。 在实际卸载 Tablet 之前，Tablet 服务器会执行另一次(通常非常快)轻微的压缩，以消除在执行第一次轻微的压缩时到达的 Tablet 服务器日志中任何剩余的未压缩状态。 完成第二次轻微的压缩后，可以将 Tablet 加载到另一台 Tablet 服务器上，而无需恢复任何日志条目。 利用不变性 除了 SSTable 缓存之外，Bigtable 系统的其他各个部分也被简化了，因为我们生成的所有 SSTable 都是不可变的。 例如，在读取 SSTables 时，我们不需要对文件系统的访问进行任何同步。 因此，可以非常有效地实现对行的并发控制。 读写都可以访问的唯一可变数据结构是 memtable。 为了减少 memtable 读取过程中的争用，我们在写入时复制每个 memtable 行，并允许读写并行进行。 由于 SSTables 是不可变的，永久删除已删除数据的问题就转化为 SSTables 的垃圾回收问题。、 每个 Tablet 的 SSTables 都注册在 METADATA 表中。 最后，SSTables 的不变性使我们能够快速拆分 Tablet。 我们让子 Tablet 共享父 Tablet 的 SSTable，而不是为每个子 Tablet 生成一组新的 SSTable。 7 性能评估 我们建立了一个包含 N 台 Tablet 服务器的 Bigtable 集群来衡量它的性能和可扩展性。 Tablet 服务器配置为使用 1 GB 内存，并写入一个 GFS 单元，该单元由 1786 台机器组成，每台机器有两个 400 GB IDE 硬盘。 N 台客户端计算机生成了用于这些测试的 Bigtable 负载。 (我们使用与 Tablet 服务器相同数量的客户端，以确保客户端不会成为瓶颈。) 每台机器有两个双核 Opteron 2 GHz 芯片，足够的物理内存来容纳所有运行进程的工作集，还有一个千兆以太网链路。 这些机器被安排在一个两级树形交换网络中，根节点的总带宽约为 100-200 gbps。 所有的机器都在同一个托管设施中，因此任何一对机器之间的往返时间都不到一毫秒。 Tablet 服务器和 Master、测试客户端以及 GFS 服务器都在同一组机器上运行。 每台机器都运行一个 GFS 服务器。 其它的机器要么运行 Tablet 服务器、要么运行客户程序、要么运行在测试过程中，使用这组机器的其它的任务启动的进程。 R 是在测试中所涉及的 Bigtable row key 的不同数目。 我们精心选择 R 的值，保证每次基准测试对每台 Tablet 服务器读/写的数据量都在 1 GB 左右。 在序列写的基准测试中，我们使用的 row key 的范围是 0 到 R-1。 这个 row key 被划分成 10 N 个大小相等的范围。 这些范围由一个中央调度器分配给N个客户机，该调度器在客户机处理完分配给它的上一个范围后，立即将下一个可用范围分配给该客户机。 这种动态分配有助于减轻客户机上运行的其他进程引起的性能变化的影响。 我们在每一 row key 下写了一个字符串。 每个字符串都是随机生成的，因此不可压缩。 随机写基准与此类似，只是在写之前立即对 row key 进行模 R 的散列，以便在基准的整个持续时间内，写负载大致均匀地分布在整个行空间中。 顺序读取的性能基准和顺序写入的基准采用完全相同的方式生成 row key，但它不在 row key 下写入， 而是读取存储在 row key 下的字符串(该字符串是通过先前调用顺序写入基准编写的)。 同样的，随机读的基准测试和随机写是类似的。 扫描的基准与顺序读取基准类似，但使用 Bigtable API 提供的支持来扫描行范围中的所有值。 单个 RPC 从 Tablet 服务器获取大量的值序列，因此可以减少由基准执行的 RPC 的数量。 随机读取(内存)基准与随机读取基准类似，但是包含基准数据的位置组被标记为内存中，因此从 Tablet 服务器的内存中就可以完成读取，而不需要读取 GFS。 对于这个基准测试，我们将每个 Tablet 服务器的数据量从 1 GB 减少到 100 MB，这样就可以轻松地放入 Tablet 服务器可用的内存中。 图 6 显示了在 Bigtable 中读写 1000 字节值时我们的基准性能的两个图表。 在表格中显示了每个 Tablet 服务器每秒的操作数；在图中显示了每秒的操作总数。 单个 Tablet 服务器的性能 让我们首先考虑一下只有一台 Tablet 服务器的性能。 随机读取比所有其他操作慢一个数量级或更多。 每次随机读取都涉及通过网络将 64 KB 的 SSTable 块从 GFS 传输到 Tablet 服务器，其中仅使用单个 1000 字节的值。 Tablet 服务器每秒执行大约 1200 次读取，也就是说读取 GFS 的速度大约为 75MB/s。 由于我们的网络堆栈、SSTable 解析和 Bigtable 代码中的开销，这样会使 Tablet 服务器 CPU 满载， 并且几乎满载我们系统中的网络链接。 大多数具有这种访问模式的 Bigtable 应用程序将块大小减小到较小的值，通常为 8 KB。 从内存中随机读取的速度要快得多，因为从 Tablet 服务器的本地内存中读取的每 1000 字节内容都能得到满足，而无需从 GFS 中获取一个 64 kb 的快。 随机写入和顺序写入的性能优于随机读取，因为每个 Tablet 服务器都将所有传入的写入附加到单个提交日志中，并使用组提交将这些写入高效地流式传输到 GFS。 随机写入和顺序写入的性能没有显著差异；在这两种情况下，对 Tablet 服务器的所有写入都记录在同一个提交日志中。 顺序读取的性能比随机读取好，因为从 GFS 获取的每个 64 KB SSTable 块都存储在块缓存中来服务接下来的 64 个读取请求。 扫描速度更快，因为 Tablet 服务器可以返回大量值来响应单个客户端，因此 RPC 的开销会分摊到大量值上。 扩展性 当我们将系统中的 Tablet 服务器数量从 1 增加到 500 时，聚合吞吐量会急剧增加，增加了超过 100 倍，随着 Tablet 服务器数量增加 500 倍， 内存随机读取的性能几乎增加了 300 倍。 但是，性能并不是线性增长的。 对于大多数基准测试，从 1 台 Tablet 服务器增加到 50 台 Tablet 服务器时，每台服务器的吞吐量会显著下降。 这种下降是由多个服务器配置中的负载不平衡引起的，通常是由于其他进程争夺 CPU 和网络。 我们的负载平衡算法试图处理这种不平衡，但由于以下两个主要原因而无法完成完美的工作： 一个是尽量减少 Tablet 的移动导致重新负载均衡能力受限(如果 Tablet 被移动了，那么通常是 1 秒内，这个 Tablet 是暂时不可用的)， 另一个是我们的基准测试程序产生的负载会有波动。 随机读取基准测试显示了最糟糕的伸缩性(服务器数量增加 500 倍时，聚合吞吐量只增加了 100 倍)。 发生这种行为的原因是(如上所述)每读取 1000 字节，我们就在网络上传输一个 64 KB 的大数据块。 这种传输使我们网络中的共享的 1 GB 链路满载，因此，随着机器数量的增加，每服务器的吞吐量显著下降。 8 真实应用 Tablet 服务器数量 集群数量 0 … 19 259 20 … 49 47 50 … 99 20 100 … 499 50 &gt; 500 12 表 1：Bigtable 集群中 Tablet 服务器数量的分布。 截止到 2006 年 8 月，Google 内部一共有 388 个非测试用的 Bigtable 集群运行在各种各样的服务器集群上，合计大约有 24500 个 Tablet 服务器。 表 1 显示了每个集群上 Tablet 服务器的大致分布情况。 这些集群中，许多用于开发目的，因此会有一段时期比较空闲。 通过观察一个由 14 个集群、8069 个 Tablet 服务器组成的群组，我们看到整体的吞吐量超过了每秒 1200000 次请求， 发送到系统的 RP C请求导致的网络负载达到了 741 MB/s，系统发出的 RPC 请求网络负载大约是 16 GB/s。 项目名 表的大小(TB) 压缩比例 元素个数 column family 个数 局域组个数 存储于内存 对延迟敏感 Crawl 800 11% 1000 16 8 0% 否 Crawl 50 33% 200 2 2 0% 否 Google Analytics 20 29% 10 1 1 0% 是 Google Analytics 200 14% 80 1 1 0% 是 Google Base 2 31% 10 29 3 15% 是 Google Earth 0.5 64% 8 7 2 33% 是 Google Earth 70 - 9 8 3 0% 否 Orkut 9 - 0.9 8 5 1% 是 Personalized Search 4 47% 6 93 11 5% 是 表 2：生产使用中几个表的特点。表的大小(压缩前测量)，元素个数取近似值，对于禁止压缩的表不提供压缩比例。 表 2 提供了一些目前正在使用的表的相关数据。 一些表存储的是用户相关的数据，另外一些存储的则是用于批处理的数据；这些表在总的大小、每个数据项的平均大小、从内存中读取的数据的比例、表的结构的复杂程度上都有很大的差别。 本节的其余部分，我们将主要描述三个产品研发团队如何使用 Bigtable 的。 8.1 Google Analytics Google Analytics 是用来帮助 Web 站点的管理员分析他们网站的流量模式的服务。 它提供了整体状况的统计数据，比如每天的独立访问的用户数量、每天每个 URL 的浏览次数；它还提供了用户使用网站的行为报告，比如根据用户之前访问的某些页面，统计出几成的用户购买了商品。 为了使用这个服务，Web 站点的管理员只需要在他们的 Web 页面中嵌入一小段 JavaScript 脚本就可以了。 这个 Javascript 程序在页面被访问的时候调用。 它记录了各种 Google Analytics 需要使用的信息，比如用户的标识、获取的网页的相关信息。 Google Analytics 汇总这些数据，之后提供给 Web 站点的管理员。 我们粗略的描述一下 Google Analytics 使用的两个表。 Row Click 表(大约有 200 TB 数据)的每一行存放了一个最终用户的会话。 行的名字是一个包含 Web 站点名字以及用户会话创建时间的元组。 这种模式保证了对同一个 Web 站点的访问会话是顺序的，会话按时间顺序存储。 这个表可以压缩到原来尺寸的 14%。 Summary 表(大约有 20 TB 的数据)包含了关于每个 Web 站点的、各种类型的预定义汇总信息。 一个周期性运行的 MapReduce 任务根据 Raw Click 表的数据生成 Summary 表的数据。 每个 MapReduce 工作进程都从 Raw Click 表中提取最新的会话数据。 系统的整体吞吐量受限于 GFS 的吞吐量。 这个表的能够压缩到原有尺寸的 29%。 8.2 Google Earth Google 通过一组服务为用户提供了高分辨率的地球表面卫星图像，访问的方式可以使通过基于 Web 的 Google Maps 访问接口(maps.google.com)， 也可以通过 Google Earth 定制的客户端软件访问。 这些软件产品允许用户浏览地球表面的图像：用户可以在不同的分辨率下平移、查看和注释这些卫星图像。 这个系统使用一个表存储预处理数据，使用另外一组表存储用户数据。 数据预处理流水线使用一个表存储原始图像。 在预处理过程中，图像被清除，图像数据合并到最终的服务数据中。 这个表包含了大约 70 TB 的数据，所以需要从磁盘读取数据。 图像已经被高效压缩过了，因此存储在 Bigtable 后不需要再压缩了。 Imagery 表的每一行都代表了一个单独的地理区域。 行都有名称，以确保毗邻的区域存储在了一起。 Imagery 表中有一个 column family 用来记录每个区域的数据源。 这个 column family 包含了大量的列：基本上市每个列对应一个原始图片的数据。 由于每个地理区域都是由很少的几张图片构成的，因此这个 column family 是非常稀疏的。 数据预处理流水线高度依赖运行在 Bigtable 上的 MapReduce 任务传输数据。 在运行某些 MapReduce 任务的时候，整个系统中每台 Tablet 服务器的数据处理速度是 1 MB/s。 这个服务系统使用一个表来索引 GFS 中的数据。 这个表相对较小(大约是 500 GB)，但是这个表必须在保证较低的响应延时的前提下，针对每个数据中心，每秒处理几万个查询请求。 因此，这个表必须在上百个 Tablet 服务器上存储数据，并且将 column family 存储在内存中。 个性化查询 个性化查询(www.google.com/psearch) 是一个双向服务；这个服务记录用户的查询和点击，涉及到各种 Google 的服务，比如 Web 查询、图像和新闻。 用户可以浏览他们查询的历史，重复他们之前的查询和点击；用户也可以定制基于 Google 历史使用习惯模式的个性化查询结果。 个性化查询使用 Bigtable 存储每个用户的数据。 每个用户都有一个唯一的用户 ID，每个用户 ID 和一个列名绑定。 一个单独的 column family 被用来存储各种类型的行为(比如，有个 column family 可能是用来存储所有的 Web 查询的)。 每个数据项都被用作 Bigtable 的时间戳，记录了相应的用户行为发生的时间。 个性化查询使用以 Bigtable 为存储的 MapReduce 任务生成用户的数据图表。 这些用户数据图表用来个性化当前的查询结果。 个性化查询的数据会复制到几个 Bigtable 的集群上，这样就增强了数据可用性，同时减少了由客户端和 Bigtable 集群间的“距离”造成的延时。 个性化查询的开发团队最初建立了一个基于 Bigtabl e的、“客户侧”的复制机制为所有的复制节点提供一致性保障。 现在的系统则使用了内建的复制子系统。 个性化查询存储系统的设计允许其它的团队在它们自己的列中加入新的用户数据， 因此，很多 Google 服务使用个性化查询存储系统保存用户级的配置参数和设置。 在多个团队之间分享数据的结果是产生了大量的 column family。 为了更好的支持数据共享，我们加入了一个简单的配额机制限制用户在共享表中使用的空间； 配额也为使用个性化查询系统存储用户级信息的产品团体提供了隔离机制。 9 经验教训 在设计、实现、维护和支持 Bigtable 的过程中，我们获得了一些有用的经验，并吸取了一些有趣的教训。 我们学到的一个教训是，大型分布式系统易受多种故障的影响，而不仅仅是标准网络分区故障还有许多分布式协议中假定的 fail-stop 类型故障。 比如，我们遇到过下面这些类型的错误导致的问题：内存数据损坏、网络中断、时钟偏差、机器挂起、扩展的和非对称的网络分区、 我们正在使用的其他系统中的错误(例如Chubby)、GFS 配额溢出以及计划内和计划外的硬件维护。 随着我们在这些问题上积累了更多的经验，我们通过改变各种协议来解决这些问题。 例如，我们在 RPC 机制中添加了校验和。 我们在设计系统的部分功能时，不对其它部分功能做任何的假设，这样的做法解决了其它的一些问题。 例如，我们不再假设给定的 Chubby 操作只能返回一组固定错误中的一个。 我们学到的另一个教训是，在清楚如何使用新特性之前，推迟添加新特性是很重要的。 例如，我们最初计划在 API 中支持通用事务。 因为我们没有立即使用它们所以没有实现。 现在我们有许多实际的应用程序在 Bigtable 上运行，我们已经能够检查它们的实际需求，并且发现大多数应用程序只需要单行事务。 当人们请求分布式事务时，最重要的用途是维护二级索引，我们计划添加一个专门的机制来满足这一需求。 与分布式事务相比，新机制的通用性较差，但效率更高( 特别是对于跨越数百行或更多行的更新)，而且非常符合我们的“跨数据中心”复制方案的优化策略。 我们从支持 Bigtable 中学到的一个实际教训是正确的系统级监视的重要性(即，监视 Bigtable 本身以及使用 Bigtable 的客户机进程)。 例如，我们扩展了 RPC 系统，以便对于 RPC 的一个示例，它保留代表该 RPC 执行的重要操作的详细跟踪。 这个特性允许我们检测和修复许多问题，比如 Tablet 数据结构上的锁争用、提交 Bigtable 变更时对 GFS 的写入速度慢， 以及 METADATA Tablet 不可用时对 METADATA 表的访问受阻。 另一个有用的监视示例是，每个 Bigtable 集群都在 Chubby 中注册。 这允许我们跟踪所有集群，发现它们有多大，查看它们运行的软件版本，它们接收的流量，以及是否存在诸如意外的大延迟之类的问题。 我们学到的最重要的一课是简单设计的价值。 考虑到我们系统的大小(大约十万行非测试代码)，以及代码以意想不到的方式随时间演化的事实，我们发现代码和设计的清晰性对代码维护和调试有巨大的帮助。 一个例子是我们的 Tablet 服务器成员协议。 我们的第一个协议很简单：Master 定期向 Tablet 服务器发出租约，如果租约到期，Tablet 服务器就会自杀。 不幸的是，该协议在出现网络问题时显著降低了可用性，并且对 Master 恢复时间也很敏感。 我们重新设计了好几次协议，直到我们有了一个性能良好的协议。 然而，产生的协议过于复杂，并且依赖一些 Chubby 很少被用到的特性。 我们发现我们浪费了大量的时间在调试一些古怪的问题，有些是 Bigtable 代码的问题，有些是 Chubby 代码的问题。 最终，我们放弃了这个协议，转而使用一个新的更简单的协议，它完全依赖于广泛使用的 Chubby 特性。 10 相关工作 Boxwood 项目 [24] 的组件在某些方面与 Chubby、GFS 和 Bigtable 重叠，因为它提供了分布式协议、锁定、分布式块存储和分布式 B-tree 存储。 在每一个有重叠的情况下，似乎 Boxwood 的组件的目标级别都比相应的 Google 服务更底层。 Boxwood 项目的目标是为构建更高级别的服务(如文件系统或数据库)提供基础设施，而 Bigtable 的目标是直接支持希望存储数据的客户端应用程序。 最近的许多项目都解决了在广域网上提供分布式存储或更高级别服务的问题，通常是在“互联网规模”上。 这其中包括了分布式的哈希表，这项工作由一些类似 CAN [29] 、Chord [32] 、Tapestry [37] 和 Pastry [30] 的项目率先发起。 些系统的主要关注点和 Bigtable 不同，比如应对各种不同的传输带宽、不可信的协作者、频繁的更改配置等； 另外，去中心化和 Byzantine 灾难冗余也不是 Bigtable 的目的。 注：Byzantine 灾难冗余(一个决策算法可以容忍多少百分比的骗子，然后仍然能够正确确定共识?) 就可能提供给应用程序开发人员的分布式数据存储模型而言，我们认为由分布式 B 树或分布式哈希表提供的密钥-值对模型有很大的局限性。 键值对是一个有用的组件，但它们不应该是提供给开发人员的唯一组件。 我们选择的模型比简单的键值对更丰富，并且支持稀疏的半结构化数据。 尽管如此，它仍然非常简单，可以非常高效地表示平面文件，并且它足够透明(通过局域组)，允许用户调整系统的重要行为。 一些数据库供应商已经开发了能够存储大量数据的并行数据库。 Oracle 的 Real Application Cluster 数据库 [27] 使用共享磁盘存储数据(Bigtable 使用 GFS)和分布式锁管理器(Bigtable 使用 Chubby)。 IBM 的 DB2 并行版 [4] 基于类似于 Bigtable 的无共享 [33] 体系结构。 每个 DB2 服务器负责一个表中的行的子集，该表存储在本地关系数据库中。 这两种产品都提供了一个完整的事务关系模型。 Bigtable 局域组提供了类似于基于列的存储方案在压缩和磁盘读取方面具有的性能；这些以列而不是行的方式组织数据的方案包括 C-Store [1,34] 和商业产品如 Sybase IQ [15,36]、SenSage [31]、KDB+ [22] 以及 MonetDB/X100 中的 ColumnBM存储层 [38]。 另一个将垂直和水平数据分割成平面文件并获得良好数据压缩比的系统是 AT&amp;T 的 Daytona 数据库 [19] 。 局域组不支持CPU缓存级别的优化，如 Ailamaki [2] 所描述的优化。 Bigtable使用 memtables 和 SSTables 存储 Tablet 更新的方式类似于日志结构的合并树 [26] 存储索引数据更新的方式。 在这两种系统中，排序后的数据在写入磁盘之前都会缓冲在内存中，读取时必须合并内存和磁盘中的数据。 C-Store 和 Bigtable 有很多相似点：两个系统都采用 Shared-nothing 架构，都有两种不同的数据结构， 一种用于当前的写操作，另外一种存放“长时间使用”的数据， 并且提供一种机制在两个存储结构间搬运数据。 两个系统在 API 接口函数上有很大的不同：C-Store 操作更像关系型数据库，而 Bigtable 提供了低层次的读写操作接口， 并且设计的目标是能够支持每台服务器每秒数千次操作。 C-Store 同时也是个“读性能优化的关系型数据库”，而 Bigtable 对读和写密集型应用都提供了很好的性能。 Bigtable 的负载均衡器必须解决一些与 shared-nothing 数据库相同的负载和内存平衡问题(例如，[11，35])。 我们的问题稍微简单一点： 我们不考虑同一数据的多个副本的可能性(可能由于视图或索引的原因而以其他形式存在) 我们让用户告诉我们哪些数据属于内存，哪些数据应该留在磁盘上，而不是试图动态地确定这一点 我们没有要执行或优化的复杂查询 11 结论 我们已经描述了 Bigtable，一个在 Google 存储结构化数据的分布式系统。 Bigtable 集群自 2005 年 4 月开始投入生产使用，在此之前，我们花了大约 7个人/年的时间来进行设计和实现。 截至 2006 年 8 月，超过 60 个项目正在使用 Bigtable。 我们的用户喜欢 Bigtable 实现提供的性能和高可用性，他们可以通过简单地向系统中添加更多的机器来扩展集群的容量， 因为随着时间的推移，他们的资源需求会发生变化。 考虑到 Bigtable 的不同寻常的接口，一个有趣的问题是我们的用户适应它有多困难。 新用户有时不确定如何最好地使用 Bigtable 接口，特别是当他们习惯于使用支持通用事务的关系数据库时。 尽管如此，许多 Google 产品成功地使用 Bigtable 的事实表明，我们的设计在实践中运行良好。 我们正在实施几个额外的 Bigtable 功能，例如支持辅助索引和基础设施，以及支持多 Master 节点的、跨数据中心复制的 Bigtable 的基础组件。 我们还开始将 Bigtable 作为服务部署到产品组中，这样各个组就不需要维护自己的集群。 随着服务集群的扩展，我们需要在 Bigtable 本身中处理更多的资源共享问题 [3,5]。 最后，我们发现在 Google 构建我们自己的存储解决方案有很大的优势。 我们从为 Bigtable 设计自己的数据模型中获得了很大的灵活性。 此外，我们对 Bigtable 实现的控制，以及 Bigtable 所依赖的其他 Google 基础设施，意味着我们可以在瓶颈和低效出现时消除它们。 致谢 We thank the anonymous reviewers, David Nagle, and our shepherd Brad Calder, for their feedback on this paper. The Bigtable system has benefited greatly from the feedback of our many users within Google. In addition, we thank the following people for their contributions to Bigtable: Dan Aguayo, Sameer Ajmani, Zhifeng Chen, Bill Coughran, Mike Epstein, Healfdene Goguen, Robert Griesemer, Jeremy Hylton, Josh Hyman, Alex Khesin, Joanna Kulik, Alberto Lerner, Sherry Listgarten, Mike Maloney, Eduardo Pinheiro, Kathy Polizzi, Frank Yellin, and Arthur Zwiegincew. 参考资料 [1] ABADI, D. J., MADDEN, S. R., AND FERREIRA, M. C. Integrating compression and execution in columnoriented database systems. Proc. of SIGMOD (2006). [2] AILAMAKI, A., DEWITT, D. J., HILL, M. D., AND SKOUNAKIS, M. Weaving relations for cache performance. In The VLDB Journal (2001), pp. 169–180. [3] BANGA, G., DRUSCHEL, P., AND MOGUL, J. C. Resource containers: A new facility for resource management in server systems. In Proc. of the 3rd OSDI (Feb. 1999), pp. 45–58. [4] BARU, C. K., FECTEAU, G., GOYAL, A., HSIAO, H., JHINGRAN, A., PADMANABHAN, S., COPELAND,G. P., AND WILSON, W. G. DB2 parallel edition. IBM Systems Journal 34, 2 (1995), 292–322 [5] BAVIER, A., BOWMAN, M., CHUN, B., CULLER, D., KARLIN, S., PETERSON, L., ROSCOE, T., SPALINK, T., AND WAWRZONIAK, M. Operating system support for planetary-scale network services. In Proc. of the 1st NSDI(Mar. 2004), pp. 253–266. [6] BENTLEY, J. L., AND MCILROY, M. D. Data compression using long common strings. In Data Compression Conference (1999) , pp. 287–295. [7] BLOOM, B. H. Space/time trade-offs in hash coding with allowable errors. CACM 13, 7 (1970), 422–426. [8] BURROWS, M. The Chubby lock service for looselycoupled distributed systems. In Proc. of the 7th OSDI(Nov. 2006). [9] CHANDRA, T., GRIESEMER, R., AND REDSTONE, J. Paxos made live — An engineering perspective. In Proc. of PODC (2007). [10] COMER, D. Ubiquitous B-tree. Computing Surveys 11, 2 (June 1979), 121–137. [11] COPELAND, G. P., ALEXANDER, W., BOUGHTER, E. E., AND KELLER, T. W. Data placement in Bubba. In Proc. of SIGMOD ( 1988), pp. 99–108. [12] DEAN, J., AND GHEMAWAT, S. MapReduce: Simplified data processing on large clusters. In Proc. of the 6th OSDI(Dec. 2004), pp. 137–150. [13] DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L., STONEBRAKER, M., AND WOOD, D. Implementation techniques for main memory database systems. In Proc. of SIGMOD (June 1984), pp. 1–8. [14] DEWITT, D. J., AND GRAY, J. Parallel database systems: The future of high performance database systems. CACM 35, 6 (June 1992), 85–98. [15] FRENCH, C. D. One size fits all database architectures do not work for DSS. In Proc. of SIGMOD (May 1995), pp. 449–450. [16] GAWLICK, D., AND KINKADE, D. Varieties of concurrency control in IMS/VS fast path. Database Engineering Bulletin 8, 2 (1985), 3–10. [17] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. of the 19th ACM SOSP (Dec. 2003), pp. 29–43. [18] GRAY, J. Notes on database operating systems. In Operating Systems — An Advanced Course, vol. 60 of Lecture Notes in Computer Science. Springer-Verlag, 1978. [19] GREER, R. Daytona and the fourth-generation language Cymbal. In Proc. of SIGMOD (1999), pp. 525–526. [20] HAGMANN, R. Reimplementing the Cedar file system using logging and group commit. In Proc. of the 11th SOSP (Dec. 1987), pp. 155–162. [21] HARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra striped network file system. In Proc. of the 14th SOSP (Asheville, NC, 1993), pp. 29–43. [22] KX.COM. kx.com/products/database.php. Product page. [23] LAMPORT, L. The part-time parliament. ACM TOCS 16, 2 (1998), 133–169. [24] MACCORMICK, J., MURPHY, N., NAJORK, M., THEKKATH, C. A., AND ZHOU, L. Boxwood: Abstractions as the foundation for storage infrastructure. In Proc. of the 6th OSDI (Dec. 2004), pp. 105–120. [25] MCCARTHY, J. Recursive functions of symbolic expressions and their computation by machine. CACM 3, 4 (Apr. 1960), 184–195. [26] O’NEIL, P., CHENG, E., GAWLICK, D., AND O’NEIL, E. The log-structured merge-tree (LSM-tree). Acta Inf. 33, 4 (1996) , 351–385. [27] ORACLE.COM. www.oracle.com/technology/products/-database/clustering/index.html. Product page. [28] PIKE, R., DORWARD, S., GRIESEMER, R., AND QUINLAN, S. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming Journal 13, 4 (2005), 227–298. [29] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SHENKER, S. A scalable content-addressable network. In Proc. of SIGCOMM (Aug. 2001), pp. 161–172. [30] ROWSTRON, A., AND DRUSCHEL, P. Pastry: Scalable, distributed object location and routing for largescale peer-to-peer systems. In Proc. of Middleware 2001 (Nov. 2001), pp. 329–350. [31] SENSAGE.COM. sensage.com/products-sensage.htm. Product page. [32] STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, M. F., AND BALAKRISHNAN, H. Chord: A scalable peer-to-peer lookup service for Internet applications. In Proc. of SIGCOMM (Aug. 2001), pp. 149–160. [33] STONEBRAKER, M. The case for shared nothing. Database Engineering Bulletin 9, 1 (Mar. 1986), 4–9. [34] STONEBRAKER, M., ABADI, D. J., BATKIN, A., CHEN, X., CHERNIACK, M., FERREIRA, M., LAU, E., LIN, A., MADDEN, S., O’NEIL, E., O’NEIL, P., RASIN, A., TRAN, N., AND ZDONIK, S. C-Store: A columnoriented DBMS. In Proc. of VLDB (Aug. 2005), pp. 553–564. [35] STONEBRAKER, M., AOKI, P. M., DEVINE, R., LITWIN, W., AND OLSON, M. A. Mariposa: A new architecture for distributed data. In Proc. of the Tenth ICDE (1994), IEEE Computer Society, pp. 54–65. [36] SYBASE.COM. www.sybase.com/products/databaseservers/sybaseiq. Product page. [37] ZHAO, B. Y., KUBIATOWICZ, J., AND JOSEPH, A. D. Tapestry: An infrastructure for fault-tolerant wide-area location and routing. Tech. Rep. UCB/CSD-01-1141, CS Division, UC Berkeley, Apr. 2001. [38] ZUKOWSKI, M., BONCZ, P. A., NES, N., AND HEMAN, S. MonetDB/X100 — A DBMS in the CPU cache. IEEE Data Eng. Bull. 28, 2 (2005), 17–22.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://wangqian0306.github.io/tags/HBase/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Bigtable","slug":"Bigtable","permalink":"https://wangqian0306.github.io/tags/Bigtable/"}]},{"title":"MapReduce shuffle","slug":"bigdata/mapreduce-shuffle","date":"2021-06-08T14:13:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/mapreduce-shuffle/","permalink":"https://wangqian0306.github.io/2021/mapreduce-shuffle/","excerpt":"","text":"MapReduce shuffle 简介 在 Hadoop 中 MapReduce 应用通常实现了 Mapper 和 Reducer 接口来提供 Map 和 Reduce 方法。 这也就构成了整个任务的核心。 注：在 Mapper 中还会发生文件溢写、排序、合并等操作，这些内容在《Hadoop 权威指南》中统一的被划分到了 shuffle 章节一同介绍。 Reducer 则主要有以下三个阶段： shuffle sort reduce 在官方文档中是这样描述 shuffle 的： 12Input to the Reducer is the sorted output of the mappers.In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP. Reducer 所需的输入是经过排序的 Mapper 输出。 在这个阶段，MapReduce 框架通过 HTTP 的方式获取所有 Mapper 输出的相关分区。 而在《Hadoop 权威指南》中的描述则是这样的： MapReduce 确保每个 Reducer 的输入都是按键排序的。 系统执行排序的过程(即将 Map 输出作为输入传入 Reducer) 称为 shuffle。 注：以下是书中的标注 事实上，shuffle 这个说法并不准确。因为在某些语境中，它只代表 Reduce 任务获取 Map 输出的过程。 在这一小节，我们将其理解为从 Map 产生输出到 Reduce 消化输入的整个过程。 整体流程梳理 虽然说官方文档仅仅将 shuffle 归类到了 Reducer 中，但是此处会参照《Hadoop 权威指南》中的表述方式梳理具体流程。 Map 端流程 Map 函数产生输出时，并不是简单地将它写到磁盘。而是利用缓冲的方式写到内存，并出于效率的考虑进行预排序。详见下图： 每个 Map 任务都有一个环形内存缓冲区用于储存任务输出。 一旦缓冲内容达到设定的阈值(io.sort.spill.percent) 一个后台线程便开始把内容溢写 (spill) 到磁盘。 在溢写的过程中如果缓冲区被填满则 Map 会阻塞直到写入完成。 在写入磁盘之前，线程首先根据数据最终要传输的 Reducer 把数据划分成相应的分区(partition)。 在每个分区中，后台线程按键进行内部排序，如果有 combiner，它就在排序后的输出上运行。 运行 combiner 会使的 Map 输出结果更紧凑，因此减少写到磁盘的数据和传递给 Reducer 的数据。 每次溢出都会产生新的文件，在任务完成后会将所有文件进行合并和排序。 如果至少存在 3 个溢出文件，combiner 会在数据文件写入磁盘之前运行。 Reduce 端流程 Map 的输出会在 task tracker 的本地磁盘上。 task tracker 会为分区文件运行 Reduce 任务。 Reducer 会通过 HTTP 的方式得到输出文件的分区 。 如果 Map 输出的数据量相当小，则会被复制到 Reduce 任务的 JVM 内存中，否则将会被复制到磁盘。 一旦内存缓冲区达到阈值大小(maperd.job.suhffle.input.buffer.percent) 或达到 Map 输出阈值(maperd.inmem.merge.threshold)，则合并溢写到磁盘中。 如果指定 combiner 则在合并期间运行它来降低写入硬盘的数据量。 随着磁盘上副本增多，后台线程会将它们合并为更大的、排好序的文件。 这会为后面的合并节省一些时间。 注意，为了合并，压缩的 Map 输出都必须在内存中被解压缩。 复制完所有 Map 输出后，Reduce 任务进入排序阶段(更恰当的说法是合并阶段，因为排序是在 Map 端进行的)，这个阶段将合并 Map 输出， 维持其顺序排序。 在最后阶段即 Reduce 阶段，直接把数据输入 Reduce 函数，从而省略一次读取和写入操作的行程。 最后的合并可以来自内存和磁盘片段。 在 Reduce 阶段，对已排序输出中的每个键调用 Reduce 函数。 此阶段的输出直接写到输出文件系统，一般为 HDFS。 如果使用 HDFS，由于 task tracker 节点(或者节点管理器)也运行数据节点，所以第一个文件块的副本被写入到本地磁盘。 Word Count 数据流样例 参考资料 MapReduce Tutorial MapReduce: 详解Shuffle过程 《Hadoop 权威指南：大数据的存储与分析》","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wangqian0306.github.io/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://wangqian0306.github.io/tags/MapReduce/"}]},{"title":"MongoDB","slug":"database/mongodb","date":"2021-06-07T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/mongodb/","permalink":"https://wangqian0306.github.io/2021/mongodb/","excerpt":"","text":"MongoDB Community Edition 简介 MongoDB 是一个通用的文档数据库，可以使用社区版的 MongoDB 进行本地使用。 部署方式 可以使用如下 docker-compose.yaml 文件部署服务： 1234567891011121314services: mongo: image: mongodb/mongodb-community-server:latest user: &quot;1000:1000&quot; environment: - MONGO_INITDB_ROOT_USERNAME=root - MONGO_INITDB_ROOT_PASSWORD=123456 - MONGO_INITDB_DATABASE=demo ports: - &quot;27017:27017&quot; volumes: - type: bind source: ./data target: /data/db 注：运行用户不要是 root 会有权限问题。 使用如下命令即可运行服务： 12mkdir -p datadocker-compose up -d 使用如下命令进入容器： 1docker-compose exec mongo bash 使用如下命令进入交互式管理工具： 1mongosh -u root -p 使用如下指令创建用户： 12use &lt;db&gt;db.createUser(&#123;user:&quot;&lt;username&gt;&quot;,pwd:&quot;&lt;password&gt;&quot;,roles:[&#123;role:&quot;readWrite&quot;,db:&quot;&lt;db&gt;&quot;&#125;]&#125;); 参考资料 官方手册","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://wangqian0306.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://wangqian0306.github.io/tags/MongoDB/"}]},{"title":"MapReduce Simplified Data Processing on Large Clusters 中文翻译版","slug":"treatise/mapreduce_simplified_data_processing_on_large_clusters","date":"2021-06-02T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/mapreduce_simplified_data_processing_on_large_clusters/","permalink":"https://wangqian0306.github.io/2021/mapreduce_simplified_data_processing_on_large_clusters/","excerpt":"","text":"MapReduce: Simplified Data Processing on Large Clusters 中文翻译版 作者： Jeffrey Dean and Sanjay Ghemawat 摘要 MapReduce 是一种编程模型同时也是一个处理和生成超大数据集的算法模型的相关实现。 用户首先需要创建一个用于处理键值对的 Map 函数，来生成中间结果数据，然后编写一个 Reduce 函数来把所有的带有相同键的中间结果进行合并和关联，生成最终结果。 如同本文所表述的一样，许多现实生活中的许多事务也能通过这种模型进行处理。 使用本文所示的编码方式可以让程序在多台普通的设备当中并行的执行。 运行时系统负责对输入数据进行分区、在一组机器上调度程序的执行、处理机器故障以及管理所需的机器间通信等细节。 这使得没有任何并行和分布式系统经验的程序员能够轻松地利用大型分布式系统的资源。 我们的 MapReduce 运行在一个大型的通用设备集群上，并且具有高度的可伸缩性：一个 MapReduce 程序可以在数千台机器上处理许多 TB 的数据。 程序员发现这个系统很容易使用：已经实现了数百个 MapReduce 程序，每天在 Googl e的集群上执行了超过 1000 个 MapReduce 作业。 1 引言 在过去的5年里，包括本文作者在内的Google的很多程序员，为了处理海量的原始数据，已经实现了数以百计的、专用的计算方法。 这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网络爬虫的程序）、Web请求日志等等； 也为了计算处理各种类型的衍生数据，比如：倒排索引，网页的图形结构的不同表现形式，每台主机上的数据爬取记录汇总，特定日期最常查询的问题等等。 大多数这样的计算在概念上都很简单。 然而，由于输入数据通常很大，为了在合理的时间内完成计算，所以必须将计算分布在成百上千台机器上。 如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。 为了解决上述复杂的问题，我们设计一个新的抽象模型，使用这个抽象模型，我们只要表述我们想要执行的简单运算即可， 而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面。 设计这个抽象模型的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 原语。 我们意识到我们大多数的运算都包含这样的操作：在输入数据的 “逻辑” 记录上应用 Map 操作得出一个键值对集合类型的中间结果， 然后在所有具有相同键的数据上应用 Reduce 操作，从而达到合并中间的数据，得到目标结果。 我们使用带有用户指定的 Map 和 Reduce 操作的函数模型，使我们能够轻松地以并行化的方式运行大量的计算，并将重新执行作为容错的主要机制。 这项工作的主要贡献是提供了一个简单而强大的接口，可以实现大规模的分布并行计算。 实现此接口，我们就可在大型通用设备集群上获得高性能。 第2节描述了基本的编程模型并给出了几个例子。 第3节描述了针对我们基于集群的计算环境定制的 MapReduce 接口的实现。 第4节描述了我们发现有用的编程模型的一些改进。 第5节给出了我们实现各种任务的性能度量。 第6节探讨了在 Google 中使用 MapReduce，包括我们在使用 MapReduce 作为重写我们的产品索引系统的基础上的经验。 第7节讨论了相关的和未来的工作。 2 编程模型 在计算过程中的输入和输出都采用了键值对的形式。 MapReduce 库的用户将计算表示为两个函数：Map 和 Reduce。 由用户编写的 Map 函数接受输入对并生成一组中间键/值对。 MapReduce 库回将所有的中间结果进行组合和关联，并将它们传递给 Reduce 函数。 Reduce 函数也由用户编写，它接受一个中间键和该键的一组值。 它将这些值合并在一起，形成一组可能更小的值。 通常每个 Reduce 调用只生成零或一个输出值。 中间值通过迭代器提供给用户的 Reduce 函数。 这允许我们处理太大而无法放入内存的数据。 2.1 样例 在此样例中我们需要计算大量文档中每个单词出现的次数。 用户将会编写的代码类似于以下伪代码： 12345678910111213map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;);reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); Map 函数会将每个单词加上相关的出现次数 (在这个简单的例子中只有 “1”)。 Reduce 函数将为特定单词发出的所有计数相加。 另外，用户编写代码，使用输入和输出文件的名字、可选的调节参数来完成一个符合 MapReduce 模型规范的对象， 然后调用 MapReduce 函数，并把这个规范对象传递给它。 用户的代码和 MapReduce 库链接在一起(用C++实现)。 附录 A 包含了这个实例的全部程序代码。 2.2 类型 尽管前面的伪代码是根据字符串输入和输出编写的，但从概念上讲，用户提供的 Map 和 Reduce 函数只要有对应的关联就可以： 12map (k1,v1) -&gt; list (k2,v2)\\reduce (k2,list(v2)) -&gt; list (v2) 比如，输入的键和值与输出的键和值属于不同的域。 此外，中间的键和值与输出的键和值属于同一域。 我们的C++中使用字符串类型作为用户自定义函数的输入输出，用户在自己的代码中对字符串进行适当的类型转换。 2.3 更多示例 下面是一些有趣程序的简单示例，这些程序可以很容易地表示为 MapReduce 计算。 分布式检索：Map 函数输出匹配某个模式的一行，Reduce 函数是一个恒等函数，即把中间数据复制到输出。 URL 访问频率统计：Map 函数处理日志中 web 页面请求的记录，然后输出 (URL,1)。 Reduce 函数把相同 URL 的值都累加起来，产生 (URL,记录总数)结果。 倒转网络链接图：Map 函数在源页面(source)中检索出所有的链接目标(target)并输出为(target,source)。 Reduce 函数把给定链接目标的链接组合成一个列表，输出(target,list(source))。 每个主机的词向量统计：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。 Map 函数为每一个输入文档解析为(主机名,检索词向量)，其中主机名来自文档的URL。 Reduce 函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的 (主机名,检索词向量)。 倒排索引：Map 函数将每个文档解析成 (词,文档号) 组成的列表， Reduce 函数接收所有词并依照文档号进行排序，最终输出 (词,list(文档号))键值对。 这样输出的所有内容就形成了一个简单的倒排索引。 这样我们就可以简单的去获取词的出处。 分布式排序：Map 函数会从每条记录当中提取 key 并输出 (key,record) 键值对。 Reduce 函数会直接输出所有的值。 此计算取决于第 4.1 节中描述的分区特性和第 4.2 节中描述的排序属性。 3 实现方式 MapReduce 有很多种不同的实现方式。 需要在具体环境中才能得出最优实现。 例如：一种实现可能适用于小型共享内存的设备，另一种适用于大型 NUMA 多处理器的设备，还有一种适用于更大的网络设备集合。 本章节描述一个适用于 Google 内部广泛使用的运算环境的实现： 用以太网交换机连接、由普通 PC 机组成的大型集群。 在我们的环境里包括： x86架构、运行Linux操作系统、双处理器、2-4GB内存的机器。 普通的网络硬件设备，每个机器的带宽为百兆或者千兆，但是远小于网络的平均带宽的一半。 集群中包含成百上千的设备，因此，设备故障是常态。 存储为廉价的内置IDE硬盘。我们用一个内部分布式文件系统[8]来管理存储在这些磁盘上的数据。 文件系统通过数据复制来在不可靠的硬件上保证数据的可靠性和有效性。 用户提交工作（job）给调度系统。每个工作(job) 都包含一系列的任务(task)，调度系统将这些任务调度到集群中多台可用的机器上。 3.1 执行概览 调度器会把数据分片(partition)成 M 份，然后 Map 操作也会被分发至这些设备上。 不同的分片就可以被不同的设备并行处理。 用户可以指定的 Reduce 函数的分区数量和分区方式。 分区函数(例如，hash(key) mod R) 会将 Map 函数生成的中间结果进行重新分区, Reduce 函数也会被分发到多台设备上执行。 图1 展示了我们实现的 MapReduce 操作执行流程。 当用户的程序调用 MapReduce 函数时，将发生以下操作 (图1中的编号标签对应于下面列表中的编号)： 在用户代码中的 MapReduce 库会把输入文件切分成 M 份，每份通常有 16-64 MB(用户可以进行配置)。 程序的副本当中的 master 是特殊的。 由 master 分发任务给其他的 worker。 总共会有 M 个 Map 任务，R 个 Reduce 任务。 master 会挑选空闲的 worker 并给他们指派一个 Map 或 Reduce 任务。 当 worker 接收到 Map 任务后会去读取对应的输入分片。 Map 函数会依照用户的函数将输入数据处理成键值对格式的中间结果，并将其缓存在内容中。 缓存中的数据会被分区函数将其分割为 R 个区域并定期的写入磁盘中然后回报给 master。 由 master 将存储地址发送给分配了 Reduce worker。 在 Reduce worker 接收到中间结果的存储位置时，它会使用 RPC(remote procedure call) 的方式读取在 Map worker 上的数据。 当 Reduce worker 读取到所有的中间数据后会按照键进行排序，以便于实现分组聚合。 排序操作是必要的因为通常需要把许多不同的键映射到一个 Reduce 任务。 如果中间结果的数据量太大，在内存中已经无法容纳就会使用外部排序的方式。 Reduce worker 会对排序后的中间数据进行迭代，对于遇到的每个唯一的中间键，它将键和相应的中间值集传递给用户的 Reduce 函数。 Reduce 函数的输出会被追加到这个 Reduce 分区最终的输出文件中。 当所有的 Map 和 Reduce 任务执行完成后，master 会唤醒用户的程序。 在此时 MapReduce 会将处理结果输出至用户代码中。 在执行成功之后，MapReduce 的输出结果会在 R 个分片(一个 Reduce 任务对应一个分片，文件名由用户指定)。 通常来说，用户不需要将文件合并成一个文件。 他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 3.2 Master 的数据结构 Master 保存了一些数据结构。 它存储了每一个 Map 和 Reduce 任务的状态(空闲 idle、工作中 in-process 或 完成 completed)， 以及 worker (非空闲任务) 的标识。 Master 就像一个数据管道，中间结果存储区域的位置信息通过这个管道从 Map 传递到 Reduce。 因此，对于每个完成的 Map 任务，master 存储了 Map 任务生成的 R 中间结果的位置和大小。 在 Map 任务执行完成后 master 会更新中间结果的位置和大小等信息。 这些信息将以增量方式推送到正在执行的 Reduce 任务。 3.3 容错性 因为 MapReduce 库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据，所以，这个库必须要能很好的处理机器故障。 worker 故障 Master 会周期性的ping worker。 如果在一定周期内没有接受到相应请求则 Master 会将该 worker 标记为失败。 在这个 worker 上运行的所有 Map 任务会被重新标记成空闲状态，因此这些任务可以被分配到其他 worker 上。 类似的，在发生故障的 worker 上正在运行的 Map 或者 Reduce 任务也将被重新标记为空闲状态，等待重新调度。 已经运行完成的 Map 任务会被重新执行，因为它们的输出存储在发生故障的设备中的硬盘上，所以这些结果无法被访问。 而完成的 Reduce 任务无需重新执行，因为它们的数据存储在全局文件系统中。 当一个 Map 任务先被 worker A 执行然后被 worker B 执行(由于节点故障)，所有运行 Reduce 任务的 worker 都会收到执行通知。 任何 Reduce 任务若没有获取到 worker A 的数据就会去 worker B 中读取。 MapReduce 可以处理大规模 worker 失效的情况。 例如，在一次 MapReduce 操作期间，运行集群上的网络维护导致有80台计算机、在几分钟内无法访问。 MapReduce 的 master 只是重新执行无法访问的工作机器所做的工作，并继续运行，最终完成了 MapReduce 操作。 master 故障 一个简单的解决办法是让 master 周期性的将上面描述的数据结构的写入磁盘，即检查点(checkpoint)。 如果 master 任务异常关闭，一个新的副本可以从最新的检查点重新启动。 然而，考虑到只有一个主控器，它不太可能失败；因此，如果主程序失败，我们当前的实现将中止 MapReduce 计算。 客户端可以检查此情况，并根据需要重试 MapReduce 操作。 在失效方面的处理机制 当用户提供的 Map 和 Reduce 操作符是其输入是确定函数时，我们的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。 我们依赖对 Map 和 Reduce 任务的输出的提交是原子性的来完成这个特性。 每个在运行的任务都会把他们的输出写在私有的临时文件中。 一个 Reduce 任务产生一个这样的文件，而一个 Map 任务会产生 R 个这样的文件(R 取决于 Reduce 任务个数)。 当 Map 任务执行完成后 worker 会向 master 发送消息，在此条消息中会说明 R 个临时文件的名称。 如果主机接受到一个已经标记为完成的 Map 任务则会忽略这条消息。 否则将会把 R 个文件的文件名记录在 master 的数据结构中。 当 Reduce 任务完成时，Reduce worker 会自动将其临时文件重命名为最终输出文件。 如果在多台机器上执行相同的 Reduce 任务，则将对同一最终输出文件执行多个 Rename 调用。 我们依靠底层文件系统提供的原子重命名操作来保证最终文件系统状态只包含 Reduce 任务的一次执行所产生的数据。 我们的 Map 和 Reduce 操作符绝大多数都是确定性的，而且在这种情况下，我们的语义约等于顺序执行，这使得程序员很容易对程序的行为进行推理。 当 Map 和/或 Reduce 操作符是不确定的时，我们提供了较弱但仍然合理的语义。 在存在非确定性运算符的情况下，特定 Reduce 任务 R1 的输出等效于由非确定性程序的顺序执行产生的 R1 的输出。 然而，不同 Reduce 任务 R2 的输出可以对应于由不确定程序的不同顺序执行产生的 R2 的输出。 考虑 Map 任务 M 和 Reduce 任务 R1、R2 的情况。 我们假设 e(Ri) 是 Ri 的执行结果(只有一个这样的结果)。 当 e(R1) 读取了由 M 一次执行产生的输出，而 e(R2) 读取了由 M 的另一次执行产生的输出，导致了较弱的失效处理。 3.4 存储位置 网络带宽在我们的环境当中是一种稀缺资源。 我们通过利用输入数据(由 GFS [8] 管理)存储在组成集群的机器的本地磁盘上这一事实来节省网络带宽。 GFS 将每个文件分为 64MB 块，并在不同的机器上存储每个块的多个副本(通常为3个副本)。 MapReduce master 会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的计算机上调度 Map 任务。 否则，它将尝试在该任务输入数据的副本附近(例如，在与包含数据的计算机位于同一网络交换机上的工作计算机上) 调度映射任务。 在集群中大部分的 worker 在运行大型 MapReduce 操作时，大多数输入数据都是本地读取的，不消耗网络带宽。 3.5 任务粒度 和上面描述的内容一致，我们将 Map 阶段分成 M 份 Reduce 阶段分成 R 份。 理想情况下，M 和 R 应该远远大于 worker 的数量。 让每个 worker 执行许多不同的任务可以优化动态负载平衡，还可以在一个 worker 失败时加快恢复速度：它完成的许多 Map 任务可以分布在所有其他worker机器上。 但是实际上，在我们的具体实现中对 M 和 R 的取值都有一定的客观限制，因为 master 必须执行 O(M+R) 次调度，并且在内存中保存 O(M*R) 个状态。 (在实际情况中内存的影响因素较小，O( M * R) 个持久化的信息当中每个 Map/Reduce 任务只需要 1 byte。) 此外，R 的数量则常常受到用户的约束，因为每个 Reduce 任务的结果最终都输出都在一个单独的文件中。 在实践中，我们倾向于选择 M 的大小，这样每个任务的输入数据大约为 16 MB 到 64 MB (此种局部优化是最有效的)， 并且我们会将 R 的数量设为我们期望使用的 worker 数量的一个小倍数。 我们经常使用 2000 台设备执行 M=200000 和 R=5000 的 MapReduce 计算。 3.6 备份任务 MapReduce 任务执行时间过长的常见原因之一是 “落伍者”: 在运算的过程中，某台设备消耗了过长的事件去处理最后几个 Map 或者 Reduce 任务。 “落伍者” 的出现原因有很多。 例如：一台设备的磁盘出现了问题，在读取时需要经常进行纠错操作，导致其读取性能从 30 MB/s 降低到 1 MB/s。 集群中的调度系统可能已经调度了机器上的其他任务，由于 CPU、内存、本地磁盘或网络带宽的竞争，导致它执行 MapReduce 代码的速度较慢。 我们最近遇到的一个问题是机器初始化代码中的一个 bug，它导致处理器缓存被禁用：受影响机器上的计算速度降低了 100 倍以上。 我们有一个通用的机制来缓解落伍者问题。 当 MapReduce 操作接近完成时，master 会安排其余正在进行的任务备份执行。 无论是主任务还是备份任务执行完成，都会将任务标记为已完成 我们已经对这个机制进行了调整，使得它通常会将操作使用的计算资源增加不超过百分之几。 我们发现，这大大缩短了完成大型 MapReduce 操作的时间。 例如，当备份任务机制被禁用时，第 5.3 节中描述的排序程序需要额外花费 44% 的时间才能完成。 4 优化方式 尽管简单地编写 Map 和 Reduce 函数所提供的基本功能足以满足大多数需求，但我们发现有一些扩展是有用的。 本节对这些问题进行了描述。 4.1 分区函数 用户需要制定 MapReduce 计算中 Reduce 任务或是输出文件的数量 R。 分区函数会将数据按照分片键进行切分。 默认的分区函数是哈希函数(例如：“hash(key) mod R”)。 这样一来我们的分区就是非常平衡的。 但是在某些情况下，使用其他函数对数据进行分区则是非常有用的。 例如：有时我们需要输出的内容包含 URL，我们想让单个主机的所有数据在同一个文件上输出。 为了支持这种情况，用户可以提供一个特殊的分区函数。 例如：使用 “hash(Hostname(urlkey)) mod R” 作为分区函数，就可以让来自同一主机的所有 URL 最终输出到同一文件中。 4.2 保证排序 我们保证在给定的分区内，键值对会对按递增键的顺序进行处理。 这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按键值随机存取的应用非常有意义，对在排序输出的数据集中也很有帮助。 4.3 Combiner 函数 在某些情况下，每个 Map 任务生成的键都有明显的重复，用户指定的 Reduce 函数是可以对这些键进行交换的和关联的。 这方面的一个很好的例子是第 2.1 节中的单词计数示例。 由于词频倾向于遵循 Zipf 分布，因此每个 Map 任务将产生数百或数千条 &lt;the，1&gt; 形式的记录。 所有这些计数都将通过网络发送到一个 Reduce 任务，然后通过 Reduce 函数相加生成一个数字。 我们允许用户指定一个可选的 Combiner 函数，在数据通过网络发送之前对其进行部分合并。 Combiner 函数会在每台运行 Map 任务的设备上执行。 通常使用相同的代码来实现 Combiner 和 Reduce 函数。 Reduce 函数和 Combiner 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。 Reduce 函数的输出被写入最终的输出文件。 Combiner 函数的输出会被写入一个中间文件，该文件将被发送到 Reduce 任务。 合并部分数据可以大大加快了特定的 MapReduce 操作。 附录 A 包含了一个使用 Combiner 的示例。 4.4 输入和输出类型 MapReduce 库支持多种不同格式的输入类型。 例如，“text” 模式输入将每一行视为键/值对：键是文件中的偏移量，值是行的内容。 另一种常见的支持格式存储按键排序的键/值对序列。 种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独的 Map 任务来进行后续处理 (例如，文本模式的范围分割必须确保仅仅在每行的边界进行范围分割)。 用户可以通过提供一个简单读取器接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。 读取器不一定需要提供从文件读取的数据。 例如：从数据库或者从内存映射的数据结构中读取数据也是很容易的。 以类似的方式，我们支持一组输出类型来生成不同格式的数据，用户代码很容易添加对新输出类型的支持。 4.5 副作用 在某些情况下，MapReduce 的用户发现从 Map 与 Reduce 操作生成辅助文件作为附加输出非常方便。 我们依靠程序writer把这种“副作用”变成原子的和幂等的。 通常，应用程序会写入临时文件，并在完全生成该文件后自动重命名该文件。 如果一个任务产生了多个输出文件，我们没有提供类似两阶段提交的原子操作支持这种情况。 因此，生成具有跨文件一致性要求的多个输出文件的任务应该是确定的。 这种限制在实践中从来都不是问题。 4.6 跳过错误数据 有时，用户代码中存在一些 bug，这些 bug 会导致 Map 或 Reduce 函数在某些记录上崩溃。 这样的 bug 会阻止 MapReduce 操作完成。 通常的做法是修复错误，但有时这是不可行的；也许这个 bug 在第三方库中，而这个库的源代码是不可修改的。 此外，有时忽略一些记录是用户可以接受的，例如在对一个大数据集进行统计分析时。 我们提供了一种可选的执行模式，其中 MapReduce 库会检测哪些记录会导致崩溃，并跳过这些记录以继续运行。 每个 worker 进程都会安装一个信号处理程序这个程序会捕捉内存段异常 (segmentation violation) 和总线错误 (bus error)。 在调用自定义的 Map 或 Reduce 操作之前，MapReduce 库会将参数的序列号存储在全局变量中。 如果用户程序触发了一个系统信号，消息处理函数将用 “最后一口气” 通过 UDP 包向 master 发送处理的最后一条记录的序号。 当 master 在某个特定记录上看到多次失败时，它指示在下一次重新执行相应的 Map 或 Reduce 任务时应跳过该记录。 4.7 本地执行 在 Map 或 Reduce 函数中的调试问题可能很棘手，因为实际的计算发生在一个分布式系统中，通常在几千台机器上，而工作分配决策是由 master 动态做出的。 为了方便调试、评测和小规模测试，我们开发了 MapReduce 库的替代实现，它在本地机器上顺序执行 MapReduce 操作的所有工作。 用户可以控制 MapReduce 操作的执行，可以把操作限制到特定的 Map 任务上。 用户通过设定特别的标志来在本地执行他们的程序，之后就可以很容易的使用本地调试和测试工具(比如gdb)。 4.8 状态信息 Master 运行了一个内部的 HTTP 服务器并导出一组状态页供用户使用。 状态页显示计算的进度，如已完成的任务数、正在进行的任务数、输入字节数、中间数据字节数、输出字节数、处理速率等。 这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。 用户可以使用这些数据来预测计算需要多长时间，以及是否应该向计算中添加更多的资源。 这些页面还可以用来计算什么时候计算速度比预期慢得多。 此外，顶级状态页还会显示哪些 worker 存在失败，以及失败时映射和减少他们正在处理的任务。 在尝试诊断用户代码中的错误时，此信息非常有用。 4.9 计数器 MapReduce 库提供了一个计数器工具来统计各种事件的发生次数。 例如，用户代码可能要计算处理的单词总数或索引的德语文档数等。 为了使用这个工具，需要在用户代码中新增一个命名为 Counter 的对象，然后在 Map 与 Reduce 函数中适当的增加计数器，例如： 12345678Counter* uppercase;uppercase = GetCounter(&quot;uppercase&quot;);map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;); 这些计数器的值周期性的从各个单独的 worker 机器上传递给 master (附加在 ping 的应答包中传递)。 Master 会聚合来自成功的 Map 和 Reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。 当前的计数器值也显示在主状态页上，以便用户可以查看实时计算的进度。 聚合计数器值时，master 消除了重复执行同一 Map 或 Reduce 任务的影响，以避免重复计数。 (重复执行可能是由于使用备份任务和由于失败而重新执行任务造成的。) MapReduce 库会自动维护一些计数器值，例如处理的输入键值对数量和生成的输出键值对数量。 计数器机制对于 MapReduce 操作的完整性检查非常有用。 例如，在某些 MapReduce 操作中，用户代码可能希望确保生成的输出键值对的数目正好等于处理的输入键值对的数目， 或者确保处理的德语文档的分数在处理的文档总数中的某个可容许分数内。 5 性能 在本节中，我们将测量在大型计算机集群上运行的两个 MapReduce 计算的性能。 一种计算方法是在大约 1 TB 的数据中搜索特定的模式。 另一种计算对大约 1 TB 的数据进行排序。 这两个程序代表了由 MapReduce 用户编写的真实程序的一个大子集– 一类程序将数据从一个表示形式转移到另一个表示形式，另一类从一个大数据集中提取少量有趣的数据。 5.1 集群配置 所有的程序都是在一个由大约 1800 台机器组成的集群上执行的。 每台机器有两个 2GHz 的 Intel Xeon 处理器，支持超线程，4 GB 内存，两个 160 GB 磁盘和一个千兆以太网链路。 这些机器被安排在一个两级树形交换网络中，根节点的总带宽约为 100 - 200 gbps。 所有的机器都在同一个托管设施中，因此任何一对机器之间的往返时间都不到一毫秒。 在 4 GB 内存中，群集上运行的其他任务保留了大约 1-1.5GB 的内存。 这些程序是在周末下午执行的，当时 CPU、磁盘和网络大部分处于空闲状态。 5.2 检索程序 检索程序会查询 10^10 个 100 byte 的记录，在其中检索相对罕见的三字符模式(共有 92337 处)。 输入被切分成大约 64 MB 的大小(M=15000)，整个输出被存放在一个文件中（R=1）。 图2 展示了计算进度与时间的关系。 Y 轴显示了输入数据的速率。 随着越来越多的机器被分配到这个 MapReduce 计算中，这个速率逐渐加快，当分配了 1764 个 worker 时，这个速率达到了超过 30 Gb/s 的峰值。 当 Map 任务完成时，速率开始下降，并在大约 80 秒的计算时间内达到零。 整个计算从开始大约需要 150 秒结束。 这包括大约一分钟的启动开销。 启动开销是由于需要将程序发送至所有 worker 以及等待 GFS 文件系统打开 1000 个输入文件集合的时间、获取相关的文件本地位置优化信息的时间。 5.3 排序程序 排序程序会处理 10^10 个 100 byte 的记录(大约 1 TB 的数据)。 这个程序是以 TeraSort 基准测试为模型的[10]。 排序程序由少于 50 行的代码构成。 三行映射函数从文本行中提取一个 10 字节的排序键，并将该键和原始文本行作为键值对输出。 我们使用了一个内置的身份函数作为 Reduce 操作符。 此函数将中间的键值对作为输出键值对原封不动地传输。 最终排序的输出被写入一组双向复制的 GFS 文件(也就是说，2 TB的输出)。 和以前一样，输入数据被分割成 64 MB 的片段 ( M=15000 ) 我们将排序后的输出划分为 4000 个文件 (R=4000)。 分区函数使用 key 的原始字节来把数据分区到 R 个片段中。 在这个基准测试中，我们使用的分区函数知道 key 的分区情况。 在一般的排序程序中，我们将添加一个预传递的 MapReduce 操作，该操作将收集 key 的样本，并使用样本 key 的分布来计算最终排序过程的分割情况。 图 3 (a) 中显示了正常执行排序程序的进度。 左上角的图表显示了读取输入的速率。 由于所有 Map 任务都在 200 秒之内完成，因此速率峰值约为 13 GB/s，并很快的执行完成。 值得注意的是，输入的速率小于处理的速率。 这是因为排序映射任务将花费大约一半的时间和I/O带宽将中间输出写入本地磁盘。 grep 的相应中间输出的大小可以忽略不计。 左侧中间的图表显示了 Map 任务通过网络输送给 Reduce 任务的速率。 当第一个 Map 任务完成时，洗牌 (shuffle) 流程就开始了。 图中的第一个峰值是第一批大约 1700 个 Reduce 任务(整个 MapReduce 被分配了 1700 台机器，每台机器一次只能执行一个 Reduce 任务)。 在大约 300 秒的计算过程中，第一批 Reduce 任务中的一些任务完成了，我们开始为剩余的 Reduce 任务对数据进行洗牌。 所有的洗牌操作是在大约 600 秒的计算时间内完成的。 左下角的图表显示了 Reduce 任务将排序后的数据写入最终输出文件的速率。 在第一个洗牌周期结束和写入周期开始之间有一个延迟，因为设备在对中间数据进行排序。 写入将以大约 2-4 GB/s 的速率持续一段时间。 在整个计算过程中所有写操作都在大约 850 秒内完成。 包括启动开销，整个计算需要 891 秒。 这是目前执行效率最高的排序，TeraSort 基准测试结果为 1057 秒[18]。 需要注意的是：由于我们的局部优化，输入速率高于洗牌速率和输出速率—大多数数据都是从本地磁盘读取的，并且绕过了相对带宽受限的网络。 我们会将写入操作执行两次来生成副本，因为这是底层文件系统提供的可靠性和可用性机制。 如果底层文件系统使用擦除编码而不是复制，则写入数据的网络带宽需求将减少[14]。 5.4 备份任务的影响 在图 3 (b) 中，我们展示了在禁用备份任务的情况下执行排序程序。 执行流与图 3 (a) 中所示的类似，只是有一个很长的尾部此时几乎没有任何写活动发生。 960 秒后，除 5 个 Reduce 任务外的所有任务都完成。 然而，最后几个掉队者要到 300 秒后才能完成。 整个计算耗时 1283 秒，耗用时间增加了 44%。 5.5 设备故障 在图 3 © 中，我们展示了排序程序的执行过程，在计算的几分钟内，我们故意杀死了 1746 个 worker 中的 200 个。 底层集群调度器立即在这些机器上重新启动新的工作进程(因为只有进程被终止，所以机器仍然正常工作)。 图三（c）显示出了一个“负”的输入数据读取速度，这是因为一些已经完成的 Map 任务丢失了 (由于相应的执行 Map 任务的 worker 进程被杀死了)，需要重新执行这些任务。 相关 Map 任务很快就被重新执行了。 整个运算在 933 秒内完成，包括了初始启动时间 (只比正常执行多消耗了5%的时间)。 6 经验 我们在 2003 年 1 月完成了第一个版本的 MapReduce 库，在 2003 年 8 月的版本有了显著的增强，这包括了输入数据本地优化、worker 机器之间的动态负载均衡等等。 从那以后，我们惊喜的发现，MapReduce 库能广泛应用于我们日常工作中遇到的各类问题。 它现在在 Google 内部各个领域得到广泛应用，包括： 大规模机器学习 为 Google News 和 Froogle products 处理问题 从公众查询产品 (比如 Google Zeitgeist) 的报告中抽取数据。 从大量的新应用和新产品的网页中提取有用信息 (比如，从大量的位置搜索网页中抽取地理位置信息)。 大规模的图形计算。 图4 显示了在我们的源代码管理系统中，随着时间推移，独立的 MapReduce 程序数量的显著增加。 从 2003 年早些时候的 0 个增长到 2004 年 9 月份的差不多 900 个不同的程序。 MapReduce 的成功取决于采用 MapReduce 库能够在不到半个小时时间内写出一个简单的程序， 这个简单的程序能够在上千台机器的组成的集群上做大规模并发处理，这极大的加快了开发和原形设计的周期。 另外，采用 MapReduce 库，可以让完全没有分布式与并行系统开发经验的程序员很容易的利用大量的资源，开发出分布式并行处理的应用。 项目 数量 任务数量 29423 平均任务执行时长 634 s 每日使用设备数 79186 输入数据量 3288 TB 中间数据产生量 758 TB 输出数据量 193 TB 每个任务平均的 worker 数量 157 平均每个任务运行失败的 worker 数量 1.2 平均每个任务 Map 任务的数量 3351 平均每个 Reduce 任务的数量 55 独立的 Map 实现 395 独立的 Reduce 实现 269 独立的 Map/Reduce 组合 426 表1 在 2004 年运行的 MapReduce 任务 在每个任务结束的时候，MapReduce 库统计计算资源的使用状况。 在表1，我们列出了 2004 年 8 月份 MapReduce 运行的任务所占用的相关资源。 6.1 大规模索引 到目前为止，MapReduce 最成功的应用就是重写了 Google 网络搜索服务所使用到的索引系统。 索引系统的输入数据是网络爬虫抓取回来的海量的文档，这些文档数据都保存在 GFS 文件系统里。 这些文档原始内容的大小超过了 20 TB。 索引程序是通过一系列的 MapReduce 操作 (大约 5 到 10 次)来建立索引。 使用MapReduce (而不是索引系统的早期版本中的 ad-hoc 分布式处理方式) 提供了以下几个好处： 索引代码更简单、更小、更易于理解，因为处理容错、分布和并行化的代码隐藏在 MapReduce 库中。 例如，当使用 MapReduce 方式时，计算的一个阶段的代码从大约 3800 行的 C++ 代码下降到大约 700 行。 MapReduce 库的性能足够好，以至于我们可以将概念上无关的计算分开，而不是将它们混合在一起，以避免对数据进行额外的传递。 这使得更改索引过程变得很容易。 例如，在我们的旧索引系统中花了几个月才做的一个更改在新系统中只花了几天就实现了。 索引过程变得更易于维护，因为大多数由机器故障、机器速度慢和网络故障引起的问题都由 MapReduce 库自动处理，无需操作员干预。 此外，通过向索引集群添加新机器，可以很容易地提高索引过程的性能。 7 相关工作 许多系统提供了限制性编程模型，并利用这些限制进行自动并行化计算。 例如，一个聚合函数可以通过并行使用 N 个处理器，在 log N 的时间内处理 N 个元素的数组来计算所有的内容 [6，9，13]。 MapReduce 可以被认为是其中一些模型的简化和升华，这些模型是基于我们对大型现实世界计算的经验。 更重要的是，我们提供了可扩展到数千个处理器的容错实现。 相比之下，大多数并行处理系统只在较小的规模上实现，并将处理机器故障的细节留给程序员。 批量同步编程 [17] 和一些 MPI 原语 [11] 提供了更高级别的抽象，使程序员更容易编写并行程序。 这些系统与 MapReduce 之间的一个关键区别是 MapReduce 利用受限编程模型自动并行化用户程序并提供透明的容错。 我们数据本地优化策略的灵感来源于active disks [12,15] 等技术，在 active disks 中，计算任务是尽量推送到数据存储的节点处理，这样就减少了网络和 IO 子系统的吞吐量。 我们通过使用商品化的处理器挂载硬盘的方式而不是在磁盘控制器上，但是一般的做法是相似的。 我们的备用任务机制和 Charlotte System [3] 提出的 eager 调度机制比较类似。 Eager调度机制的一个缺点是如果一个任务反复失效，那么整个计算就不能完成。 我们通过忽略引起故障的记录的方式在某种程度上解决了这个问题。 MapReduce 的实现依赖于一个内部的集群管理系统，这个集群管理系统负责在一个超大的、共享机器的集群上分布和运行用户任务。 虽然这个不是本论文的重点，但是有必要提一下，这个集群管理系统在理念上和其它系统，如 Condor [16] 是一样的。 MapReduce 库的排序机制和 NOW-Sort [1] 的操作上很类似。 读取输入源的机器(Map workers)把待排序的数据进行分区后，发送到 R 个 Reduce worker中的一个进行处理。 每个 Reduce worker 在本地对数据进行排序 (尽可能在内存中排序)。 当然，NOW-Sort 没有给用户自定义的 Map 和 Reduce 函数的机会，因此不具备 MapReduce 库广泛的实用性。 River [2] 提供了一个编程模型，其中进程通过在分布式队列上发送数据来相互通信。 与 MapReduce 一样，River 系统试图提供良好的平衡性能，即使存在由异构硬件或系统扰动引入的不一致性。 River 通过仔细安排磁盘和网络传输来实现这一点，从而实现平衡的完成时间。 MapReduce 使用不同的方式实现。 通过限制编程模型，MapReduce 框架能够将问题划分为大量细粒度任务。 这些任务在可用的 worker 上进行动态调度，以便更快的 worker 处理更多的任务。 受限编程模型还允许我们在接近作业结束时安排任务的冗余执行，这大大减少了在存在不一致(例如缓慢或卡住的 worker)的情况下的完成时间。 BAD-FS [5] 的编程模型与 MapReduce 非常不同，它的目标是跨广域网执行作业。 然而，有两个基本的相似之处。 两个系统都使用冗余执行来从故障导致的数据丢失中恢复。 两者都使用位置感知调度来减少通过拥塞的网络链路发送的数据量。 TACC [7] 是一个旨在简化高可用网络服务构建的系统。 与 MapReduce 一样，它依赖于重新执行作为实现容错的机制。 8 结论 MapReduce 编程模型在 Google 内部成功应用于多个领域。 我们把这种成功归结为几个方面：首先，由于 MapReduce 封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得 MapReduce 库易于使用。 即便对于完全没有并行或者分布式系统开发经验的程序员而言；其次，大量不同类型的问题都可以通过 MapReduce 简单的解决。 比如，MapReduce 用于生成 Google 的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统； 第三，我们实现了一个在数千台计算机组成的大型集群上灵活部署运行的 MapReduce。 这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决 Google 遇到的其他很多需要大量计算的问题。 我们也从 MapReduce 开发过程中学到了不少东西。 首先，约束编程模式使得并行和分布式计算非常容易，也易于构造容错的计算环境；其次，网络带宽是稀有资源。 大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 第三，多次执行相同的任务可以减少性能缓慢的机器带来的负面影响，同时解决了由于机器失效导致的数据丢失问题。 9 致谢 Josh Levenberg has been instrumental in revising and extending the user-level MapReduce API with a number of new features based on his experience with using MapReduce and other people’s suggestions for enhancements. MapReduce reads its input from and writes its output to the Google File System [8]. We would like to thank Mohit Aron, Howard Gobioff, Markus Gutschke, David Kramer, Shun-Tak Leung, and Josh Redstone for their work in developing GFS. We would also like to thank Percy Liang and Olcan Sercinoglu for their work in developing the cluster management system used by MapReduce. Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike, and Debby Wallach provided helpful comments on earlier drafts of this paper. The anonymous OSDI reviewers, and our shepherd, Eric Brewer, provided many useful suggestions of areas where the paper could be improved. Finally, we thank all the users of MapReduce within Google’s engineering organization for providing helpful feedback, suggestions, and bug reports. 参考资料 [1] Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson. High-performance sorting on networks of workstations. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, Tucson, Arizona, May 1997. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick.Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. Charlotte: Metacomputing on the web. In Proceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. [4] Luiz A. Barroso, Jeffrey Dean, and Urs H¨olzle. Web search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28, April 2003. [5] John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, and Miron Livny. Explicit control in a batch-aware distributed file system. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation NSDI, March 2004. [6] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(11), November 1989. [7] Armando Fox, Steven D. Gribble, Yatin Chawathe, Eric A. Brewer, and Paul Gauthier. Cluster-based scalable network services. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 78–91, Saint-Malo, France, 1997. [8] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google file system. In 19th Symposium on Operating Systems Principles, pages 29–43, Lake George, New York, 2003. [9] S. Gorlatch. Systematic efficient parallelization of scan and other list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, Euro-Par’96. Parallel Processing, Lecture Notes in Computer Science 1124, pages 401–408. Springer-Verlag, 1996. [10] Jim Gray. Sort benchmark home page. http://research.microsoft.com/barc/SortBenchmark/. [11] William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, Cambridge, MA, 1999. [12] L. Huston, R. Sukthankar, R. Wickremesinghe, M. Satyanarayanan, G. R. Ganger, E. Riedel, and A. Ailamaki. Diamond: A storage architecture for early discard in interactive search. In Proceedings of the 2004 USENIX File and Storage Technologies FAST Conference, April 2004. [13] Richard E. Ladner and Michael J. Fischer. Parallel prefix computation. Journal of the ACM, 27(4):831–838, 1980. [14] Michael O. Rabin. Efficient dispersal of information for security, load balancing and fault tolerance. Journal of the ACM, 36(2):335–348, 1989. [15] Erik Riedel, Christos Faloutsos, Garth A. Gibson, and David Nagle. Active disks for large-scale data processing. IEEE Computer, pages 68–74, June 2001. [16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed computing in practice: The Condor experience. Concurrency and Computation: Practice and Experience, 2004. [17] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1997. [18] Jim Wyllie. Spsort: How to sort a terabyte quickly. http://alme1.almaden.ibm.com/cs/spsort.pdf. 附录 A 词频统计 本节包含了一个完整的程序，用于统计在一组命令行指定的输入文件中，每一个不同的单词出现频率。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &quot;mapreduce/mapreduce.h&quot;// User’s map functionclass WordCounter : public Mapper &#123; public: virtual void Map(const MapInput&amp; input) &#123; const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) &#123; // Skip past leading whitespace while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // Find word end int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++; if (start &lt; i) Emit(text.substr(start,i-start),&quot;1&quot;); &#125; &#125;&#125;;REGISTER_MAPPER(WordCounter);// User’s reduce functionclass Adder : public Reducer &#123; virtual void Reduce(ReduceInput* input) &#123; // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-&gt;done()) &#123; value += StringToInt(input-&gt;value()); input-&gt;NextValue(); &#125; // Emit sum for input-&gt;key() Emit(IntToString(value)); &#125;&#125;;REGISTER_REDUCER(Adder);int main(int argc, char** argv) &#123; ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into &quot;spec&quot; for (int i = 1; i &lt; argc; i++) &#123; MapReduceInput* input = spec.add_input(); input-&gt;set_format(&quot;text&quot;); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class(&quot;WordCounter&quot;); &#125;// Specify the output files:// /gfs/test/freq-00000-of-00100// /gfs/test/freq-00001-of-00100// ...MapReduceOutput* out = spec.output();out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);out-&gt;set_num_tasks(100);out-&gt;set_format(&quot;text&quot;);out-&gt;set_reducer_class(&quot;Adder&quot;);// Optional: do partial sums within map// tasks to save network bandwidthout-&gt;set_combiner_class(&quot;Adder&quot;);// Tuning parameters: use at most 2000// machines and 100 MB of memory per taskspec.set_machines(2000);spec.set_map_megabytes(100);spec.set_reduce_megabytes(100);// Now run itMapReduceResult result;if (!MapReduce(spec, &amp;result)) abort();// Done: ’result’ structure contains info// about counters, time taken, number of// machines used, etc.return 0;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://wangqian0306.github.io/tags/MapReduce/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"Spark shuffle","slug":"bigdata/spark-shuffle","date":"2021-06-02T14:13:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2021/spark-shuffle/","permalink":"https://wangqian0306.github.io/2021/spark-shuffle/","excerpt":"","text":"Spark shuffle 简介 Spark 的核心数据结构在计算过程中是保持不变的，这意味着它们在创建之后无法更改。 在实际的使用过程中我们需要将操作相关内容发送给 Spark 而这个过程被称为转换。 转换又分为两类： 窄依赖 宽依赖 区别宽窄依赖的方式是明确操作是否会对多个输出分区造成影响。 窄依赖仅仅会决定一个输出分区的转换，而宽依赖转换则相反。 通常宽依赖关系的转换经常被称为洗牌 (shuffle) 操作，它会在整个集群中执行互相交换分区数据的功能。 触发洗牌 手动重新分区 在程序执行中可以手动将数据进行分区，这一操作同时也会触发洗牌。常用的方式如下： 获取分区信息 1234// in Scaladf.rdd.getNumPartitions // 1# in Pythondf.rdd.getNumPartitions() # 1 手动重新分区 123456789101112// in Scaladf.repartition(5)# in Pythondf.repartition(5)// in Scaladf.repartition(col(&quot;DEST_COUNTRY_NAME&quot;))# in Pythondf.repartition(col(&quot;DEST_COUNTRY_NAME&quot;))// in Scaladf.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;))# in Pythondf.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)) 合并分区操作（不会全面洗牌，但是尝试合并分区） 1234// in Scaladf.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).coalesce(2)# in Pythondf.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).coalesce(2) 数据操作 ByKey 操作(除了 counting) groupBy reduceByKey cogroup join 执行流程及逻辑 注：目前版本的 Spark 只使用 SortShuffleManager。 官方说明 在基于排序的 shuffle 中，传入的记录根据其目标分区 ID 进行排序，然后写入单个输出文件。 Reducers 获取该文件的连续区域，以便读取它们需要的 Map 输出。 在 Map 输出数据太多而无法放入内存的情况下，输出的排序子集可以溢写到磁盘， 通过合并文件以生成最终输出文件。 基于排序的 shuffle 有两种不同的写入路径来生成其地图输出文件： 序列化排序：当以下三个条件都成立时使用： shuffle 的依赖项指定没有 map-side combine。 shuffle 序列化器支持序列化值的重定位 (目前 KryoSerializer 和 Spark SQL 的自定义序列化器支持此功能)。 shuffle 产生少于或等于 16777216 个输出分区。 反序列化排序 (bypass)：用于处理所有其他情况。 序列化排序方式 在序列化排序模式下，传入的记录一旦传递到 shuffle writer 就会马上被序列化并在排序的过程中进入缓冲。 这种方式有以下几项优化: 它的排序对序列化的二进制数据而不是 Java 对象进行操作，从而减少了内存消耗和 GC 开销。 此优化要求记录序列化器具有某些属性，以允许对序列化的记录进行重新排序，而无需反序列化。 有关更多详细信息，请参阅 SPARK-4550，该优化是首次提出并实施的。 它使用专门的缓存高效排序器 (ShuffleExternalSorter) 对压缩记录指针和分区 ID 的数组进行排序。 通过在排序数组中每条记录仅使用 8 个字节的空间，这可以将更多的数组放入缓存中。 溢出合并过程对属于同一分区的序列化记录块进行操作，并且在合并期间不需要反序列化记录。 当溢写压缩编解码器支持压缩数据的连接时，溢写合并操作会简单的连接序列化和压缩的溢出分区以产生最终输出分区。 有关这些优化的更多详细信息，请参阅 SPARK-7081。 参考资料 Spark Shuffle 概念及 shuffle 机制 Spark 中的 Spark Shuffle 详解 Spark Shuffle 管理器SortShuffleManager内核原理深入剖析 Spark The Definitive Guide","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"Kafka 消息顺序","slug":"bigdata/kafka-order","date":"2021-05-31T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/kafka-order/","permalink":"https://wangqian0306.github.io/2021/kafka-order/","excerpt":"","text":"Kafka 消息顺序 简介 在需要保证消息顺序的情况下应当采用如下的方式使用 Kafka。 概念说明 在 Kafka 中每个 topic 会有一个或多个 partition，而在含有多个 partition 的情况下就无法保证消息有序了。 那我们可以通过限制 partition 数量的方式来确保顺序。 注： 由于 Kafka 的分区数量可以增加所以此处还需额外针对 key 和 partition 进行维护。 官方说明 123Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition&#x27;s events in exactly the same order as they were written. 注：节选自 Kafka 官方文档 1.1 节 Main Concepts and Terminology 实现方式 所以可以得出以下结论： 在同 key 在同个 partition 中，Kafka 的消息顺序是可以保证的。 所以在实际使用中，我们可以为需要排序的内容指定 partition 的方式来确保消息顺序。 可能遇到的问题 当 Kafka 在遇到消息发送失败的情况下会进行如下图所示的重试： 而这种情况又会导致 乱序。 为了解决此问题有两种解决方案： 抛弃重试策略 (不可接受) 在重试时不接受其他消息 在《Kafka 权威指南》这本书里的描述是这样的： Kafka 可以保证同一个分区里的消息是有序的。也就是说，如果生产者按照 一定的顺序发送消息，broker 就会按照这个顺序把它们写入分区，消费者也 会按照同样的顺序读取它们。在某些情况下，顺序是非常重要的。例如，往 一个账户存入 100 元再取出来，这个与先取钱再存钱是截然不同的！不过， 有些场景对顺序不是很敏感。 如果把 retries 设为非零整数，同时把 max.in.flight.requests.per.connection 设为比 1 大的数，那么，如果第一个批次消息写入失败，而第二个批次写入 成功，broker 会重试写入第一个批次。如果此时第一个批次也写入成功，那 么两个批次的顺序就反过来了。 一般来说，如果某些场景要求消息是有序的，那么消息是否写入成功也是 很关键的，所以不建议把 retries 设为 0。可以把 max.in.flight.requests. per.connection 设为 1，这样在生产者尝试发送第一批消息时，就不会有其 他的消息发送给 broker。不过这样会严重影响生产者的吞吐量，所以只有在 对消息的顺序有严格要求的情况下才能这么做。 参考资料 Kafka 官方文档 Kafka : Ordering Guarantees Kafka The Definitive Guide 注：《Kafka 权威指南》有中文版","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"Mendix 试用","slug":"low_code/mendix-toturial","date":"2021-05-17T12:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/mendix-tutorial/","permalink":"https://wangqian0306.github.io/2021/mendix-tutorial/","excerpt":"","text":"简介 Mendix 平台旨在在整个应用程序开发生命周期（从构思到开发，部署以及对云或本地应用程序组合的持续管理）中加速企业应用程序交付。 Mendix 不仅提供 无代码 （可视化编辑模式），也提供 低代码（高度可扩展的集成工具，以支持跨职能团队协作工作）。诸如分析师和普通的开发人员之类的业务领域专家可以与专家开发人员一起工作，以实现更高水平的业务敏捷性，从而加快交付速度，而该平台的云原生架构和自动化工具则支持高可用性的部署，管理和监视。企业级应用程序。 生态 Online Project Management (在线项目管理) App Store (插件中心) Atlas UI (UI 设计语言) Mendix Studio (在线编辑器) Mendix Studio Pro (本地编辑器) Support (7*24邮件技术支持) Commuity (开发社区) Academy (官方学院) 简单使用 Online Project Management 访问 https://sprintr.home.mendix.com/ 选择要管理的项目 选择控制栏左侧的 Stories 选项卡即可 App Store https://marketplace.mendix.com/ Mendix Stuido 访问 https://sprintr.home.mendix.com/ 选择项目然后点击 Edit in Mendix Studio 即可 Mendix Studio Pro 注：Mendix Studio Pro 当前仅支持 Windows 平台 下载链接： https://marketplace.mendix.com/link/studiopro/ 社区地址 https://community.mendix.com/ Academy 在线课程 https://academy.mendix.com/link/home 参考资料 官方文档 https://docs.mendix.com/","categories":[{"name":"Low_Code","slug":"Low-Code","permalink":"https://wangqian0306.github.io/categories/Low-Code/"}],"tags":[{"name":"Low_Code","slug":"Low-Code","permalink":"https://wangqian0306.github.io/tags/Low-Code/"}]},{"title":"ZeroMQ 入门","slug":"mq/zeromq","date":"2021-04-25T12:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/zeromq/","permalink":"https://wangqian0306.github.io/2021/zeromq/","excerpt":"","text":"简介 ZeroMQ（也称为ØMQ，0MQ或ZMQ）是一种高性能的异步消息传递库，旨在用于分布式或高并发的应用程序中。 ZeroMQ 提供了一个消息队列，但是与传统面向消息的中间件不同，ZeroMQ 可以在没有专有消息代理服务的情况下运行。 官方文档 连接例程 安装依赖包 1pip3 install pyzmq --user 编写服务端代码 1234567891011121314151617181920212223## Hello World server in Python# Binds REP socket to tcp://*:5555# Expects b&quot;Hello&quot; from client, replies with b&quot;World&quot;#import timeimport zmqcontext = zmq.Context()socket = context.socket(zmq.REP)socket.bind(f&quot;tcp://*:5555&quot;)while True: # Wait for next request from client message = socket.recv() print(f&quot;Received request: &#123;message&#125;&quot;) # Do some &#x27;work&#x27; time.sleep(1) # Send reply back to client socket.send(b&quot;World&quot;) 编写客户端代码 1234567891011121314151617181920212223## Hello World client in Python# Connects REQ socket to tcp://localhost:5555# Sends &quot;Hello&quot; to server, expects &quot;World&quot; back#import zmqcontext = zmq.Context()# Socket to talk to serverprint(&quot;Connecting to hello world server…&quot;)socket = context.socket(zmq.REQ)socket.connect(f&quot;tcp://localhost:5555&quot;)# Do 10 requests, waiting each time for a responsefor request in range(10): print(f&quot;Sending request &#123;request&#125; …&quot;) socket.send(b&quot;Hello&quot;) # Get the reply. message = socket.recv() print(f&quot;Received reply &#123;request&#125; [ &#123;message&#125; ]&quot;)","categories":[{"name":"MQ","slug":"MQ","permalink":"https://wangqian0306.github.io/categories/MQ/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"ZeroMQ","slug":"ZeroMQ","permalink":"https://wangqian0306.github.io/tags/ZeroMQ/"}]},{"title":"Mosquitto 入门","slug":"mq/mosquitto","date":"2021-04-23T14:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2021/mosquitto/","permalink":"https://wangqian0306.github.io/2021/mosquitto/","excerpt":"","text":"简介 Eclipse Mosquitto 是一款实现了 MQTT 协议 5.0，3.1.1 和 3.1 版本的开源消息代理软件（ EPL/EDL 许可证）。 Mosquitto 非常轻量化，能够在低功耗的单板计算机和完整服务器的上使用。 官方文档 安装方式 (Docker) 编辑 mosquitto.conf 配置文件 12listener 1883 0.0.0.0allow_anonymous true 编辑 docker-compose.yaml 12345678910111213services: app-mosquitto: container_name: mosquitto hostname: mosquitto image: eclipse-mosquitto:latest ports: - 1883:1883 - 8883:8883 read_only: true volumes: - type: bind source: ./mosquitto.conf target: /mosquitto/config/mosquitto.conf 启动服务 1docker-compose up -d 关闭服务 1docker-compose down 常用命令 订阅 1mosquitto_sub -t &lt;topic&gt; -u &lt;user&gt; -P &lt;password&gt; 发布 1mosquitto_pub -h &lt;host&gt; -t &lt;topic&gt; -m &quot;&lt;message&gt;&quot; -u &lt;user&gt; -P &lt;password&gt; 连接例程 安装软件包 1pip3 install paho-mqtt --user 编写连接程序 12345678910111213141516171819import paho.mqtt.client as mqttdef on_connect(mq_client, user_data, flags, rc): print(&quot;Connected with result code &quot; + rc) client.subscribe(&quot;#&quot;)def on_message(mq_client, user_data, msg): print(msg.topic + &quot; &quot; + str(msg.payload))if __name__ == &quot;__main__&quot;: client = mqtt.Client() client.on_connect = on_connect client.on_message = on_message client.connect(&quot;0.0.0.0&quot;, 1883) client.publish(&quot;test&quot;, b&quot;nice&quot;) client.loop_forever()","categories":[{"name":"MQ","slug":"MQ","permalink":"https://wangqian0306.github.io/categories/MQ/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"Mosquitto","slug":"Mosquitto","permalink":"https://wangqian0306.github.io/tags/Mosquitto/"},{"name":"MQTT","slug":"MQTT","permalink":"https://wangqian0306.github.io/tags/MQTT/"}]},{"title":"EdgeX 平台初探","slug":"go/edgex","date":"2021-04-16T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/edgex/","permalink":"https://wangqian0306.github.io/2021/edgex/","excerpt":"","text":"EdgeX 总体架构 EdgeX 由以下四个基本模块构成 设备服务层 (DEVICE SERVICES) 核心服务层 (CORE SERVICES) 支持服务层 (SUPPORT SERVICES) 应用服务层 (APPLICATION SERVICES) 在上述层级的基础上扩展了以下服务 安全服务 (SECURITY) 系统管理服务 (MANAGEMENT) 设备服务层 设备服务层当中的服务可以自行选择进行安装，目前支持的内容如下： 测试设备 随机数生成器(Go) 虚拟设备(Go) 官方支持的设备 Modbus (Go) MQTT (Go) Grove © Camera (Go) Rest (Go) BACnet © 研发中的设备 OPC UA © Bluetooth © GPS © 收费的连接器(IOTech 公司提供) Modbus BACnet Bluetooth CAN EtherCAT EtherNet/IP FILE zigbee GPS MQTT OPC UA 核心服务层 核心服务层由以下微服务构成，必须在系统中安装： 服务名 英文名 描述 核心数据存储微服务 Core data 集成了数据总线，完成接收数据并进行存储的功能 基础控制微服务 Command 调配系统上下层控制 元数据存储微服务 Metadata 存储元数据，关联设备与服务 配置中心微服务 Registry and Configuration 存储微服务配置 注：配置中心微服务由 Consul 实现。 支持服务层 支持服务包括范围广泛的微服务，包括边缘分析。 正常的软件应用程序职责，如日志记录、调度和数据清理由支持服务层中的微服务执行。 支持服务层由以下微服务构成，如下服务都可以选择安装： 服务名 英文名 描述 规则引擎微服务 Rules Engine 完成部分边缘分析功能 计划微服务 Scheduling 定时触发系统内的 REST API 警报微服务 Alerts and Notifications 输出报警信息 注: 目前的规则引擎服务由 EMQ X Kuiper 实现。 应用服务层 本层中尚且没有提供成品的微服务，而是提供了 SDK。 在 SDK 中主要实现了以下功能 基于 EdgeX 的消息总线触发器(支持 ZeroMQ, MQTT, Redis Streams) 基于 MQTT 的触发器 基于 HTTP 的触发器 安全服务 安全服务分为了以下功能 密钥管理（密钥） 网关 用户管理 ACL 管理 以上功能是结合下列项目实现的: KONG Vault 注: 目前 KONG 项目支持 REST API 调用，Vault 有多种语言支持。 系统管理服务 系统管理服务目前有两种控制方式分别为 System Management Agent 通过对外开放接口的形式来让中心节点操控边缘端 System Management Executor 在集群内使用可执行文件的方式完成边缘端控制","categories":[{"name":"Go","slug":"Go","permalink":"https://wangqian0306.github.io/categories/Go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://wangqian0306.github.io/tags/go/"},{"name":"EdgeX","slug":"EdgeX","permalink":"https://wangqian0306.github.io/tags/EdgeX/"}]},{"title":"Go 语言入门","slug":"go/go","date":"2021-04-01T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/go/","permalink":"https://wangqian0306.github.io/2021/go/","excerpt":"","text":"安装 使用如下命令即可安装： 1234wget https://go.dev/dl/go&lt;vesion&gt;.linux-amd64.tar.gzsudo rm -rf /usr/local/go sudo tar -C /usr/local -xzf go&lt;vesion&gt;.linux-amd64.tar.gzexport PATH=$PATH:/usr/local/go/bin 注：PATH 环境变量记得写入对应 profile 里。 gvm gvm 是一款 go 语言的管理工具，使用如下命令即可安装： 12sudo apt-get install bisonbash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) 使用如下命令即可完成特定版本的安装： 1gvm install go1.23.3 软件源配置 配置国内软件源 1go env -w GOPROXY=https://goproxy.cn,direct 安装第三方项目或命令 由于安装的权限与位置是和运行的用户有关系的所以需要配置如下环境变量： 1export PATH=$PATH:$(go env GOPATH)/bin 常用 Web 框架 Beego 注：此框架被很多国内公司采用。官网列出的企业有：华为企业云，京东，淘宝，美团，链家等。 Gin Echo 注: 滴滴采用。 Iris 注：官网描述为等价于 expressjs 的 go 语言框架 revel 常用学习资料 官网 在线编程学习网站","categories":[{"name":"Go","slug":"Go","permalink":"https://wangqian0306.github.io/categories/Go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://wangqian0306.github.io/tags/go/"}]},{"title":"AsciiDoc","slug":"tmp/asciidoc","date":"2021-03-08T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/asciidoc/","permalink":"https://wangqian0306.github.io/2021/asciidoc/","excerpt":"","text":"Asciidoc 简介 AsciiDoc 是一种文本文档格式，可以用于书写文档，文章，手册，书籍和 UNIX 手册。AsciiDoc文件可以使用asciidoc命令转换成HTML和DocBook文件格式。AsciiDoc结构先进：AsciiDoc语法和输出标签(几乎可以转换成任意的 SGML/XML 标记)都可以由用户自己定义和扩展。 适用场景： 可自定义的表格及表注释 多文件进行合并渲染 掺杂图表和数学公示以及代码分行注释 可以使用各种主题 注：Asciidoc 像是增强版的 Markdown 相较于 LaTeX 来说更为简便。 处理器 AsciiDoc 语言可以编写文档，这是一种基于文本的书写格式。AsciiDoc 语言被设计为不显眼且简洁，以简化写作。但是 AsciiDoc 本身并不是一种发布格式。它更像是一种速记方式。这就是 AsciiDoc 处理器的用武之地。 AsciiDoc 有很多的处理器(例如 Asciidoctor)读取 AsciiDoc 源并将其转换为可发布的格式，例如 HTML 5 或 PDF。它还可以将其转换为本身可以由发布工具链（例如 DocBook）处理的格式。 与 IDEA 集成 首先需要在 IDEA 的插件商店安装如下插件 AsciiDoc 注：虽然名字是 AsciiDoc 但实际上是 Asciidoctor 的插件。 Asciidoctor 环境安装 Asciidoctor 需要 ruby 环境才能进行安装具体命令如下： 123dnf install ruby -ygem install asciidoctorgem install asciidoctor-pdf --pre 此外由于 Asciidoctor 不支持中文，所以还需要自行安装中文字体和字形，可以使用 参考字库及文件 中的 notosans-cjk-sc.zip 压缩包来预览渲染结果，使用命令如下： 1asciidoctor-pdf -a pdf-theme=default-notosans-cjk-sc-theme.yml -a pdf-fontsdir=. test.adoc 参考资料 语法文档 处理器文档 PDF 主题配置 自定义字体和主题 参考字库及文件 Noto Sans CJK 字库 中文开源字体集","categories":[],"tags":[{"name":"Asciidoc","slug":"Asciidoc","permalink":"https://wangqian0306.github.io/tags/Asciidoc/"}]},{"title":"PlantUML","slug":"tmp/uml","date":"2021-03-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2021/plantuml/","permalink":"https://wangqian0306.github.io/2021/plantuml/","excerpt":"","text":"PlantUML 简介 PlantUML 是一个开源项目，支持快速绘制： 时序图 用例图 类图 对象图 活动图 组件图 部署图 状态图 定时图 同时还支持以下非 UML 图: JSON Data YAML Data Network diagram (nwdiag) 线框图形界面 架构图 规范和描述语言 (SDL) Ditaa diagram 甘特图 MindMap diagram Work Breakdown Structure diagram 以 AsciiMath 或 JLaTeXMath 符号的数学公式 Entity Relationship diagram 此项目就像 Markdown 一样，在编写时采用固定的规范即可书写，但是在显示时却可以展示出图片。非常适合在项目文档中使用，无需将源文件与绘图文件分开保存。 在 IDEA 中使用 首先需要在 IDEA 的插件商店安装如下插件 PlantUML integration 然后就可以在 IDEA 中新建 .puml 文件或者说在 Markdown 中编写代码块，并将其类型指定为 puml 注：在 Markdown 文件编写代码后会在左侧出现下载插件的标识，点击安装后重新渲染文件即可获得图片。 常见问题 找不到 Graphviz 包 可以访问 Graphviz 下载地址 获取软件包，在下载安装完成后需要重启 IDEA 找不到 Graphviz.dot 在 IDEA puml 文件的工具栏中找到设置按钮，点击 Open Settings 选项，然后编辑下面的配置项至指定文件即可 1Graphviz dot executable: &lt;graphviz 安装路径&gt;/bin/dot 与 Hexo 集成 在 package.json 配置文件的 dependencies 部分新增如下插件即可 1hexo-filter-plantuml 获取颜色 在绘图时可以选择使用十六进制代码来指定颜色，或者可以采用已经完成编码的模板颜色。 模板颜色如下： 也可以通过如下文件展示模板颜色 123@startumlcolor@enduml 可以使用如下的写法为元素更改颜色 123@startumlrectangle DEMO #Lightblue@enduml 获取架构图中可用图形 在绘制架构图时可以使用如下命令，展示可以使用的图形 123@startumllistsprite@enduml 结果如下 绘制云平台等复杂图形 plantuml 官方还提供了很多的官方标准库，用于扩展绘图元素。具体内容请参见 官方标准库 及其 标准库项目 参考资料 中文手册 官网 官方标准库","categories":[],"tags":[{"name":"UML","slug":"UML","permalink":"https://wangqian0306.github.io/tags/UML/"}]},{"title":"MyBatis-Plus","slug":"java/mybatis-plus","date":"2021-03-03T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2021/mybatis-plus/","permalink":"https://wangqian0306.github.io/2021/mybatis-plus/","excerpt":"","text":"MyBatis-Plus 简介 MyBatis-Plus(简称 MP)是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，可以简化开发、提高效率。 依赖 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.velocity&lt;/groupId&gt; &lt;artifactId&gt;velocity-engine-core&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt;&lt;/dependency&gt; 代码生成工具 12345678910111213141516171819202122import com.baomidou.mybatisplus.annotation.FieldFill;import com.baomidou.mybatisplus.generator.FastAutoGenerator;import com.baomidou.mybatisplus.generator.config.TemplateType;import com.baomidou.mybatisplus.generator.fill.Column;public class CodeGenerator &#123; public static void main(String[] args) &#123; String pkgPath = System.getProperty(&quot;user.dir&quot;) + &quot;/src/main/java&quot;; FastAutoGenerator.create(&quot;jdbc:mysql://&lt;host&gt;/&lt;database&gt;&quot;, &quot;&lt;username&gt;&quot;, &quot;&lt;password&gt;&quot;) .globalConfig(builder -&gt; builder.outputDir(pkgPath).author(&quot;&lt;author&gt;&quot;) .disableOpenDir()) .packageConfig(builder -&gt; builder.parent(&quot;&lt;package&gt;&quot;)) .templateConfig(builder -&gt; builder.disable(TemplateType.XML)) .strategyConfig((scanner, builder) -&gt; builder.addInclude(&quot;&lt;table-name&gt;&quot;) .controllerBuilder().enableRestStyle().enableHyphenStyle() .entityBuilder().enableLombok().addTableFills( new Column(&quot;create_time&quot;, FieldFill.INSERT) ).build()) .execute(); &#125;&#125; 参考资料 官方文档 样例代码","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wangqian0306.github.io/tags/MyBatis/"}]},{"title":"IoTDB 的初步搭建及基本使用","slug":"bigdata/iotdb","date":"2021-02-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/iotdb/","permalink":"https://wangqian0306.github.io/2021/iotdb/","excerpt":"","text":"IoTDB 的初步搭建及基本使用 简介 Apache IoTDB（物联网数据库）是一体化收集、存储、管理与分析物联网时序数据的软件系统。 安装 Docker 版 新建 docker-compose.yaml 文件，填入如下内容即可 12345678910111213141516171819202122services: iotdb-service: image: 192.168.2.129:5000/mirror/iotdb:latest hostname: iotdb-service container_name: iotdb-service ports: - &quot;1883:1883&quot; environment: - cn_internal_address=iotdb-service - cn_internal_port=10710 - cn_consensus_port=10720 - cn_seed_config_node=iotdb-service:10710 - dn_rpc_address=iotdb-service - dn_internal_address=iotdb-service - dn_rpc_port=6667 - dn_mpp_data_exchange_port=10740 - dn_schema_region_consensus_port=10750 - dn_data_region_consensus_port=10760 - dn_seed_config_node=iotdb-service:10710 volumes: - ./data:/iotdb/data - ./logs:/iotdb/logs 基础使用 命令行 使用如下命令可以进入交互式命令行： 1start-cli.bat -h &lt;host&gt; -p &lt;port&gt; -u &lt;user&gt; -pw &lt;password&gt; IoTDB 采用了类似于 SQL 的语句，常见内容如下： 命令 作用 SHOW STORAGE GROUP 展示存储组 SET STORAGE GROUP TO &lt;user&gt;.&lt;group&gt; 切换或者创建组 CREATE TIMESERIES &lt;user&gt;.&lt;group&gt;.&lt;table&gt; WITH DATATYPE=INT32,ENCODING=RLE; 创建时间序列 SHOW DEVICES 显示所有用户的组？？ SHOW TIMESERIES &lt;user&gt;.&lt;group&gt; 查看用户组中所有的时间序列 insert into root.demo(timestamp,s0) values(1,1); 插入数据点 SELECT * FROM root.demo 检索数据 Python 使用如下命令安裝依赖： 12pip install thirftpip install apache-iotdb 使用如下代码即可链接到服务器： 12345678910from iotdb.Session import Sessionip = &quot;127.0.0.1&quot;port_ = &quot;6667&quot;username_ = &quot;root&quot;password_ = &quot;root&quot;session = Session(ip, port_, username_, password_)session.open(False)zone = session.get_time_zone()session.close() Java 引入如下依赖： 123dependencies &#123; implementation &#x27;org.apache.iotdb:iotdb-session:1.3.1&#x27;&#125; 编写配置类 IoTDBSessionConfig： 123456789101112131415161718192021222324252627282930313233import org.springframework.context.annotation.Configuration;import org.springframework.stereotype.Component;import org.apache.iotdb.session.pool.SessionPool;import org.springframework.beans.factory.annotation.Value;@Component@Configurationpublic class IoTDBSessionConfig &#123; @Value(&quot;$&#123;spring.iotdb.username:root&#125;&quot;) private String username; @Value(&quot;$&#123;spring.iotdb.password:root&#125;&quot;) private String password; @Value(&quot;$&#123;spring.iotdb.ip:127.0.0.1&#125;&quot;) private String ip; @Value(&quot;$&#123;spring.iotdb.port:6667&#125;&quot;) private int port; @Value(&quot;$&#123;spring.iotdb.maxSize:10&#125;&quot;) private int maxSize; private static SessionPool sessionPool; public SessionPool getSessionPool() &#123; if (sessionPool == null) &#123; sessionPool = new SessionPool(ip, port, username, password, maxSize); &#125; return sessionPool; &#125;&#125; 编写测试类 TestController ： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.apache.iotdb.isession.SessionDataSet;import org.apache.iotdb.rpc.IoTDBConnectionException;import org.apache.iotdb.rpc.StatementExecutionException;import org.apache.iotdb.session.pool.SessionPool;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;@Slf4j@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource private IoTDBSessionConfig iotDBSessionConfig; private static Object convertValueByType(SessionDataSet.DataIterator dataIterator, String columnName, String columnType) throws StatementExecutionException &#123; return switch (columnType) &#123; case &quot;BOOLEAN&quot; -&gt; dataIterator.getBoolean(columnName); case &quot;TEXT&quot; -&gt; dataIterator.getString(columnName); case &quot;VECTOR&quot; -&gt; dataIterator.getObject(columnName); case &quot;INT32&quot; -&gt; dataIterator.getInt(columnName); case &quot;INT64&quot; -&gt; dataIterator.getLong(columnName); case &quot;FLOAT&quot; -&gt; dataIterator.getFloat(columnName); case &quot;DOUBLE&quot; -&gt; dataIterator.getDouble(columnName); case &quot;TIMESTAMP&quot; -&gt; dataIterator.getTimestamp(columnName); default -&gt; throw new IllegalArgumentException(&quot;Unsupported DataType: &quot; + columnType); &#125;; &#125; @GetMapping public String select() throws IoTDBConnectionException, StatementExecutionException &#123; SessionPool sessionPool = iotDBSessionConfig.getSessionPool(); SessionDataSet dataSet = sessionPool.executeQueryStatement(&quot;select * from root.sg.d1&quot;).getSessionDataSet(); List&lt;String&gt; columnNames = dataSet.getColumnNames(); List&lt;String&gt; columnTypes = dataSet.getColumnTypes(); List&lt;Map&lt;String, Object&gt;&gt; dataList = new ArrayList&lt;&gt;(); SessionDataSet.DataIterator dataIterator = dataSet.iterator(); while (dataIterator.next()) &#123; Map&lt;String, Object&gt; row = new HashMap&lt;&gt;(); for (int i = 0; i &lt; columnNames.size(); i++) &#123; row.put(columnNames.get(i), convertValueByType(dataIterator, columnNames.get(i), columnTypes.get(i))); &#125; dataList.add(row); &#125; log.error(dataList.toString()); return &quot;success&quot;; &#125;&#125; 进阶配置 MQTT 服务 修改文件 iotdb-common.properties 中的如下内容： 1enable_mqtt_service=true 注：原先的配置文件位于 /iotdb/conf/iotdb-common.properties 中，可以先启动服务然后使用 docker cp 命令将文件复制到本地进行修改。 之后修改 docker-compose.yaml 文件即可： 1234567ports: - &quot;6667:6667&quot; - &quot;1883:1883&quot;volumes: - ./conf/iotdb-common.properties:/iotdb/conf/iotdb-common.properties - ./data:/iotdb/data - ./logs:/iotdb/logs 之后即可使用 默认账户 向 MQTT 中写入如下 JSON ： 123456&#123; &quot;device&quot;:&quot;root.sg.d1&quot;, &quot;timestamp&quot;:1586076045524, &quot;measurements&quot;:[&quot;s1&quot;,&quot;s2&quot;], &quot;values&quot;:[0.530635,0.530635]&#125; 写入完成后即可在 iotdb 内检索到数据。 代码编译 注： IoTDB 向 Hadoop 存储数据的软件包需要重新编译。 在官网下载源码包并解压 使用如下命令进行构建 1mvn clean package -pl server,hadoop -am 构建完成后将 IoTData 的 Hadoop 模块中的 hadoop-tsfile-0.10.0-jar-with-dependencies.jar 拷贝至 server/target/iotdb-server-&lt;version&gt;/lib 下 编译常见问题 maven 无法拉取依赖包 ISSUE 解决方式： 从可执行软件包中获取 jar 包，然后手动将其拷贝至 maven 本地目录中 maven 无法拉取 thrift exe 库文件 进入源码中的 thrift/target/tools 目录，手动下载库文件 1wget https://github.com/jt2594838/mvn-thrift-compiler/raw/master/thrift_0.12.0_0.13.0_linux.exe 重新编译 1mvn package -pl server,hadoop -am 参考资料 官网地址 MQTT 插件 spring boot使用IoTDB的两种方式","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"IoTDB","slug":"IoTDB","permalink":"https://wangqian0306.github.io/tags/IoTDB/"}]},{"title":"Hive CLI","slug":"bigdata/hive-cli","date":"2021-01-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/hive-cli/","permalink":"https://wangqian0306.github.io/2021/hive-cli/","excerpt":"","text":"Hive CLI 简介 Hive 基本命令整理 Hive 命令行梳理 查看数据库清单 1show databases; 使用数据库 1use &lt;name&gt;; 查看数据表清单 1show tables; 创建表 1234567891011CREATE TABLE page_view( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT &#x27;IP Address of the User&#x27;) COMMENT &#x27;This is the page view table&#x27; PARTITIONED BY (dt STRING, country STRING) STORED AS SEQUENCEFILE; 查看表详情 1DESCRIBE [EXTENDED] page_view; 查看表分区 1SHOW PARTITIONS page_view; 重命名表 1ALTER TABLE old_table_name RENAME TO new_table_name; 重命名列 1ALTER TABLE old_table_name REPLACE COLUMNS (col1 TYPE, ...); 新增列 1ALTER TABLE tab1 ADD COLUMNS (c1 INT COMMENT &#x27;a new int column&#x27;, c2 STRING DEFAULT &#x27;def val&#x27;); 删除分区 1ALTER TABLE pv_users DROP PARTITION (ds=&#x27;2008-08-08&#x27;) 从 HDFS 载入数据至 Hive 1LOAD DATA INPATH &#x27;/tmp/test.txt&#x27; INTO TABLE page_view; 从 HDFS 导入数据至 Hive 123IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=&quot;value&quot;[, ...])]] FROM &#x27;source_path&#x27; [LOCATION &#x27;import_target_path&#x27;] 从 Hive 导出数据至 HDFS 1EXPORT TABLE page_view to &#x27;/page_view&#x27;; 参考资料 基本命令官方文档 导入导出官方文档 查询语句官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Hive","slug":"Hive","permalink":"https://wangqian0306.github.io/tags/Hive/"}]},{"title":"HBase 命令行整理","slug":"bigdata/hbase-cli","date":"2021-01-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/hbase-cli/","permalink":"https://wangqian0306.github.io/2021/hbase-cli/","excerpt":"","text":"HBase 命令行整理 内部命令 查看用户名 1whoami 查看集群信息 1status 列出所有表 1list 建表 1create &#x27;&lt;table&gt;&#x27;,&#x27;&lt;column_family_1&gt;&#x27;,&#x27;&lt;column_family_2&gt;&#x27;,... 注： 使用 SPLITS =&gt; [‘10’, ‘20’, ‘30’, ‘40’] 指定分片方式。 使用 {NUMREGIONS =&gt; 15, SPLITALGO =&gt; ‘HexStringSplit’} 指定分片数量和分片依据。 插入或修改数据 1put &#x27;&lt;table&gt;&#x27;,&#x27;&lt;row_key&gt;&#x27;,&#x27;&lt;column_family:column&gt;&#x27;,&#x27;&lt;value&gt;&#x27; 列出所有数据 1scan &#x27;&lt;table&gt;&#x27;,&#x27;&lt;column_family&gt;&#x27; 读取数据 1get &#x27;&lt;table&gt;&#x27;,&#x27;&lt;row_key&gt;&#x27;,&#x27;&lt;column_family&gt;&#x27; 统计表中的数据量 1count &#x27;&lt;table&gt;&#x27; 注：如果数据量较大建议使用外部命令部分的统计功能 禁用并删除表 12disable &#x27;&lt;table&gt;&#x27;drop &#x27;&lt;table&gt;&#x27; 外部命令 统计表行数 1hbase org.apache.hadoop.hbase.mapreduce.RowCounter &#x27;&lt;table&gt;&#x27; 常见问题 Python 链接出现 TSocket read 0 bytes 进入 HBase Thrift Server 配置项当中，关闭如下配置项： hbase.regionserver.thrift.compact hbase.regionserver.thrift.framed 然后进入 HBase 配置项中，关闭如下配置项： hbase.regionserver.thrift.http hbase.thrift.support.proxyuser REGION_SERVER_COMPACTION_QUEUE 错误日志如下： 123The health test result for REGION_SERVER_COMPACTION_QUEUE has become disabled: Test disabled while the role is stopping: Test of whether the RegionServer&#x27;s compaction queue is too full. 根因分析： HBase 在数据写入时会先将数据写到内存中的 MemStore，然后再将数据刷写到磁盘的中。 每次 MemStore Flush 都会为每个列族创建一个 HFile，频繁的 Flush 就会创建大量的 HFile，并且会使得 HBase 在检索的时候需要读取大量的 HFile，较多的磁盘 IO 操作会降低数据的读性能。 而在此时写入的 StoreFile 数量增加，触发了 Compaction，将任务放入了队列中。所以想要解决此问题就要扩大 Compaction 队列的数量或者增加 MemStore Flush 的文件大小。 相关配置项如下： 12345hbase.hregion.memstore.flush.size=256Mhbase.hregion.memstore.block.multiplier=8hbase.hstore.compaction.min=10hbase.hstore.compaction.max=20hbase.hstore.blockingStoreFiles=50 参考资料 官方文档 HBase异常问题分析 Hbase Compaction 队列数量较大分析(压缩队列、刷新队列）","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"HBase","slug":"HBase","permalink":"https://wangqian0306.github.io/tags/HBase/"}]},{"title":"Hive HA","slug":"bigdata/hive-ha","date":"2021-01-13T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2021/hive-ha/","permalink":"https://wangqian0306.github.io/2021/hive-ha/","excerpt":"","text":"Hive HA 简介 本 HA 方案采用将 Hive 与 ZooKeeper 结合的方式，将多个 HiveServer 进行整合，并通过 ZooKeeper 对外提供服务。 配置 HiveServer2 进入设置 筛选 HiveServer2 服务 搜索 xml 配置 新增如下配置项 1234&lt;property&gt; &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 1234&lt;property&gt; &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt; &lt;value&gt;hiveserver2&lt;/value&gt;&lt;/property&gt; 保存并重启服务 注：除了此种方案之外还可以使用 HAProxy 具体参见 官方文档 Hive Metastore 进入设置 筛选 Hive Metastore Server 服务 选择 Advanced 类型 找到 Hive Metastore Delegation Token Store 配置项 将其修改为 org.apache.hadoop.hive.thrift.DBTokenStore 保存并重启服务 官方文档 连接样例 pom.xml 12345678910111213141516171819202122&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.0.0-cdh6.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.0.0-cdh6.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.1-cdh6.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.1.1-cdh6.3.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.security.UserGroupInformation;import java.io.IOException;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class Test &#123; private static final String jdbc_class = &quot;org.apache.hive.jdbc.HiveDriver&quot;; private static final String connection = &quot;jdbc:hive2://&lt;zookeeper_host_1&gt;:&lt;zookeeper_port_1&gt;,&lt;zookeeper_host_2&gt;:&lt;zookeeper_port_2&gt;/&lt;db&gt;;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&quot;; public static String krb5ConfigPath = &quot;&lt;krb5.conf_path&gt;&quot;; public static String krb5KeytabPath = &quot;&lt;keytab_path&gt;&quot;; public static String krb5Username = &quot;&lt;principal&gt;&quot;; public static void main(String[] args) throws Exception &#123; System.setProperty(&quot;java.security.krb5.conf&quot;, krb5ConfigPath); Configuration configuration = new Configuration(); configuration.set(&quot;hadoop.security.authentication&quot;, &quot;kerberos&quot;); try &#123; UserGroupInformation.setConfiguration(configuration); UserGroupInformation.loginUserFromKeytab(krb5Username, krb5KeytabPath); &#125; catch (IOException e) &#123; System.out.println(&quot;auth error&quot;); e.printStackTrace(); &#125; String sql = &quot;SHOW tables&quot;; try &#123; Class.forName(jdbc_class); Connection con = DriverManager.getConnection(connection); Statement stmt = con.createStatement(); ResultSet rs = stmt.executeQuery(sql); while (rs.next()) &#123; System.out.println(rs.getString(1)); &#125; con.close(); &#125; catch (Exception e) &#123; System.out.println(&quot;connection error or execute error&quot;); System.out.println(e.getMessage()); &#125; &#125;&#125; 注: 如果没有开启 Kerberos 则可以删去 String sql 之前的内容。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Hive","slug":"Hive","permalink":"https://wangqian0306.github.io/tags/Hive/"}]},{"title":"The Google File System 中文翻译版","slug":"treatise/the_google_file_system","date":"2021-01-07T14:26:13.000Z","updated":"2025-01-08T02:56:21.490Z","comments":true,"path":"2021/the_google_file_system/","permalink":"https://wangqian0306.github.io/2021/the_google_file_system/","excerpt":"","text":"The Google File System 中文翻译版 作者： Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung 原版的版权说明 12345678Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee.SOSP’03, October 19–22, 2003, Bolton Landing, New York, USA.Copyright 2003 ACM 1-58113-757-5/03/0010 ...$5.00. 摘要 我们设计并实现了谷歌文件系统，这是一个面向大规模数据密集型应用的、可伸缩的分布式文件系统。 GFS 虽然运行在廉价的硬件设备上，但是它依然了提供灾难冗余的能力，为大量客户机提供了高性能的服务。 虽然 GFS 与许多传统的分布式文件系统有很多共同的目标，但是由于我们是基于目前现有应用程序的负载和技术环境而驱动 GFS 设计的， 所以在目前和可以预见的以后，GFS 和早期的分布式文件系统有明显的不同。这促使我们重新审视传统文件系统，衍生出了完全不同的设计思路。 GFS 完全满足了我们对存储的需求。 GFS 已经被广泛的部署在 Google 内部作为存储平台，存放我们的服务产生和处理的数据，同时还用于那些需要大规模数据集的研究和开发工作。 最大的一个集群基于数千个服务器和数千个硬盘提供了数百 TB 的存储空间并同时为数百个客户端服务。 在此论文中，我们会展示为了支持分布式应用程序的文件系统扩展程序，讨论一些设计方面的内容， 并且提供一些小规模性能测试结果以及真实生产系统的测试结果。 1 引言 为了满足谷歌迅速增长的数据处理需求，我们设计并实现了谷歌文件系统(GFS)。 GFS 与传统的分布式文件系统有着很多相同的设计目标，比如，性能、可伸缩性、可靠性以及可用性。 但是由于我们是基于目前现有应用程序的负载和技术环境而驱动 GFS 设计的，所以在目前和可以预见的以后，GFS 和早期的分布式文件系统有明显的不同。 我们重新审视传统文件系统，衍生出了完全不同的设计思路。 第一，组件失效被认为是常态事件，而不是意外事件。 GFS 可能由数百甚至数千个使用廉价硬件设备而搭建的存储设备组成，它还能被相当数量的客户端访问。 GFS 组件的数量和质量导致在事实上，任何给定时间内都有可能发生某些组件无法工作，某些组件无法从它们目前的失效状态中恢复。 我们遇到过各种各样的问题，比如程序错误、系统错误、人为失误，甚至还有硬盘、内存、线材、网络以及电源造成的问题。 因此持续监控，错误检测，灾难冗余和自动恢复的机制必须集成在 GFS 中。 第二，文件相对于传统标准来说比较大。几个 GB 的文件是最常见的。每个文件通常包含有多个应用程序的对象，例如 Web 文档。 当我们经常需要处理快速增长的、并且由数亿个对象构成的、数以 TB 的数据集时，采用管理数亿个 KB 大小的小文件的方式是非常不明智的， 尽管有些文件系统支持这样的管理方式。 因此，设计的假设条件和参数，比如I/O操作和Block的尺寸都需要重新考虑。 第三，绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。 对文件的随机写入操作在实际中几乎不存在。 一旦写入完毕之后，对文件的操作就只有读，而且通常是按顺序读。 大量的数据符合这些特性。 比如有些数据可能会构成大型存储库，数据分析程序会扫描这些存储库。 有些是正在运行的应用程序生成的数据流； 有些是数据存档； 有些由一台机器生成、另外一台机器处理的中间数据，这些中间数据的处理可能是同时进行的、也可能是后续才处理的。 对于这种针对海量文件的访问模式，客户端对数据块缓存是没有意义的，数据的追加操作是性能优化和原子性保证的主要考量因素。 第四，应用程序和文件系统 API 的协同设计提高了整个系统的灵活性。 例如，我们放松了 GFS 的一致性模型，以极大地简化文件系统，而不会给应用程序带来繁重的负担。 我们引入了原子性的记录追加操作，从而保证多个客户端能够并行进行追加操作，不需要额外的同步操作来保证数据的一致性。 本文后面还有对这些问题的细节的详细讨论。 目前我们已经为了不同的需求搭建了多套 GFS 集群。 最大的集群有1000个存储节点，超过 300 TB 的存储空间，并且被不同机器上的数百个客户端连续不断的频繁访问。 2 设计概述 2.1 设计预期 在设计满足我们需求的文件系统时，无数的机遇和挑战指引着我们。 之前我们已经提到了一些需要关注的关键点，这里我们将对设计预期的细节展开讨论。 该系统由许多经常发生故障的廉价组件组成。它必须不断地自我监控，定期检测，容忍错误并能从组件故障中迅速恢复。 系统中存在一定数量大文件。我们推测将会有几百万个文件，每个文件的大小通常为 100 MB或更大。 数个 GB 的文件是常见情况，应进行有效管理。 此系统还必须支持小文件的存储，但是我们不需要对其进行优化。 此系统的负载主要由以下两种读取方式构成：大型流式读取和小型随机读取。 在大型流式读取的过程中每个独立的操作通常读取几百 KB，更常见的是 1 MB 甚至更多。 来自同一客户端的连续操作通常会读取同一文件的连续区域。少量随机读取通常会以任意偏移量读取几个 KB。 如果应用程序对性能非常关注，通常的做法是把小规模的随机读取操作合并并排序，之后按顺序批量读取，这样就避免了在文件中前后来回的移动读取位置。 在系统中的负载也有很多是大型有序的追加写入操作。在此过程中的操作大小和读取操作应该是类似的。一旦写入之后文件就不会经常变动了。 本系统还支持对文件的任意位置进行小型的写入操作，但是效率不一定高。 系统必须高效的、行为定义明确的实现多客户端并行追加数据到同一个文件里。 我们的文件通常被用于生产-消费者队列中的合并操作。数百个生产者服务每个服务运行在一台设备上，同时对一个文件进行追加操作。 使用最小的同步开销来实现的原子的多路追加数据操作是必不可少的。 这个文件可能之后才会被读取或者一个消费者正在同时读取文件。 高的持续带宽比低延迟更重要。通常我们的目标程序都非常重视以高速率处理大量数据，而很少有对单个读取或写入有严格响应时间要求的应用程序。 2.2 程序接口 GFS 提供了与文件系统类似的程序接口，虽然没有实现诸如 POSIX 之类的标准 API。 文件以分层目录的形式组织，用路径名来标识。 我们支持常用的操作，如创建新文件、删除文件、打开文件、关闭文件、读和写文件。 注： POSIX 维基百科 此外，GFS具有快照和记录追加操作。 快照可以花费很低的成本来创建一个文件或者目录树的拷贝。 记录追加可以同时把多个客户端的将数据追加到同一文件中，并且保证每个客户端的追加的原子性。 这对于实现多路结果合并，以及生产者-消费者队列非常有用，多个客户端可以在不需要额外的同步锁定的情况下，同时对一个文件追加数据。 我们发现这些类型的文件在构建大型分布式应用程序中具有不可估量的价值。 快照和记录追加操作将在3.4和3.3节分别讨论。 2.3 系统架构 GFS 集群由一个主节点，多个存储节点和访问他们的多个客户端构成，就像图1一样。 每一个组件都以用户级别的进程运行在普通的 Linux 设备上。 在机器资源允许的情况下，我们可以在同一台机器上同时运行存储服务和客户端程序，并且我们能够接受不可靠的应用程序代码带来的稳定性降低的风险。 文件会被分解成固定大小的文件块。每一个文件块在创建的时候都会被主节点分配一个不可变且全局唯一的 64bit 标识。 存储节点会把文件块当做 Linux 文件存放在本地磁盘上，并且会根据全局标识和操作的字节范围来读取或是写入这个文件块。 为了提高可靠性，每一个文件块都会被复制到多个服务器上。 我们默认会将文件存储三份，但是用户可以为不同命名空间下的不同区域指定复制的数量。 主节点维护着所有文件系统当中的元数据。元数据包含有文件的命名空间，访问控制信息，文件和块的映射关系以及文件块所处的位置。 主节点还控制着系统级别的操作，比方说文件块的管理，文件块的垃圾回收和文件块的移动。 主节点使用心跳信息周期地和每个存储节点通讯，并发送指令到各个存储节点并接收存储节点的状态信息。 GFS 客户端为应用程序实现了文件系统 API 来与主节点和存储节点通信实现读取或写入操作。 GFS 客户端只和主节点进行元数据方面的交互，具体的数据通信都是直接和存储节点实现的。 我们不提供 POSIX API，所以就不需要和 Linux 的 vnode 层有关系了。 注：要理解 vnode 可以先看看 inode inode 维基百科 , vnode 文档 所有的 GFS 客户端和存储节点都是不缓存文件数据的。 客户端缓存文件几乎没有什么意义，因为大多数应用程序读取的都是很大的数据流或者说是数据集，这些内容根本没法缓存。 如果不做缓存也就没了缓存与文件不一致的问题，简化了客户端和整个系统。 （但是客户端还是缓存元数据的） 因为 Linux 的缓冲区缓存已经将经常访问的数据加载到了内存中而存储节点将文件块以本地文件的方式存储在硬盘上，所以数据存储节点也不需要实现文件缓存功能。 2.4 单个主节点 只有一个主节点可以极大的简化我们的设计，并且可以让主节点统筹全局的信息来制定杂的文件块存储和复制方案。 另外，我们必须减少对主节点的读写，避免主节点成为系统的瓶颈。 客户端并不通过主节点读写文件数据。 客户端只会向主节点查询它的要访问的存储节点。 客户端会在很小的一段时间内缓存这样的信息并直接与存储节点进行数据操作。 让我来利用 图1 解释一下一次简单的文件读取流程。 首先客户端会使用固定的分块大小将文件名和字节偏移量转化为分块索引。 然后客户端会把文件名和分块索引传送给主节点。 主节点会返回文件块的标识和副本的存储位置。 客户端利用文件名和分块索引作为 key 然后把主节点返回的内容进行缓存。 客户端通常会向最近的一个存储节点发送一次请求。 这个请求内会说明文件块的标识和大小。 在对这个文件块的后续操作过程中，客户端都不需要再和主节点进行通讯了，除非缓存过期或者说文件被重新打开了。 事实上，客户端通常会在一次请求中查询多个文件块的信息，主节点也可能会回应紧跟着这些文件块的其他文件块。 在实际应用中，这些额外的信息在没有任何代价的情况下，避免了客户端和主节点未来可能会发生的几次通讯。 2.5 文件块的大小 文件块的大小是一个重要的设计参数。 我们选择了 64 MB，这样的设计比一般的文件系统大很多。 每个文件块的副本都以普通 Linux 文件的形式保存在存储节点上，只有在需要的时候才进行扩大。 惰性空间分配策略避免了因内部碎片造成的空间浪费，内部碎片或许是对选择这么大的文件块最具争议一点。 增大文件块可以产生下面几个好处。 第一，可以减少客户端与主节点的交流次数，因为在单个文件块的情况下的读和写操作都只需要与主节点进行一次数据交互来读取文件块的存储位置。 这样减少主节点的负载对系统会产生明显的优化，因为大多数的应用程序是顺序读写大文件的。 第二，由于使用了大的文件块，客户机更可能在一个文件块上执行操作，因此它可以通过在一段较长的时间内保持与存储节点的 TCP 连接来减少网络开销。 第三，这样做允许我们把元数据存储在内存当中，这样做带来的好处我们会在 2.6.1 节讨论。 在另一方面，大的文件块和惰性空间分配也会有它们的缺点。 小文件由少量的文件块组成，或许只有一个。 如果许多客户机正在访问同一个文件，那么存储这些块的存储节点可能会变成热点。 在实际使用过程中热点问题并没有成为重大的问题，因为我们的应用程序通常在读取有序的多个文件块。 然而，当我们第一次把 GFS 用于批处理队列系统的时候，热点的问题还是产生了: 一个可执行文件在 GFS 上保存为单个文件块，之后这个可执行文件在数百台机器上同时启动。 存储这个可执行文件的几个存储节点被数百个并发请求访问过载。 我们提高了可执行文件的存储副本数量，并错开了批处理队列系统启动时间，解决了这个问题。 一个可能的长期解决方案是允许客户机在这种情况下从其他客户机读取数据。 2.6 元数据 主节点存储着以下三种主要的元数据：文件和文件块的命名空间，文件与文件块的映射关系，文件块的副本位置。 所有的元数据都储存在主节点的内存当中。 前两种类型的文件(命名空间和文件与文件块的映射关系)也会被持久化保存在变更日志中，而变更日志文件存储在本地磁盘上，同时日志会被复制到其他的远程主节点上。 使用日志的这种方式允许我们可以简单可靠的更新主节点状态，还可以避免在主节点崩溃时造成的一致性问题。 主节点不会持久化的保存文件块的位置。 反而，主节点会在启动的时候或者是集群新增存储节点的时候向存储节点查询它们存储的文件块。 2.6.1 内存中的数据结构 因为元数据保存在内存中，所以主节点的操作速度非常快。 并且，主节点以在后台简单而高效的周期性扫描自己保存的全部状态信息。 这种定期扫描用于实现文件块的垃圾回收，以及在存储服务器出现故障时重新复制文件块和文件块的迁移，平衡存储服务器之间的负载和磁盘空间。 第 4.3 和 4.4 节我们会详细讨论这些功能。 对于这种只使用内存的方式，一个潜在的问题是文件块的数量以及整个系统的容量受到主机内存量的限制。 这在实践中并不是一个严重的问题。 主节点中只需要 64 Bytes 的元数据就能管理一个文件块(64 MB)。 大部分的文件块都是满的因为大多数文件都由多个文件块组成，也就是说只有最后一个文件块可能有部分是空闲的。 同理，文件的命名空间通常也小于 64 Bytes ,因为保存的文件名是用压缩算法压缩过的。 即便是需要支持更大的文件系统，为主节点增加额外内存的费用是很少的，而通过增加有限的费用，我们就能够把元数据全部保存在内存里，增强了系统的简洁性、可靠性、高性能和灵活性。 2.6.2 文件块的存储位置 主节点并不会持久性的保存文件块副本与存储节点的对应关系。 主节点只会在启动时拉取存储节点的文件块存储信息。 此后，主节点可以保持自身的最新状态，因为它控制着所有文件块的位置，并使用常规心跳监视存储服务器的工作状态。 我们最初尝试过将文件块的信息持久化存储在主节点上，但是我们发现在启动之后或者说定期的从存储节点获取这些数据要简单的多。 这种设计简化了在存储节点加入，离开集群、更名、失效、以及重启的时候，主节点和存储节点数据同步的问题。 在集群中通常有数百台服务器，这样的问题经常发生。 可以从另外一个角度去理解这个设计决策：只有存储节点才能最终确定一个文件块是否在它的硬盘上。 试图在主节点上保持此信息的一致性是没有意义的，因为存储节点上的错误可能会导致文件块自动消失（例如，磁盘可能损坏并被禁用），或者操作员可能会重命名存储节点。 2.6.3 操作日志 操作日志中存储了重要的元数据变更记录。 这对 GFS 非常重要。 它不仅是元数据的唯一持久记录，而且它也作为判断同步操作顺序的逻辑时间基线。 文件和文件块和他们的版本(4.5 节)，都由它们创建的逻辑时间唯一的、永久的标识。 操作日志非常关键，我们必须可靠地存储它，并且直到元数据修改完成并且持久化之后，日志对于客户端来说才是可见的。 否则的话，即使这些文件块本身仍然存在，我们仍然会失去整个文件系统或是客户端最近的操作。 因此，我们才会将操作日志复制到多台设备上，并且只有在日志完整写入磁盘然后同步至远程设备之后才会响应客户端的操作。 主节点会通过批量将一些记录一起写入磁盘的方式来减少写入磁盘和复制对系统整体性能的影响。 主节点也会通过重演操作日志的方式来完成灾难恢复。 为了减少启动时长，我们必须让操作日志尽肯能的少。 每当日志增长到超过一定大小时，主服务器就会检查其状态，以便存储节点可以通过从本地磁盘加载最新的检查点然后只需要重演少量的操作日志就可以完成恢复。 检查点采用了类似于 B树的紧凑形式，这种形式可以直接映射到内存当中，在用于命名空间查询时无需额外的解析。 这一特性进一步加快了恢复速度并且提高了可用性。 因为构建一个检查点需要花费一些时间，因此存储主机的内部状态的数据结构要能在不影响现有操作的前提下保存检查点。 主节点会切换到一个全新的日志文件当中并且使用单独的线程创建检查点。 新的检查点中会包含切换前的所有数据变化。 只需要一分钟就可以建立一个包含数百万个文件的检查点。 在检查点建立完成之后会写入本地和远程地址中。 只需要有最新的完整检查点和此检查点之后的日志我们就可以完成数据恢复了。 旧的检查点和日志文件可以被删除，但是为了应对灾难性的故障我们通常会多保存一些历史文件。 检查点建立时出现的故障并不会影响集群恢复，因为在恢复的时候会自动检测并跳过不完整的检查点。 2.7 一致性模型 GFS 使用了较为宽松的一致性模型，该模型可以很好地支持我们高度分散的应用程序，实现起来相对简单有效。 本节我们讨论 GFS 的一致性的保障机制，以及对应用程序的意义。 我们还将重点介绍 GFS 是如何维持一致性的，但是细节就留给了本文中的其他部分。 2.7.1 GFS 一致性保障机制 文件命名空间的变化(例如:创建文件)是原子性的。 它们仅由主节点控制: 命名空间锁提供了原子性和正确性(4.1 节); 主节点的操作日志定义了这些操作在全局的顺序(2.6.3 节)。 数据修改后区域的状态取决于操作的类型、成功与否、以及是否同步修改。 表1 总结了各种操作的结果。 如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文件的这一区域是一致的(consistent)。 如果对文件的数据修改之后，区域是一致的，并且客户端能够看到写入操作全部的内容，那么这个区域是已定义的(defined)。 当一个数据修改操作成功执行，并且没有受到同时执行的其他写入操作的干扰，那么影响的区域就是已定义的(隐含了一致性): 所有的客户端都可以看到写入的内容。 并行修改操作成功完成之后，区域处于一致的、未定义的状态：所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的数据。 通常情况下，区域内包含了来自多个修改操作的、混杂的数据片段。 失败的修改操作导致一个区域处于不一致状态（同时也是未定义的）：不同的客户端在不同的时间会看到不同的内容。 后面我们将描述应用如何区分已定义和未定义的区域。 应用程序没有必要再去细分未定义区域的不同类型。 数据修改操作分为写入或者记录追加两种。 写入操作把数据写在应用程序指定的文件偏移位置上。 即使有多个修改操作并行执行时，记录追加操作至少可以把数据原子性的追加到文件中一次，但是偏移位置是由 GFS 选择的(3.3 节) (相比之下，“常规”追加只是客户端认为是文件当前结尾的偏移量处的写操作。) GFS 会返回给客户端偏移量，并且标记定义的数据的起始区域。 此外，GFS 可能会在文件中插入填充数据或重复记录。 这些数据占据的文件区域被认定是不一致的，这些数据通常比用户数据小的多。 经过了一系列的成功的修改操作之后，GFS 确保被修改的文件区域是已定义的，并且包含最后一次修改操作写入的数据。 GFS 通过以下手段确保上述结果： (a) 对文件块的所有副本采用相同的顺序进行修改操作。 (b) 利用文件块的版本号来侦测副本是否因为它所在的存储节点宕机(4.5 节)而错过了修改操作而导致其失效。 旧版本的文件块是不会被改变的，主节点也不会将它的地址返回给客户端。 它们会被垃圾回收机制尽早的回收掉。 因为客户端有可能会缓存文件块的存储位置，所以说可能会在数据刷新前客户端读取到了陈旧文件副本的问题， 这样的问题只会出现在缓存过期的时间和文件下一次被打开的时间的间隔形成的时间窗内，因为文件打开时会清除缓存中与该文件有关的所有文件块的位置信息。 而且，由于追加操作是最常见的操作行为，所以陈旧的副本通常返回的是缺失的数据而不是过时的数据。 当读取端重新联系主节点的时候，它将立即获得当前的文件块存储位置。 即使在修改操作成功执行很长时间之后，组件的失效也可能损坏或者删除数据。 GFS 通过主节点与所有存储节点之间的定期握手来识别故障的存储节点，并通过校验的方式检测数据损坏(5.2 节)。 一旦发现了问题，集群将尽快通过有效副本恢复数据(4.3 节)。 只有在 GFS 做出反应之前(通常在几分钟内)，所有的副本都丢失了，才会不可逆的丢失文件块。 即使在这种情况下，文件块也只是不可用了而不是损坏了：应用程序会接收到明确的错误信息而不是损坏的数据。 2.7.2 程序的实现 使用 GFS 的应用程序可以利用一些简单技术实现这个宽松的一致性模型，这些技术也用来实现一些其他的目标功能，包括：尽量采用追加写入而不是覆盖，检查点和自验证的写入操作，自记录标识。 我们的大多数应用程序都是通过追加而不是覆盖来改变文件内容。 典型的使用方式是从头到尾编写文件。 在写入操作完成后应用程序会原子性的设置一个永久保存的文件名，或者定期建立检查点确保成功写入了多少数据。 检查点中还可能包含有应用程序级别的校验和。 读取器仅验证和处理直到最后一个检查点的文件区域，我们知道该检查点有一定处于已定义状态。 无论一致性和并发性有怎么样的问题，这种方法都为我们提供了很好的服务。 与随机读写相比追加写入的方式有着更好的效率，并且对程序失败的处理会更有弹性。 检查点这种机制允许写入端逐渐的完成启动操作，并且阻止读取端处理应用程序看来已经成功写入但是尚未完成全部操作的数据。 另一种典型的使用方式是，将许多写入端并发的追加写入同一个文件，通过这种方式来合并结果或者说实现生产者-消费者队列。 追加写入至少写入文件一次的语义可以保证写入端的输出。 读取端通过下面的方式来处理偶然的填充数据和重复内容。 在写入端编写的每条记录中都包含有校验和之类的额外内容，通过这种方式可以验证数据的有效性。 读取端可以使用校验和来识别并丢弃填充数据。 如果不能接受偶尔的数据重复的话(例如：如果他们会触发非幂等的操作)，则可以使用记录中的唯一标识符号对数据进行过滤，唯一标识符也经常被命名程序中用于处理的实体对象，例如 web 文档。 这些功能(除了剔除重复数据)都被放在了库，并且适用于谷歌内部的其他的文件接口实现。 这样一来，相同顺序的记录加上较为罕见的重复数据就会被稳定的提供给读取端。 3 系统交互 此系统的设计主旨在于减少主节点在所有操作当中的参与程度。 在此背景下，我们现在来描述下 客户端，主节点和存储节点如何进行交互完成 变更数据，原子性的记录追加以及快照功能。 3.1 租约和变更顺序 数据变更是一个会改变文件块内容或者元数据的操作，比如写入操作或者记录追加操作。 每一次的数据变更都会执行在所有此文件块的副本中。 我们使用租约来维持多个副本之间数据变更顺序的一致性。 主节点会将租约授予给一个副本，我们将这个副本成为主副本。 主副本会文件块为指定数据变更的执行顺序。 所有的其他副本会根据这个顺序实现数据变更操作。 因此，修改操作全局的顺序首先由主节点选择的租约的顺序决定，然后由租约中主副本分配的序列号决定。 租约机制旨在于最大程度上减少主节点在数据管理上的开销。 租约的过期时间被设定为了默认60秒。 但是如果文件块发生了数据变更，那主副本就可以向主节点无限期的重新申请延长租约。 这些扩展请求会被装载在主机和存储节点定期交换的心跳记录上。 主节点有时也会尝试在租约到期之前撤销租约(例如：主机希望终止正在重命名的文件上执行的文件变更)。 即使主服务器和主副本失去通信，它们也在租约到期后安全的将新租约授予另一个副本。 在图2 中，我们遵循如下的步骤，展现写入操作的控制流程。 客户端向主节点查询哪个存储节点持有该块的当前租约以及其他副本的位置。如果没有租约则主节点将选择一个副本授予租约。(未标明) 主节点将主副本的标识和位置和其他副本(次要副本)的位置返回给客户端。客户端缓存这些数据，并在以后用于数据变更。仅当主节点无法访问或者不再回复租约时，它才需要再与主节点产生联系。 客户端将数据推送到所有副本。客户端可以按照任何顺序进行上述操作。每个存储节点会将数据存储在内部 LRU 缓存区的高速缓存中直到这些数据使用完成或者缓存过期为止。通过将数据流与控制流分离，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不用关注哪个存储节点是主要的。3.2 节将对此进行进一步讨论。 一旦所有副本都确认已接收到数据，客户端就会向主副本发送写请求。这个请求标识了早前推送到所有副本的数据。主副本会为接收到所有的数据变化分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。然后主副本会按照序列号来顺序执行变更操作改变文件状态。 主副本将写入请求转发至所有其他副本。每个其他副本都依据主副本的序列号采用相同的顺序文件变更。 其他副本回复主副本，表示依据完成了操作。 主副本答复其他副本。在这个过程中副本遇到的任何问题都要汇报给客户端。在出现错误的情况下，写入操作可能在主符合和一些其他副本执行成功。(如果操作在主副本上失败了，操作就不会被分配序列号，也不会被传递。)客户端的请求被确认为失败之后，被修改的区域会处于不一致的状态。我们的客户代码通过重试失败的数据变更来处理此类错误。在从头开始重复执行之前，客户端会针对步骤 3 到步骤 7 执行少量的重试操作。 如果应用程序发起的写入操作很大或者说跨越了文件块的便捷，GFS 客户端代码会将其分解为多个写操作。 它们都遵循上述控制流程，但可能与其他客户端的并发操作交错并被其覆盖。 因此共享的文件区域可能最终包含来自不同客户端的数据片段，因为每个操作在所有副本上都以相同的顺序正确完后程了，所以这些副本将是相同的。 如 2.7 节所述，这会使文件区域处于一致但未定义的状态。 3.2 数据流 我们将数据流与控制流分离开来，以有效地利用网络。 当控制流从客户端流向主副本，然后流至所有其他副本时，数据将以流的方式沿着精心挑选的存储节点链进行线性推送。 我们的目标是充分利用每台计算机的网络带宽，避免出现网络瓶颈和高延迟链接，并最小化推送所有数据的延迟。 为了充分利用每台机器的网络带宽，数据会沿着存储节点链进行线性推送，而不是以其他拓扑结构（例如，树型结构）。 因此，每台机器的全部出口带宽用于尽可能快地传输数据，而不是在多个接收者之间分配带宽。 为了尽可能避免网络瓶颈和高延迟链接（例如，交换机间链接经常同时出现），每台计算机都会将数据转发到尚未接收到数据的网络拓扑中“最近”的计算机。 假设客户端将数据推送到存储节点 S1 至 S4。 它将数据发送到最近的存储节点，例如 S1。 S1 通过最接近 S1 的 S4(例如S2) 将其转发到最接近的存储节点 S2。 同样，S2 将其转发到 S3 或 S4，以更接近 S2 的那个为准，依此类推。 我们的网络拓扑非常简单，可以从 IP 地址准确估计“距离”。 最后我们使用流水化的作业模式将数据通过 TCP 连接进行传输并以此来最大程度的减少延迟。 一旦存储节点接受到一些数据，它将立刻开始转发。 流水线的工作模式对我们来说特别有用，因为我们使用具有全双工连接的交换网络。 立即发送数据不会降低接收率。 在没有网络拥塞的情况下，将 B 字节传输到 R 副本的理想时间是 B/T+RL，其中 T 是 网络吞吐量，L 是在两台机器之间传输字节的等待时间。 我们的网络链路通常为 100 Mbps（T），L 远远低于 1 ms。 因此，理想情况下，可以在大约 80 毫秒内分配 1 MB。 3.3 原子性的数据追加 GFS 提供了一个原子性的追加操作称为记录追加。 在传统的写入操作中，客户端需要指定写入数据的偏移量。 但是并发向同一个区域中进行写入是不可以被序列化实现的：该区域可能会包含来自多个客户端的数据片段。 但是，在记录追加操作中，客户端仅需要提供数据。 GFS 会原子性的把数据追加放入文件中至少一次(即写入一个顺序的 Byte 流)并将该偏移量返回给客户端。 这类似于在 Unix 环境中以 O APPEND 模式打开文件的情况下，多个写入端没有竞争的并发写入内容。 记录追加被我们的分布式应用程序大量使用，许多客户端在不同机器上的同时向同一文件追加内容。 如果客户端使用传统写入操作，则它们将需要其他复杂且昂贵的同步操作，例如通过分布式锁管理器。 在我们的工作负载中，此类文件通常充当多生产者/单消费者队列，或来为许多不同客户端合并结果。 记录追加是一种数据变更操作，它也遵循 3.1 节描述的控制流程，仅仅是在主副本端新增了一些额外的逻辑。 客户端将数据推送到文件最后一个文件块的所有副本之后，将其请求发送到主副本。 主数据库检查是否将记录追加到当前文件块上是否会导致该文件块超过最大大小（64 MB）。 如果会超过它将填充数据块到最大大小，告诉剩余的存储节点执行相同的操作，然后回复客户端以指示应在下一个文件块上重试该操作。 （记录追加的大小被限制为最大文件块大小的四分之一，这样一来最坏情况的也能使碎片保持在可接受的水平。） 如果记录在最大大小范围内（通常是这样），则主副本将数据追加到其文件中，告诉其他副本将数据写入其具有确切偏移量的位置，最后将成功回复给客户端。 如果记录追加操作在任何副本上均执行失败，则客户端将重试该操作。 重新进行记录追加的结果是，同一个文件块的不同副本可能包含不同的数据，存在部分或者全部都是重复的数据。 GFS 不保证所有副本在字节级别上都是相同的。 它仅保证数据作为整体的原子单位至少被写入一次。 从简单的观察很容易得出这种特性，即为了报告操作成功，必须在某个文件块的所有副本上以相同的偏移量写入数据。 这之后，所有副本的长度至少与记录的长度一样长，因此，即使以后有其他副本成为主副本，任何将来的记录也将被分配更高的偏移量或者写入其他文件块。 就我们的一致性保障模型而言，成功完成记录追加操作的文件区域写入的数据是已定义的(因此是一致的)，而介于写入操作中间的文件区域则是不一致的(因此是未定义的)。 正如我们在 2.7.2节中讨论的那样，我们的应用程序可以处理不一致的区域。 3.4 快照 快照操作几乎可以瞬间复制文件或目录树(源)，同时最大程度地减少正在进行的突变的中断。 我们的用户使用它来快速创建大型数据集的分支副本(通常是递归创建这些副本的副本)，或者用于改动测试使得可以轻松的提交或者回滚原来的状态。 像 AFS [5] 一样，我们使用标准的写时复制技术来实现快照。 当主节点接受到快照请求时，它首先要撤销掉和此快照相关的文件块上的未完成的租约。 这确保了对这些文件块的任何后续写入操作都将需要与主节点机进行交互以找到租约持有者。 这也就给了主节点创建文件块副本的机会。 租约被撤销或到期后，主节点将此操作记录到磁盘。 然后，主节点通过复制源文件或者目录的元数据的方式，把这条日志记录的变化保存在内存中。 新创建的快照文件指向与源文件相同的块。 快照操作之后，客户端第一次要写入文件块 C 的时候，它将向主节点发送请求以查找当前的租约持有者。 主节点注意到文件块 C 的引用计数大于 1。 主节点推迟了对客户请求的答复，而是选择了一个新的文件块句柄 C’。 然后，主节点要求具有当前文件块 C 的每个存储节点创建一个称为 C’ 的新文件块。 通过在与原始存储节点相同的节点上创建新的文件块，我们可以保证文件是通过本地而不是使用网络进行复制的。(我们磁盘的读写速度大约是我们 100 Mb 网络的三倍) 从这一点来看，请求处理与任何文件块都没有什么不同：主节点向存储副本之一授予新文件块 C’ 的租约，然后回复客户端，客户端可以正常地写入该文件块，而不知道该文件块是通过原现的文件块刚刚创建的。 4 主节点操作 主节点运行了所有的命名空间操作。 此外，主节点管理了整个系统中的文件块副本：它还会决定文件块的存储位置，创建新块以及副本，并协调各种系统范围内的活动以保持文件块的副本，平衡所有存储节点上的负载并回收未使用的存储空间。 现在我们来分别进行讨论。 4.1 命名空间的管理和锁定 主节点的许多操作都可能花费很久的时间：例如，快照操作必须撤消存储节点上此快照所覆盖的所有文件块上的租约。 我们不想因为快照这样的操作而影响其他操作产生延迟。 因此，我们允许多个操作处于活动状态，并在名称空间的区域上使用锁以确保正确的序列化。 与许多传统文件系统不同，GFS 没有针对每个目录实现能够列出目录下所有文件的数据结构。 它也不支持相同文件或目录的别名（即，Unix 术语中的硬链接或软链接）。 GFS 的命名空间在逻辑上就是将完整路径名映射到元数据的查找表。 使用前缀压缩，可以在内存中有效地表示该表。 命名空间树中的每个节点(绝对文件名或绝对目录名)都有一个关联的读写锁。 主节点的每个操作都需要一组锁。 通常情况下如果我们 主节点的许多操作都可能花费很久的时间：例如，快照操作必须撤消存储节点上此快照所覆盖的所有文件块上的租约。 我们不想因为快照这样的操作而影响其他操作产生延迟。 因此，我们允许多个操作处于活动状态，并在名称空间的区域上使用锁以确保正确的序列化。 与许多传统文件系统不同，GFS 没有针对每个目录实现能够列出目录下所有文件的数据结构。 它也不支持相同文件或目录的别名（即，Unix 术语中的硬链接或软链接）。 GFS 的命名空间在逻辑上就是将完整路径名映射到元数据的查找表。 使用前缀压缩，可以在内存中有效地表示该表。 命名空间树中的每个节点(绝对文件名或绝对目录名)都有一个关联的读写锁。 主节点的每个操作都需要一组锁。 通常情况下如果我们要操作 /d1/d2/.../dn/leaf，它需要以下目录的读取锁 /d1,/d1/d2,/d1/d2/../dn 并且在完整路径上获取写入锁/d1/d2/.../dn/leaf。 请注意，取决于操作不同，最终获取写入锁的位置可能是文件夹或者目录。 现在，我们说明该锁定机制如何防止在将 /home/user 快照到 /save/user 时创建文件 /home/user/foo。 快照操作在 /home 和 /save 上获取读取锁, 在 /home/user 和 /save/user 上获取写入锁。 在创建文件的时候需要获取以下目录的锁，/home 和 /home/user 上的读取锁，与 /home/user/foo 上的写入锁。 这两个操作将被正确序列化，因为它们试图在 /home/user 上获取同样的锁。 创建文件不需要在父目录上获取写入锁，因为没有类似于&quot;目录&quot;或者是 inode 一样的需要保护不能被修改的数据结构。 而在相应目录上的读取锁足以保证目录不被删除。 这种方案还有一个好处就是它允许在同一个目录中发生文件变更。 例如，可以在同一个目录中同时执行多个文件创建，每个文件的创建操作都在目标目录获取读取锁，然后在目标文件上获取写入锁。 在目录上的读取锁就已经可以防止目录被删除，重命名或者快照了。 对文件名的写入锁可以保证序列化的创建操作无法将目标指定为相同的文件名。 由于命名空间可以有许多节点，因此读写锁对象会被延迟分配，并且在不使用时将其删除。 同样，获取锁也必须通过一致的全局顺序来避免死锁：所有的锁会在命名空间中按照层级进行排列，然后按照字典序排列每层元素。 4.2 副本分配 一个 GFS 集群通常分布在多个层次上。 它包含许多机架上的数百个存储节点。 而存储节点也会被来自相同或不同机架上的客户端访问。 不同机架上的两台计算机之间的通信可能会通过一个或多个网络交换机。 此外，进出机架的带宽可能小于机架内所有计算机的总带宽。 多级分发的方式在可伸缩型，可靠性和可用性上给了我们一个独特的挑战。 每个文件块的副本分配方式遵循下面两个原则:最大化数据可靠性和可用性，最大化网络带宽利用率。 为了实现这两点内容，仅仅是把副本分散在多台设备中是不够的，这样做只能防止硬盘故障或者设备失效并且利用每个设备上的网络带宽。 我们必须把文件分散在不同的机架之间。 这样做才能确保即使在整个机架损坏或者说下线的时候(例如在一些共享的资源比如说电源或者网络故障的情况下)仍然有些文件块的副本可以留存下来并且可用。 这还意味着在网络流量方面，尤其是针对文件块的读操作，能够有效利用多个机架的整合带宽。 另一方面，写操作必须和多个机架上的设备进行网络通信，但是这个代价是我们愿意付出的。 4.3 创建，重新复制，重新平衡 创建文件块的副本有以下三种原因：文件块的创建，重新复制和重新平衡。 当主节点创建文件块的时候会选择初始内容为空的副本存储位置。 主节点会考虑以下几个因素。 (1) 我们想把数据放置在磁盘使用率较低的存储节点上。最终这样做可以平衡不同存储节点上的使用率。 (2) 我们想把限制每个节点上创建操作的数量。虽然创建操作需要的资源比较少，但是创建操作也意味着随之会有大量的写入数据的操作，因为在数据写入的情况下会创建文件块，在我们一次插入多次读取的工作负载设计中，一旦它们完全被写入，它们通常实际上变成只读的。 (3) 如上所述，我们希望跨机架的散布文件块的副本。 一旦可用副本的数量低于用户设定的参数，主服务器就会重新复制文件块。 这种情况可能有多种原因，例如：存储节点不可用，存储节点报告文件副本损坏，磁盘由于故障而被禁用，或者用户修改了复制数量。 需要重新复制的每个文件块都基于以下几个因素进行排序。 一个是距离目标的数量还有多少。 例如，缺失两个副本的文件的优先级大于缺失一个副本。 另外，我们优先重新复制活跃文件的文件块而不是最近刚被删除的文件块（查看 4.4 节）。 最后，为了最大程度地减少故障对正在运行的应用程序的影响，我们提高了阻止客户端进度的文件块优先级。 主节点会选择优先级最高的文件，并通过指示某些存储节点从现在的有效副本中复制数据来完成&quot;克隆&quot;。 而放置新文件块的存储节点也要遵循类似于创建操作的需求：平衡磁盘利用率，限制活动的克隆操作数量以及为分发跨机架的副本。 为了防止克隆操作阻塞客户端的通信，主节点会限制群集和每个存储节点上活动的克隆操作数。 此外，每个存储节点通过限制其对源块服务器的读取请求来限制其在每个克隆操作上花费的带宽量。 最后，主节点会定期重新平衡副本：它检查当前副本的分布情况，并通过移动副本的方式优化磁盘空间和平衡存储负载。 在这个过程当中，主节点也会逐步的填满一个新的存储节点而不是快速的使用文件块和数据流充满存储节点，以至于过载。 新副本的分布标准与上面讨论的相似。 此外，主节点还必须选择要删除的现有副本。 通常，它倾向于删除空闲空间低于平均水平的块服务器上的磁盘，以使磁盘空间使用量相等。 4.4 垃圾回收机制 删除文件后，GFS 不会立即回收可用的物理存储。 GFS 空间回收采用惰性的策略，只在文件和文件块级的常规垃圾回收时进行。 我们发现这种方法使系统更加简单和可靠。 4.4.1 回收算法 当应用程序删除文件时，主节点将立即记录该删除，就像其他更改一样。 但是，不会立即回收资源，而是将文件重命名为包含删除时间戳记的隐藏文件。 在主节点常规扫命名空间时，如果发现这些文件日期距离当时超过了三天(此时间段可配置)那就会删除这些文件。 在此之前，仍可以使用新的特殊名称读取文件，并且可以通过将其重命名为正常的名称来取消删除操作。 当从名称空间中删除隐藏文件时，其内存元数据将被删除。 这有效地切断了其所有文件块的链接。 在文件块命名空间的扫描中也是相似的，主节点识别到孤立的文件块(即，无法从任何文件访问的文件块)并清除这些文件块的元数据。 在存储节点定期与主节点交换心跳信息中，每个存储节点会向主节点汇报其拥有的文件块子集，然后主节点会回复给子节点已经不在元数据当中存储的文件块。 存储节点就可以自由的删除这些文件块和副本了。 4.4.2 讨论 尽管分布式垃圾回收是一个棘手的问题，需要在编程语言的上下文中提供复杂的解决方案，但是在我们的案例中，这非常简单。 我们可以轻松地识别所有对文件块的引用：只有主节点在文件中存储并维护了所有文件块的对应关系。 我们还可以轻松地识别所有文件块的副本：它们是每个存储节点上指定目录下的 Linux 文件。 所有主节点不能识别的副本都是 “垃圾”。 垃圾回收相比与直接删除有以下几个优势。 首先，在大型分布式文件系统中组建故障是很常见的，采用垃圾回收的方式即简单又可靠。 文件块的创建可能在某些存储节点上成功但在另一些节点中会失败，这样主节点就无法知道这些副本是否存在。 副本删除的消息可能丢失，主节点必须记录这些错误然后在失败之后重新发送失败的命令，包括自身的和存储节点的。 垃圾回收提供了一种统一且可靠的方式来清理所有未知有用的副本。 其次，它将存储回收操作合并到了主节点的常规后台活动中，例如命名空间的常规扫描和与存储节点的握手。 因此操作会被批量的执行，开销了分散。 而且仅当主节点相对空闲时才执行此操作。 主节点可以更迅速地回复需要及时相应的客户请求。 再次，延迟删除的策略为意外的不可逆转的删除操作提供了安全保障。 根据我们的经验，这种方式主要缺点是，延迟回收会影响用户调优存储空间，特别是当存储空间比较紧缺的时候。 应用程序会重复的创建和删除临时文件，这样做会导致存储空间无法被马上使用。 如果删除的文件再次被明确删除，我们将通过加快存储回收来解决这些问题。 我们还允许用户将不同的复制和回收策略应用于命名空间的不同部分。 例如，用户可以制定某个目录树中文件的所有文件块都应存储而不复制，并且所有已删除的文件都会立即且不可撤消地从文件系统状态中删除。 4.5 过期失效的副本检测 如果存储节点发生故障并且在其关闭时丢失了数据变更操作，那副本可能会过期。 主节点维护了每个文件块的版本号，并使用这个版本号来区分最新的和过时的副本。 每当主节点授予文件块上的新租约时，它都会增加文件块的版本号并通知最新的副本。 主节点和副本都把最新的版本号记录到持久状态信息中。 这个行为发生在任何客户端被通知之前，因此可以在客户端写入数据之前完成。 如果另一个副本当前不可用，则其块版本号将不会被更改。 存储节点会在启动时检测现有的文件块和版本号并将它们发送给主节点，这样以来主节点就能检测到过期文件块。 如果主节点接收的版本号大于其记录中的版本号，则主节点会认为在授予租约时失败了，因此将更高的版本更新为最新版本。 主节点会在常规的垃圾回收中删除陈旧的副本。 在此之前，当它答复客户端对块信息的请求时，它实际上认为陈旧的副本根本不存在。 另一种保护措施是，当主节点在克隆操作中通知客户端哪个存储节点持有文件块的租约时或当它指示存储节点从另一个存储节点读取文件块时，主节点也会传输文件块的版本号。 客户端或存储节点在执行操作时会验证版本号，以便它始终在访问最新数据。 5 容错和诊断 在设计系统时，我们最大的挑战之一就是应对频繁出现的组件故障。 组件的数量和质量让这些问题出现的频率远远超过一般系统意外发生的频率：我们不能完全信任机器，也不能完全信任磁盘。 组件故障可能导致系统不可用，或者更糟的是损坏数据。 我们讨论我们如何面对这些挑战，以及当组件失效不可避免的发生时，使用自带工具诊断系统故障。 5.1 高可用性 在 GFS 群集中的数百台服务器中，某些服务器在任何给定时间都将不可用。 我们通过两种简单而有效的策略来保持整个系统的高可用性：快速恢复和复制。 5.1.1 快速恢复 主节点和存储节点的设计目标就是无论它们怎样终止都能在数秒之内启动并完成恢复。 实际上，我们不区分正常终止和异常终止。 通常都通过杀死进程来关闭服务器。 客户端和其他的服务器会感觉到系统不稳定，正在发出的请求会超时，需要重新连接到重启后的服务器，然后重新发送请求。 6.2.2 节记录了实际的启动时间。 5.1.2 文件块的副本 如前所述，每个文件块都复制在不同机架上的多个存储节点上。 用户可以为文件命名空间的不同部分指定不同的复制级别。 默认值为三。 主节点根据需要克隆现有的副本，以在存储节点脱机或通过校验和验证检测到损坏的副本时保持每个文件块的完全复制(请参阅5.2节)。 尽管复制为我们提供了很好的服务，但我们仍在探索其他形式的跨服务器冗余，例如奇偶校验或擦除代码，以满足日益增长的只读存储需求。 我们认为在这样松耦合的系统中实现这样复杂的冗余方案是非常有挑战性，但还是可以实现，因为我们的数据流大多是追加方式的写入和读取操作，极少情况下采取随即写入操作。 5.1.3 主节点的复制 通过复制主节点的方式可以满足可靠性需求。 主节点的操作日志和检查点会被复制到多台主机上。 仅在操作日志被记录到磁盘上并且分发到所有的主节点副本上时数据变更操作才会被确定是完成的。 为了简化整个系统，单个主进程会负责管理所有的数据变更操作和后台的活动，例如在内部修改集群状态的垃圾回收操作。 当主节点故障时，它几乎可以立即重新启动。 如果是服务器或者是磁盘故障，则 GFS 外部的监视服务将在其他位置使用复制的操作日志启动新的主节点进程。 而客户端通常使用别名的方式来连接集群(例如，gfs-test)，而别名是 DNS 服务提供的(CNAME),可以通过改变目标地址的方式连接新的服务器。 此外，&quot;影子&quot;主节点会在主节点宕机时为应用程序提供只读的文件系统。 因为这些节点是&quot;影子&quot;而不是镜像，所以它们的日志和检查点会比主节点落后一些，通常是几分之一秒。 它们提高了没有经过数据变化的文件和不介意获得略微陈旧文件的应用程序的读取可用性。 实际上由于实际的数据是从存储节点读取的，所以应用程序不会发现陈旧的文件内容。 在这个短暂的时间窗内，过期的可能是文件的元数据，比如目录的内容或者访问控制信息。 为了随时了解情况，影子主机读取了不断增长的操作日志的副本，并且和主节点保持一致的元数据修改操作。 像主节点一样，它在启动时(且以后很少)向存储节点拉取文件块副本信息并与它们交换频繁的握手消息以监视其状态。 它仅依赖于主节点上的副本位置更新，该更新是由主节点决定创建和删除副本而导致的。 5.2 数据完整性 每个存储节点使用校验和来检测存储数据的是否损坏。 鉴于 GFS 群集通常在数百台服务器上具有数千个磁盘，因此它经常会遇到磁盘故障，从而导致数据丢失或读写路径丢失。 （原因请参见第7节。） 我们通过其他节点的文件块副本进行故障恢复，但是通过比较不同存储节点的文件副本的方式来检测故障是不切实际的。 此外，具有歧义的副本可能是合法的: 在 GFS 数据变化的语义中，特别是前文中描述的原子性的记录追加情况下，并不能保证副本完全一致。 因此，每个存储节点必须通过维护校验和的方式来独立验证其副本的数据完整性。 一个文件块会被分成 64 KB大小的数据块。 每个都有对应的32位校验和。 像其他元数据一样，校验和也保存在内存中，并通过日志记录与用户数据分开存储。 在读取操作时，存储节点会在任何数据返回给无论是客户端还是其他存储节点这样的请求者之前，会验证与当前读取范围重叠的数据块的校验和。 因此，存储节点不会将损坏的数据传播到其他设备中。 如果数据块与校验和不匹配，存储节点会将错误信息返回给请求者，并向主节点汇报此情况。 此时请求者将从其他副本中读取数据，而主节点会从其他副本克隆数据块。 在新的副本写入完成后主节点会命令存储节点删除错误的文件副本。 由于多种原因，校验和检测的方式对读取性能影响很小。 由于我们的大多数读取操作至少跨越几个数据块，因此我们仅需要读取和校验和相对少量的额外数据即可进行验证。 GFS 客户端代码通过把读取操作对齐在数据块边界上，来进一步减少了开销。 此外，无需任何 I/O 即可完成对存储节点的校验和查找和比较，并且校验和计算通常可以与 I/O 重叠。 由于校验和计算在我们的工作负载中占主导地位，因此它针对附加到文件块末尾的写入（与覆盖现有数据的写入相反）进行了严格的优化。 我们只是增量的更新最后不完整的数据块校验和，并为追加操作写满的新数据块计算校验和。 即使最后一个数据块以及损坏了并且我们也没有能检测出来，新生成的校验和也会和存储的数据出现不匹配的情况，并且在下次读取到此数据块时将照常检测到损坏。 在另一方面，如果写入覆盖了该文件块的现有范围，则我们必须读取并验证要覆盖范围的第一个和最后一个数据块，然后执行写入操作，最后计算并记录新的校验和。 如果我们在部分覆盖掉第一个和最后一个数据块之前没有对其进行验证，则新的校验和可能会隐藏未覆盖区域中存在的损坏。 在空闲的时候，存储节点但可以扫描和验证当前处于未被使用的文件块。 这使我们能够检测很少被读取的文件块中的损坏。 一旦检测到数据损坏，主节点即可创建新的未损坏文件的副本，并删除损坏的副本。 这样可以防止不活动但已损坏的文件块副本欺骗主节点，使其认为它具有足够的有效副本。 5.3 诊断工具 广泛而详细的诊断日志仅需要花费很少的成本就会在问题隔离，系统调试和性能分析方面提供了不可估量的帮助。 如果没有日志，则很难知道设备之间的瞬时，不可重复的交互内容。 GFS 会在服务器上生成诊断日志，该日志记录许多重要事件(例如，存储节点的启动和关闭)以及所有 RPC 请求和相应内容。 这些诊断日志可以被任意删除，而不会影响系统正常运行。 但是我们会在空间允许的情况下尽量存储这些日志。 RPC 日志包括在网络上发送的确切请求和响应内容，但不会记录读取或写入的文件数据内容。 通过收集和比对不同设备上的 PPC 日志中的请求和相应内容，我们可以重现交互历史来诊断问题。 日志还可以用来跟踪负载测试和性能分析。 日志记录对性能的影响很小(远小于它带来的好处)，因为这些日志是按顺序和异步写入的。 最新事件也保存在内存中，可用于在线监控。 6 性能测试 在本节中，我们提供一些微观基准测试，以说明 GFS 架构和实施中固有的瓶颈，以及 Google 使用的实际集群中的一些真实数据。 6.1 小规模基准测试 我们在由1个主节点，2个主节点副本，16个存储节点和16个客户端组成的 GFS 群集上测量了性能。 请注意，此配置是为了便于测试而设置的。 典型的集群有数百个存储节点和数百个客户端。 所有设备都配有2个 1.4 GHz 的 PIII 处理器，2 GB 内存，2个 80 GB 5400 转的机械硬盘，并且有 100Mbps 全双工网络连接在 HP 2524 交换机上。 所有19台 GFS 服务器都连接到一台交换机，所有16台客户机都连接到另一台。 两个交换机通过 1 Gbps 链路连接。 6.1.1 读取性能 N 个客户端同时从文件系统中进行读取。 每个客户端会从 320 GB 的文件集中读取随机选择的 4 MB 区域。 重复执行256次，以便每个客户端最终读取 1 GB 数据。 存储节点共有 32 GB 内存，因此我们希望最多有 10% 的数据是从高速缓冲区中读取的。 我们的结果应该接近冷缓存结果。 图3(a) 显示了 N 个客户端的总读取速率及其理论极限。 当两个交换机之间的 1 Gbps 链路达到饱和时，限制峰值总计为 125 MB/s；如果客户端的 100 Mbps 网络接口达到饱和，则每个客户端的峰值为 12.5 MB/s (以适用者为准)。 当仅一个客户端正在读取时，观察到的读取速率为 10 MB/s，即每个客户端只能达到极限速度的 80%。 对于16个客户端同时读取时，总读取速率达到 94 MB/s， 约为 125 MB/s 链接极限的 75%，也就是说每个客户端的速度为 6 MB/s。 效率从 80% 下降到 75%，因为随着读取端数量的增加，多个读取端同时从同一存储节点读取数据的可能性也增加了。 6.1.2 写入性能 N个客户端同时写入N个不同的文件。 每个客户端以 1 MB 的写入顺序将 1 GB 的数据写入新文件。 总写入速率及其理论极限如图3(b) 所示。 理论极限速度应当稳定在 67 MB/s，因为我们需要将每个字节写入16台存储节点中的3台，而每个存储节点具有 12.5 MB/s 的输入带宽。 单个客户端的写入速度为 6.3 MB/s，约为理论极限性能的一半。 造成这种情况的主要原因是网络。 它与我们推送数据到文件块副本时采用的管道模式不相适应。 将数据从一个副本传播到另一个副本的延迟会降低总体写入速率。 16个客户端的总写入速率达到 35 MB/s(每个客户端 2.2 MB/s)，约为理论极限性能的一半。 与读取的情况一样，随着客户端数量的增加，多个客户端并发写入同一存储节点的可能性更高。 而且，与16个读取端相比16个写入端遇到冲突的可能性更大，因为每次写入都会涉及到3个不同设备上的副本。 写入速度比我们预想的慢。 实际上，这并不是的主要问题，因为即使它可见的增加了各个客户端的延迟，但也不会显著的影响文件系统传递给大量客户端的聚合写入带宽。 6.1.3 追加性能 图3© 显示了记录追加的性能。 N 个客户端同时附加到单个文件。 实际性能受存储文件最后一个文件块的存储节点网络带宽的限制，而与客户端的数量无关。 对于一个客户端，速度从 6.0 MB/s 开始，而在16个客户端使它的速度降至 4.8 MB/s，这主要是由于不同的客户端上的网络拥塞以及网络传输速度的不同而导致的。 我们的应用程序倾向于同时生成多个此类文件。 换句话说，N 个客户端同时附加到 M 个共享文件中，其中 N 和 M 都有数十个或数百个。 因此，在我们的实验中，存储节点的网络拥塞实际上不是一个重要的问题，因为客户端可以在写入一个文件出现阻塞时转而去写入另一个文件。 注：顶部曲线显示了我们网络设备的理论极限。底部曲线为实测吞吐量。 它们具有显示95％置信区间的误差线，在某些情况下由于测量值的低差异而无法辨认。它们具有显示 95% 置信区间的误差线，在某些情况下由于测量值的低差异而无法辨认。 6.2 真实集群测试数据 现在，我们看一下 Google 内部正在使用的两个集群，它们可以代表其他几个集群。 超过100名工程师定期使用 A 集群进行研发。 一个典型的任务是由人类用户发起的，并且长达几个小时。 它会读取从几 MB 到几 TB 的数据，转换或分析数据，然后将结果写回群集。 B 群集主要用于生产数据的处理。 这些任务持续的时间更长，并且仅在偶尔的人工干预下就可以连续生成和处理多 TB 数据集。 在这两种情况下，单个“任务”都由许多计算机上的多个进程同时读取和写入许多文件组成。 6.2.1 数据存储 如表中的前五个条目所示，两个集群都有数百个存储节点，支持许多 TB 的存储，并且相当但不是完全满。 “已用空间”包括所有块副本。 几乎所有文件都被复制3次。 因此，群集分别存储 18 TB 和 52 TB 的文件数据。 这两个群集的文件数量相似，但是 B 集群的过期文件比例更高，即已删除或替换为新版本但其存储空间尚未被收回。 B 集群还具有更多的文件块，因为其文件往往更大。 6.2.2 元数据 存储节点会聚合存储数十 GB 的元数据，其中的内容通常是 64 KB 的用户数据块和校验和。 在存储节点上保存的唯一的元数据是 4.5 节中讨论的文件块版本号。 保留在主节点上的元数据要小得多，只有几十 MB，或者平均每个文件约100个字节。 这与我们的假设一致，即主节点的内存大小实际上不会限制系统的容量。 每个文件的元数据大多数是以前缀压缩形式存储的文件名。 其他元数据包括文件所有权和权限，从文件和文件块的映射以及每个文件块的当前版本。 另外，对于每个块，我们存储当前副本位置和应用计数，并以此来实现写时拷贝。 每个单独的服务器(主节点和存储节点)都只有50到100 MB 的元数据。 因此，恢复速度很快：在服务器能够在相应查询之前，只需几秒钟即可从磁盘读取此元数据。 但是，主节点会挂起一段时间(通常是30到60秒)，直到它从所有存储节点获取到文件块的位置信息为止。 6.2.3 读写操作比例 表3 显示了不同时间段的读写速率。 进行这些测量时，两个群集都已经运行了大约一周。 (群集最近已重新启动，以升级到新版本的 GFS。) 自重启以来，平均写入速率小于 30 MB/s。 当我们进行这些测量时，B 集群正在处理突发的写入操作，该操作产生大约 100MB/s 的数据，由于数据需要传播到三个副本，因此产生了 300 MB/s 的网络负载。 读取速率远高于写入速率。 如我们所假设的那样，总工作量包含的读取次数多于写入次数。 两个集群都有很多重度的读取操作。 特别是 A 集群在前一周一直保持 580 MB/s 的读取速率。 它的网络配置可以支持 750 MB/s，因此可以有效地利用其资源。 B 群集可以支持 1300 MB/s 的峰值读取速率，但其应用程序仅使用了 380 MB/s。 6.2.4 主节点负载 表3 还表示了发送到主节点的操作速率约为每秒 200 到 500 个操作。 主节点可以轻松地跟上这个速度，因此对于这样的工作负载而言，主节点不是瓶颈。 在早期版本的 GFS 中，主节点有时是某些工作负载的瓶颈。 它花了大部分时间顺序扫描大型目录(包含成千上万个文件)以查找特定文件。 此后，我们更改了主节点的数据结构，使其能通过命名空间进行二分查找来搜索改善效率。 现在，它可以轻松地支持每秒数千个文件访问。 如有必要，我们可以通过在命名空间数据结构之前放置名称缓存并以此来进一步加快查询速度。 6.2.5 故障恢复时间 在存储节点出现故障后，某些文件块的会缺失副本的数量，必须对其进行克隆操作来达到目标的副本数量。 恢复所有的此类文件块所需的时间取决于需要恢复多少文件块。 在一次实验中，我们杀死了 B 集群的一个存储节点。 这个存储节点有大约 15000 个文件块，其中包含 600 GB 的数据。 为了限制测试对现有应用程序的影响并为集群调度决策留出余地，我们的默认参数将集群内并发的克隆操作数限制为了 91 个(40%的存储节点数量)，其中每个克隆操作最多允许消耗 6.25 MB/s (50 Mbps) 带宽。 所有文件块均在 23.2 分钟内恢复，有效复制速率为 440 MB/s。 在另一次实验中，我们杀死了两个带有大约 16,000 个块和 660 GB 数据的存储节点。 这次的双重故障使 266 个文件块减少为只有一个副本。 这 266 个文件块以较高的优先级进行克隆，并在 2 分钟内全部恢复到至少含有 2 个副本，从而使群集处于可以忍受另一个存储节点故障而不会丢失数据的状态。 6.3 工作负载分析 在本节中，我们将详细介绍了两个 GFS 群集上的工作负载情况，这些情况与 6.2 节中的工作负载情况类似，但不完全相同。 X 集群用于研发，Y 集群用于生产数据处理。 6.3.1 方法和注意事项 这些测试结果仅包括客户端发出的请求，因此它们反映了我们的应用程序为整个文件系统生成的工作量。 它们中并不包含执行客户端请求时服务器间的内部请求或后台活动，例如集群内数据的转发写入或重新平衡。 I/O 操作的统计信息是从 GFS 服务器中的 RPC 请求日志中推导重建出来的。 例如，GFS 客户端代码可能将读取拆分为多个 RPC，以增加并行度，从中我们可以推断出原始读取负载。 由于我们的访问模式是高度程式化的，因此我们推测任何错误都是来自于误差。 应用程序的显式日志记录可能会提供更准确的数据，但是从逻辑上讲，重新编译并重新启动成千上万个正在运行的客户端是不可能的，而且收集在很多设备上的运行结果这也是项非常繁重的工作。 应该注意的是不要过度概括我们的工作量。 由于 Google 完全控制了 GFS 及其应用程序，因此这些应用程序应当倾向于针对 GFS 进行调整，但事实完全相反，GFS 是为这些应用程序设计的。 在通用应用程序和文件系统之间也可能存在这种相互影响，但是在我们的案例中，这种影响可能更加明显。 6.3.2 存储节点工作负载 注: 对于读取操作而言，表中的大小是实际读取和传输的数据量，而不是请求的数据量。 表4 显示了不同大小之间操作分布情况。 读取操作的结果是明显的双峰分布。 小型读取操作(小于 64 KB)来自于搜索密集型的客户端，这些客户端在大文件中查询少量数据。 大型读取操作(超过 512 KB)来自于对整个文件的长时间读取。 在 Y 集群上有很多的读取操作甚至没有返回任何数据。 我们的应用程序，尤其是生产系统中的应用程序，经常使用文件作为生产者－消费者队列。 生产者将同时向文件发起追加操作，而使用者则会读取文件的末尾。 有时，当消费者读取的位置超过生产者写入的位置时就不会返回任何数据。 X 集群则不会有这样的情况，因为它通常用于短期的数据分析任务，而不是长期运行的分布式应用程序。 写入操作的结果也是明显的双峰分布。 大型的写入操作(超过 256 KB)通常是写入端的大量缓冲造成的。 写入端会缓存一部分数据，更多情况下是记录存档点或者是进行同步操作，又或者是为小型的写入操作(小于 64 KB)分配账户。 注: 此处翻译可能不准确，原文是: Writers that buffer less data, checkpoint or synchronize more often, or simply generate less data account for the smaller writes (under 64 KB). 至于记录追加操作，Y 集群的大型追加操作所占的百分比远远大于 X 集群，这是因为 Y 集群是我们的生产集群，所以做了更贴合 GFS 的相关调优。 注: 对于读取操作而言，表中的大小是实际读取和传输的数据量，而不是请求的数据量。 如果读取尝试读取超出文件末尾的数据，则两者可能会有所不同，这在设计上在我们的工作负载中并不罕见。 表5 列出了各种大小的操作中传输的数据总量。 对于所有类型的操作，较大的操作(超过 256 KB)通常占用传输的大多数字节。 因为检索操作相关的负载，所以小型读取操作(小于 64 KB)会传输一部分重要的数据。 6.3.3 追加和写入的对比 在我们的生产环境中大量使用了记录追加操作。 在 X 集群中，写入操作和追加操作的字节传输比例是 108:1，操作计数的比例是 8:1。 对于生产环境的 Y 集群来说，这样的比例是 3.7:1 和 2.5:1。 而且，这些比例表明，对于这两个集群来说，记录追加操作往往大于写入操作。 但是对于 X 集群来说，在测量期间追加操作的使用率特别的低，因此结果可能会被一两个选用特定大小缓冲区的应用程序影响。 不出所料，我们的数据变更的工作负载主要是附加而不是覆盖。 我们测量了主副本上覆盖的数据量。 这近似于客户端故意覆盖先前写入的数据而不是附加新数据的情况。 对于 X 群集，重写修改了不足 0.0001% 的数据并且数据变更操作占比不足 0.0003% 。 对于 Y 群集，比例均为0.05％。 尽管比较小，但仍然比我们预期的要高。 事实证明，这些覆盖操作大多数是由于错误或超时而导致的客户端重试。 它们本身不是工作量的一部分，而是由重试机制引发的后果。 6.3.4 主节点负载对比 表6 显示了主节点接受到的请求类型占比。 大多数的请求都是在查询文件块的位置信息以及为数据变更请求租约持有者信息。 X 集群和 Y 集群上的删除请求数有明显的不同，因为 Y 集群中存储的是数据都是生产数据，这些数据会被定期的重建并且替换成最新的版本。 某些差异进一步隐藏在打开文件的请求中，因为旧版本的文件可能会通过从头写入的操作而被隐式删除(类似 UNIX 的 open 函数中的 “w” 模式)。 FindMatchingFiles 是一个模式匹配请求，它支持 “ls” 和类似的文件系统操作。 与其他请求主节点的操作不同，这样的操作可能会处理很大一部分的命名空间数据，因此可能花费的性能和时间比较多。 在 Y 集群中经常见到这样的操作，因为自动的数据处理任务倾向于检查文件系统的各个组成部分以了解全局应用程序的状态。 相比之下，X 集群的应用程序则受到了更明确的控制，通常事先知道所需文件的名称。 7 经验 在构建和部署 GFS 的过程中，我们遇到了许多问题，一些是运营方面的，有些是技术方面的。 最初，GFS 被我们认为是生产环境的后端文件系统。 随着时间推移，它的使用涉及了研究和开发任务。 它最初几乎不支持权限和配额之类的东西，但现在已经包含了这些内容。 尽管生产系统受到严格的纪律和控制，但用户有时却没有。 需要更多的基础架构来防止用户之间相互干扰。 我们最大的困难是与磁盘和一些 Linux 相关的问题。 我们的许多磁盘都向 Linux 驱动程序声明它们支持多种 IDE 协议版本，但实际上仅对较新的版本做出了可靠的响应。 由于协议版本非常相似，因此这些驱动大多数都可以工作，但是偶尔的不兼容会导致驱动和内核对于驱动状态有不一样的判断。 这会使内核中的问题静默的破坏数据。 这个问题促使我们使用校验和来检测数据损坏，而在同时我们也修改了内核以处理这些协议不匹配。 在早期，由于 fsync() 的系统开销，我们在 Linux 2.2 内核上遇到了一些问题。 这种系统开销与文件的大小成正比而不是与修改部分的大小成正比。 对于我们的大型操作日志而言，这是一个问题，尤其是在实施检查点之前。 在一段时间中我们使用同步写入的方式解决此问题，但最后还是迁移到了 Linux 2.4。 另一个关于 Linux 的问题是单个的读取-写入锁，当程序从磁盘中读取数据时(读取锁)和在 mmap() 修改地址空间时在地址空间中的任何线程都必须持有锁。 在轻负载下，我们检查到系统中出现了瞬时超时，并努力定位资源瓶颈或是偶发的硬件故障。 最终，我们发现，当磁盘线程将数据映射至分页时，单个锁会阻止主网络线程将新数据映射到内存中。 由于我们主要带宽限制是在网络而不是内存上，故我们通过使用 pread() 替换 mmap() 的方式来解决此问题，但此方法额外消耗了一次拷贝操作。 尽管偶尔出现问题，Linux 代码的可用性还是一次又一次地帮助我们探索和理解系统行为。 在适当的时候，我们会把内核相关的改进并与开源社区共享。 8 相关的工作 像 AFS [5] 等其它大型分布式文件系统一样，GFS 提供了独立于物理位置的命名空间，这可以使数据透明的进行移动来实现负载均衡和容错。 但与 AFS 不同，GFS 以类似于 xFS [1] 和 Swift [3] 的方式在服务器之间进行数据分发，以提供高性能并增强容错能力。 由于磁盘相对便宜，并且复制比复杂的 RAID [9] 方法更简单，因为 GFS 当前仅使用复制的方式来实现冗余，所以会比 xFS 或 Swift 消耗更多的存储资源。 与 AFS，xFS，Frangipani [12] 和 Intermezzo [6] 等系统相比，GFS 在文件系统接口之下不提供任何缓存功能。 我们的目标工作负载在单个应用程序的运行中几乎没有重用，因为在实际情况中要么是流式处理大量的数据集，要么是随机搜索每次只读取少量的数据。 例如 Frangipani, xFS, Minnesota’s GFS [11] and GPFS [10] 等分布式文件系统选择移除中心节点，并依赖分布式算法的方式来保证一致性和管理集群。 我们选择集中化的方式来简化设计，提高可靠性，获取灵活性。 特别是中心的主节点可以让复杂的文件块分发和复制策略变得容易很多，因为主节点已经有了大多数的相关信息兵器能够控制这些内容。 我们通过保持少量的主机状态数据并在其它计算机上完全复制这些数据来解决容错问题。 而影子主机机制则提供了可伸缩性和高可用性(在读取操作上)。 主节点的状态更新是通过追加写入持久化存储的预写日志实现的。 因此，我们可以像 Harp [7] 那样使用主节点复制的方案来提供更强的一致性的高可用性。 我们在处理为大量用户提供统一的性能的问题时选择了类似于 Lustre [8] 的处理方案。 但是，我们通过专注于应用程序的需求而不是构建 POSIX 兼容的文件系统的方式大大的简化了问题。 此外，GFS 推测有大量不可靠的硬件，因此容错对于我们的设计至关重要。 GFS 更像是 NASD 架构 [4]。 尽管 NASD 架构是基于网络连接的磁盘驱动器，但是 GFS 和 NASD 原型一样都是使用了则是一般市面上的计算机作为存储节点。 与 NASD 工作方式不同，我们的存储节点使用的是延迟分配的固定大小的文件块而不是可变长度的对象。 此外，GFS 实现了生产环境中所需的某些功能，例如重新平衡，复制和恢复。 与 Minnesota’s GFS 和 NASD 不同，我们去改变存储设备的型号。 我们专注于满足使用现有商品级别的硬件来构建复杂分布式系统以满足日常数据处理需求。 由原子性的记录追加操作实现的生产者-消费者队列解决了与 River [2] 类似的分布式队列问题。 与 River 使用基于内存的分布式队列加上严格的数据流控制的实现方式不同，GFS 采用了可以被多个生产者并发写入的持久化文件的方案。 River 模型支持了 m 对 n 的分布式队列，但缺乏持久存储附带的容错能力，而 GFS 仅有效支持 m 对 1 队列。 多个消费者可以同时读取一个文件，但是它们必须协调的分配传入的负载。 9. 结论 谷歌文件系统展示了支持商品级硬件上的大规模数据处理工作负载所必需的品质。 虽然一些设计决策是针对独特设置指定的，但许多决策可能适用于规模和成本意识相似的数据处理任务。 首先，我们根据我们当前和预期的工作负载和技术环境重新检查传统文件系统的假设。 我们的观察结果导致了设计方面的根本区别。 我们将组件故障视为正常现象，而不是例外情况，针对大型文件进行优化，因为这些文件通常会被追加(也许同时进行)，然后再读取(通常是有序进行)，并通过扩展和放松标准文件系统接口的方式改善整个系统。 我们的系统通过持续监控，复制关键数据以及快速自动恢复来提供容错能力。 文件块复制的机制使我们能够容忍存储节点故障。 这些故障的发生率激发了一种新颖的在线修复机制，该机制定期透明地修复损坏的文件块并尽快补充复制集。 此外，我们使用校验和来检测磁盘或 IDE 子系统级别的数据损坏，鉴于系统中的磁盘数量，这种情况变得非常普遍。 我们的设计为执行各种任务的许多并发读写端提供了高额的总吞吐量。 我们通过将主节点的文件系统控制与直接在存储节点和客户端之间传递的数据传输的内容进行分离的方式实现了这样的效果。 通过大型文件块和租约的方式可以将主节点涉及到的常见操作减至最小，这样的方式给了主副本进行数据变更的权限。 这使得集群中存在一个简单，集中不会成为瓶颈的主节点成为可能。 我们相信，在网络方面的改进将解除当前单个客户端上的写入吞吐量的限制。 GFS 已成功满足了我们的存储需求，并已在 Google 内部广泛将它作为研究，开发以及生产数据处理的存储平台。 它是使我们持续创新和解决整个 Web 范围内的难题的一个重要工具。 致谢 123456789101112We wish to thankthe following people for their contributionsto the system or the paper. Brain Bershad (our shepherd)and the anonymous reviewers gave us valuable commentsand suggestions. Anurag Acharya, Jeff Dean, and David desJardins contributed to the early design. Fay Chang workedon comparison of replicas across chunkservers. Guy Edjlali worked on storage quota. Markus Gutschke workedon a testing frameworkand security enhancements. DavidKramer worked on performance enhancements. Fay Chang,Urs Hoelzle, Max Ibel, Sharon Perl, Rob Pike, and DebbyWallach commented on earlier drafts of the paper. Many ofour colleagues at Google bravely trusted their data to a newfile system and gave us useful feedback. Yoshka helped withearly testing. 引用 [1] Thomas Anderson, Michael Dahlin, Jeanna Neefe, David Patterson, Drew Roselli, and Randolph Wang. Serverless networkfile systems. In Proceedings of the 15th ACM Symposium on Operating System Principles, pages 109–126, Copper Mountain Resort, Colorado, December 1995. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Luis-Felipe Cabrera and Darrell D. E. Long. Swift: Using distributed diskstriping to provide high I/O data rates. Computer Systems, 4(4):405–436, 1991. [4] Garth A. Gibson, David F. Nagle, Khalil Amiri, Jeff Butler, Fay W. Chang, Howard Gobioff, Charles Hardin, ErikRiedel, David Rochberg, and Jim Zelenka. A cost-effective, high-bandwidth storage architecture. In Proceedings of the 8th Architectural Support for Programming Languages and Operating Systems, pages 92–103, San Jose, California, October 1998. [5] John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. Scale and performance in a distributed file system. ACM Transactions on Computer Systems, 6(1):51–81, February 1988. [6] InterMezzo. http://www.inter-mezzo.org, 2003. [7] Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. Replication in the Harp file system. In 13th Symposium on Operating System Principles, pages 226–238, Pacific Grove, CA, October 1991. [8] Lustre. http://www.lustreorg, 2003. [9] David A. Patterson, Garth A. Gibson, and Randy H. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, pages 109–116, Chicago, Illinois, September 1988. [10] FrankSchmuckand Roger Haskin. GPFS: A shared-diskfile system for large computing clusters. In Proceedings of the First USENIX Conference on File and Storage Technologies, pages 231–244, Monterey, California, January 2002. [11] Steven R. Soltis, Thomas M. Ruwart, and Matthew T. O’Keefe. The Gobal File System. In Proceedings of the Fifth NASA Goddard Space Flight Center Conference on Mass Storage Systems and Technologies, College Park, Maryland, September 1996. [12] Chandramohan A. Thekkath, Timothy Mann, and Edward K. Lee. Frangipani: A scalable distributed file system. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 224–237, Saint-Malo, France, October 1997.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"GFS","slug":"GFS","permalink":"https://wangqian0306.github.io/tags/GFS/"},{"name":"HDFS","slug":"HDFS","permalink":"https://wangqian0306.github.io/tags/HDFS/"}]},{"title":"minikube 环境安装","slug":"kubernetes/minikube","date":"2020-12-29T13:41:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/minikube/","permalink":"https://wangqian0306.github.io/2020/minikube/","excerpt":"","text":"minikube 环境安装 简介 minikube 是单机简化版的 kubernetes 环境，目前提供了多种方式运行集群，本文采用了 Docker 的方式完成了部署和使用操作。 安装 Linux 注: 此处需要 Docker 作为基础环境，并且需要一个可以使用 docker 和 sudo 的用户来运行下面的命令。 安装软件包 12curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64sudo install minikube-linux-amd64 /usr/local/bin/minikube Windows 安装软件包 1choco install minikube 服务配置 启动服务 注：此处使用 centos7 原版的 docker 发现了程序bug，在使用 docker-ce 的时候没有遇到问题。 1minikube start --driver=docker --image-mirror-country=&#x27;cn&#x27; --image-repository=&#x27;registry.cn-hangzhou.aliyuncs.com/google_containers&#x27; 注：如果需要查看详细日志可以使用 --alsologtostderr -v=1 参数。 安装操作命令 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet kubeadm kubectl 基础使用 创建外部服务(LoadBalancer) 123kubectl create deployment hello-node --image=registry.cn-hangzhou.aliyuncs.com/google_containers/echoserver:1.4kubectl expose deployment hello-node --type=LoadBalancer --port=8080minikube service hello-node 创建外部服务(NodePort) 123kubectl create deployment hello-node --image=registry.cn-hangzhou.aliyuncs.com/google_containers/echoserver:1.4kubectl expose deployment hello-node --type=NodePort --port=8080minikube service hello-node 清除服务 12kubectl delete service hello-nodekubectl delete deployment hello-node 常用命令 端口转发 1kubectl port-forward service/hello-node 7080:8080 转发所有 LoadBalancer 端口 1minikube tunnel 开启图形界面 1minikube dashboard 暂停 Kubernetes 集群 1minikube pause 停止集群 1minikube stop 删除所有 minikube 集群 1minikube delete --all 注意事项 minikube 在创建的过程中会在 Docker 网卡上创建独立的虚拟网卡。主机可能无法访问虚拟机中的内容。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"Chrony 入门","slug":"linux/chrony","date":"2020-12-28T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/chrony/","permalink":"https://wangqian0306.github.io/2020/chrony/","excerpt":"","text":"Chrony 服务安装及配置流程 简介 Chrony 是一个时钟同步程序。 软件安装和配置 服务端 软件安装 1yum -y install chrony 服务配置 1vim /etc/chrony.conf 可以按照如下样例进行配置 1234567891011121314server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburstdriftfile /var/lib/chrony/driftmakestep 1.0 3rtcsyncallow &lt;area&gt;logdir /var/log/chrony 注：此处的 area 需要调整为目标网段，例如: 192.168.1.0/24。 启动服务并配置开机启动 12systemctl enable chronydsystemctl restart chronyd 配置服务端防火墙 12firewall-cmd --permanent --add-service=ntpfirewall-cmd --reload 手动刷新时间 1chronyc sources 注：此命令的结果列表即是目标服务器列表，如发现问题请及时修改。 客户端 软件安装 1yum -y install chrony 配置同步目标服务器 1vim /etc/chrony.conf 然后填入如下内容 1server &lt;ip&gt; iburst 注：此处的 IP 需要填写之前配置服务端的 IP 地址。 启动服务并配置开机启动 12systemctl enable chronydsystemctl restart chronyd 手动刷新时间 1chronyc sources 注：此命令的结果列表即是目标服务器列表，如发现问题请及时修改。 测试 在服务端和客户端都可以使用如下命令确定服务运行情况： 检查服务状态 1timedatectl 如果发现下面两项内容则证明服务正常: 12NTP enabled: yesNTP synchronized: yes","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Emoji 使用说明","slug":"tmp/emoji","date":"2020-12-28T13:57:04.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2020/emoji/","permalink":"https://wangqian0306.github.io/2020/emoji/","excerpt":"","text":"Emoji 使用说明 简介 本文翻译是 https://gitmoji.dev/ 的翻译整理版。 使用表 表情 代码 作用 🎨 :art: 改进代码结构和缩进 ⚡ :zap: 提升性能 🔥 :fire: 删除文件 🐛 :bug: 修复 Bug 🚑 :ambulance: 程序补丁 ✨ :sparkles: 新功能 📝 :memo: 更新文档 🚀 :rocket: 部署版本 💄 :lipstick: 推送界面或者样式更新 🎉 :tada: 初始提交 ✅ :white_check_mark: 提交单元测试 🔒 :lock: 解决安全问题 🔖 :bookmark: 发布版本 🚨 :rotating_light: 修复 Warning 🚧 :construction: 暂存代码 💚 :green_heart: 修复CI构建问题 ⬇️ :arrow_down: 降级依赖 ⬆️ :arrow_up: 升级依赖 📌 :pushpin: 固定依赖版本 👷 :construction_worker: CI 相关推送 📈 :chart_with_upwards_trend: 性能分析相关推送 ♻️ :recycle: 重构代码 ➕ :heavy_plus_sign: 引入依赖 ➖ :heavy_minus_sign: 移除依赖 🔧 :wrench: 更新配置 🔨 :hammer: 新建或者更新代码 🌐 :globe_with_meridians: 国际化或者本地化 ✏️ :pencil2: 修复拼写错误和错别字 💩 :poop: 写了一坨 ⏪ :rewind: 版本回退 🔀 :twisted_rightwards_arrows: 合并分支 📦 :package: 推送编译的文件或包 👽 :alien: 外部更新 🚚 :truck: 移动项目调整目录 📄 :page_facing_up: 更新 License 💥 :boom: 重大调整 🍱 :bento: 更新静态资源 ♿ :wheelchair: 改进可用性 💡 :bulb: 新增注释评论 🍻 :beers: 瞎写的 💬 :speech_balloon: 修改公共说明 🗃️ :card_file_box: 修改数据库相关内容 🔊 :loud_sound: 添加 Log 🔇 :mute: 移除 Log 👥 :busts_in_silhouette: 新增合作者 🚸 :children_crossing: 改进使用体验 🏗️ :building_construction: 架构变化 📱 :iphone: 响应式设计 🤡 :clown_face: 虚拟内容 🥚 :egg: 新增菜单 🙈 :see_no_evil: 推送忽略文件 📸 :camera_flash: 代码快照 ⚗️ :alembic: 实验性质内容 🔍 :mag: 改善搜索引擎优化 🏷️ :label: 新增类型 🌱 :seedling: ? 🚩 :triangular_flag_on_post: 推送特性标识 🥅 :goal_net: 步骤错误 💫 :dizzy: 推送动画和过渡 🗑️ :wastebasket: 移除文件 🛂 :passport_control: 权限管理","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wangqian0306.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"Podman 安装和基础命令","slug":"docker/podman","date":"2020-12-25T13:41:32.000Z","updated":"2025-01-16T08:43:58.094Z","comments":true,"path":"2020/podman/","permalink":"https://wangqian0306.github.io/2020/podman/","excerpt":"","text":"Podman 安装 使用如下命令进行安装： 12yum install docker epel-release -yyum install podman-compose -y 关闭 SELinux 修改服务状态 1setenforce 0 全局配置 1vim /etc/selinux/config 修改如下内容 1SELINUX=disabled 代理配置 Podman 不需要修改代理相关文件，直接使用系统代理，可以使用如下方法进行拉取： 12345export HTTP_PROXY=&quot;http://127.0.0.1:8888/&quot;export HTTPS_PROXY=&quot;http://127.0.0.1:8888/&quot;podman pull &lt;image&gt;unset HTTP_PROXYunset HTTPS_PROXY 基本操作 Podman 的基本操作与 Docker 基本相同只不过在拉取镜像的时候有个参数差异。 拉取镜像 1podman pull &lt;image&gt; 注：如果需要拉取 http registry 当中的镜像则需要加入 --tls-verify=false 参数。 新功能 podman 引入了部分 Kubernetes 的相关功能。目前可以在软件安装完成后使用 pod 部分的相关功能。 部署容器 1podman play kube demo.yml 管理容器 1podman pod &lt;command&gt;","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"MariaDB HA 搭建流程整理","slug":"database/mariadb-ha","date":"2020-12-10T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/mariadb-ha/","permalink":"https://wangqian0306.github.io/2020/mariadb-ha/","excerpt":"","text":"MariaDB HA 搭建流程整理 简介 本次搭建采用 主从同步 + KeepAlived 方案实现。两台设备分别为 mariadb1,mariadb2 主主互备 软件安装 1yum install mariadb-server 服务配置 1vim /etc/my.cmf 1234567891011121314[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysql# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0max_connections=1000log-bin=mysql-binserver-id=1binlog_format=MIXED[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid 注: 此处的 server-id 需要配置成不一样的值。 配置 Open Files Limit 1systemctl edit mariadb.service 123[Service]LimitNOFILE=infinity 启动服务 1systemctl enable mariadb --now 创建用户 123create user mysync@&#x27;%&#x27; identified by &#x27;123456&#x27;;grant REPLICATION SLAVE ON *.* to mysync@&#x27;%&#x27;;FLUSH PRIVILEGES; 检查数据状态 1show master status; 配置从机同步(A-&gt;B 和 B-&gt;A) 12345678use mysql;change master tomaster_host=&#x27;&lt;mariadb2&gt;&#x27;,master_user=&#x27;mysync&#x27;,master_password=&#x27;123456&#x27;,master_log_file=&#x27;&lt;mariadb2_file&gt;&#x27;,master_log_pos=&lt;mariadb2_pos&gt;;start slave; 12345678use mysql;change master tomaster_host=&#x27;&lt;mariadb1&gt;&#x27;,master_user=&#x27;mysync&#x27;,master_password=&#x27;123456&#x27;,master_log_file=&#x27;&lt;mariadb1_file&gt;&#x27;,master_log_pos=&lt;mariadb1_pos&gt;;start slave; 注：master_log_pos 和 master_log_file 要从主机的数据状态当中获得。 状态检查 1show slave status; 如果观察到如下内容则代表配置正常。 12Slave_IO_Running: YesSlave_SQL_Running: Yes Keepalived 安装软件 1yum -y install keepalived 编写检测脚本(所有设备) 1vim /etc/keepalived/check_mysql.sh 1234567891011#!/bin/bashMYSQL_PING=`mysqladmin ping`MYSQL_OK=&quot;mysqld is alive&quot;if [[ &quot;$MYSQL_PING&quot; != &quot;$MYSQL_OK&quot; ]] then echo &quot;mysql is not running&quot; killall keepalived else echo &quot;mysql is running&quot;fi 1chmod +x /etc/keepalived/check_mysql.sh 获取网卡信息 1nmcli connection show 注：获取到的网卡名需要填入下一步的配置文件中，两台设备都记得要检查 配置 Keepalived 服务 (主要节点) 注：配置中的 interface 要指定当前的网卡，并且需要根据设备优先级设定 priority，且虚拟 IP 地址要与目前 IP 不冲突。 12mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bkvim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script check_mysql &#123; script &quot;/etc/keepalived/check_mysql.sh&quot; interval 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface &lt;name&gt; virtual_router_id 51 priority 100 advert_int 1 nopreempt authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; &lt;virtual_ip&gt; &#125; track_script &#123; check_mysql &#125;&#125; 配置 Keepalived 服务 (次要节点) 1vim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script check_mysql &#123; script &quot;/etc/keepalived/check_mysql.sh&quot; interval 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface &lt;name&gt; virtual_router_id 51 priority 90 advert_int 1 nopreempt authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; &lt;virtual_ip&gt; &#125; track_script &#123; check_mysql &#125;&#125; 启动服务 1systemctl enable keepalived --now 运行状态检测 使用 ping &lt;virtual_ip&gt; 可以用来检测服务运行情况 在 keepalived 的设备上输入 ip a 然后检查绑定的网卡上是否存在 IP 挂载的情况即可。 常见问题 keepalived.pid 文件不存在 如果出现了 keepalived.pid 文件不存在而引发的服务运行异常，请检查 /etc/keepalived 目录中是否有正确的配置文件。 网卡配置调整 在网卡经过配置后虚拟 IP 连接异常 注：在网卡相关信息有修改的时候需要重启 keepalived 服务。 1systemctl restart keepalived","categories":[{"name":"MariaDB","slug":"MariaDB","permalink":"https://wangqian0306.github.io/categories/MariaDB/"}],"tags":[{"name":"MariaDB","slug":"MariaDB","permalink":"https://wangqian0306.github.io/tags/MariaDB/"}]},{"title":"CDH 本地源搭建流程整理","slug":"bigdata/cdh-mirror","date":"2020-12-09T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/cdh-mirror/","permalink":"https://wangqian0306.github.io/2020/cdh-mirror/","excerpt":"","text":"CDH 本地源搭建流程整理 简述 注：CDH 已经关闭了免费下载途径。此文档仅供参考。 将公网上的 CDH 软件源下载至本地可以满足在内网安装和下载加速的功能，具体的搭建方式可以采用以下方式。 手动搭建 Nexus 软件源搭建 手动搭建方式 Cloudera 官方文档 安装 httpd 服务 1yum install -y httpd 创建同步文件夹 1mkdir -p /var/www/html/cloudera-repos/cm6 拉取软件包 注：Cloudera 官方已经关闭了免费下载源，所以需要自行寻找让软件包。 修改文件权限 12chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6 补充 repo(若出现 repo 访问异常) 注：进入报错的文件夹输入如下命令。 1createrepo -g repodata/repomd.xml . 启动服务 12systemctl enable httpdsystemctl start httpd 检查服务 1curl localhost/cloudera-repos/ 若出现文件夹则证明环境搭建正常。 配置客户端 在需要安装 CDH 的客户机上新增 /etc/yum.repos.d/cm.repo 文件即可。 12345678[cloudera-manager]name=Cloudera Manager 6.3.1baseurl=http://&lt;server_ip&gt;/cloudera-repos/cm6/6.3.1/redhat7/yum/gpgkey=http://&lt;server_ip&gt;/cloudera-repos/cm6/6.3.1/redhat7/yum/RPM-GPG-KEY-clouderagpgcheck=0enabled=1autorefresh=0type=rpm-md Nexus 软件源方式(已经失效) Neuxs 可以作为 Proxy 缓存外网上的软件包。 注：此处采用 Docker 的方式运行 Nexus 软件源，需要 Docker 和 Docker-Compose 软件。 选定安装位置创建 nexus 文件夹 注：此处建议安装在 /opt 目录下 1mkdir /opt/nexus 在 nexus 文件夹中新增 docker-compose.yaml 文件 1vim /opt/nexus/docker-compose.yaml 1234567services: nexus: image: sonatype/nexus3 volumes: - ./nexus-data:/nexus-data ports: - 8081:8081 开启服务 12cd /opt/nexusdocker-compose up -d 查看默认密码 1docker-compose exec nexus cat /nexus-data/admin.password 登录界面 访问 http://&lt;ip&gt;:8081 并使用 admin 账户进行登录。 配置代理 根据界面提示新增 yum (proxy) 类型代理，然后根据提示使用即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"}]},{"title":"Phoenix 搭建流程整理","slug":"bigdata/phoenix","date":"2020-12-06T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/phoenix/","permalink":"https://wangqian0306.github.io/2020/phoenix/","excerpt":"","text":"Phoenix 搭建流程整理 简介 Phoenix 是一个开源的HBASE SQL层应用程序。 安装流程 CSD 激活 在 CM 主机上执行如下命令放入激活文件 12mkdir -p /opt/cloudera/csd/wget https://archive.cloudera.com/phoenix/6.2.0/csd/PHOENIX-1.0.jar -P /opt/cloudera/csd/ Parcel 配置 进入 CDH 然后配置如下 parcel 1https://archive.cloudera.com/phoenix/6.2.0/parcels/ 安装分配并激活此 parcel 即可 安装服务 在 Parcel 配置完成后需要重启 scm-server 服务才可以进行安装 1systemctl restart cloudera-scm-server 之后使用界面安装即可 测试 Python 1pip3 install -i https://mirrors.cloud.tencent.com/pypi/simple --user phoenixdb 123456789101112131415import phoenixdbimport phoenixdb.cursordatabase_url = &#x27;http://192.168.2.116:8765/&#x27;conn = phoenixdb.connect(database_url, autocommit=True)cursor = conn.cursor()#cursor.execute(&quot;CREATE TABLE asd (id INTEGER PRIMARY KEY, username VARCHAR)&quot;)cursor.execute(&quot;UPSERT INTO asd VALUES (?, ?)&quot;, (2, &#x27;wq&#x27;))#cursor.execute(&quot;SELECT * FROM asd&quot;)#print(cursor.fetchall())#cursor = conn.cursor(cursor_factory=phoenixdb.cursor.DictCursor)#cursor.execute(&quot;SELECT * FROM USERS WHERE id=1&quot;)#print(cursor.fetchone()[&#x27;USERNAME&#x27;]) Java 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;wqphoenix&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;5.0.0.7.1.4.0-203&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.0.0-cdh6.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;version&gt;1.18.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package org.example.wqphoenix;import lombok.extern.slf4j.Slf4j;import java.sql.Connection;import java.sql.DriverManager;import java.sql.Statement;import java.util.Properties;@Slf4jpublic class Demo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.setProperty(&quot;phoenix.query.timeoutMs&quot;, &quot;100000&quot;); props.setProperty(&quot;hbase.rpc.timeout&quot;, &quot;100000&quot;); props.setProperty(&quot;hbase.client.scanner.timeout.period&quot;, &quot;100000&quot;);// props.setProperty(&quot;phoenix.schema.isNamespaceMappingEnabled&quot;, &quot;true&quot;); try &#123; Class.forName(&quot;org.apache.phoenix.jdbc.PhoenixDriver&quot;); String url = &quot;jdbc:phoenix:cdh-3.rainbowfish11000.local:2181&quot;; DriverManager.setLoginTimeout(10); Connection conn = DriverManager.getConnection(url, props); Statement statement = conn.createStatement(); long time = System.currentTimeMillis(); String create_sql = &quot;CREATE TABLE Test (id INTEGER PRIMARY KEY, username VARCHAR)&quot;; statement.execute(create_sql); String insert_sql = &quot;UPSERT INTO Test VALUES (3, &#x27;demo&#x27;)&quot;; statement.executeUpdate(insert_sql); String sql = &quot;SELECT * FROM Test&quot;; ResultSet rs = statement.executeQuery(sql); while (rs.next()) &#123; System.out.println(&quot;*****************&quot;); System.out.println(String.valueOf(rs.getInt(1))); System.out.println(rs.getString(2)); &#125; rs.close(); long timeUsed = System.currentTimeMillis() - time; System.out.println(&quot;time &quot; + timeUsed + &quot;mm&quot;); statement.close(); conn.commit(); conn.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考资料 参考文档","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Phoenix","slug":"Phoenix","permalink":"https://wangqian0306.github.io/tags/Phoenix/"}]},{"title":"Pulsar 初探","slug":"bigdata/pulsar","date":"2020-12-06T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/pulsar/","permalink":"https://wangqian0306.github.io/2020/pulsar/","excerpt":"","text":"Pulsar 初探 简介 最近经常看到 Pulsar 相关的文章，所以初步了解了一下 Pulsar 项目。 相较于 Kafka 来说，Pulsar 主要做了以下两件事来更好的提供服务： 云原生(方便扩容缩容) 使用 Bookkeeper 存储数据 与 Kafka 类似 Pulsar 目前也使用了 ZooKeeper 存储元数据，大致结构如下图所示。 在 Pulsar 集群中大致分为以下三个部分： 一个或多个 broker 负载平衡和处理来自生产者的传入消息，向消费者分发消息，与 Pulsar 配置存储通信以处理各种协调任务，将消息存储在 BookKeeper 实例（又名 bookies）中，在某些情况下依赖于特定于集群的 ZooKeeper 集群任务等等。 一个或多个 bookie 组成的 BookKeeper 集群处理消息的持久存储。 特定于该集群的 ZooKeeper 集群处理 Pulsar 集群之间的协调任务。 如下图所示： Pulsar 架构 初步使用 新建 docker-compose.yaml 文件然后填入如下内容： 12345678910services: pulsar: image: apachepulsar/pulsar:latest container_name: pulsar ports: - &quot;6650:6650&quot; - &quot;8080:8080&quot; command: bin/pulsar standalone volumes: - ./volume/data:/pulsar/data 安装客户端库 1pip install pulsar-client 编写消费者 123456789101112import pulsarclient = pulsar.Client(&#x27;pulsar://localhost:6650&#x27;)consumer = client.subscribe(&#x27;my-topic&#x27;, subscription_name=&#x27;my-sub&#x27;)while True: msg = consumer.receive() print(&quot;Received message: &#x27;%s&#x27;&quot; % msg.data()) consumer.acknowledge(msg)client.close() 编写生产者 123456789import pulsarclient = pulsar.Client(&#x27;pulsar://localhost:6650&#x27;)producer = client.create_producer(&#x27;my-topic&#x27;)for i in range(10): producer.send((&#x27;hello-pulsar-%d&#x27; % i).encode(&#x27;utf-8&#x27;))client.close() 参考资料 https://www.bilibili.com/video/BV1tV41127PD?from=search&amp;seid=14981968776096759677","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"https://wangqian0306.github.io/tags/Pulsar/"}]},{"title":"CDH 集群修改元数据库","slug":"bigdata/switch-db","date":"2020-12-04T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/cdh-switch-db/","permalink":"https://wangqian0306.github.io/2020/cdh-switch-db/","excerpt":"","text":"CDH 集群修改元数据库 CDH 集群可能存在如下数据库: 组件 数据库 Cloudera Manager Server scm Activity Monitor amon Reports Manager rman Hue hue Hive Metastore Server metastore Sentry Server sentry Cloudera Navigator Audit Server nav Cloudera Navigator Metadata Server navms Oozie oozie Cloudera Manager Server 数据库修改 关闭集群大数据服务 关闭 Cloudera Management Service 服务 关闭服务(所有设备) 12systemctl restart cloudera-scm-serversystemctl restart cloudera-scm-agent 迁移数据库 在迁移时可以使用 Navicat Premium 的数据传输功能，具体位置如下： 1工具-&gt;数据传输 选择需要传输的源地址和目标地址之后按照界面提示操作即可。 重新初始化数据库 1/opt/cloudera/cm/schema/scm_prepare_database.sh -h &lt;new_host&gt; mysql scm mariadb &lt;password&gt; 或者可以手动编辑配置文件 1vim /etc/cloudera-scm-server/db.properties 启动集群(所有设备) 12systemctl start cloudera-scm-serversystemctl start cloudera-scm-agent 注：先启动 cloudera-scm-server 待页面工作正常时再启动 cloudera-scm-agent, 在观测到服务运行正常后再启动服务 启动服务 其他服务数据库修改 除了 Cloudera Manager Server 之外所有的服务都可以在页面当中进行配置,具体位置如下： 1Configuration -&gt; Database Settings","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"}]},{"title":"CDH 集群设备 Hostname 修改","slug":"bigdata/switch-host","date":"2020-12-04T14:26:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/cdh-switch-host/","permalink":"https://wangqian0306.github.io/2020/cdh-switch-host/","excerpt":"","text":"CDH 集群设备 Hostname 修改 关闭集群 在页面中正常关闭集群和 Cloudera Management Service 关闭 cloudera-scm-agent 服务 1systemctl stop cloudera-scm-agent 关闭 cloudera-scm-server 服务 1systemctl stop cloudera-scm-server 修改 hostname 修改配置项 1vim /etc/sysconfig/network 1HOSTNAME=&lt;hostname&gt; 全局配置 1hostnamectl set-hostname &lt;hostname&gt; 修改数据库 登录 SCM 数据库，然后修改如下表 HOSTS HOSTS_AUD 将其中的域名全部修改为最新值。 启动集群 12systemctl start cloudera-scm-serversystemctl start cloudera-scm-agent 重新部署配置并启动集群 在集群配置中需要注意含有数据库的配置项，如果数据库主机 hostname 变化则可能造成集群启动异常。 在 CDH 集群中需要数据库进行持久化存储的服务如下： Cloudera Manager Server Activity Monitor Reports Manager Hue Hive Metastore Server Sentry Server Cloudera Navigator Audit Server Cloudera Navigator Metadata Server Oozie 常见问题 Hadoop Failover Controller 启动异常 由于 HDFS HA 是通过 ZooKeeper 来实现的所以可以通过如下命令进行重置 1hdfs zkfc -formatZK -force -nonInteractive","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"}]},{"title":"CDH 搭建流程整理","slug":"bigdata/cdh","date":"2020-12-03T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/cdh/","permalink":"https://wangqian0306.github.io/2020/cdh/","excerpt":"","text":"CDH 搭建流程整理 简述 注：CDH 已经关闭了免费下载途径。此文档仅供参考。 本文只是简要描述了 CDH 集群的搭建步骤，适用于测试和研发环境。如在安装过程中遇到问题可以参照 官方文档 。 基础环境准备 在 CDH 安装的所有主机上都需要执行以下命令 配置主机名 1hostnamectl set-hostname &lt;hostname&gt; 配置域内解析 1vim /etc/hosts 或者使用 DNS 。 配置网络主机名 1vim /etc/sysconfig/network 1HOSTNAME=&lt;hostname&gt; 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld 关闭 SELinux 12setenforce 0vim /etc/selinux/config 将状态设置为 disable 时钟同步 123yum install -y ntpsystemctl start ntpdsystemctl enable ntpd 配置内存交换比例 12sysctl vm.swappiness=10echo &quot;\\nvm.swappiness=10&quot; &gt; /etc/sysctl.conf 配置THP 123echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho &quot;\\necho never &gt; /sys/kernel/mm/transparent_hugepage/defrag\\necho never &gt; /sys/kernel/mm/transparent_hugepage/enabled\\n&quot; &gt; /etc/rc.local JDK 安装及配置 12yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-develalternatives --config java JDBC 配置 12345wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gztar -zxvf mysql-connector-java-5.1.46.tar.gzmkdir -p /usr/share/java/cd mysql-connector-java-5.1.46cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 安装软件源 注：此处需要修改为本地源 1vim /etc/yum.repos.d/clouder-scm.repo 12345678[cloudera-manager]name=Cloudera Manager 6.3.1baseurl=http://&lt;server_ip&gt;/cloudera-repos/cm6/6.3.1/redhat7/yum/gpgkey=http://&lt;server_ip&gt;/cloudera-repos/cm6/6.3.1/redhat7/yum/RPM-GPG-KEY-clouderagpgcheck=0enabled=1autorefresh=0type=rpm-md 安装数据库 在 CDH 集群中需要寻找出一台设备来安装数据库 注：Cloudera 官方建议数据库和需要用到持久化的软件安装在同一台设备上。 安装数据库 123yum install -y mariadb-server mariadb-devel mariadbsystemctl enabel mariadbsystemctl start mariadb 创建用户和数据库 1mysql 12345678910111213use mysql;create user mariadb@&#x27;%&#x27; identified by &#x27;&lt;password&gt;&#x27;;grant all privileges on *.* to mariadb@&#x27;%&#x27; identified by &#x27;123456&#x27;;flush privileges;CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; 注：数据库对应关系参见官方文档 初始化数据库 1/opt/cloudera/cm/schema/scm_prepare_database.sh -h &lt;db_host&gt; mysql scm mariadb &lt;password&gt; 服务安装 在 CDH 主机上执行下列命令 安装软件 1yum install -y cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server 启动服务 12systemctl start cloudera-scm-serversystemctl enable cloudera-scm-server 等待3分钟之后检查 http://&lt;ip_address&gt;:7180 页面是否正常响应即可。 错误检查 1tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 服务增强 Oozie WebUI 根据界面提示安装软件 下载依赖包 1wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip -P /var/lib/oozie 解压并修改权限 1234yum install -y unzipcd /var/lib/oozie/unzip ext-2.2.zipchown -R oozie:oozie ext-2.2 数据备份及恢复 备份 1curl -u &lt;admin_username&gt;:&lt;admin_password&gt; &quot;http://&lt;cm_server_host&gt;:7180/api/v32/cm/deployment&quot; &gt; cm-deployment.json 恢复 注：此处功能需要企业版授权才可以使用。 关闭集群 登录 Cloudera Manager Server 所在设备 1curl -H &quot;Content-Type: application/json&quot; --upload-file cm-deployment.json -u &lt;admin_username&gt;:&lt;admin_password&gt; http://&lt;cm_server_host&gt;:7180/api/v32/cm/deployment?deleteCurrentDeployment=true","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"}]},{"title":"Cloudera Manager HA 搭建流程整理","slug":"bigdata/cm-ha","date":"2020-12-03T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/cm-ha/","permalink":"https://wangqian0306.github.io/2020/cm-ha/","excerpt":"","text":"Cloudera Manager HA 搭建流程整理 简介 Cloudera Manager 有以下五个部分： Cloudera Manager Server Cloudera Management Service Activity Monitor Alert Publisher Event Server Host Monitor Service Monitor Reports Manager Cloudera Navigator Audit Server 关系型数据库 文件存储 Cloudera Manager Agent 注：目前 Cloudera Manager 的 HA 方式是采用负载均衡器实现的且目前仅支持主备的HA方案，并不支持双活。HAProxy 和 NFS 等服务的 HA 在 Cloudera 官方文档中并未进行说明。 为了实现 HA 的效果需要如下三种服务 HAProxy (端口转发) NFS(文件同步) Pacemaker, Corosync (故障转移) 注意事项： 不要在现有 CDH 集群中的任何主机上托管 Cloudera Manager 或 Cloudera Management Service 角色，因为这会使故障转移配置变得复杂，并且重叠的故障域可能导致故障和错误跟踪的问题。 采用相同配置的硬件设备部署主备服务。这样做能确保故障转移不会导致性能下降。 为主机和备机配置不同的电源和网络，这样做能限制重叠的故障域。 其中 Cloudera Management Service 所在设备会有 Hostname 修改,CDH 集群会无法对其进行管理故不能安装任何大数据组件。 本次部署时的 NFS 服务不能搭建在 MGMT 主机上。 为了方便说明，下面将采用如下所示的简写 简写 主机 CMS CMSHostname MGMT MGMTHostname NFS NFS Store DB Databases 如果在配置过程中遇到了问题请参阅 官方文档 前置准备 如果之前已经安装了 CDH 集群则需要在界面上停止集群并且使用如下命令关闭集群和服务。 12systemctl stop cloudera-scm-serversysyemctl stop cloudera-scm-agent 并且在新部署的设备上执行如下命令： 配置主机名 1hostnamectl set-hostname &lt;hostname&gt; 配置域内解析 1vim /etc/hosts 或者使用 DNS 。 配置网络主机名 1vim /etc/sysconfig/network 1HOSTNAME=&lt;hostname&gt; 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld 关闭 SELinux 12setenforce 0vim /etc/selinux/config 将状态设置为 disable 时钟同步 123yum install -y ntpsystemctl start ntpdsystemctl enable ntpd 配置内存交换比例 12sysctl vm.swappiness=10echo &quot;\\nvm.swappiness=10&quot; &gt; /etc/sysctl.conf 配置THP 123echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho &quot;\\necho never &gt; /sys/kernel/mm/transparent_hugepage/defrag\\necho never &gt; /sys/kernel/mm/transparent_hugepage/enabled\\n&quot; &gt; /etc/rc.local JDK 安装及配置 12yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-develalternatives --config java JDBC 配置 12345wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gztar -zxvf mysql-connector-java-5.1.46.tar.gzmkdir -p /usr/share/java/cd mysql-connector-java-5.1.46cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 安装软件源 12wget https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/rpm --import https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/RPM-GPG-KEY-cloudera CMS 配置 安装服务 1yum install haproxy -y 服务初始化配置(可选) 1vim /etc/haproxy/haproxy.cfg 在服务检测部分有转发限定的相关的配置，为了方便测试可以通过注释下面的内容来关闭此项检测 1option forwardfor except 127.0.0.0/8 在服务安装完成后内部存在默认样例，在实际环境中可以注释如下内容关闭此样例 123456frontend main *:5001 acl url_static path_beg -i /static /images /javascript /stylesheets acl url_static path_end -i .jpg .gif .png .css .js use_backend static if url_static default_backend app 123backend static balance roundrobin server static 127.0.0.1:4331 check 123456backend app balance roundrobin server app1 127.0.0.1:5001 check server app2 127.0.0.1:5002 check server app3 127.0.0.1:5003 check server app4 127.0.0.1:5004 check 关闭 SE-Linux 12setenforce 0vim /etc/selinux/config 开放连接 1setsebool -P haproxy_connect_any=1 编辑 HAProxy 配置文件 1vim /etc/haproxy/haproxy.cnf Cloudera Manager Server 端口转发配置 1234567891011121314151617listen cmf :7180 mode tcp option tcplog server cmfhttp1 &lt;CMS1&gt;:7180 check server cmfhttp2 &lt;CMS2&gt;:7180 checklisten cmfavro :7182 mode tcp option tcplog server cmfavro1 &lt;CMS1&gt;:7182 check server cmfavro2 &lt;CMS2&gt;:7182 checklisten cmfhttps :7183 mode tcp option tcplog server cmfhttps1 &lt;CMS1&gt;:7183 check server cmfhttps2 &lt;CMS2&gt;:7183 check 检测配置项 1haproxy -f /etc/haproxy/haproxy.cfg -c 启动服务 1systemctl enable haproxy --now MGMT 配置 安装服务 1yum install haproxy -y 服务初始化配置(可选) 1vim /etc/haproxy/haproxy.cfg 在服务检测部分有转发限定的相关的配置，为了方便测试可以通过注释下面的内容来关闭此项检测 1option forwardfor except 127.0.0.0/8 在服务安装完成后内部存在默认样例，在实际环境中可以注释如下内容关闭此样例 123456frontend main *:5001 acl url_static path_beg -i /static /images /javascript /stylesheets acl url_static path_end -i .jpg .gif .png .css .js use_backend static if url_static default_backend app 123backend static balance roundrobin server static 127.0.0.1:4331 check 123456backend app balance roundrobin server app1 127.0.0.1:5001 check server app2 127.0.0.1:5002 check server app3 127.0.0.1:5003 check server app4 127.0.0.1:5004 check 关闭 SE-Linux 12setenforce 0vim /etc/selinux/config 开放连接 1setsebool -P haproxy_connect_any=1 编辑 HAProxy 配置文件 1vim /etc/haproxy/haproxy.cfg Activity Monitor 端口转发配置 1234567891011121314151617181920listen am1 :8087 mode tcp option tcplog server am1a &lt;MGMT1&gt;:8087 check server am1b &lt;MGMT2&gt;:8087 checklisten am2 :9087 mode tcp option tcplog server am2a &lt;MGMT1&gt;:9087 check server am2b &lt;MGMT2&gt;:9087 checklisten am3 :9998 mode tcp option tcplog server am3a &lt;MGMT1&gt;:9998 check server am3b &lt;MGMT2&gt;:9998 checklisten am4 :9999 mode tcp option tcplog server am4a &lt;MGMT1&gt;:9999 check server am4b &lt;MGMT2&gt;:9999 check Alert Publisher 端口转发配置 12345listen ap1 :10101 mode tcp option tcplog server ap1a &lt;MGMT1&gt;:10101 check server ap1b &lt;MGMT2&gt;:10101 check Event Server 端口转发配置 123456789101112131415listen es1 :7184 mode tcp option tcplog server es1a &lt;MGMT1&gt;:7184 check server es1b &lt;MGMT2&gt;:7184 checklisten es2 :7185 mode tcp option tcplog server es2a &lt;MGMT1&gt;:7185 check server es2b &lt;MGMT2&gt;:7185 checklisten es3 :8084 mode tcp option tcplog server es3a &lt;MGMT1&gt;:8084 check server es3b &lt;MGMT2&gt;:8084 check Host Monitor 端口转发配置 1234567891011121314151617181920listen hm1 :8091 mode tcp option tcplog server hm1a &lt;MGMT1&gt;:8091 check server hm1b &lt;MGMT2&gt;:8091 checklisten hm2 :9091 mode tcp option tcplog server hm2a &lt;MGMT1&gt;:9091 check server hm2b &lt;MGMT2&gt;:9091 checklisten hm3 :9994 mode tcp option tcplog server hm3a &lt;MGMT1&gt;:9994 check server hm3b &lt;MGMT2&gt;:9994 checklisten hm4 :9995 mode tcp option tcplog server hm4a &lt;MGMT1&gt;:9995 check server hm4b &lt;MGMT2&gt;:9995 check Service Monitor 端口转发配置 1234567891011121314151617181920listen sm1 :8086 mode tcp option tcplog server sm1a &lt;MGMT1&gt;:8086 check server sm1b &lt;MGMT2&gt;:8086 checklisten sm2 :9086 mode tcp option tcplog server sm2a &lt;MGMT1&gt;:9086 check server sm2b &lt;MGMT2&gt;:9086 checklisten sm3 :9996 mode tcp option tcplog server sm3a &lt;MGMT1&gt;:9996 check server sm3b &lt;MGMT2&gt;:9996 checklisten sm4 :9997 mode tcp option tcplog server sm4a &lt;MGMT1&gt;:9997 check server sm4b &lt;MGMT2&gt;:9997 check Cloudera Manager Agent 端口转发配置 12345listen mgmt-agent :9000 mode tcp option tcplog server mgmt-agenta &lt;MGMT1&gt;:9000 check server mgmt-agentb &lt;MGMT2&gt;:9000 check Reports Manager 端口转发配置 12345678910listen rm1 :5678 mode tcp option tcplog server rm1a &lt;MGMT1&gt;:5678 check server rm1b &lt;MGMT2&gt;:5678 checklisten rm2 :8083 mode tcp option tcplog server rm2a &lt;MGMT1&gt;:8083 check server rm2b &lt;MGMT2&gt;:8083 check Cloudera Navigator Audit Server 端口转发配置 123456789101112131415listen cn1 :7186 mode tcp option tcplog server cn1a &lt;MGMT1&gt;:7186 check server cn1b &lt;MGMT2&gt;:7186 checklisten cn2 :7187 mode tcp option tcplog server cn2a &lt;MGMT1&gt;:8083 check server cn2b &lt;MGMT2&gt;:8083 checklisten cn3 :8089 mode tcp option tcplog server cn3a &lt;MGMT1&gt;:8089 check server cn3b &lt;MGMT2&gt;:8089 check 检测配置项 1haproxy -f /etc/haproxy/haproxy.cfg -c 启动服务 1systemctl enable haproxy --now 检查服务状态 1systemctl status haproxy NFS 配置 注：NFS 设备最好搭建在 CDH 集群之外，方便后期维护。 服务安装 1yum install -y nfs-utils 创建挂载文件夹 123456789mkdir -p /home/cloudera-scm-servermkdir -p /home/cloudera-host-monitormkdir -p /home/cloudera-scm-agentmkdir -p /home/cloudera-scm-eventservermkdir -p /home/cloudera-scm-headlampmkdir -p /home/cloudera-service-monitormkdir -p /home/cloudera-scm-navigatormkdir -p /home/etc-cloudera-scm-agentmkdir -p /home/cloudera-scm-csd 编写连接配置文件 1vim /etc/exports.d/cloudera-manager.exports 填入如下内容 123456789/home/cloudera-scm-server *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-host-monitor *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-scm-agent *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-scm-eventserver *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-scm-headlamp *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-service-monitor *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-scm-navigator *(rw,sync,no_root_squash,no_subtree_check)/home/etc-cloudera-scm-agent *(rw,sync,no_root_squash,no_subtree_check)/home/cloudera-scm-csd *(rw,sync,no_root_squash,no_subtree_check) 启动服务 123456firewall-cmd --permanent --add-service=mountdfirewall-cmd --permanent --add-service=rpc-bindfirewall-cmd --permanent --add-service=nfsfirewall-cmd --reloadsystemctl enable rpcbind --nowsystemctl enable nfs --now 查看服务运行状态 1showmount -e 若显示的出上文中配置的文件夹则证明服务运行正常，如果遇到问题可以试试下面的命令。 刷新挂载列表 1exportfs -a Cloudera Manager Server 偏移测试 在完成上述内容后就可以进行 Cloudera Manager Server 偏移测试了。 千万注意主备不能同时启动，如果主备同时启动可能损坏数据库 CMS 1 配置（主） 安装服务并初始化数据库 若 CMS 1 之前已经安装过了 Cloudera Manager Server 则可以跳过此步骤。 12yum install -y cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server/opt/cloudera/cm/schema/scm_prepare_database.sh -h &lt;db_host&gt; mysql scm mariadb &lt;password&gt; 安装 NFS 1yum install -y nfs-utils 拷贝原有文件 若 CMS 1 没有装过 Cloudera Manager Server 则可以执行如下命令将配置文件备份传输至 NFS 服务器。 1scp -r /var/lib/cloudera-scm-server/* &lt;user&gt;@&lt;NFS&gt;:/home/cloudera-scm-server/ 注：如果 CDH 集群中安装过扩展 Parcels 则需要同步主备两台设备上的 csd 授权文件。具体位置位于 Administration &gt; Settings &gt; Custom Service Descriptors &gt; Local Descriptor Repository Path。 创建挂载目录 12rm -rf /var/lib/cloudera-scm-servermkdir -p /var/lib/cloudera-scm-server 挂载存储目录 1mount -t nfs &lt;NFS&gt;:/home/cloudera-scm-server /var/lib/cloudera-scm-server 配置开机挂载 1vim /etc/fstab 填入如下内容 1&lt;NFS&gt;:/home/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 启动服务 1systemctl start cloudera-scm-server 检查服务状态 访问 http://&lt;CMS1&gt;:7180 查看服务运行情况 关闭 HTTP Referer Check 进入 Administration-&gt;Settings-&gt;Category-&gt;Security 配置中 关闭 HTTP Referer Check 属性 检查负载均衡服务状态 访问 http://&lt;CMS&gt;:7180 查看服务运行情况 关闭主机 1systemctl stop cloudera-scm-server CMS 2 配置（备） 安装服务 1yum install -y cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server 安装 NFS 1yum install -y nfs-utils 创建挂载目录 12rm -rf /var/lib/cloudera-scm-servermkdir -p /var/lib/cloudera-scm-server 挂载存储目录 1mount -t nfs &lt;NFS&gt;:/home/cloudera-scm-server /var/lib/cloudera-scm-server 配置开机挂载 1vim /etc/fstab 填入如下内容 1&lt;NFS&gt;:/home/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 拷贝数据库配置文件 12mkdir -p /etc/cloudera-scm-serverscp &lt;ssh-user&gt;@&lt;CMS1&gt;:/etc/cloudera-scm-server/db.properties /etc/cloudera-scm-server/db.properties 关闭开机启动 1systemctl disable cloudera-scm-server 启动备机 1systemctl start cloudera-scm-server 检查服务状态 访问 http://CMS2:7180 查看服务运行情况 检查负载均衡服务状态 访问 http://CMS:7180 查看服务运行情况 关闭备机 1systemctl stop cloudera-scm-server 完成测试 在备机关闭完成后，需要返回至主机，开启主机上的 cloudera-scm-server 服务 1systemctl start cloudera-scm-server Agent 配置 修改 Agent 连接的目标主机 修改 CDH 集群中除了 MGMT1,MGMT2,CMS1,CMS2 设备上的所有 Agent 配置 1vim /etc/cloudera-scm-agent/config.ini 修改如下配置项： 1server_host=&lt;CMS&gt; 重启 Agent 服务 1systemctl restart cloudera-scm-agent Cloudera Mangement Service 偏移测试 由于 Cloudera Management Service 当中存储的内容没什么用，所以此处采用了删除重新安装的方式。 MGMT 1 配置 安装服务 1yum install -y cloudera-manager-daemons cloudera-manager-agent 拷贝配置文件至 NFS 1scp -R /etc/cloudera-scm-agent &lt;user&gt;@&lt;NFS&gt;:/home/etc-cloudera-scm-agent 安装 NFS 1yum install -y nfs-utils 创建存储目录 1234567mkdir -p /var/lib/cloudera-host-monitormkdir -p /var/lib/cloudera-scm-agentmkdir -p /var/lib/cloudera-scm-eventservermkdir -p /var/lib/cloudera-scm-headlampmkdir -p /var/lib/cloudera-service-monitormkdir -p /var/lib/cloudera-scm-navigatormkdir -p /etc/cloudera-scm-agent 挂载存储目录 1234567mount -t nfs &lt;NFS&gt;:/home/cloudera-host-monitor /var/lib/cloudera-host-monitormount -t nfs &lt;NFS&gt;:/home/cloudera-scm-agent /var/lib/cloudera-scm-agentmount -t nfs &lt;NFS&gt;:/home/cloudera-scm-eventserver /var/lib/cloudera-scm-eventservermount -t nfs &lt;NFS&gt;:/home/cloudera-scm-headlamp /var/lib/cloudera-scm-headlampmount -t nfs &lt;NFS&gt;:/home/cloudera-service-monitor /var/lib/cloudera-service-monitormount -t nfs &lt;NFS&gt;:/home/cloudera-scm-navigator /var/lib/cloudera-scm-navigatormount -t nfs &lt;NFS&gt;:/home/etc-cloudera-scm-agent /etc/cloudera-scm-agent 配置开机挂载 1vim /etc/fstab 填入如下内容 1234567&lt;NFS&gt;:/home/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 修改 Agent 配置 1vim /etc/cloudera-scm-agent/config.ini 修改如下配置项 12server_host=&lt;CMS&gt;listening_hostname=&lt;MGMT&gt; 配置 Hosts 1vim /etc/hosts 新增如下内容 1&lt;MGMT1_IP&gt; &lt;MGMT&gt; 检查 Hosts 1ping &lt;MGMT&gt; 若目标地址 IP 与 MGMT1_IP(本机 IP) 相同则证明配置无误。 配置 UID 和 GID 注：此处需要和 MGMT1 完成统一配置且与两台设备的其他服务不冲突。 检查设备上cloudera-scm用户的 UID 和 GID 1cat /etc/passwd 配置设备上的 UID 和 GID 12usermod -u &lt;UID&gt; cloudera-scmgroupmod -g &lt;UID&gt; cloudera-scm 检查之前用户的遗留文件夹 1find / -user &lt;UID_Origin&gt; -type d 切换权限至当前用户 1chown -R cloudear-scm:cloudear-scm &lt;dir&gt; 修改文件夹所属用户 123456chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserverchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigatorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agentchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp 启动服务 1systemctl restart cloudera-scm-agent 检查注册情况 访问 http://&lt;CMS&gt;:7180 查看主机列表，若 &lt;MGMT&gt; 出现在列表中则证明安装成功。 删除并重新安装服务 在页面当中删除 Cloudera Management Service 服务，然后点击右上角的 Add 按钮在新设备上安装服务 关闭 MGMT系列设备上的 Hostname 检查 由于此处采用了 Hosts 文件完成了解析所以需要根据页面上的红色提示关闭 Hostname 检查 服务启动测试 在安装完成后需要开启集群和Cloudera Management Service 服务进行检查。 关闭服务 在界面上关闭集群和 Cloudera Management Service 服务 关闭 Agent 服务 1systemctl stop cloudera-scm-agent MGMT 2 配置 安装服务 1yum install -y cloudera-manager-daemons cloudera-manager-agent 安装 NFS 1yum install -y nfs-utils 挂载存储目录 1234567mount -t nfs &lt;NFS&gt;:/home/cloudera-host-monitor /var/lib/cloudera-host-monitormount -t nfs &lt;NFS&gt;:/home/cloudera-scm-agent /var/lib/cloudera-scm-agentmount -t nfs &lt;NFS&gt;:/home/cloudera-scm-eventserver /var/lib/cloudera-scm-eventservermount -t nfs &lt;NFS&gt;:/home/cloudera-scm-headlamp /var/lib/cloudera-scm-headlampmount -t nfs &lt;NFS&gt;:/home/cloudera-service-monitor /var/lib/cloudera-service-monitormount -t nfs &lt;NFS&gt;:/home/cloudera-scm-navigator /var/lib/cloudera-scm-navigatormount -t nfs &lt;NFS&gt;:/home/etc-cloudera-scm-agent /etc/cloudera-scm-agent 配置开机挂载 1vim /etc/fstab 填入如下内容 1234567&lt;NFS&gt;:/home/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0&lt;NFS&gt;:/home/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 修改 Agent 配置 1vim /etc/cloudera-scm-agent/config.ini 修改如下配置项 12server_host=&lt;CMS&gt;listening_hostname=&lt;MGMT&gt; 配置 Hosts 1vim /etc/hosts 新增如下内容 1&lt;MGMT2_IP&gt; &lt;MGMT&gt; 检查 Hosts 1ping &lt;MGMT&gt; 若目标地址 IP 与 MGMT2_IP(本机 IP) 相同则证明配置无误。 配置 UID 和 GID 注：此处需要和 MGMT1 完成统一配置且与两台设备的其他服务不冲突。 检查设备上cloudera-scm用户的 UID 和 GID 1cat /etc/passwd 配置设备上的 UID 和 GID 12usermod -u &lt;UID&gt; cloudera-scmgroupmod -g &lt;UID&gt; cloudera-scm 启动服务 1systemctl start cloudera-scm-agent 注：如果因为用户修改的情况造成启动失败，可以尝试重启设备。 检查注册情况 访问 http://&lt;CMS&gt;:7180 查看主机列表，若 &lt;MGMT&gt; 出现在列表中并且 IP 地址变为了 MGMT2_IP则证明安装成功。 服务启动测试 在页面上启动 Cloudera Management Service 服务，然后在本机执行下列语句查看是否服务运行在本机上。 1ps -elf | grep &quot;scm&quot; 关闭服务 在页面上关闭服务，然后输入如下命令关闭备机上的所有服务。 1systemctl stop cloudera-scm-agent 完成测试 在备机关闭完成后，需要返回至主机，开启主机上的 cloudera-scm-agent 服务 1systemctl start cloudera-scm-agent 故障检测配置 在 CMS1,CMS2,MGMT1,MGMT2 四台设备上都需要执行下面的命令： 安装软件 12wget http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/network:ha-clustering:Stable.repo -P /etc/yum.repos.d/yum install -y pacemaker corosync crmsh corosync 配置 如果存在 /etc/default/corosync 文件则需要修改如下配置项 1START=yes 关闭corosync开机启动 1systemctl disable corosync CMS1 和 CMS 2 编辑 corosync 配置文件 1vim /etc/corosync/corosync.conf 12345678910111213141516171819202122totem &#123;version: 2secauth: offcluster_name: cmftransport: udpu&#125;nodelist &#123; node &#123; ring0_addr: &lt;CMS1&gt; nodeid: 1 &#125; node &#123; ring0_addr: &lt;CMS2&gt; nodeid: 2 &#125;&#125;quorum &#123;provider: corosync_votequorumtwo_node: 1&#125; 创建检测脚本 1vim /etc/rc.d/init.d/cloudera-scm-server 填入如下内容： 12345678910111213141516171819#!/bin/bashcase &quot;$1&quot; in start) systemctl start cloudera-scm-server.service ;; stop) systemctl stop cloudera-scm-server.service ;; status) systemctl status -l cloudera-scm-server.service ;; restart) systemctl restart cloudera-scm-server.service ;; *) echo &#x27;$1 = start|stop|status|restart&#x27; ;;esac 修改脚本权限 1chmod 755 /etc/rc.d/init.d/cloudera-scm-server 启动服务 12systemctl start corosyncsystemctl enab corosync 关闭 cloudera-scm-server 服务自启动和服务 12systemctl disable cloudera-scm-serversystemctl stop cloudera-scm-server 启动 pacemaker 12systemctl start pacemakersystemctl enable pacemaker 检查节点状态 1crm status 若 CMS1 和 CMS2 都处在节点列表中则证明配置正确。 配置 pacemaker 123crm configure property no-quorum-policy=ignorecrm configure property stonith-enabled=falsecrm configure rsc_defaults resource-stickiness=100 将 cloudera-scm-server 进行托管 1crm configure primitive cloudera-scm-server lsb:cloudera-scm-server 注：此处在第二台设备中运行此命令时提示任务已经存在。 启动服务 1crm resource start cloudera-scm-server 检测服务状态 1crm status 然后访问 http://&lt;CMS&gt;:7180 检查服务运行状态。 测试服务迁移 1crm resource move cloudera-scm-server &lt;CMS2&gt; 检测服务状态 1crm status 然后访问 http://&lt;CMS&gt;:7180 检查服务运行状态。 注：在迁移完成后建议到备机上检测服务运行状态，确保服务运行正常。 MGMT1 和 MGMT2 编辑 corosync 配置文件 1vim /etc/corosync/corosync.conf 12345678910111213141516171819202122totem &#123;version: 2secauth: offcluster_name: mgmttransport: udpu&#125;nodelist &#123; node &#123; ring0_addr: &lt;MGMT1&gt; nodeid: 1 &#125; node &#123; ring0_addr: &lt;MGMT2&gt; nodeid: 2 &#125;&#125;quorum &#123;provider: corosync_votequorumtwo_node: 1&#125; 创建检测脚本 1vim /etc/rc.d/init.d/cloudera-scm-agent 填入如下内容： 12345678910111213141516171819#!/bin/bashcase &quot;$1&quot; in start) systemctl start cloudera-scm-agent.service ;; stop) systemctl stop cloudera-scm-agent.service ;; status) systemctl status -l cloudera-scm-agent.service ;; restart) systemctl restart cloudera-scm-agent.service ;; *) echo &#x27;$1 = start|stop|status|restart&#x27; ;;esac 修改脚本权限 1chmod 755 /etc/rc.d/init.d/cloudera-scm-agent 启动服务 12systemctl start corosyncsystemctl enable corosync 关闭 cloudera-scm-agent 服务自启动和服务 12systemctl disable cloudera-scm-agentsystemctl stop cloudera-scm-agent 启动 pacemaker 12systemctl start pacemakersystemctl enable pacemaker 检查节点状态 1crm status 若 MGMT1 和 MGMT2 都处在节点列表中则证明配置正确。 配置 pacemaker 123crm configure property no-quorum-policy=ignorecrm configure property stonith-enabled=falsecrm configure rsc_defaults resource-stickiness=100 创建 OCF( Open Cluster Framework) 1mkdir -p /usr/lib/ocf/resource.d/cm 编辑配置文件 1vim /usr/lib/ocf/resource.d/cm/agent 填入如下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#!/bin/sh######################################################################## CM Agent OCF script############################################################################################################################################### Initialization:: $&#123;__OCF_ACTION=$1&#125;OCF_SUCCESS=0OCF_ERROR=1OCF_STOPPED=7#######################################################################meta_data() &#123; cat &lt;&lt;END&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE resource-agent SYSTEM &quot;ra-api-1.dtd&quot;&gt;&lt;resource-agent name=&quot;Cloudera Manager Agent&quot; version=&quot;1.0&quot;&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;longdesc lang=&quot;en&quot;&gt; This OCF agent handles simple monitoring, start, stop of the Cloudera Manager Agent, intended for use with Pacemaker/corosync for failover.&lt;/longdesc&gt;&lt;shortdesc lang=&quot;en&quot;&gt;Cloudera Manager Agent OCF script&lt;/shortdesc&gt;&lt;parameters /&gt;&lt;actions&gt;&lt;action name=&quot;start&quot; timeout=&quot;20&quot; /&gt;&lt;action name=&quot;stop&quot; timeout=&quot;20&quot; /&gt;&lt;action name=&quot;monitor&quot; timeout=&quot;20&quot; interval=&quot;10&quot; depth=&quot;0&quot;/&gt;&lt;action name=&quot;meta-data&quot; timeout=&quot;5&quot; /&gt;&lt;/actions&gt;&lt;/resource-agent&gt;END&#125;#######################################################################agent_usage() &#123;cat &lt;&lt;END usage: $0 &#123;start|stop|monitor|meta-data&#125; Cloudera Manager Agent HA OCF script - used for managing Cloudera Manager Agent and managed processes lifecycle for use with Pacemaker.END&#125;agent_start() &#123; systemctl start cloudera-scm-agent if [ $? = 0 ]; then return $OCF_SUCCESS fi return $OCF_ERROR&#125;agent_stop() &#123; systemctl stop cloudera-scm-agent if [ $? = 0 ]; then return $OCF_SUCCESS fi return $OCF_ERROR&#125;agent_monitor() &#123; # Monitor _MUST!_ differentiate correctly between running # (SUCCESS), failed (ERROR) or _cleanly_ stopped (NOT RUNNING). # That is THREE states, not just yes/no. systemctl status cloudera-scm-agent if [ $? = 0 ]; then return $OCF_SUCCESS fi return $OCF_STOPPED&#125;case $__OCF_ACTION inmeta-data) meta_data exit $OCF_SUCCESS ;;start) agent_start;;stop) agent_stop;;monitor) agent_monitor;;usage|help) agent_usage exit $OCF_SUCCESS ;;*) agent_usage exit $OCF_ERR_UNIMPLEMENTED ;;esacrc=$?exit $rc 修改配置权限 1chmod 770 /usr/lib/ocf/resource.d/cm/agent 将 cloudera-scm-server 进行托管 1crm configure primitive cloudera-scm-agent ocf:cm:agent 开启服务 1crm resource start cloudera-scm-agent 测试服务偏移 1crm resource move cloudera-scm-agent &lt;MGMT2&gt; 常见问题 crm 权限不足 如果出现这样的问题可以再次修改 crm 所需文件的权限，然后使用如下命令： 1crm resource cleanup &lt;service&gt; 然后重新管理服务即可 1crm resource &lt;option&gt; &lt;service&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"}]},{"title":"Hue 的配置","slug":"bigdata/hue","date":"2020-12-03T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/hue/","permalink":"https://wangqian0306.github.io/2020/hue/","excerpt":"","text":"Hue 的配置 常见问题 Hue 无法访问 HBase Hue 访问 HBase 异常 TSocket read 0 bytes 此问题需要修改 HBase 的配置项 进入 HBase 配置页 筛选 HBase Thrift Server 服务 关闭 hbase.regionserver.thrift.compact 和 hbase.regionserver.thrift.framed 搜索 xml 新增如下配置项 1234&lt;property&gt; &lt;name&gt;hbase.table.sanity.checks&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 重新启动服务","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Hue","slug":"Hue","permalink":"https://wangqian0306.github.io/tags/Hue/"}]},{"title":"Kerberos HA 搭建流程整理","slug":"bigdata/kerbeors","date":"2020-12-02T15:25:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/kerberos/","permalink":"https://wangqian0306.github.io/2020/kerberos/","excerpt":"","text":"Kerberos HA 搭建流程整理 简介 Kerberos 是一款身份认证软件。本次 HA 搭建采用了crontab+kprop的方式实现，具体内容参见 官方文档 。 主机安装 安装软件(主机)： 1yum install -y krb5-server krb5-libs krb5-workstation 连接配置 1vim /etc/krb5.conf 12345678910111213141516171819202122232425262728293031323334includedir /etc/krb5.conf.d/[logging]default = FILE:/var/log/krb5libs.logkdc = FILE:/var/log/krb5kdc.logadmin_server = FILE:/var/log/kadmind.log[libdefaults]default_realm = &lt;realm&gt;dns_lookup_kdc = falsedns_lookup_realm = falseticket_lifetime = 86400renew_lifetime = 604800forwardable = truedefault_tgs_enctypes = des3-hmac-sha1 aes256-ctsdefault_tkt_enctypes = des3-hmac-sha1 aes256-ctspermitted_enctypes = des3-hmac-sha1 aes256-ctsudp_preference_limit = 1kdc_timeout = 3000rdns = falsepkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crtdefault_ccache_name = KEYRING:persistent:%&#123;uid&#125;[realms]&lt;realm&gt; = &#123;kdc = &lt;kdc_host1&gt;admin_server = &lt;kdc_host1&gt;kdc = &lt;kdc_host2&gt;# admin_server = &lt;kdc_host2&gt;&#125;[domain_realm].&lt;domain&gt; = &lt;realm&gt;&lt;domain&gt; = &lt;realm&gt; 加密配置 1vim /var/kerberos/krb5kdc/kdc.conf 1234567891011121314[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] &lt;realm&gt; = &#123; #master_key_type = aes256-cts max_life = 1d max_renewable_life= 7d 0h 0m 0s acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal &#125; 权限配置 1vim /var/kerberos/krb5kdc/kadm5.acl 1*/admin@&lt;realm&gt; * 创建数据库: 1kdb5_util create -s -r &lt;realm&gt; 启动服务 1234systemctl enable kadminsystemctl enable krb5kdcsystemctl start kadminsystemctl start krb5kdc 配置防火墙 1234firewall-cmd --permanent --add-service kerberosfirewall-cmd --permanent --add-service kadminfirewall-cmd --permanent --add-service kpropfirewall-cmd --reload 新建账号 1kadmin.local 12345addprinc cloudera-scm/adminaddprinc -randkey host/&lt;kdc_ip1&gt;addprinc -randkey host/&lt;kdc_ip2&gt;ktadd host/&lt;kdc_ip1&gt;ktadd host/&lt;kdc_ip2&gt; 备机安装 安装软件 1yum install -y krb5-server krb5-libs krb5-workstation 拷贝配置文件 12345scp &lt;user&gt;@&lt;kdc_host1&gt;:/etc/krb5.conf /etc/krb5.confscp &lt;user&gt;@&lt;kdc_host1&gt;:/etc/krb5.keytab /etc/krb5.keytabscp &lt;user&gt;@&lt;kdc_host1&gt;:/var/kerberos/krb5kdc/kadm5.acl /var/kerberos/krb5kdc/kadm5.aclscp &lt;user&gt;@&lt;kdc_host1&gt;:/var/kerberos/krb5kdc/kdc.conf /var/kerberos/krb5kdc/kdc.confscp &lt;user&gt;@&lt;kdc_host1&gt;:/var/kerberos/krb5kdc/.k5.&lt;realm&gt; /var/kerberos/krb5kdc/.k5.&lt;realm&gt; 编辑同步文件 1vim /var/kerberos/krb5kdc/kpropd.acl 12host/&lt;kdc_host1&gt;@&lt;realm&gt;host/&lt;kdc_host2&gt;@&lt;realm&gt; 配置防火墙 1234firewall-cmd --permanent --add-service kerberosfirewall-cmd --permanent --add-service kadminfirewall-cmd --permanent --add-service kpropfirewall-cmd --reload 启动同步服务 1systemctl start kprop --now 手动同步数据库 主机 备份数据库 1kdb5_util dump /var/kerberos/krb5kdc/master.dump 同步数据至备机 1kprop -f /var/kerberos/krb5kdc/master.dump &lt;kdc_host2&gt; 注：若出现 SUCCESS 字样则代表传输成功。 备机 关闭 kprop 服务 1systemctl stop kprop 移除 kprop 同步配置 1mv /var/kerberos/krb5kdc/kpropd.acl /var/kerberos/krb5kdc/kpropd.acl.bk 修改备机配置 1vim /etc/krb5.conf 12345678910111213141516171819202122232425262728293031323334includedir /etc/krb5.conf.d/[logging]default = FILE:/var/log/krb5libs.logkdc = FILE:/var/log/krb5kdc.logadmin_server = FILE:/var/log/kadmind.log[libdefaults]default_realm = &lt;realm&gt;dns_lookup_kdc = falsedns_lookup_realm = falseticket_lifetime = 86400renew_lifetime = 604800forwardable = truedefault_tgs_enctypes = des3-hmac-sha1 aes256-ctsdefault_tkt_enctypes = des3-hmac-sha1 aes256-ctspermitted_enctypes = des3-hmac-sha1 aes256-ctsudp_preference_limit = 1kdc_timeout = 3000rdns = falsepkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crtdefault_ccache_name = KEYRING:persistent:%&#123;uid&#125;[realms]&lt;realm&gt; = &#123;kdc = &lt;kdc_host1&gt;admin_server = &lt;kdc_host1&gt;kdc = &lt;kdc_host2&gt;# admin_server = &lt;kdc_host2&gt;&#125;[domain_realm].&lt;domain&gt; = &lt;realm&gt;&lt;domain&gt; = &lt;realm&gt; 启动备机服务 12systemctl start krb5kdcsystemctl start kadmin 测试数据库同步情况 1kadmin.local 1listprins 注: 若用户目录对应则证明此处同步正常。 配置服务开机启动 12systemctl enable krb5kdc --nowsystemctl stop kadmin 修改连接配置 1vim /etc/krb5.conf 12345678910111213141516171819202122232425262728293031323334includedir /etc/krb5.conf.d/[logging]default = FILE:/var/log/krb5libs.logkdc = FILE:/var/log/krb5kdc.logadmin_server = FILE:/var/log/kadmind.log[libdefaults]default_realm = &lt;realm&gt;dns_lookup_kdc = falsedns_lookup_realm = falseticket_lifetime = 86400renew_lifetime = 604800forwardable = truedefault_tgs_enctypes = des3-hmac-sha1 aes256-ctsdefault_tkt_enctypes = des3-hmac-sha1 aes256-ctspermitted_enctypes = des3-hmac-sha1 aes256-ctsudp_preference_limit = 1kdc_timeout = 3000rdns = falsepkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crtdefault_ccache_name = KEYRING:persistent:%&#123;uid&#125;[realms]&lt;realm&gt; = &#123;kdc = &lt;kdc_host1&gt;admin_server = &lt;kdc_host1&gt;kdc = &lt;kdc_host2&gt;# admin_server = &lt;kdc_host2&gt;&#125;[domain_realm].&lt;domain&gt; = &lt;realm&gt;&lt;domain&gt; = &lt;realm&gt; 修改远程同步配置 1mv /var/kerberos/krb5kdc/kpropd.acl.bk /var/kerberos/krb5kdc/kpropd.acl 启动同步服务 1systemctl start kprop 注：此处的备机如果拥有全部数据库的话可以将 kpropd.acl 文件移除，暂时做为主机使用。 配置自动同步 在主机上编写同步脚本 1vim /var/kerberos/krb5kdc/kprop_sync.sh 1234567#!/bin/bashDUMP=/var/kerberos/krb5kdc/master.dumpSLAVE=&quot;&lt;kdc_host2&gt;&quot;TIMESTAMP=`date`echo &quot;Start at $TIMESTAMP&quot;kdb5_util dump $DUMPkprop -f $DUMP -d $SLAVE 测试脚本运行情况 12chmod 700 /var/kerberos/krb5kdc/kprop_sync.shbash /var/kerberos/krb5kdc/kprop_sync.sh 配置定时任务 1crontab -e 10 */30 * * * root /var/kerberos/krb5kdc/kprop_sync.sh &gt; /var/kerberos/krb5kdc/lastupdate 开启定时任务 1systemctl enable crond --now 测试 关闭主机 kdc 服务 1systemctl stop krb5kdc 登录测试（账号密码） 1kinit cloudera-scm/admin@&lt;realm&gt; 查看目前的票据 1klist 销毁票据 1kdestroy 生成登录秘钥 1kadmin.local 1xst -norandkey -k admin.keytab cloudera-scm/admin@&lt;realm&gt; 检查登录密钥： 12345678[xxxx@xxxx xxxx]# ktutilktutil: read_kt admin.keytabktutil: listslot KVNO Principal---- ---- --------------------------------------------------------------------- 1 2 xxxx/xxxxx@&lt;realm&gt; ....ktutil: quit 使用秘钥登录 1kinit -kt admin.keytab cloudera-scm/admin@&lt;realm&gt; 客户端环境安装 软件安装 1yum install -y krb5-workstation krb5-libs 拷贝配置 1scp &lt;user&gt;@&lt;ip&gt;:/etc/krb5.conf /etc/krb5.conf Windows 平台的注意事项 Windows Server 和 Windows 专业版系统上可以使用 Windows 自己的 Kerberos (AD) Windows 平台的 Kerberos 与 MIT Kerberos (Linux) 并不是完全兼容的，如果有在 Windows 平台上开发应用程序的需求请选用 Windows AD。 MIT Kerberos 提供了 Windows 版的客户端，支持浏览器调取 Kerberos 认证。 Firefox 访问 Kerberos 认证服务 在访问开启 Kerberos 鉴权情况下的 WebHDFS 界面的时候需要如下配置浏览器： 在地址栏输入 about:config 进入配置模式 查找如下配置项目 1network.negotiate-auth.trusted-uris 将目标服务器加入此列表中 注：如果你使用的是 Windows 则还要把 network.auth.use-sspi 配置为 false 重启浏览器 常见问题 Message stream modified (41) 此问题需要注释 renew_lifetime 配置项 Hue 更新令牌 如果遇到 Hue 令牌相关的更新问题可以使用如下命令进行配置： 1kadmin.local 12modprinc -maxrenewlife 90day krbtgt/&lt;relam&gt;modprinc -maxrenewlife 90day +allow_renewable hue/&lt;host&gt;@&lt;relam&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kerberos","slug":"Kerberos","permalink":"https://wangqian0306.github.io/tags/Kerberos/"}]},{"title":"BIND 服务安装及配置流程","slug":"linux/bind","date":"2020-12-01T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/bind/","permalink":"https://wangqian0306.github.io/2020/bind/","excerpt":"","text":"BIND 服务安装及配置流程 简介 BIND 是 DNS 服务软件，在本文中会有 bind1,bind2 两台设备完成此次安装。 主机搭建 配置 Hostname 1hostnamectl set-hostname &lt;bind1_host&gt;.&lt;domain&gt; 安装软件： 1yum -y install bind bind-utils 配置服务 12mv /etc/named.conf /etc/named.conf.bkvim /etc/named.conf 填入如下配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051options &#123; listen-on port 53 &#123; any; &#125;; listen-on-v6 &#123; any; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query &#123; any; &#125;; allow-transfer &#123; localhost; &lt;bind-2_ip&gt;; &#125;; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file &quot;/etc/named.root.key&quot;; managed-keys-directory &quot;/var/named/dynamic&quot;; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;&#125;;logging &#123; channel default_debug &#123; file &quot;data/named.run&quot;; severity dynamic; &#125;;&#125;;zone &quot;.&quot; IN &#123; type hint; file &quot;named.ca&quot;;&#125;;include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;;zone &quot;&lt;domain&gt;&quot; IN &#123; type master; file &quot;&lt;domain&gt;.lan&quot;; allow-update &#123; none; &#125;;&#125;;zone &quot;&lt;reverse_ip_area&gt;.in-addr.arpa&quot; IN &#123; type master; file &quot;&lt;reverse_ip_area&gt;.db&quot;; allow-update &#123; none; &#125;;&#125;; 注：reverse_ip_area 为反向书写的IP地址(去掉最后一位)，例如： 1.168.192 配置正向解析 1vim /var/named/&lt;domain&gt;.lan 123456789101112131415161718$TTL 86400@ IN SOA dlp.&lt;domain&gt;. &lt;bind1_host&gt;.&lt;domain&gt;. ( 2019100301 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL) IN NS dlp.&lt;domain&gt;. IN A &lt;bind1_ip&gt; IN MX 10 dlp.&lt;domain&gt;.dlp IN A &lt;bind1_ip&gt;&lt;bind1_host&gt; IN A &lt;bind1_ip&gt;&lt;bind2_host&gt; IN A &lt;bind2_ip&gt; 配置反向解析 1vim /var/named/&lt;reverse_ip_area&gt;.db 12345678910111213$TTL 86400@ IN SOA dlp.&lt;domain&gt;. &lt;bind1_host&gt;.&lt;domain&gt;. ( 2019100301 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL) IN NS dlp.&lt;domain&gt;.&lt;bind1_ip&gt; IN PTR dlp.&lt;domain&gt;.&lt;bind1_ip&gt; IN PTR &lt;bind1_host&gt;.&lt;domain&gt;.&lt;bind2_ip&gt; IN PTR &lt;bind2_host&gt;.&lt;domain&gt;. 注：此处 IP 只填写最后一位 启动服务 123systemctl enable --now namedfirewall-cmd --add-service=dns --permanentfirewall-cmd --reload 从机搭建 从机搭建逻辑与主机相同，但是只需要配置 /etc/named.conf 文件即可 配置 Hostname 1hostnamectl set-hostname &lt;bind2_host&gt;.&lt;domain&gt; 安装软件： 1yum -y install bind bind-utils 配置服务 12mv /etc/named.conf /etc/named.conf.bkvim /etc/named.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455options &#123; listen-on port 53 &#123; any; &#125;; listen-on-v6 &#123; any; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query &#123; any; &#125;; allow-transfer &#123; localhost; &lt;bind-2_ip&gt;; &#125;; recursion yes; dnssec-enable yes; dnssec-validation yes; bindkeys-file &quot;/etc/named.root.key&quot;; managed-keys-directory &quot;/var/named/dynamic&quot;; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;&#125;;logging &#123; channel default_debug &#123; file &quot;data/named.run&quot;; severity dynamic; &#125;;&#125;;zone &quot;.&quot; IN &#123; type hint; file &quot;named.ca&quot;;&#125;;include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;;zone &quot;&lt;domain&gt;&quot; IN &#123; type slave; masters &#123; &lt;bind1_ip&gt;; &#125;; masterfile-format text; file &quot;slaves/&lt;domain&gt;.lan&quot;; notify no;&#125;;zone &quot;&lt;reverse_ip_area&gt;.in-addr.arpa&quot; IN &#123; type slave; masters &#123; &lt;bind1_ip&gt;; &#125;; masterfile-format text; file &quot;slaves/reverse_ip_area.db&quot;; notify no;&#125;; 启动服务 123systemctl enable --now namedfirewall-cmd --add-service=dns --permanentfirewall-cmd --reload 检查配置文件同步 1ls /var/named/slaves/ 若出现配置文件则证明解析正常。 使用配置好的 BIND 服务器 获取网卡信息 1nmcli connection show 修改网卡配置 12nmcli connection modify &lt;name&gt; ipv4.dns &lt;dns_server_ip&gt;nmcli connection down &lt;name&gt;; nmcli connection up &lt;name&gt; 动态配置 bind 服务适配了 RFC2136 规范，如果在 /etc/named.conf 文件中打开配置项，即可完成动态更新。 首先需要在外部 DNS 服务器上运行如下命令，生成密钥： 1tsig-keygen -a hmac-sha256 externaldns-key 应该得到这样的输出，将其放置在 /etc/named.conf 文件中，并将其保存成密钥文件 key.txt： 1234key &quot;externaldns&quot; &#123; algorithm hmac-sha256; secret &quot;&lt;secret&gt;&quot;;&#125;; 之后在需要更新的 zone 部分进行如下配置即可： 12345678910zone &quot;xxxx&quot; &#123; type master; file &quot;xxxx&quot;; allow-transfer &#123; key &quot;externaldns-key&quot;; &#125;; update-policy &#123; grant externaldns-key zonesub ANY; &#125;;&#125;; 测试命令如下： 1nsupdate -k key.txt 然后输入如下内容： 12345&gt; server &lt;server_ip&gt;&gt; zone &lt;domain&gt;&gt; update add &lt;host&gt; 86400 A &lt;ip&gt;&gt; send&gt; quit 注：若没有额外的错误输出则证明配置完成。 之后即可进行如下测试： 1ping &lt;host&gt; 注：若出现配置的 IP 则证明动态更新成功。 常见问题 Permission Denied 此处问题大多是由于 SELinux 权限限制的问题可以暂时关闭 SELinux 进行测试 1setenforce 0 注: 此处命令只能是单次生效如果需要完全关闭则还需要修改配置文件 1vim /etc/selinux/conf 然后修改配置项即可 SELINUX=disabled 部分域名无法解析 此问题可能是 Bind 在转发 DNS 时遇到了网络问题，可以按照如下逻辑修改配置项： 12345678options &#123; .... dnssec-enable no; dnssec-validation no; forward only; forwarders &#123; 114.114.114.114; &#125;; ....&#125;; 注：此处的 114.114.114.114 (电信公共 DNS) 仅仅是示例，可以根据网络条件进行填写。 参考资料 官方文档 RFC2136","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"DNS","slug":"DNS","permalink":"https://wangqian0306.github.io/tags/DNS/"},{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"Nexus 服务安装及配置流程","slug":"linux/nexus","date":"2020-12-01T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/nexus/","permalink":"https://wangqian0306.github.io/2020/nexus/","excerpt":"","text":"Nexus 服务安装及配置流程 简介 Neuxs 可以作为 Proxy 缓存外网上的软件包。 安装 注：此处采用 Docker 的方式运行 Nexus 软件源，需要 Docker 和 Docker-Compose 软件。 选定安装位置创建 nexus 文件夹 注：此处建议安装在 /opt 目录下 1mkdir /opt/nexus 在 nexus 文件夹中新增 docker-compose.yaml 文件 1vim /opt/nexus/docker-compose.yaml 1234567services: nexus: image: sonatype/nexus3 volumes: - ./nexus-data:/nexus-data ports: - 8081:8081 开启服务 12cd /opt/nexusdocker-compose up -d 查看默认密码 1docker-compose exec nexus cat /nexus-data/admin.password 登录界面 访问 http://&lt;ip&gt;:8081 并使用 admin 账户进行登录。 软件源配置 在 Nexus 中软件源分为以下三种： hosted (本地直接存储) proxy (远程文件代理) group (资源组) 其中 hosted 需要本地上传，proxy 需要指定远程目录地址，而 group 可以将这两种资源整合起来对外提供服务。 清理容器 在使用 Nexus 作为容器仓库的时候，容易产生数据的积压。默认的清除策略无法支持保存历史版本的容器历史文件。 解决方法如下： 自定义容器清理策略，指定按照名称(正则)进行清除 使用 nexus-cli 进行清理 注：nexus-cli 很久没有更新了，而且只支持到了 nexus2，故建议采用第一种方法。 参考资料 Nexus nexus-cli","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"SDKMAN","slug":"linux/sdk","date":"2020-12-01T13:57:04.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2020/sdk/","permalink":"https://wangqian0306.github.io/2020/sdk/","excerpt":"","text":"SDKMAN 简介 SDKMAN 是一款在大多数基于 Unix 的系统上管理多个并行版本软件开发工具包的软件。它提供了命令行工具和 API，用于安装、切换、删除和列出候选项。 安装方式 首先需要确保以下依赖已经正确安装： unzip zip curl sed 然后可以使用如下命令安装： 1curl -s &quot;https://get.sdkman.io&quot; | bash 如果需要采用不同的 sdk 可以使用如下命令： 查看 java 版本： 1sdk list java 安装 java： 1sdk install java 22.3.r17-nik 使用此版本 java： 1sdk default java 22.3.r17-nik 参考资料 官方文档","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"OLAP 引擎对比","slug":"bigdata/OLAP","date":"2020-07-21T13:06:58.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/olap/","permalink":"https://wangqian0306.github.io/2020/olap/","excerpt":"","text":"简述 数据处理大致可以分成两大类： 联机事务处理OLTP(on-line transaction processing) 联机分析处理OLAP(On-Line Analytical Processing) 通常来说 以支持业务处理为主要目的是OLTP型 以支持决策管理分析为主要目的是OLAP型 而 OLAP 基于实现方式细分可以分为以下三类： 关系OLAP-ROLAP(RelationalOLAP) 多维OLAP-MOLAP(MultidimensionalOLAP) 混合OLAP-HOLAP(HybridOLAP) 其中： ROLAP 依赖操作 DB 数据，通过 SQL 的 WHERE 条件实现传统的切片切块功能 MOLAP 则是在开始的时候就将数据存在了多位数据集中 HOLAP 则希望将二者结合起来获取更快的性能 样例产品如下： ROLAP 有：Presto 和 Impala MOLAP 有：Kylin 和 Druid Kylin 简介 Apache Kylin 是一个开源的、分布式的分析型数据仓库，提供Hadoop/Spark 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据， 最初由 eBay 开发并贡献至开源社区。 它能在亚秒内查询巨大的表。 从存储结构上来说 Kylin 采用 Cube 的方式来存储数据，并将数据按照如下方式构建模型： 星型模型(star schema) 雪花模型(snowflake schema) 在 Cube 创建之后 Kylin 会将数据按照指定的模型进行聚合并将数据存储至 HBase 中。 数据源 Kylin 目前可以使用如下方式接入数据源： Hive Kafka JDBC 然后使用如下的方式来构建 Cube Hadoop MapReduce Spark Flink 特点 可以便捷的使用 SQL 查询数据 与分析工具结合很方便，例如 Tableau 和 Power BI 需要进行预计算 Druid Apache Druid 是一个开源的分布式数据存储组件。 Druid 的核心设计结合了数据仓库，时间序列数据库和搜索系统的思想，从而创建了一个统一的系统，可对各种用例进行实时分析。 Druid 将这三个系统中的每个系统的关键特征合并到其接收层，存储格式，查询层和核心体系结构中。 Druid 采用了列式存储的方式，根据列的类型（字符串，数字等），将应用不同的压缩和编码方法。 Druid 还会根据列类型构建不同类型的索引。 与搜索系统类似，Druid 为字符串列构建反向索引，以进行快速搜索和过滤。 与时间序列数据库类似，Druid 按时间对数据进行智能分区，以实现快速的面向时间的查询。 数据源 Druid 可以通过以下两种方式完成数据接入 流式数据接入 Kafka Amazon Kinesis 批量数据接入 本地批处理 Hadoop 集群数据接入 特点 便于查询具有时间成分的数据 不适合进行更新","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"OLAP","slug":"OLAP","permalink":"https://wangqian0306.github.io/tags/OLAP/"},{"name":"Druid","slug":"Druid","permalink":"https://wangqian0306.github.io/tags/Druid/"},{"name":"Kylin","slug":"Kylin","permalink":"https://wangqian0306.github.io/tags/Kylin/"}]},{"title":"状态模式","slug":"design_pattern/pattern-state","date":"2020-07-13T13:06:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-state/","permalink":"https://wangqian0306.github.io/2020/design-pattern-state/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940abstract class State &#123; void doWork() &#123; &#125;&#125;class Happy extends State &#123; @Override void doWork() &#123; System.out.println(&quot;happy&quot;); &#125;&#125;class Sad extends State &#123; @Override void doWork() &#123; System.out.println(&quot;sad&quot;); &#125;&#125;class Context &#123; private State state; public void setState(State state) &#123; this.state = state; &#125; public void work() &#123; state.doWork(); &#125;&#125;public class StatePatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(); context.setState(new Happy()); context.work(); context.setState(new Sad()); context.work(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"数据仓库概念梳理","slug":"theory/data_warehouse","date":"2020-07-12T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2020/data_warehouse/","permalink":"https://wangqian0306.github.io/2020/data_warehouse/","excerpt":"","text":"数据仓库概念梳理 简介 注：此处内容来源于维基百科。 在数据处理领域中，数据仓库(DW 或 DWH)，也被称为企业数据仓库(EDW)。 它是一种用于业务报告和数据分析的系统，通常被认为是商业智能(BI)的核心组件。 数据仓库是一个集成了来自于一个或者多个不同来源的系统数据的中央存储库。 它将当前和历史数据存储在一个位置，用于为整个企业的员工创建分析报告。 存储在仓库中的数据是从运营系统(如营销或者销售)上传的。 数据可能会通过 ODS 与数据清理等操作来确保数据质量，然后再用于最终的分析报告。 ETL 和 ELT 是两种建设数据仓库的主要方式。 注：提取(Extract)、转换(Transform)、加载(Load) 建设数仓的好处 数据仓库维护来自源系统的信息副本。这种架构提供了以下的好处： 将来自多个来源的数据集成到单个数据库和数据模型中。因为我们将很多的数据聚合到单个数据库中，所以可以使用单个查询引擎在 ODS 中呈现数据。 缓解由于在 OLTP 数据库中运行大型、长时间运行的分析查询而导致的数据库隔离级别锁争用问题。 维护数据历史，即使源系统没有。 集成来自多个源系统的数据，实现整个企业的中央视图。 通过提供一致的代码和描述、标记甚至修复错误数据来提高数据质量。 一致地呈现组织信息。 不管数据的来源如何，统一为所有相关的数据提供单一通用的数据模型。 重构数据模型，使其对业务有实际意义。 重构数据模型，使其即使对于复杂的分析查询也能提供出色的查询性能，而不会影响原有系统。 为运营业务应用程序增加价值。 使决策支持查询更容易编写。 组织和消除重复数据的歧义。 数据集市 数据集市是在数据仓库环境中的一种数据结构或是访问模式，它经常被用于检索面向客户的数据。 数据集市通常是数据仓库面向特定的业务线或团队的一个子集。 数据仓库通常管理企业范围，而数据集市中的信息则属于单个部门。 在某些情况中，每个部门或业务单位都被视为其数据集市的所有者，包括所有硬件、软件和数据。 这使每个部门能够隔离其数据的使用、操作和开发。 在使用一致维度的其他部署中，此业务单元所有权不适用于客户、产品等共享维度。 建立仓库和数据集市是因为数据库中的信息没有按照易于访问的方式进行组织。 这种组织方式过于复杂、以至于难以访问或进行资源密集型的查询。 虽然 OLTP 旨在更新数据，但数据仓库或集市是只读的。 数据仓库旨在访问大量相关记录。 数据集市通过以支持一组用户的集体视图的方式提供数据，允许用户访问他们最常查看的特定类型的数据，从而缩短了最终用户的响应时间。 数据集市基本上是数据仓库的浓缩和更集中的版本，它反映了组织内每个业务部门的法规和流程规范。 每个数据集市专用于特定的业务功能或区域。 该数据子集可能跨越企业的许多或所有功能主题领域。 通常使用多个数据集市来满足各个业务部门的需求。 通用内容 数据仓库和集市的通用环境包括以下内容： 向仓库或集市提供数据的源系统 数据集成技术和工作流 在面对不同用户时提供不同的存储架构 为各种用户提供不同的工具和应用程序 充分的元数据、完整的数据质量和完善的治理流程， 特性 数据仓库定义中数据的基本特性包括 面向主题(特定的数据分析领域与目标) 数据集成(数据仓库一般都要涉及多个分析主题) 反应历史变化(数据仓库中的数据反映历史变化) 稳定(数据仓库中的数据是稳定的) 数仓分层模型 在阿里巴巴的数据体系中数据仓库被分成了三层，自上而下为： 数据应用层(ADS，Application Data Service) 个性化指标加工：定制化、复杂性指标(复合指标) 基于应用的数据组装：宽表集市、趋势指标 数据公共层(CDM，Common Data Model) 纬度表(DIM，Dimension)：建立一致数据分析维表、降低数据计算口径和算法不统一风险 公共汇总层(DWS，Data WareHouse Service)：构建命名规范、口径一致的统计指标，为上层提供公共指标，建立汇总宽表 明细事实表(DWD，Data Warehouse Detail)：基于维表建模，明细宽表，服用关联计算，减少数据扫描 数据引入层(ODS，Operation Data Store) 同步：结构化数据增量或全量同步 结构化：非结构化数据进行结构化处理 保存历史、清洗：根据业务、审计、稽查的需求保留历史数据或进行清洗 在阿里的样例中数据流向如下图所示： 此外在其他文章中还有这样的数据流向： 建模方法论 目前业内有如下建模方法，大致可以进行如下的区分： 具体的构建关键点为： 需求 模型 数据 注：根据不同的方法论可以决定起点和方向。 具体特性如下： 特性 Kimball InMon 时间 快速交付 长 开发难度 小 大 维护难度 大 小 技能要求 入门级 专家级 数据要求 特定业务 企业级","categories":[{"name":"大数据概念","slug":"大数据概念","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://wangqian0306.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"Data warehouse","slug":"Data-warehouse","permalink":"https://wangqian0306.github.io/tags/Data-warehouse/"},{"name":"Data Mart","slug":"Data-Mart","permalink":"https://wangqian0306.github.io/tags/Data-Mart/"},{"name":"数据集市","slug":"数据集市","permalink":"https://wangqian0306.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82/"}]},{"title":"Spring Boot 缓存","slug":"spring/cache","date":"2020-07-12T13:32:58.000Z","updated":"2025-01-20T06:56:22.175Z","comments":true,"path":"2020/caching/","permalink":"https://wangqian0306.github.io/2020/caching/","excerpt":"","text":"简介 Spring Boot 可以使用 Spring Framework 提供的功能组件来实现缓存的功能。 支持的存储库 可以使用如下所示的缓存 Generic JCache (JSR-107) (EhCache 3, Hazelcast, Infinispan, and others) EhCache 2.x Hazelcast Infinispan Couchbase Redis Caffeine Cache2k Simple 详情请参阅官方文档 注：后续将使用 Redis 作为缓存库进行说明。 Redis 环境依赖 使用 Redis 缓存需要新增下面的依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 链接配置 由于使用了 Redis 作为存储组件，所以需要配置 Redis 的链接。 详细内容请参照官方文档 单机模式 简单使用和测试的话可以使用单机模式进行配置，仅需要在配置文件中写入如下内容即可: 123456spring: data: redis: host: &lt;host&gt; port: 6379 database: 0 主从 + 哨兵模式 主从加哨兵模式可以使用如下的配置项: 1234567spring: data: redis: sentinel: master: mymaster nodes: 192.168.1.1:26379,192.168.1.2:26379,192.168.1.3:26379 password: &lt;password&gt; 集群模式 集群模式可以使用如下的配置项: 12345spring: data: redis: cluster: nodes: 192.168.1.1:16379,192.168.1.2:16379,192.168.1.3:16379 Caffeine 简介 Caffeine 是一款本地缓存的框架，详细技术栈参照如下文档： Design Of A Modern Cache Design Of A Modern Cache—Part Deux 环境依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 1234&lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;caffeine&lt;/artifactId&gt;&lt;/dependency&gt; 配置项 12345spring: cache: cache-names: &quot;cache1,cache2&quot; caffeine: spec: &quot;maximumSize=500,expireAfterAccess=600s&quot; 相关注解及说明 注解 说明 @EnableCaching 启用缓存功能 @Cacheable 缓存方法的标识 @CachePut 强制更新缓存 @CacheEvict 强制删除缓存 @Caching 自定义缓存功能，做功能拼接 @CacheConfig 缓存配置项 详情请参阅官方文档 简单试用 编写如下 application.yaml 配置文件： 12345678910spring: application: name: cache-test data: redis: host: 192.168.2.77 port: 6379 cache: redis: time-to-live: 23h 编写 TestService.java ： 1234567891011121314151617181920212223import org.springframework.cache.annotation.CacheEvict;import org.springframework.cache.annotation.Cacheable;import org.springframework.stereotype.Service;import java.time.LocalDateTime;@Servicepublic class TestService &#123; @Cacheable(cacheNames = &#123;&quot;echo&quot;&#125;) public String echo(Integer id) &#123; try &#123; Thread.sleep(4000); &#125; catch (Exception ignore) &#123; &#125; return LocalDateTime.now() + &quot; &quot; + id; &#125; @CacheEvict(cacheNames = &#123;&quot;echo&quot;&#125;) public void evict(Integer id) &#123; &#125;&#125; 编写 TestController.java : 123456789101112131415161718192021import jakarta.annotation.Resource;import org.springframework.web.bind.annotation.*;@RestController@RequestMapping(&quot;/test&quot;)public class TestController &#123; @Resource private TestService testService; @GetMapping(&quot;/cache/&#123;id&#125;&quot;) public String cache(@PathVariable int id) &#123; return testService.echo(id); &#125; @DeleteMapping(&quot;/cache/&#123;id&#125;&quot;) public void delete(@PathVariable int id) &#123; testService.evict(id); &#125;&#125; 编写测试文件 test.http ： 12345678### set and read cacheGET http://localhost:8080/test/cache/1### delete cacheDELETE http://localhost:8080/test/cache/1### test read with out cacheGET http://localhost:8080/test/cache/2 之后运行服务，然后尝试调用测试文件即可。 缓存的常见配置 FastJson 序列化 在使用 Redis 的时候需要将缓存数据进行传输和下载，所以需要将对象进行序列化，可以通过如下代码实现： 123456789101112131415161718192021222324252627import com.alibaba.fastjson2.support.spring6.data.redis.GenericFastJsonRedisSerializer;import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheConfiguration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.serializer.RedisSerializationContext.SerializationPair;import org.springframework.data.redis.serializer.StringRedisSerializer;@Configuration@EnableCachingpublic class CacheConfig &#123; @Bean public CacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) &#123; GenericFastJsonRedisSerializer fastJsonRedisSerializer = new GenericFastJsonRedisSerializer(); RedisCacheConfiguration defaultCacheConfig = RedisCacheConfiguration.defaultCacheConfig() .serializeKeysWith(SerializationPair.fromSerializer(new StringRedisSerializer())) .serializeValuesWith(SerializationPair.fromSerializer(fastJsonRedisSerializer)); return RedisCacheManager.builder(redisConnectionFactory) .cacheDefaults(defaultCacheConfig) .build(); &#125;&#125; 定义不同的过期时间 123456789101112131415161718192021222324252627282930import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheConfiguration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import java.time.Duration;import java.util.HashMap;import java.util.Map;@Configuration@EnableCachingpublic class CacheConfig &#123; @Bean public CacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) &#123; RedisCacheConfiguration defaultCacheConfig = RedisCacheConfiguration.defaultCacheConfig(); Map&lt;String, RedisCacheConfiguration&gt; cacheConfigurations = new HashMap&lt;&gt;(); cacheConfigurations.put(&quot;ocean&quot;, defaultCacheConfig.entryTtl(Duration.ofDays(1))); cacheConfigurations.put(&quot;weather&quot;, defaultCacheConfig.entryTtl(Duration.ofHours(2))); return RedisCacheManager.builder(redisConnectionFactory) .cacheDefaults(defaultCacheConfig) .withInitialCacheConfigurations(cacheConfigurations) .build(); &#125;&#125; 利用浏览器的缓存机制 在响应结果中加入 LastModified 头可以让浏览器缓存响应结果，节省开销： 1234567891011121314151617181920212223import org.springframework.cache.annotation.Cacheable;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpStatusCode;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Service;import java.util.Calendar;@Servicepublic class TestService &#123; @Cacheable(cacheNames = &#123;&quot;test&quot;&#125;) public ResponseEntity&lt;String&gt; test() &#123; HttpHeaders headers = new HttpHeaders(); Calendar calendar = Calendar.getInstance(); calendar.set(Calendar.MINUTE, 0); calendar.set(Calendar.SECOND, 0); calendar.set(Calendar.MILLISECOND, 0); headers.setLastModified(calendar.getTime().toInstant()); return new ResponseEntity&lt;&gt;(&quot;test&quot;, headers, HttpStatusCode.valueOf(200)); &#125;&#125; 缓存常见问题如何解决 缓存雪崩 缓存雪崩的意思是大量缓存同时过期导致大量请求被发送到了数据库的问题。 注：此处内容尚且没有进行过验证，抽时间再补下吧。 此处可以使用自定义CacheManager(RedisCacheManager)的方式来进行实现。 缓存穿透 缓存击穿指的是 查询必然不存在的数据来直击数据库 通常来说此问题可以通过 @Cacheable 注解中的unless 配置项配合缓存空对象或者布轮过滤器的方式来进行补充和完善。 缓存空对象 可以直接使用下面的配置项开启空对象缓存。 1spring.cache.redis.cache-null-values=true 布隆过滤器 注：此处内容尚且没有进行过验证，抽时间再补下吧。 此处可以使用自定义CacheManager(RedisCacheManager)的方式来进行实现。 缓存击穿 缓存击穿，是指非常热点的数据在失效的瞬间，持续的大量请求就穿破缓存，直接请求数据库的情况。 解决此处的问题可以配置 @Cacheable 注解中的 sync 配置项为 True 来解决。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"}]},{"title":"spring-rest-docs","slug":"spring/spring-rest-docs","date":"2020-07-12T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2020/spring-rest-docs/","permalink":"https://wangqian0306.github.io/2020/spring-rest-docs/","excerpt":"","text":"Spring REST Docs 简介 Spring REST Docs 是一款接口文档生成工具。主要是通过将 AsciiDoctor 和 Spring MVC Test 自动生成的请求片段相互结合的方式编写文档。 Spring MVC Test 自动生成的请求片段内容如下： curl-request http-request http-response httpie-request request-body response-body 注：此工具生成的文档还是有些简陋，建议酌情选用。 使用方式 首先需要引入相关依赖包： Maven 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.restdocs&lt;/groupId&gt; &lt;artifactId&gt;spring-restdocs-mockmvc&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; Gradle 123dependencies &#123; testImplementation &quot;org.springframework.restdocs:spring-restdocs-mockmvc&quot;&#125; 然后按需编写测试类即可，样例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.restdocs.RestDocumentationContextProvider;import org.springframework.restdocs.RestDocumentationExtension;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit.jupiter.SpringExtension;import org.springframework.test.context.web.WebAppConfiguration;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.setup.MockMvcBuilders;import org.springframework.web.context.WebApplicationContext;import static org.springframework.restdocs.mockmvc.MockMvcRestDocumentation.document;import static org.springframework.restdocs.mockmvc.MockMvcRestDocumentation.documentationConfiguration;import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;@WebAppConfiguration@ContextConfiguration(classes = DemoApplication.class)@ExtendWith(&#123;RestDocumentationExtension.class, SpringExtension.class&#125;)public class DemoTest &#123; @Autowired private WebApplicationContext context; private MockMvc mockMvc; @BeforeEach public void setUp(RestDocumentationContextProvider restDocumentation) &#123; this.mockMvc = MockMvcBuilders.webAppContextSetup(context) .apply(documentationConfiguration(restDocumentation)).build(); &#125; @Test public void test() throws Exception &#123; String url = &quot;/test&quot;; mockMvc.perform(get(url)).andExpect(status().is2xxSuccessful()).andDo(document(&quot;sample&quot;)); &#125;&#125; 注：在测试完成后需要指定 andDo(document(&quot;&lt;dir&gt;&quot;)) 方法，标明本测试生成的文件夹路径。在运行测试完成后会在 target/generated-snippets 文件夹内找到生成的 adoc格式文档。如有需求可以结合 asciidoctor-maven-plugin 生成完整文档。 参考资料 官方手册 官方例程","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Spring REST Docs","slug":"Spring-REST-Docs","permalink":"https://wangqian0306.github.io/tags/Spring-REST-Docs/"}]},{"title":"springdoc-openapi","slug":"spring/springdoc-openapi","date":"2020-07-12T13:32:58.000Z","updated":"2025-01-08T02:56:21.482Z","comments":true,"path":"2020/springdoc-openapi/","permalink":"https://wangqian0306.github.io/2020/springdoc-openapi/","excerpt":"","text":"springdoc-openapi 简介 springdoc-openapi 是一款类似于 springfox 的社区项目，可以通过注解生成文档，并且提供了 swagger-ui 方便调试。 使用方式 引入项目依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springdoc&lt;/groupId&gt; &lt;artifactId&gt;springdoc-openapi-starter-webmvc-ui&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 注: Spring Boot 3.0 修改了引入包。 配置 swagger 地址： 1springdoc.swagger-ui.path=/swagger-ui.html 然后在程序代码中加入如下注解及其参数即可： controller @Tag @Parameter @Operation @ApiResponse module @Schema 注：相关注解的详细说明参见 swagger 文档。 参考资料 官方网站 官方例程 注解说明文档 swagger 官网","categories":[{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"Swagger","slug":"Swagger","permalink":"https://wangqian0306.github.io/tags/Swagger/"}]},{"title":"代理模式","slug":"design_pattern/pattern-proxy","date":"2020-07-09T14:10:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-proxy/","permalink":"https://wangqian0306.github.io/2020/design-pattern-proxy/","excerpt":"","text":"静态代理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748interface Image &#123; void display();&#125;class RealImage implements Image &#123; private String fileName; public RealImage(String fileName)&#123; this.fileName = fileName; loadFromDisk(fileName); &#125; @Override public void display() &#123; System.out.println(&quot;Displaying &quot; + fileName); &#125; private void loadFromDisk(String fileName)&#123; System.out.println(&quot;Loading &quot; + fileName); &#125;&#125;class ProxyImage implements Image&#123; private RealImage realImage; private String fileName; public ProxyImage(String fileName)&#123; this.fileName = fileName; &#125; @Override public void display() &#123; if(realImage == null)&#123; realImage = new RealImage(fileName); &#125; realImage.display(); &#125;&#125;public class ProxyPatternDemo &#123; public static void main(String[] args) &#123; Image image = new ProxyImage(&quot;test_10mb.jpg&quot;); image.display(); &#125;&#125; JDK 动态代理 12345678910111213141516171819202122232425262728293031323334353637383940import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;interface Image &#123; void display();&#125;class RealImage implements Image &#123; @Override public void display() &#123; System.out.println(&quot;Displaying RealImage&quot;); &#125;&#125;class InvocationHandlerImpl implements InvocationHandler &#123; private final Object subject; public InvocationHandlerImpl(Object subject) &#123; this.subject = subject; &#125; @Override public Object invoke(Object o, Method method, Object[] objects) throws Throwable &#123; System.out.println(&quot;调用前&quot;); Object returnValue = method.invoke(subject, objects); System.out.println(&quot;调用后&quot;); return returnValue; &#125;&#125;public class ProxyPatternDemo &#123; public static void main(String[] args) &#123; InvocationHandlerImpl handlerImpl = new InvocationHandlerImpl(new RealImage()); Image image = (Image) Proxy.newProxyInstance(Image.class.getClassLoader(), new Class[]&#123;Image.class&#125;, handlerImpl); image.display(); &#125;&#125; CGLIB 动态代理 可以通过如下 maven 样例引入 cglib jar: 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 样例代码如下： 12345678910111213141516171819202122232425262728293031323334import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;class RealImage &#123; public void display() &#123; System.out.println(&quot;Displaying RealImage&quot;); &#125;&#125;class MethodInterceptorImpl implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;调用前&quot;); Object object = proxy.invokeSuper(obj, args); System.out.println(&quot;调用后&quot;); return object; &#125;&#125;public class ProxyPatternDemo &#123; public static void main(String[] args) &#123; Enhancer enhancer = new Enhancer(); enhancer.setClassLoader(RealImage.class.getClassLoader()); enhancer.setSuperclass(RealImage.class); enhancer.setCallback(new MethodInterceptorImpl()); RealImage image = (RealImage) enhancer.create(); image.display(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Elasticsearch 入门","slug":"database/elasticsearch","date":"2020-07-04T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/elasticsearch/","permalink":"https://wangqian0306.github.io/2020/elasticsearch/","excerpt":"","text":"Elasticsearch 入门 简介 Elasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎。 本地安装 在 CentOS 中可以使用如下命令配置软件源 12rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchvim /etc/yum.repos.d/elastic.repo 写入如下配置项即可 12345678[elasticsearch]name=Elasticsearch repository for 8.x packagesbaseurl=https://artifacts.elastic.co/packages/8.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=0autorefresh=1type=rpm-md 在写入完成后可以使用如下命令安装软件 1sudo yum install --enablerepo=elasticsearch elasticsearch 配置 软件配置在 /etc/elasticsearch 目录中。 默认日志在 /var/log/elasticsearch 目录中。 JDK 配置建议 Xms 和 Xms 设置成一样。 XMx 不要超过机器内存的 50 %，不要超过 30 GB。 启动服务 123sudo /bin/systemctl daemon-reloadsudo /bin/systemctl enable elasticsearch.servicesudo systemctl start elasticsearch.service 关闭服务 1sudo systemctl stop elasticsearch.service 容器化安装 编写如下 Docker Compose 文件 123456789101112services: es: image: docker.elastic.co/elasticsearch/elasticsearch:7.15.1 environment: - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 ports: - 9200:9200 - 9300:9300 使用如下命令启动运行 1docker-compose up -d 注：建议与 kibana 一起部署，详情参见 kibana 章节。 生产环境配置推荐 开启 X-Pack 用户管理机制 使用自签名 SSL 证书(https)保证集群内部沟通加密 设置 3 个主节点做故障冗余 日志类的应用，单个分片不要大于 50 GB 搜索类的应用，单个分片不要超过 20 GB 基本命令整理 查看集群信息 12345### 查看基本信息GET /### 查看健康状态GET /_cat/health? 索引操作 12345678910111213141516171819202122### 创建索引PUT demo### 获取索引列表GET _cat/indices### 查询索引详情GET demo### 修改索引PUT /demo/_settings&#123; &quot;aliase&quot;: &#123; &quot;test&quot;: &#123;&#125; &#125;, &quot;index&quot; : &#123; &quot;refresh_interval&quot; : null &#125;&#125;### 删除索引DELETE demo 索引模板 12345678910111213141516171819### 创建索引模板PUT _template/&lt;name&gt;&#123; &quot;index_patterns&quot;: [ &quot;&lt;patterns&gt;&quot; ], &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123;&#125; &#125;&#125;### 查询索引模板详情GET _template/demo_template### 获取索引模板列表GET _cat/templates### 删除索引模板DELETE _template/demo_template 字段映射： 12345678910111213141516&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;distance&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;route_length_miles&quot;: &#123; &quot;type&quot;: &quot;alias&quot;, &quot;path&quot;: &quot;distance&quot; &#125;, &quot;transit_mode&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125; 数据类型： aggregate_metric_* alias arrays binary boolean completion date date_nanos dense_vector flattened geo_point geo_shape histogram ip join keyword nested numeric long integer short byte double float half_float scaled_float unsigned_long percolator point integer_range float_range long_range double_range date_range ip_range rank_feature rank_features search_as_you_type orientation shape orientation ignore_malformed ignore_z_value coerce text version 数据操作 12345678910111213141516171819202122### 插入数据PUT demo/_doc/1&#123; &quot;name&quot;: &quot;demo&quot;, &quot;age&quot;: 18&#125;### 根据 ID 获取数据GET demo/_doc/1### 查询数据GET demo/_search### 修改数据POST demo/_doc/1&#123; &quot;name&quot;: &quot;demo&quot;, &quot;age&quot;: 20&#125;### 删除数据DELETE demo/_doc/1 查询操作： 注：此处内容为查询接口输入参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&#123; &quot;from&quot;: 0, &quot;size&quot;: 20, &quot;_source&quot;: [&quot;result_field_1&quot;,&quot;result_field_2&quot;], &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [], &quot;filter&quot;: [], &quot;must_not&quot;: [], &quot;should&quot;: [], &quot;minimum_should_match&quot;: 1, &quot;boost&quot;: 1.0 &#125;, &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &#125;, &quot;negative&quot;: &#123; &#125;, &quot;negative_boost&quot;: 0.5 &#125;, &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &#125;, &quot;boost&quot;: 1.2 &#125;, &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ ], &quot;tie_breaker&quot;: 0.7 &#125;, &quot;function_score&quot;: &#123; &quot;query&quot;: &#123;&#125;, &quot;boost&quot;: &quot;5&quot;, &quot;random_score&quot;: &#123;&#125;, &quot;boost_mode&quot;: &quot;multiply&quot; &#125;, &quot;intervals&quot;: &#123;&#125;, &quot;match&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;match_bool_prefix&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;match_phrase&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;match_phrase_prefix&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;combined_fields&quot;: &#123; &quot;query&quot;: &quot;&lt;content&gt;&quot;, &quot;fields&quot;: [ &quot;&lt;field_1&gt;&quot;, &quot;&lt;field_2&gt;&quot; ] &#125;, &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;&lt;content&gt;&quot;, &quot;fields&quot;: [ &quot;&lt;field_1&gt;&quot;, &quot;&lt;field_2&gt;&quot; ] &#125;, &quot;query_string&quot;: &#123; &quot;query&quot;: &quot;&lt;content&gt;&quot;, &quot;default_field&quot;: &quot;content&quot; &#125;, &quot;simple_query_string&quot;: &#123; &quot;query&quot;: &quot;&lt;content&gt;&quot;, &quot;fields&quot;: [ &quot;&lt;field_1&gt;&quot;, &quot;&lt;field_2&gt;&quot; ], &quot;default_operator&quot;: &quot;and&quot; &#125;, &quot;term&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;range&quot;: &#123; &quot;&lt;key&gt;&quot;: &#123; &quot;gte&quot;: &quot;&lt;date&gt;&quot;, &quot;gt&quot;: &quot;&lt;date&gt;&quot;, &quot;lte&quot;: &quot;&lt;date&gt;&quot;, &quot;lt&quot;: &quot;&lt;date&gt;&quot; &#125; &#125;, &quot;geo_bounding_box&quot;: &#123;&#125;, &quot;geo_distance&quot;: &#123;&#125;, &quot;geohash_grid&quot;: &#123;&#125;, &quot;geo_polygon&quot;: &#123;&#125;, &quot;geo_shape&quot;: &#123;&#125;, &quot;shape&quot;: &#123;&#125;, &quot;nested&quot;: &#123; &quot;path&quot;: &quot;&lt;field&gt;&quot;, &quot;query&quot;: &#123;&#125; &#125;, &quot;has_child&quot;: &#123;&#125;, &quot;has_parent&quot;: &#123;&#125;, &quot;parent_id&quot;: &#123;&#125;, &quot;match_all&quot;: &#123;&#125;, &quot;span_containing&quot;: &#123;&#125;, &quot;span_near&quot;: &#123;&#125;, &quot;span_first&quot;: &#123;&#125;, &quot;span_not&quot;: &#123;&#125;, &quot;span_or&quot;: &#123;&#125;, &quot;span_term&quot;: &#123;&#125;, &quot;span_within&quot;: &#123;&#125;, &quot;distance_feature&quot;: &#123;&#125;, &quot;more_like_this&quot;: &#123;&#125;, &quot;percolate&quot;: &#123;&#125;, &quot;rank_feature&quot;: &#123;&#125;, &quot;script&quot;: &#123;&#125;, &quot;script_score&quot;: &#123;&#125;, &quot;wrapper&quot;: &#123;&#125;, &quot;pinned&quot;: &#123;&#125;, &quot;exists&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;fuzzy&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;ids&quot;: &#123;&quot;values&quot;: [&quot;&lt;id_1&gt;&quot;,&quot;&lt;id_2&gt;&quot;]&#125;, &quot;prefix&quot;: &#123;&quot;&lt;key&gt;&quot;: &quot;&lt;content&gt;&quot;&#125;, &quot;regexp&quot;: &#123;&#125;, &quot;terms&quot;: &#123;&quot;&lt;key&gt;&quot;: [&quot;&lt;content_1&gt;&quot;,&quot;&lt;content_2&gt;&quot;]&#125;, &quot;terms_set&quot;: &#123;&#125;, &quot;wildcard&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;&lt;field&gt;&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 聚合操作： 123456789&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123;&#125;, &quot;aggs&quot;: &#123; &quot;&lt;result_field&gt;&quot;: &#123; &quot;AGG_TYPE&quot;: &#123;&#125; &#125; &#125;&#125; 例如： 123456789101112131415&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;result&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;&lt;field&gt;&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;&lt;field&gt;&quot; &#125; &#125; &#125; &#125;&#125; 按照时间聚合： 12345678910111213141516171819202122232425&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_xxx&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;xxx&quot; &#125;, &quot;aggs&quot;: &#123; &quot;window_by_hour&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;xxx&quot;, &quot;interval&quot;: &quot;1d&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_xxx&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;xxx&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 常见问题 索引不可写入 使用如下方式修改配置即可 POST: http://&lt;host&gt;:&lt;port&gt;/_settings 1234567&#123; &quot;index&quot;: &#123; &quot;blocks&quot;: &#123; &quot;read_only_allow_delete&quot;: &quot;false&quot; &#125; &#125;&#125; 注：若修改之后不生效请检查磁盘剩余空间。 参考资料 官方文档 安装手册","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wangqian0306.github.io/tags/Elasticsearch/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"}]},{"title":"Oozie 基础使用","slug":"bigdata/oozie","date":"2020-07-02T14:43:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/oozie/","permalink":"https://wangqian0306.github.io/2020/oozie/","excerpt":"","text":"简介 Oozie 通常被用来提交和调度 Hadoop 集群中的任务。 在 CDH 集群中几乎所有的 Hue 操作都是由 Oozie 实现的。 安装和配置 服务可以直接添加，但是 Web UI 需要额外进行操作，样例如下： CDH 123curl http://archive.cloudera.com/gplextras/misc/ext-2.2.zip -o ext-2.2.zipunzip ext-2.2.zip -d /var/lib/ooziechown -R oozie:oozie /var/lib/oozie/ext-2.2 官方文档 HDP 123curl http://archive.cloudera.com/gplextras/misc/ext-2.2.zip -o ext-2.2.zipunzip ext-2.2.zip -d /usr/hdp/current/oozie-server/libextchown -R oozie:hadoop /usr/hdp/current/oozie-server/libext 运行流程 Oozie 任务的创建需要两个部分一部分为配置文件，一部分为运行流程的XML。 在运行时需要配置如下变量： 配置项 说明 样例 nameNode HDFS NameNode 地址 hdfs://localhost:8020 jobTracker Yarn ResourceManager 地址 localhost:8032 queueName 任务位于的 Yarn 队列 default oozie.wf.application.path workflow.xml 文件所在的 HDFS 文件夹 hdfs://localhost:8020/demo oozie.use.system.libpath 使用默认系统包 true security_enabled 权限认证开关 False user.name 任务所属用户 admin Java Client 可以在项目中加入下面的依赖，来引入项目相关包 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.oozie&lt;/groupId&gt; &lt;artifactId&gt;oozie-client&lt;/artifactId&gt; &lt;version&gt;5.1.0-cdh6.3.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 注：如果需要与 SpringBoot 项目集成的话需要添加 exclusion。如果系统环境中不存在 slf4j 或者 log4j 的依赖问题可以直接省略。 123456789101112131415161718192021import org.apache.oozie.client.OozieClient;import org.apache.oozie.client.OozieClientException;import org.apache.oozie.client.WorkflowAction;import org.apache.oozie.client.WorkflowJob;public class OozieDemo &#123; public static void main(String[] args) &#123; OozieClient oc = new OozieClient(&quot;http://localhost:11000/oozie/&quot;); Properties conf = oc.createConfiguration(); conf.setProperty(&quot;nameNode&quot;, &quot;hdfs://localhost:8020&quot;); conf.setProperty(&quot;...&quot;, &quot;...&quot;); String jobId; try &#123; jobId = oc.run(conf, xmlString); &#125; catch (Exception e) &#123; System.out.println(e.toString()); System.exit(1); &#125; System.out.println(jobId); &#125;&#125; XML 文件样例 Shell 任务样例 12345678910111213141516171819&lt;workflow-app name=&quot;shell-demo&quot; xmlns=&quot;uri:oozie:workflow:0.5&quot;&gt; &lt;start to=&quot;shell-0001&quot;/&gt; &lt;kill name=&quot;Kill&quot;&gt; &lt;message&gt;Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;action name=&quot;shell-0001&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.1&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;exec&gt;wqecho.sh&lt;/exec&gt; &lt;file&gt;/wq/wqecho.sh#wqecho.sh&lt;/file&gt; &lt;argument&gt;test&lt;/argument&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to=&quot;End&quot;/&gt; &lt;error to=&quot;Kill&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;End&quot;/&gt;&lt;/workflow-app&gt; Spark 任务样例 1234567891011121314151617181920&lt;workflow-app name=&quot;spark-demo&quot; xmlns=&quot;uri:oozie:workflow:0.5&quot;&gt; &lt;start to=&quot;spark2-0002&quot;/&gt; &lt;kill name=&quot;Kill&quot;&gt; &lt;message&gt;Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;action name=&quot;spark2-0002&quot;&gt; &lt;spark xmlns=&quot;uri:oozie:spark-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;master&gt;yarn&lt;/master&gt; &lt;mode&gt;client&lt;/mode&gt; &lt;name&gt;BatchSpark2&lt;/name&gt; &lt;jar&gt;demo.py&lt;/jar&gt; &lt;file&gt;/wq/demo.py#demo.py&lt;/file&gt; &lt;/spark&gt; &lt;ok to=&quot;End&quot;/&gt; &lt;error to=&quot;Kill&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;End&quot;/&gt;&lt;/workflow-app&gt; Sqoop 任务样例 12345678910111213141516&lt;workflow-app name=&quot;sqoop-demo&quot; xmlns=&quot;uri:oozie:workflow:0.5&quot;&gt; &lt;start to=&quot;sqoop-0003&quot;/&gt; &lt;kill name=&quot;Kill&quot;&gt; &lt;message&gt;Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;action name=&quot;sqoop-0003&quot;&gt; &lt;sqoop xmlns=&quot;uri:oozie:sqoop-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;command&gt;import --connect jdbc:mysql://localhost:3306/mysql --username mariadb --password mariadb --table user --target-dir /tmp/demo -m 1&lt;/command&gt; &lt;/sqoop&gt; &lt;ok to=&quot;End&quot;/&gt; &lt;error to=&quot;Kill&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;End&quot;/&gt;&lt;/workflow-app&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Oozie","slug":"Oozie","permalink":"https://wangqian0306.github.io/tags/Oozie/"}]},{"title":"Logstash 入门","slug":"database/logstash","date":"2020-07-01T15:09:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/logstash/","permalink":"https://wangqian0306.github.io/2020/logstash/","excerpt":"","text":"简介 Logstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到“存储库”中。 安装 在 CentOS 中可以使用如下命令配置软件源 12sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchvim /etc/yum.repos.d/logstash.repo 写入如下配置项即可 12345678[logstash-8.x]name=Elastic repository for 8.x packagesbaseurl=https://artifacts.elastic.co/packages/8.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md 在写入完成后可以使用如下命令安装 logstash 软件 1sudo yum install logstash -y 配置 软件配置在 /etc/logstash 目录中。 默认日志在 /var/log/logstash 目录中。 要使用 Logstash 还需要针对特定场景编写配置文件。 将编写好的配置文件放入 /etc/logstash/conf.d/ 目录中，然后重启 Logstash 服务即可。 官方文档 注：具体配置文件参见配置样例及说明部分。 使用方式 启动服务 1systemctl start logstash 关闭服务 1systemctl stop logstash 查看服务状态 1systemctl status logstash 配置服务开机自启动 1systemctl enable logstash 关闭服务开机自启 1systemctl disable logstash 配置样例及说明 开启服务接收日志 12345678910111213141516171819input &#123; tcp &#123; port =&gt; 5044 codec =&gt; json_lines &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;http://localhost:9200&quot;] index =&gt; &quot;springboot-logs-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125; 读取 Kafka 将数据写入 Elasticsearch 例如：kafka-to-es.conf 建议先去 ES 里创建 IndexTemplate 和 KafkaTopic 然后再来编写如下样例程序： 123456789101112131415161718192021222324input &#123; kafka &#123; bootstrap_servers =&gt; [&quot;rbfish-07.rainbowfish11000.prod:9092,rbfish-08.rainbowfish11000.prod:9092,rbfish-09.rainbowfish11000.prod:9092&quot;] auto_commit_interval_ms =&gt; &quot;1000&quot; group_id =&gt; &quot;LOGSTASH&quot; codec =&gt; &quot;json&quot; auto_offset_reset =&gt; &quot;latest&quot; consumer_threads =&gt; 1 decorate_events =&gt; true topics =&gt; [&quot;ShipAnalise&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;https://xxx.xxx.xxx.xxx:9200&quot;] index =&gt; &quot;xxxx-%&#123;+YYYY.MM.dd&#125;&quot; template_name =&gt; &quot;xxxx&quot; ssl =&gt; true cacert =&gt; &quot;/xxxx/xxx/http_ca.crt&quot; user =&gt; &quot;xxxxx&quot; password =&gt; &quot;xxxxxx&quot; &#125;&#125; 注：cacert 别放到 /etc/logstash 里，有权限问题。 从 Elasticsearch 迁移至 Elasticsearch 例如：es-to-es.conf 123456789101112131415161718input &#123; elasticsearch &#123; hosts =&gt; &quot;demo.wqnice.local&quot; index =&gt; &quot;mydata-*&quot; query =&gt; &#x27;&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; &quot;query&quot;: &quot;*&quot; &#125; &#125; &#125;&#x27; size =&gt; 500 scroll =&gt; &quot;5m&quot; docinfo =&gt; true &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;demo.wqnice.prod&quot; index =&gt; &quot;%&#123;[@metadata][_index]&#125;&quot; document_type =&gt; &quot;%&#123;[@metadata][_type]&#125;&quot; document_id =&gt; &quot;%&#123;[@metadata][_id]&#125;&quot; &#125;&#125; 参考资料 官方文档 安装手册 Elastic Spring Boot Integration spring-boot-elk-sample","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"},{"name":"Logstash","slug":"Logstash","permalink":"https://wangqian0306.github.io/tags/Logstash/"}]},{"title":"Gradle 的初步使用","slug":"java/gradle","date":"2020-07-01T15:09:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/gradle-quick-start/","permalink":"https://wangqian0306.github.io/2020/gradle-quick-start/","excerpt":"","text":"Gradle 的初步使用 使用 IDEA 如何配置 Gradle 在 IDEA 2020.1 中默认会下载并配置 Gradle。所以在没有安装的情况下也可以正常使用 Gradle。 需要注意的是在使用 IDEA 自带 Gradle 的时候无法使用 gradle 命令。 如果有特殊需求可以通过项目中的 gradlew 脚本文件进行使用。 配置软件源 在 ~/.gradle/ 目录中创建 init.gradle 文件并填入下面的内容即可配置阿里云加速： 123456789101112131415allprojects&#123; repositories &#123; def ALIYUN_REPOSITORY_URL = &#x27;https://maven.aliyun.com/repository/public&#x27; def ALIYUN_JCENTER_URL = &#x27;https://maven.aliyun.com/repository/jcenter&#x27; maven &#123; url ALIYUN_REPOSITORY_URL allowInsecureProtocol true &#125; maven &#123; url ALIYUN_JCENTER_URL allowInsecureProtocol true &#125; google() &#125;&#125; 切换本地的 Gradle 在 IDEA 的 Settings -&gt; Build, Execution, Deployment -&gt; Build Tools -&gt; Gradle 中可以对 Gradle 进行配置。 如果需要选用本地的 Gradle 可以选择 Gradle projects 栏中的 Use Gradle from 选项卡，将其内容配置为 Specified location 即可。 常用命令 生成模板项目 1gradle init 注：在生成的时候可以仔细观察输入参数，可以简单的生成多模板项目等内容。 编译 java 文件 1gradle compileJava 运行程序 1gradle run 构建程序 1gradle build 指定程序主类 在 build.gradle 文件中新增如下内容即可 12345jar &#123; manifest &#123; attributes &#x27;Main-Class&#x27;: &#x27;xxx.xxx.xxx&#x27; &#125;&#125; 包的引入模式 在 dependencies 模块中有如下的引入模式 implementation(正常引入) compileOnly(仅编译) compileClasspath testImplementation testRuntimeOnly testCompileClasspath 参考资料 官方文档","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"Gradle","slug":"Gradle","permalink":"https://wangqian0306.github.io/tags/Gradle/"}]},{"title":"HTTPS 知识整理","slug":"protocol/https","date":"2020-06-30T15:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2020/https/","permalink":"https://wangqian0306.github.io/2020/https/","excerpt":"","text":"HTTPS 四次握手的流程 客户端请求建立链接（443 端口），并向服务端发送一个随机数 client random 和客户端支持的加密方法，比如RSA公钥加密，此时是明文传输。 服务端回复一种客户端支持的加密方法、一个随机数 server random 和服务器证书和非对称加密的公钥。 客户端收到服务端的回复后利用服务端的公钥，加上新的随机数 premaster secret 通过服务端下发的公钥及加密方法进行加密，发送给服务器（80端口）。 服务端收到客户端的回复，利用已知的加解密方式进行解密，同时利用 client random、server random 和 premaster secret 通过一定的算法生成 HTTP 链接数据传输的对称加密 key – session key。 CA 在 HTTPS 当中的作用 CA 会用私钥将服务器端的公钥+服务器域名+公钥的摘要进行加密形成证书并将其将传输至客户端。 客户端会读取密文使用操作系统中内置的 CA 公钥进行解密，获取到服务端的公钥。 也就是说在第二次握手的过程中 CA 负责保证目标地址可信。 相关的关键词和说明 HTTPS HTTPS (全称：Hyper Text Transfer Protocol over SecureSocket Layer），是以安全为目标的 HTTP 通道，在HTTP的基础上通过传输加密和身份认证保证了传输过程的安全性。 SSL 和 TLS SSL(Secure Sockets Layer 安全套接字协议),及其继任者传输层安全（Transport Layer Security，TLS）是为网络通信提供安全及数据完整性的一种安全协议。 X.509 X.509 是密码学里公钥证书的格式标准。 X.509 证书己应用在包括 TLS/SSL 在内的众多 Internet 协议里.同时它也用在很多非在线应用场景里。 免费 CA Let’s Encrypt","categories":[{"name":"Web","slug":"Web","permalink":"https://wangqian0306.github.io/categories/Web/"}],"tags":[{"name":"HTTPS","slug":"HTTPS","permalink":"https://wangqian0306.github.io/tags/HTTPS/"}]},{"title":"原型模式","slug":"design_pattern/pattern-prototype","date":"2020-06-30T14:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-prototype/","permalink":"https://wangqian0306.github.io/2020/design-pattern-prototype/","excerpt":"","text":"1234567891011121314151617181920212223242526272829public abstract class Shape implements Cloneable &#123; private String id; protected String type; abstract void draw(); public String getType()&#123; return type; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Object clone() &#123; Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return clone; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.Hashtable;class Square extends Shape &#123; public Square()&#123; type = &quot;Square&quot;; &#125; @Override public void draw() &#123; System.out.println(&quot;draw square&quot;); &#125;&#125;class Circle extends Shape &#123; public Circle()&#123; type = &quot;Circle&quot;; &#125; @Override public void draw() &#123; System.out.println(&quot;draw circle&quot;); &#125;&#125;class ShapeCache &#123; private static final Hashtable&lt;String, Shape&gt; shapeMap = new Hashtable&lt;&gt;(); public static Shape getShape(String shapeId) &#123; Shape cachedShape = shapeMap.get(shapeId); return (Shape) cachedShape.clone(); &#125; public static void loadCache() &#123; Circle circle = new Circle(); circle.setId(&quot;1&quot;); shapeMap.put(circle.getId(),circle); Square square = new Square(); square.setId(&quot;2&quot;); shapeMap.put(square.getId(),square); &#125;&#125;public class PrototypePatternDemo &#123; public static void main(String[] args) &#123; ShapeCache.loadCache(); Shape clonedShape = (Shape) ShapeCache.getShape(&quot;1&quot;); System.out.println(&quot;Shape : &quot; + clonedShape.getType()); Shape clonedShape2 = (Shape) ShapeCache.getShape(&quot;2&quot;); System.out.println(&quot;Shape : &quot; + clonedShape2.getType()); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"建造者模式","slug":"design_pattern/pattern-builder","date":"2020-06-27T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-builder/","permalink":"https://wangqian0306.github.io/2020/design-pattern-builder/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154import java.util.ArrayList;import java.util.List;interface Packing &#123; String pack();&#125;interface Item &#123; String name(); Packing packing(); float price();&#125;class Wrapper implements Packing &#123; @Override public String pack() &#123; return &quot;Wrapper&quot;; &#125;&#125;class Bottle implements Packing &#123; @Override public String pack() &#123; return &quot;Bottle&quot;; &#125;&#125;abstract class Burger implements Item &#123; @Override public Packing packing() &#123; return new Wrapper(); &#125; @Override public abstract float price();&#125;abstract class ColdDrink implements Item &#123; @Override public Packing packing() &#123; return new Bottle(); &#125; @Override public abstract float price();&#125;class VegBurger extends Burger &#123; @Override public float price() &#123; return 25.0f; &#125; @Override public String name() &#123; return &quot;Veg Burger&quot;; &#125;&#125;class ChickenBurger extends Burger &#123; @Override public float price() &#123; return 50.5f; &#125; @Override public String name() &#123; return &quot;Chicken Burger&quot;; &#125;&#125;class Coke extends ColdDrink &#123; @Override public float price() &#123; return 30.0f; &#125; @Override public String name() &#123; return &quot;Coke&quot;; &#125;&#125;class Pepsi extends ColdDrink &#123; @Override public float price() &#123; return 35.0f; &#125; @Override public String name() &#123; return &quot;Pepsi&quot;; &#125;&#125;class Meal &#123; private List&lt;Item&gt; items = new ArrayList&lt;Item&gt;(); public void addItem(Item item)&#123; items.add(item); &#125; public float getCost()&#123; float cost = 0.0f; for (Item item : items) &#123; cost += item.price(); &#125; return cost; &#125; public void showItems()&#123; for (Item item : items) &#123; System.out.print(&quot;Item : &quot;+item.name()); System.out.print(&quot;, Packing : &quot;+item.packing().pack()); System.out.println(&quot;, Price : &quot;+item.price()); &#125; &#125;&#125;class MealBuilder &#123; public Meal prepareVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new VegBurger()); meal.addItem(new Coke()); return meal; &#125; public Meal prepareNonVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new ChickenBurger()); meal.addItem(new Pepsi()); return meal; &#125;&#125;public class BuilderDemo &#123; public static void main(String[] args) &#123; MealBuilder mealBuilder = new MealBuilder(); Meal vegMeal = mealBuilder.prepareVegMeal(); System.out.println(&quot;Veg Meal&quot;); vegMeal.showItems(); System.out.println(&quot;Total Cost: &quot; +vegMeal.getCost()); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"工厂模式","slug":"design_pattern/pattern-factory","date":"2020-06-27T12:32:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-factory/","permalink":"https://wangqian0306.github.io/2020/design-pattern-factory/","excerpt":"","text":"简单工厂模式 12345678910111213141516171819202122232425262728293031323334abstract class Pizza &#123; public abstract void prepare();&#125;class CheesePizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare cheese pizza&quot;); &#125;&#125;class GreekPizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare greek pizza&quot;); &#125;&#125;public class PizzaFactory &#123; public static Pizza createPizza(String orderType) &#123; switch (orderType) &#123; case &quot;cheese&quot;: return new CheesePizza(); case &quot;greek&quot;: return new GreekPizza(); default: System.out.println(&quot;no such type&quot;); return null; &#125; &#125;&#125; 工厂方法模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172abstract class Pizza &#123; public abstract void prepare();&#125;class BJCheesePizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare bj cheese pizza&quot;); &#125;&#125;class BJGreekPizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare bj greek pizza&quot;); &#125;&#125;class LDCheesePizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare ld cheese pizza&quot;); &#125;&#125;class LDGreekPizza extends Pizza &#123; @Override public void prepare() &#123; System.out.println(&quot;prepare ld greek pizza&quot;); &#125;&#125;abstract class OrderPizza &#123; public abstract Pizza makePizza(String type);&#125;class BJOrderPizza extends OrderPizza &#123; @Override public Pizza makePizza(String type) &#123; switch (type) &#123; case &quot;cheese&quot;: return new BJCheesePizza(); case &quot;greek&quot;: return new BJGreekPizza(); default: System.out.println(&quot;no such type&quot;); return null; &#125; &#125;&#125;class LDOrderPizza extends OrderPizza &#123; @Override public Pizza makePizza(String type) &#123; switch (type) &#123; case &quot;cheese&quot;: return new LDCheesePizza(); case &quot;greek&quot;: return new LDGreekPizza(); default: System.out.println(&quot;no such type&quot;); return null; &#125; &#125;&#125; 抽象工厂模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110interface Shape &#123; void draw();&#125;class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Rectangle::draw() method.&quot;); &#125;&#125;class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125;interface Color &#123; void fill();&#125;class Red implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Inside Red::fill() method.&quot;); &#125;&#125;class Blue implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Inside Blue::fill() method.&quot;); &#125;&#125;abstract class AbstractFactory &#123; public abstract Color getColor(String color); public abstract Shape getShape(String shape) ;&#125;class ShapeFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; return null; &#125; @Override public Color getColor(String color) &#123; return null; &#125;&#125;class ColorFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; return null; &#125; @Override public Color getColor(String color) &#123; if(color == null)&#123; return null; &#125; if(color.equalsIgnoreCase(&quot;RED&quot;))&#123; return new Red(); &#125; else if(color.equalsIgnoreCase(&quot;BLUE&quot;))&#123; return new Blue(); &#125; return null; &#125;&#125;class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase(&quot;SHAPE&quot;))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase(&quot;COLOR&quot;))&#123; return new ColorFactory(); &#125; return null; &#125;&#125;public class AbstractFactoryDemo &#123; public static void main(String[] args) &#123; AbstractFactory shapeFactory = FactoryProducer.getFactory(&quot;SHAPE&quot;); Shape shape = shapeFactory.getShape(&quot;CIRCLE&quot;); shape.draw(); AbstractFactory colorFactory = FactoryProducer.getFactory(&quot;COLOR&quot;); Color color = colorFactory.getColor(&quot;RED&quot;); color.fill(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"JAVA 的进程与线程","slug":"java/thread","date":"2020-06-25T13:39:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/java-thread/","permalink":"https://wangqian0306.github.io/2020/java-thread/","excerpt":"","text":"进程与线程的概述 进程 当一个程序进入内存运行时，即编程一个进程。进程是处于运行过程中的程序，并具有一定的独立功 能，进程是系统进行资源分配和调度的一个独立单位。一般而言进程通常包含如下三个特性 独立性 进程是系统中独立存在的实体，它可以用用自己独立的资源，每一个进程都拥有自己私有的地址空间 。在没有经过进程本身允许的情况下，一个用户进程不可以直接访问其他进程的地址空间。 动态性 进程与程序的区别在于，程序只是一个静态的指令集合，而进程是一个正在系统中活动的指令集合。 在进程中多了时间的概念。进程具有自己的生命周期和状态，这些概念在程序中都是不具备的。 并发性 多个进程可以在单个处理器上并发执行，多个进程之间不会互相影响。 线程 线程也被称作轻量级进程，是进程的执行单元。一个进程可以拥有多个线程，一个线程必须有一个 父进程。线程可以拥有自己的堆栈、自己的程序计数器和自己的局部变量，但不拥有系统资源，它 与父进程的其他线程共享该进程所拥有的全部资源。 多线程的实现方式 创建线程可以使用如下三种方式 继承 Thread 类 实现 Runnable 接口 使用 Callable 和 Future 接口 线程的生命周期 线程的生命周期包含如下5种状态 新建 就绪 运行 阻塞 死亡","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Sqoop 基础使用","slug":"bigdata/sqoop-cli","date":"2020-06-24T14:13:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/sqoop-cli/","permalink":"https://wangqian0306.github.io/2020/sqoop-cli/","excerpt":"","text":"Sqoop 版本说明 目前常见的 Sqoop 分成 1(1.4.7)和 2(1.99.7) 两个版本。 Sqoop2与1相比主要新增了 REST 接口，但是在导入数据的时候缺失了导入至 Hive 的重点功能。 注：目前主流的方案还是采用 Sqoop 1，所以本文也仅针对于 Sqoop1。 安装 在 CDH 和 HDP 集群中都可以通过新增组件的方式安装 Sqoop 命令梳理和技巧整理 基础命令样例 导入单表 样例命令如下： 1sqoop import --connect jdbc:mysql://localhost:3306/mysql --username mariadb --password mariadb --table user --target-dir /tmp/demo -m 1 整库导入 1sqoop import-all-tables --connect jdbc:mysql://localhost:3306/mysql --username mariadb --password mariadb 增量导入 1sqoop import --connect jdbc:mysql://localhost:3306/test --username mariadb --password mariadb --table demo --target-dir /tmp/demo --check-column id --incremental append --last-value 3 -m 1 参数说明 参数说明： 参数 说明 –connect JDBC 链接字符串 –username 数据库用户名 –password 数据库链接密码 –table 导入表名 –target-dir 导入 HDFS 的路径 -m Map 数量(最终生成的文件数) –append 追加模式 –columns 导入列名(可以使用&quot;包裹列名) –delete-target-dir 如果目标路径存在则删除目标路径 –query 使用 SQL 查询获取数据 –hive-import 导入至 Hive –hive-overwrite 重写 Hive 数据 –hive-database Hive 数据库名 –hive-table Hive 表名 –check-column 关键列,该列的类型不得为 CHAR / NCHAR / VARCHAR / VARNCHAR / LONGVARCHAR / LONGNVARCHAR –incremental 增量导入模式，可选：append(递增追加),lastmodified(最新修改) –last-value 上次导入的值 Sqoop 任务 可以使用任务的方式来免去输入 last-value 的值。 创建任务 1sqoop job --create demo -- import --connect jdbc:mysql://localhost:3306/mysql --username mariadb --password mariadb --table user --target-dir /tmp/demo -m 1 列出任务 1sqoop job --list 查看任务配置 1sqoop job --show demo 运行任务 1sqoop job --exec myjob -- --username demo --password demo 删除任务 1sqoop jon --delete demo 注意事项 在运行导入命令时需要切换至当前用户所属的路径，因为在命令的运行过程中会产生 表名.java 的表结构文件。 Sqoop 所需的数据库链接 jar 包需要放置在 Hadoop 的 Classpath 下。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"https://wangqian0306.github.io/tags/Sqoop/"}]},{"title":"Livy 的基础使用","slug":"bigdata/livy","date":"2020-06-23T15:25:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/livy/","permalink":"https://wangqian0306.github.io/2020/livy/","excerpt":"","text":"Livy 基础介绍 Livy 是一个使用 REST API 调度 Spark 任务的组件。 安装及配置 对于 HDP 平台来说可以通过新增组件的方式进行安装。CDH 的话需要自行制作或者找到对应的 Parcel 包。 当然也可以通过直接在官网下载 tar 包的形式进行安装。 在安装之前需要配置如下环境变量： 配置项 说明 SPARK_HOME SPARK 安装路径 HADOOP_CONF_DIR Hadoop 配置文件所在路径 注：CDH 默认的 HADOOP_CONF_DIR 为 /etc/alternatives/hadoop-conf 在配置完环境变量之后需要配置 livy.conf 文件，具体配置项如下： 配置项 说明 样例 livy.server.host 绑定域名 0.0.0.0 livy.server.port 绑定端口 8998 livy.spark.master 调度方案 yarn livy.file.local-dir-whitelist 读取本地文件的白名单路径 / livy.server.csrf-protection.enabled CSRF 保护 false 在配置完成后运行如下命令即可启动服务 1./bin/livy-server start 注：服务启动的功能比较少，仅有 start stop status 三项可选。 使用方式 Livy 提供了 REST API 和 Java Client 两种方式进行调度： REST JAVA Client 注意事项 多文件依赖 对于多个文件的 Python 项目来说需要将代码打包成为 zip 包的方式来进行传输。 REST API 时效性 Livy 在 REST API 中的运行数据是会被定时清除的，所以需要单独编码来实现日志和任务记录功能。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"},{"name":"Livy","slug":"Livy","permalink":"https://wangqian0306.github.io/tags/Livy/"}]},{"title":"CSRF 知识整理","slug":"protocol/csrf","date":"2020-06-21T13:44:32.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2020/csrf/","permalink":"https://wangqian0306.github.io/2020/csrf/","excerpt":"","text":"简述 CSRF(Cross-site request forgery)，中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 原理 攻击者通过一些技术手段欺骗用户的浏览器去访问一个自己以前认证过的站点并运行一些操作。 本质上来说CSRF攻击是利用了 Web 端程序的隐式身份验证机制。对于后台而言虽然可以认为请求是来自于某个已登录用户的浏览器发出的，但却无法保证该请求是用户批准发送的。 如何进行保护 前后端融合的项目 对于前后台融合的项目可以通过“模板”相关的技术来解决此问题。 对于 Java 来说有 JSP, Python 相关的技术有 Jinja2 等等。 前后端分离的项目 对于前后端分离的项目来说就有些难办，可以通过后台操作 Cookie 的方式来讲数据传输至浏览器，然后由前端读取 Cookie 再传输回后台进行校验。 具体实现上来说可以使用下面的样例(Spring Security) 1234567891011import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.web.csrf.CookieCsrfTokenRepository;public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.csrf() .csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()); &#125;&#125;","categories":[{"name":"Web","slug":"Web","permalink":"https://wangqian0306.github.io/categories/Web/"}],"tags":[{"name":"CSRF","slug":"CSRF","permalink":"https://wangqian0306.github.io/tags/CSRF/"}]},{"title":"JAVA 版本的短网址算法","slug":"algorithm/short_url","date":"2020-06-20T12:04:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/short_url/","permalink":"https://wangqian0306.github.io/2020/short_url/","excerpt":"","text":"简介 在线上活动中经常需要用到短网址来做活动或者推广，但是好像一直没有思考过具体实现的方案。 实现思路 Hash 取余 在这个问题第一次出现在我脑海里的时候我想用类似于布轮过滤器类似的方案，通过取余数配合进制转换的方案实现功能。 采取多次 Hash 来减少 Hash 重复和取余相同的问题 将短网址和代加密的长网址对应关系存储至数据库，并设定为唯一键，捕捉取余相同的问题 在 Hash 前拼接时间字符串来为异常做捕捉 各种转 “62进制” 比方说可以用： Unix 时间戳 雪花算法 数据库自增键 在这里就简单写下进制转换的方法吧，具体实现可以随便嵌套。 12345678910111213141516171819202122232425import java.time.LocalDateTime;import java.time.ZoneOffset;public class ShortURLUtil &#123; static final char[] DIGITS = &#123; &#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;q&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;, &#x27;J&#x27;, &#x27;K&#x27;, &#x27;L&#x27;, &#x27;M&#x27;, &#x27;N&#x27;, &#x27;O&#x27;, &#x27;P&#x27;, &#x27;Q&#x27;, &#x27;R&#x27;, &#x27;S&#x27;, &#x27;T&#x27;, &#x27;U&#x27;, &#x27;V&#x27;, &#x27;W&#x27;, &#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;&#125;; public static String getShortURL(long seq) &#123; StringBuilder sBuilder = new StringBuilder(); do &#123; int remainder = (int) (seq % 62); sBuilder.append(DIGITS[remainder]); seq = seq / 62; &#125; while (seq != 0); return sBuilder.toString(); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"HTTP 请求相关的知识整理","slug":"protocol/http","date":"2020-06-19T15:26:13.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2020/http/","permalink":"https://wangqian0306.github.io/2020/http/","excerpt":"","text":"前言 之前一直没有关注过协议相关的内容，所以导致自己好像犯了些很蠢的错误。如今回头看看捡捡应该对自己的成长有很大的益处吧！ 超文本传输协议原文 请求报文 注：请求报文部分仅仅为了对之前的知识进行整理和补充，在下面的内容才是协议相关的内容。 HTTP 报文的格式如下 1234567请求方法 URL 协议版本 // 请求行头部字段名: 对应值 // 请求头...头部字段名: 对应值请求数据 // 请求体 请求方法总览 协议中规定的请求方法有以下几种 请求方法 说明 GET 检索信息 HEAD HEAD方法与GET相同，不同之处在于服务器在响应中不得发送消息正文 POST 向指定资源提交数据处理请求 PUT 从客户端向服务器传送的数据取代指定的文档的内容 DELETE 移除目标资源 CONNECT 经过目标资源验证后建立一条与服务器通信的管道 OPTIONS 描述与目标资源的通信选项 TRACE 与目标资源之间的回路测试 注：这个官方文档读起来真费劲。。。。 安全方法 如果请求方法的定义语义本质上是只读的，则将其视为“安全方法“。 在协议中将 GET, HEAD, OPTIONS, TRACE 方法定义为了安全方法。 注：此处的只读指的是针对于目标数据而言，日志记录等操作不在考虑范围内。 幂等方法 如果使用该方法的多个相同请求在服务器上的预期效果与单个此类请求的效果相同，则该请求方法被视为“幂等方法” 。 在协议中将 PUT, DELETE 方法规定为了幂等方法。 注：此处的幂等指的是针对于目标数据而言，日志记录等操作不在考虑范围内。 可缓存的方法 可以将结果数据进行存储以供后来者取用的方法就是可缓存的方法。 在协议中将 GET, HEAD, POST 方法规定为了可缓存的方法。 状态码说明 在协议中规定了如下四类状态码 提示信息 (1xx) 状态代码的1xx类别指示临时响应，用于在完成请求的操作并发送最终响应之前传达连接状态或请求进度。 状态码 说明 100(Continue) 持续接受请求 101(Switching Protocols) 服务器理解并向客户端指出要升级到的协议 成功请求 (2xx) 状态代码的2xx类表示服务器已成功接收并理解和接受了客户的请求。 状态码 说明 200(OK) 请求成功 201(Created) 请求成功并创建了新资源 202(Accepted) 该请求已被接受进行处理，但处理尚未完成 203(Non-Authoritative Information) 所得到的内容和原始服务器的返回内容可能不完全一致 204(No Content) 服务器执行成功但是没有任何数据返回 205(Reset Content) 表单重置 重定向 (3xx) 状态代码的3xx类标识用户需要采取进一步的措施才能满足请求。 状态码 说明 300(Multiple Choices) 目标有多种 301(Moved Permanently) 已为目标资源分配了新的地址 302(Found) 目标资源暂时位于其他地址 303(See Other) 正在重定向用户请求到其他地址 305(Use Proxy) 已弃用 306(Unused) 已弃用 307(Temporary Redirect) 临时重定向 客户端异常 (4xx) 状态代码的4xx类表明客户端似乎已出错。 状态码 说明 400(Bad Request) 服务器无法处理用户传入的请求 402(Payment Required) 协议留用 403(Forbidden) 权限不足 404(Not Found) 资源不存在 405(Method Not Allowed) 请求方法异常 406(Not Acceptable) 目标资源与请求资源不符 408(Request Timeout) 服务器在限定时间内未收到完整的请求消息 409(Conflict) 与目标资源的当前状态冲突而无法完成请求 410(Gone) 在服务器上不再可以访问目标资源 411(Length Required) 必须传入Content-Length 413(Payload Too Large) 请求有效负载大于服务器愿意或能够处理的请求 414(URI Too Long) 请求目标长度大于服务器愿意解释的长度 415(Unsupported Media Type) 服务器拒绝为请求提供服务，因为有效负载的格式不受目标资源上此方法的支持 417(Expectation Failed) 服务器无法满足请求设定的Expect参数 426(Upgrade Required) 服务器拒绝使用当前协议执行请求 服务器端异常 (5xx) 状态代码的5xx类表明服务器知道它已错误或无法执行所请求的方法。 状态码 说明 500(Internal Server Error) 服务器遇到意外状况，无法满足用户请求 501(Not Implemented) 服务器不支持满足请求所需的功能 502(Bad Gateway) 服务器充当网关或代理时从目标服务器得到了无效的相应 503(Service Unavailable) 服务器当前由于暂时的过载或计划的维护而无法处理该请求 504(Gateway Timeout) 服务器充当网关或代理时无法从目标服务器获得相应 505(HTTP Version Not Supported) 服务器不支持或拒绝支持请求中的HTTP版本 请求头说明 注：此处内容尚未进行整理，仅仅只是暂存。 一般而言请求头内的数据有以下 Key 头部字段名 说明 Accept 客户端能够接收的内容类型 Accept-Encoding 浏览器可以支持的web服务器返回内容压缩编码类型 Connection 是否需要持久连接 Authorization 授权信息，常见的有 Basic 或 Bearer Cache-Control 指定请求和响应遵循的缓存机制 Cookie Cookie(这个没啥好说的) Content-Length 请求的内容长度 Content-Type 请求数据类型(MIME) Host 指定请求的服务器的域名和端口号 User-Agent 发送的浏览器和系统信息 一般而言相应头内的数据有以下 Key 头部字段名 说明 Accept-Ranges 表明服务器是否支持指定范围请求及哪种类型的分段请求 Age 从原始服务器到代理缓存形成的估算时间(以秒计，非负) Last-Modified 请求资源的最后修改时间 Cache-Control 告诉所有的缓存机制是否可以缓存及哪种类型 Content-Length 数据长度 Content-Encoding 服务器支持的返回内容压缩编码类型 Content-Type 返回数据类型(MIME) Date 原始服务器消息发出的时间 Expires 响应过期的日期和时间 Server web服务器软件名称","categories":[{"name":"Web","slug":"Web","permalink":"https://wangqian0306.github.io/categories/Web/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://wangqian0306.github.io/tags/HTTP/"}]},{"title":"JAVA 当中的锁","slug":"java/synchronized","date":"2020-06-18T13:54:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/java lock/","permalink":"https://wangqian0306.github.io/2020/java%20lock/","excerpt":"","text":"锁的类型与实现方式 线程是否要锁同步资源 锁住 悲观锁 (synchronized，接口Lock的实现类) 不锁 乐观锁 (CAS算法) 被锁导致同步资源失败，线程是否阻塞 阻塞 自旋锁 不阻塞 适应性锁 多个线程竞争同步资源的流程细节有没有区别 不锁住资源，多线程中只有一个能成功修改资源其他的线程会重试 无锁 同一个线程执行同步资源时自动获取资源 偏向锁 多个线程竞争同步资源时，没有获取资源的线程自旋等待锁被释放 轻量级锁 多个线程竞争同步资源时，没有获取资源的线程阻塞等待唤醒 重量级锁 多个线程竞争锁时是否要排队 排队 公平锁 先尝试插队，插队失败再排队 非公平锁 一个线程的多个流程能不能获取同一把锁 能 可重入锁 不能 非可重入锁 多个线程能不能共享一把锁 能 共享锁 不能 排他锁 CAS 算法及 ABA 问题 Compare And Set（CAS）即比较与交换算法是通过 内存地址 操作对象原始值 目标值 三个内容的对比来实现功能的。而在特殊的情况下由于原始值可能经历过了多次修改又变回了和原来 一样的值，在这种情况下再去做修改就不符合实际场景了，这样的问题叫ABA问题。 注：在对比时加入数据的版本号即可解决ABA问题。 synchronized 关键字的使用 synchronized 关键字有以下三种用法 同步实例方法，锁当前实例对象 同步类方法(静态方法)，锁当前类对象 同步代码块，锁括号中的对象 锁的膨胀升级过程 在只有一个线程进入临界区时会采用偏向锁。 在多线程未竞争或者竞争不激烈时采用轻量级锁。 在多线程竞争时采用重量级锁","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"垃圾回收算法知识整理","slug":"java/gc","date":"2020-06-17T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/gc/","permalink":"https://wangqian0306.github.io/2020/gc/","excerpt":"","text":"垃圾回收的作用区域 垃圾回收主要针对的是： 方法区 堆内存 其中堆是垃圾收集器的工作重点。 垃圾回收的阶段 标记阶段 引用计数 如果对象被引用了就将引用计数器+1，当引用失效的时候再-1即可。只要当引用计数器为0，就代表此对象可以被回收了。 注：Python 主要使用引用计数法进行 GC 而 JAVA 没有使用此种方案。 可达性分析 如果对象与根对象的集合(GC Roots)存在直接或间接关系则证明对象不可回收。 在 Java 中 GC Roots 包含以下几类元素： 虚拟机栈中引用的对象 本地方法栈内引用的对象 方法区中静态属性引用的对象 方法区中常量引用的对象 所有被 synchronized 持有的对象 Java 虚拟机内部的引用 反应 Java 虚拟机内部情况的 JMXBean，JVMTI 中注册的回调、本地代码缓存等 清除阶段 标记清除算法 当堆中的有效内存空间被耗尽的时候，就会停止程序(Stop The World) 然后进行标记和清除操作。 标记操作 由回收器从根节点开始遍历并标记所有被引用的对象。 清除操作 由回收器对堆内存从头到尾进行遍历，如果发现对象未被标记则将其地址存储在空闲地址列表中。 复制算法 将内存空间分为两份，每次仅使用其中一份。当垃圾回收的时候将存活的对象复制到未被使用的另一 份内存中，然后清除正在使用的所有对象。在执行完成后交换两个内存的角色，最后完成垃圾回收。 注：新生代的垃圾回收算法。 标记压缩算法 标记压缩算法和标记清除算法类似第一阶段都会从根节点开始标记所有的应用对象， 而在第二阶段则会把所有的存活对象压缩到内存的一端并按顺序排放。之后再去清理外界空间。 注：老年代的垃圾回收算法。 针对 STW 缺陷的弥补方案 增量收集算法 在处理的时候通过切换垃圾回收线程和应用程序线程的交替执行的方式来实现内存回收。 分区算法 将内存区分成多块，然后根据目标的停顿时间，每次合理的回收若干个小区间来减少停顿。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"JVM 内存区域和类的加载","slug":"java/jvm-class-loading","date":"2020-06-16T13:05:12.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/jvm-class-loading/","permalink":"https://wangqian0306.github.io/2020/jvm-class-loading/","excerpt":"","text":"内存区域 线程私有的空间有： 程序计数器 虚拟机栈 本地方法栈 线程共享的空间有： 堆 方法区 直接内存 注: 在 Java 1.8 后位于直接内存中的元空间替换了方法区。 在堆内存中又分为： 新生代 老年代 永久代(在 Java 1.8 后被元空间所取代) 类的加载 类从被加载到虚拟机内存中刚开始到卸载出内存为止总共需要经过以下几个阶段。 加载(Loading) 验证(Verification) 准备(Preparation) 解析(Resolution) 初始化(Initialization) 使用(Using) 卸载(UnLoading) 加载 加载阶段虚拟机做了下面三件事： 通过一个类的全限定名获取定义此类的二进制字节流。 将这个字节流所代表的的静态存储结构转化为方法区(元空间)运行时的数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 链接 在链接的过程中需要完成验证，准备，解析三个步骤 验证 确保字节流中的信息符合虚拟机的要求，保证类的加载不会危害虚拟机自身安全。 在具体实现时采用了以下四种检验动作： 文件格式检验 元数据验证 字节码验证 符号引用验证 准备 在此阶段中会为类变量分配内存并且设置该类变量的默认初始值。 注：在准备阶段不会为 final 修饰的类变量分配内存并赋值。因为 final 在编译的时候就已经分配了相应空间，在准备阶段仅会完成显式初始化。 解析 虚拟机将常量池内的符号引用转化为直接引用。 初始化 初始化阶段是执行类构造器 &lt;clinit&gt;() 方法的过程。 注：clinit 方法是编译器将类变量与静态代码块的语句合并生成的，合并的顺序依照源文件中的位置决定。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"单例模式","slug":"design_pattern/pattern-singleton","date":"2020-06-15T14:12:58.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/design-pattern-singleton/","permalink":"https://wangqian0306.github.io/2020/design-pattern-singleton/","excerpt":"","text":"饿汉式 12345678910111213class Demo &#123; private static Demo instance; private Demo() &#123;&#125; static &#123; instance = new Demo(); &#125; public static Demo getInstance() &#123; return instance; &#125;&#125; 注：在类加载的时候就吃资源。 懒汉式 12345678910111213141516class Demo &#123; private static volatile Demo instance; private Demo() &#123;&#125; public static Demo getInstance() &#123; if (instance==null)&#123; synchronized (Demo.class) &#123; if (instance==null)&#123; instance = new Demo(); &#125; &#125; &#125; return instance; &#125;&#125; 注：在调用时才实例化对象。 静态内部类 12345678910111213class Demo &#123; private static volatile Demo instance; private Demo() &#123;&#125; private static class DemoInstance &#123; private static final Demo INSTANCE = new Demo(); &#125; public static Demo getInstance() &#123; return DemoInstance.INSTANCE; &#125;&#125; 枚举 123456enum Demo &#123; INSTANCE; public void echo() &#123; System.out.println(&quot;echo&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"}]},{"title":"Hadoop CLI","slug":"bigdata/hadoop-cli","date":"2020-06-14T14:26:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/hadoop-cli/","permalink":"https://wangqian0306.github.io/2020/hadoop-cli/","excerpt":"","text":"CDH 与 HDP 的超级权限获取 在 HDP 集群上可以直接使用如下命令切换至 HDFS 用户，然后就可以为所欲为了 1su hdfs 但是在 CDH 集群上可以通过配置环境变量的方式来切换 HDFS 用户 1export HADOOP_USER_NAME=hdfs 又或者可以通过命令前置的环境变量方式来进行使用 1HADOOP_USER_NAME=hdfs hdfs dfs -ls / HDFS 命令行梳理 列出目录 1hdfs dfs -ls &lt; path &gt; 创建多层文件夹 1hdfs dfs -mkdir -p &lt; path &gt; 从本地目录上传 1hdfs dfs -put &lt; local_path &gt; &lt; path &gt; 从远程地址下载 1hdfs dfs -get &lt; path &gt; &lt; local_path &gt; 修改文件夹权限 1hdfs dfs -chmod -R ??? &lt; path &gt; 统计文件夹大小 1hdfs dfs -du -h &lt; path &gt; 删除文件夹 1hdfs dfs -rm -r -f -skipTrash &lt; path &gt; 重新整理集群之间的文件分布 1hdfs balancer 多集群之间的文件拷贝 1hadoop distcp hdfs://&lt;from_host&gt;:&lt;from_port&gt;/&lt;from_path&gt; hdfs://&lt;to_host&gt;:&lt;to_port&gt;/&lt;to_path&gt; 重新生成 HA 配置项 1hdfs zkfc -formatZK -force -nonInteractive 查看 Namenode 状态 1hdfs haadmin -getServiceState &lt;service_name&gt; 注：service_name 在配置项 dfs.ha.namenodes.&lt;area&gt;配置项中查看。 强制启用 Namenode 1hdfs haadmin -transitionToActive --forcemanual &lt;service_name&gt; Yarn 命令行梳理 列出目前的任务列表 1yarn application -list 根据任务状态筛选任务 1yarn application -list -appStates &lt; Status &gt; 常用的任务状态分为以下几种 任务 说明 ACCEPTED 队列中 RUNNING 正在运行 FAILED 运行失败 查看任务状态 1yarn application -status &lt; Application ID &gt; 终止任务 1yarn application -kill &lt; Application ID &gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://wangqian0306.github.io/tags/Hadoop/"}]},{"title":"Neo4j 图数据库的基本使用","slug":"database/neo4j","date":"2020-06-13T14:12:59.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/neo4j/","permalink":"https://wangqian0306.github.io/2020/neo4j/","excerpt":"","text":"简介 Neo4j 是一款图数据库，相较于其他数据库可以轻松的查询数据之间的具体关系。在遇到复杂关系时使用 Neo4j 可以减少很多的工作量。 注：企业版本需要收费。但是目前官网有对初创公司的支持项目。 安装 请参照官方文档 或者可以试试 Docker 版 123456789services: neo4j: image: neo4j:latest container_name: neo4j privileged: true restart: always ports: - &quot;7474:7474&quot; - &quot;7687:7687&quot; 配置文件说明 配置文件的具体位置请参见配置文件位置表 配置项 说明 dbms.directories.data=&lt; path &gt; 数据地址 dbms.directories.plugins=&lt; path &gt; 插件地址 dbms.directories.logs=&lt; path &gt; 日志地址 dbms.directories.import=&lt; path &gt; 文件导入地址(在实际使用时可以) dbms.security.auth_enabled=&lt; boolean &gt; 鉴权开关 dbms.memory.heap.initial_size=&lt; size &gt; 初始内存分配大小 dbms.memory.heap.max_size=&lt; size &gt; 最大内存分配大小 dbms.connectors.default_listen_address=&lt; ip &gt; 默认监听地址 dbms.connector.http.enabled=&lt; boolean &gt; web界面开关 dbms.connector.http.listen_address=&lt; host &gt;:&lt; port &gt; 界面地址和端口 启动服务 在配置完成后可以使用如下命令操作 neo4j 启动服务 1neo4j start 关闭服务 1neo4j stop 查看服务状态 1neo4j status 查看数据库版本 1neo4j version 注: 如果是用 repo 进行安装的话还是可以使用 systemctl 命令进行配置的。 简单使用 在安装完成后可以访问 http://&lt;host&gt;:7474 来访问 Web UI。 在 Web UI 中有使用的具体示例，当然也可以访问 官方文档。 除了基本的查询语句之外还可以使用下面的命令来浏览一些支持性的内容 查看最近操作历史记录 1:history 查看索引信息 1:schema 播放引导提示 1:play start 查看系统状态 1:sysinfo CSV 数据导入 Neo4j 提供的导入方式有语句和导入命令两种，语句的使用更为灵活简便，所以此处仅对语句进行说明。 12LOAD CSV FROM &#x27;demo.csv&#x27; AS line FIELDTERMINATOR &#x27;;&#x27;CREATE (:Artist &#123;name: line[1], year: toInteger(line[2])&#125;) 注：在导入实际文件的时候需要将对应文件放置在 dbms.directories.import 配置项所设置的目录下。或者此处使用远程地址。 数据导出和恢复 由于备份功能为企业版专属，所以如果需要使用备份功能只可以关闭 Neo4j 服务，然后使用如下命令将数据导出成 dump 文件。 数据导出 1neo4j-admin dump --database=neo4j --to=&lt; filename &gt;.dump 数据恢复 1neo4j-admin load --from=&lt; filename &gt;.dump --database=neo4j --force 注：如果是版本是 3.x 的话需要使用的数据库是 graph.db 密码恢复 可以使用如下命令来进行密码重置，在登录 Web 界面之后使用默认密码 neo4j登录然后设定新密码即可。 1neo4j-admin set-initial-password secret --require-password-change 与 SpringBoot 集成 坦率的讲 spring-data-neo4j 根本不好用。。。还是推荐使用官方的原生包。","categories":[{"name":"Neo4j","slug":"Neo4j","permalink":"https://wangqian0306.github.io/categories/Neo4j/"}],"tags":[{"name":"Neo4j","slug":"Neo4j","permalink":"https://wangqian0306.github.io/tags/Neo4j/"}]},{"title":"NFS 服务安装及配置流程","slug":"linux/nfs","date":"2020-06-12T15:52:33.000Z","updated":"2025-01-08T02:56:21.478Z","comments":true,"path":"2020/nfs/","permalink":"https://wangqian0306.github.io/2020/nfs/","excerpt":"","text":"NFS 服务安装及配置流程 简介 NFS 是 Linux 的共享文件夹。 安装 安装 NFS 服务 1yum -y install nfs-utils 远程目录配置 新增 NFS 远程目录 1mkdir -p &lt;path&gt; 新增远程链接配置文件 1vim /etc/exports.d/&lt;config&gt;.exports 然后可以填入下面的配置，来开放远程链接 1&lt;path&gt; *(&lt;opreation&gt;,&lt;opreation1&gt;...) 其中具体的配置信息参见下表 配置项 说明 rw 可读可写 ro 只读 sync 在数据被写入持久化存储时才进行同步 async 数据持续同步 no_root_squash 当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的UID与GID都会变成nobody身份。 root_squash 如果登录 NFS 主机使用共享目录的使用者是 root，那么对于这个共享的目录来说，它具有root的权限。 no_squash 访问用户先与本机用户匹配，匹配失败后再映射为匿名用户或用户组 网络配置 使用如下命令进行防火墙配置 1234firewall-cmd --permanent --add-service=mountdfirewall-cmd --permanent --add-service=rpc-bindfirewall-cmd --permanent --add-service=nfs-serverfirewall-cmd --reload 在配置完成防火墙后可以启动服务 12systemctl enable rpcbind --nowsystemctl enable nfs-server --now 如果新增了目录可以使用如下命令刷新服务 1exportfs -a 检测 在需要连接的客户机上可以使用如下命令进行配置检测 1showmount -e &lt;host_ip&gt; 如果可以看到之前配置的目录则证明安装正常。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"OpenShift","slug":"OpenShift","permalink":"https://wangqian0306.github.io/tags/OpenShift/"},{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"NFS","slug":"NFS","permalink":"https://wangqian0306.github.io/tags/NFS/"}]},{"title":"大数据组件调优知识整理","slug":"bigdata/performance","date":"2020-06-11T15:17:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/performance/","permalink":"https://wangqian0306.github.io/2020/performance/","excerpt":"","text":"HDFS 调优 相关资料网站 hdfs+yarn 参数调优 HDFS Settings for Better Hadoop Performance 参数调整 core-site.xml io.file.buffer.size 建议值与Ambari默认值一致为131072 较大的缓存都可以提供更高的数据传输，但这也就意味着更大的内存消耗和延迟 fs.trash.interval 文件的删除时间，建议值为1440默认值为360 dfs.namenode.handler.count 默认值40建议修改值400 每个 threads 使用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加。 dfs.datanode.readahead.bytes 默认值4M 建议修改值64M 网络连接数 系统内核的net.core.somaxconn默认值128建议修改值1024 ipc.server.listen.queue.size与上文保持一致 增大打开文件数据和网络连接上限，提高hadoop集群读写速度和网络带宽使用率(对系统修改大，不建议修改) dfs.replication 默认值3建议修改值2 减少副本块可以节约存储空间和时间 hdfs-site.xml dfs.client.read.shortcircuit建议值与Ambari默认值一致为True dfs.domain.socket.path建议值与Ambari默认值一致为/var/lib/hadoop-hdfs/dn_socket dfs.namenode.avoid.read.stale.datanode建议值与Ambari默认值一致为True dfs.namenode.avoid.write.stale.datanode建议值与Ambari默认值一致为True Yarn调优 相关资料网站 YARN的Memory和CPU调优配置详解 参数调整 containers = min (2*CORES, 1.8*DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) 配置文件 配置设置 默认值 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb 8192 MB = containers * RAM-per-container yarn-site.xml yarn.scheduler.minimum-allocation-mb 1024MB = RAM-per-container yarn-site.xml yarn.scheduler.maximum-allocation-mb 8192MB = containers * RAM-per-container yarn-site.xml(check) yarn.app.mapreduce.am.resource.mb 1536MB = 2 * RAM-per-container yarn-site.xml(check) yarn.app.mapreduce.am.command-opts -Xmx1024m = 0.8 * 2 * RAM-per-container mapred-site.xml mapreduce.map.memory.mb 1024MB = RAM-per-container mapred-site.xml mapreduce.reduce.memory.mb 1024MB = 2 * RAM-per-container mapred-site.xml mapreduce.map.java.opts - = 0.8 * RAM-per-container mapred-site.xml mapreduce.reduce.java.opts - = 0.8 * 2 * RAM-per-container 参数计算 参数调整计算脚本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#!/usr/bin/env pythonimport optparsefrom pprint import pprintimport loggingimport sysimport mathimport ast&#x27;&#x27;&#x27; Reserved for OS + DN + NM, Map: Memory =&gt; Reservation &#x27;&#x27;&#x27;reservedStack = &#123; 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, 128:24, 256:32, 512:64&#125;&#x27;&#x27;&#x27; Reserved for HBase. Map: Memory =&gt; Reservation &#x27;&#x27;&#x27; reservedHBase = &#123;4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, 128:24, 256:32, 512:64&#125;GB = 1024def getMinContainerSize(memory): if (memory &lt;= 4): return 256 elif (memory &lt;= 8): return 512 elif (memory &lt;= 24): return 1024 else: return 2048 passdef getReservedStackMemory(memory): if (reservedStack.has_key(memory)): return reservedStack[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 1 return retdef getReservedHBaseMem(memory): if (reservedHBase.has_key(memory)): return reservedHBase[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 2 return ret def main(): log = logging.getLogger(__name__) out_hdlr = logging.StreamHandler(sys.stdout) out_hdlr.setFormatter(logging.Formatter(&#x27; %(message)s&#x27;)) out_hdlr.setLevel(logging.INFO) log.addHandler(out_hdlr) log.setLevel(logging.INFO) parser = optparse.OptionParser() memory = 0 cores = 0 disks = 0 hbaseEnabled = True parser.add_option(&#x27;-c&#x27;, &#x27;--cores&#x27;, default = 16, help = &#x27;Number of cores on each host&#x27;) parser.add_option(&#x27;-m&#x27;, &#x27;--memory&#x27;, default = 64, help = &#x27;Amount of Memory on each host in GB&#x27;) parser.add_option(&#x27;-d&#x27;, &#x27;--disks&#x27;, default = 4, help = &#x27;Number of disks on each host&#x27;) parser.add_option(&#x27;-k&#x27;, &#x27;--hbase&#x27;, default = &quot;True&quot;, help = &#x27;True if HBase is installed, False is not&#x27;) (options, args) = parser.parse_args() cores = int (options.cores) memory = int (options.memory) disks = int (options.disks) hbaseEnabled = ast.literal_eval(options.hbase) log.info(&quot;Using cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;GB&quot; + &quot; disks=&quot; + str(disks) + &quot; hbase=&quot; + str(hbaseEnabled)) minContainerSize = getMinContainerSize(memory) reservedStackMemory = getReservedStackMemory(memory) reservedHBaseMemory = 0 if (hbaseEnabled): reservedHBaseMemory = getReservedHBaseMem(memory) reservedMem = reservedStackMemory + reservedHBaseMemory usableMem = memory - reservedMem memory -= (reservedMem) if (memory &lt; 2): memory = 2 reservedMem = max(0, memory - reservedMem) memory *= GB containers = int (min(2 * cores, min(math.ceil(1.8 * float(disks)), memory/minContainerSize))) if (containers &lt;= 2): containers = 3 log.info(&quot;Profile: cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;MB&quot; + &quot; reserved=&quot; + str(reservedMem) + &quot;GB&quot; + &quot; usableMem=&quot; + str(usableMem) + &quot;GB&quot; + &quot; disks=&quot; + str(disks)) container_ram = abs(memory/containers) if (container_ram &gt; GB): container_ram = int(math.floor(container_ram / 512)) * 512 log.info(&quot;Num Container=&quot; + str(containers)) log.info(&quot;Container Ram=&quot; + str(container_ram) + &quot;MB&quot;) log.info(&quot;Used Ram=&quot; + str(int (containers*container_ram/float(GB))) + &quot;GB&quot;) log.info(&quot;Unused Ram=&quot; + str(reservedMem) + &quot;GB&quot;) log.info(&quot;yarn.scheduler.minimum-allocation-mb=&quot; + str(container_ram)) log.info(&quot;yarn.scheduler.maximum-allocation-mb=&quot; + str(containers*container_ram)) log.info(&quot;yarn.nodemanager.resource.memory-mb=&quot; + str(containers*container_ram)) map_memory = container_ram reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram am_memory = max(map_memory, reduce_memory) log.info(&quot;mapreduce.map.memory.mb=&quot; + str(map_memory)) log.info(&quot;mapreduce.map.java.opts=-Xmx&quot; + str(int(0.8 * map_memory)) +&quot;m&quot;) log.info(&quot;mapreduce.reduce.memory.mb=&quot; + str(reduce_memory)) log.info(&quot;mapreduce.reduce.java.opts=-Xmx&quot; + str(int(0.8 * reduce_memory)) + &quot;m&quot;) log.info(&quot;yarn.app.mapreduce.am.resource.mb=&quot; + str(am_memory)) log.info(&quot;yarn.app.mapreduce.am.command-opts=-Xmx&quot; + str(int(0.8*am_memory)) + &quot;m&quot;) log.info(&quot;mapreduce.task.io.sort.mb=&quot; + str(int(0.4 * map_memory))) passif __name__ == &#x27;__main__&#x27;: try: main() except(KeyboardInterrupt, EOFError): print(&quot;\\nAborting ... Keyboard Interrupt.&quot;) sys.exit(1) 用法示例 1python test.py -c 32 -m 128 -d 2 -k True 样例输出 12345678910111213141516Using cores=32 memory=128GB disks=2 hbase=TrueProfile: cores=32 memory=81920MB reserved=48GB usableMem=80GB disks=2Num Container=4Container Ram=20480MBUsed Ram=80GBUnused Ram=48GByarn.scheduler.minimum-allocation-mb=20480yarn.scheduler.maximum-allocation-mb=81920yarn.nodemanager.resource.memory-mb=81920mapreduce.map.memory.mb=20480mapreduce.map.java.opts=-Xmx16384mmapreduce.reduce.memory.mb=20480mapreduce.reduce.java.opts=-Xmx16384myarn.app.mapreduce.am.resource.mb=20480yarn.app.mapreduce.am.command-opts=-Xmx16384mmapreduce.task.io.sort.mb=8192 Kafka调优 broker 处理消息的最大线程数（默认为3） num.network.threads=cpu核数+1 broker 处理磁盘IO的线程数 num.io.threads=cpu核数*2","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wangqian0306.github.io/tags/Hadoop/"},{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"}]},{"title":"Kafka CLI","slug":"bigdata/kafka-cli","date":"2020-06-09T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/kafka-cli/","permalink":"https://wangqian0306.github.io/2020/kafka-cli/","excerpt":"","text":"Kafka 相关命令梳理 容器化试用 Confluent 版本 可以使用如下命令安装项目： 123git clone https://github.com/sknop/kafka-docker-composercd kafka-docker-composerpip3 install jinjia2 然后使用如下命令创建 docker-compose 文件： Zookeeper 版 1python3 kafka_docker_composer.py -b 1 -z 1 KRaft 版 1python3 kafka_docker_composer.py --brokers 1 --controllers 1 开启容器 1docker-compose up -d 官方镜像版 编写 docker-compose.yaml 文件，然后输入如下内容 12345678910111213141516171819202122services: broker: image: apache/kafka:latest hostname: broker container_name: broker ports: - &#x27;9092:9092&#x27; environment: KAFKA_NODE_ID: 1 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: &#x27;CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT&#x27; KAFKA_ADVERTISED_LISTENERS: &#x27;PLAINTEXT_HOST://localhost:9092,PLAINTEXT://broker:19092&#x27; KAFKA_PROCESS_ROLES: &#x27;broker,controller&#x27; KAFKA_CONTROLLER_QUORUM_VOTERS: &#x27;1@broker:29093&#x27; KAFKA_LISTENERS: &#x27;CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092&#x27; KAFKA_INTER_BROKER_LISTENER_NAME: &#x27;PLAINTEXT&#x27; KAFKA_CONTROLLER_LISTENER_NAMES: &#x27;CONTROLLER&#x27; CLUSTER_ID: &#x27;4L6g3nShT-eMCtK--X86sw&#x27; KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 KAFKA_LOG_DIRS: &#x27;/tmp/kraft-combined-logs&#x27; 开启容器 1docker-compose up -d kafka-topics.sh 参数解释: 参数名 说明 --alter 增加主题分区数量(在已经使用的话题中永远不要增加新分区) --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --command-config &lt;path&gt; 配置文件地址 --config &lt;name&gt;=&lt;value&gt; 配置项 --create 创建话题标识 --delete 删除话题标识 --describe 展示话题标识 --list 话题列表的标识 --replication-factor &lt;num&gt; 副本因子 --partitions &lt;num&gt; 数据分区 --topic &lt;name&gt; 话题名 注：如果需要删除话题需要在 server.properties 配置文件中编辑 delete.topic.enable=true 配置项。在标记删除的话题停止使用后 Kafka 才会真正清除话题。 config 配置项目如下： 参数名 说明 cleanup.policy 数据清理策略 compression.type 压缩类型 delete.retention.ms 删除时间周期 file.delete.delay.ms 文件删除延时 flush.messages 写入磁盘的消息数量 flush.ms 写入磁盘的时间间隔 follower.replication.throttled.replicas 应在从属节点端限制日志复制的副本列表 index.interval.bytes 索引间隔文件大小 leader.replication.throttled.replicas 应在领导节点端限制日志复制的副本列表 max.compaction.lag.ms 消息在日志中不符合压缩条件的最长时间 max.message.bytes 允许的最大记录批量大小 message.format.version 消息格式版本呢 message.timestamp.difference.max.ms broker 接收消息时的时间戳与消息中指定的时间戳之间允许的最大差异 message.timestamp.type 消息时间戳的类型 min.cleanable.dirty.ratio 日志压缩频率 min.compaction.lag.ms 未压缩日志的保存时间 min.insync.replicas 最少的同步副本数(ack=-1) preallocate 预分配磁盘文件 retention.bytes 保留文件大小 retention.ms 保留文件时间长度 segment.bytes 日志文件分片大小 具体内容请参见官方手册。 kafka-console-producer.sh 参数解释: 参数名 说明 --batch-size &lt;num&gt; 发送缓冲区,默认为200 --broker-list &lt;kafka_host&gt;:&lt;kafka_port&gt;,&lt;kafka_host1&gt;:&lt;kafka_port1&gt; kafka Broker --compression-codec 数据压缩,可选 none,gzip,snappy,lz4,zstd 默认为 gzip --producer-property &lt;name&gt;=&lt;value&gt; 配置项 --producer.config &lt;path&gt; 配置文件地址 --topic &lt;name&gt; 话题名 kafka-console-consumer.sh 参数解释: 参数名 说明 --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --consumer-property &lt;name&gt;=&lt;value&gt; 配置项 --consumer.config &lt;path&gt; 配置文件地址 --from-beginning 从头开始消费 --group &lt;name&gt; 消费组 --offset &lt;offset&gt; 消费偏移量，可填入数字或者 earliest 和 latest，默认为 latest --partition 分区 --topic &lt;name&gt; 话题名 kafka-reassign-partitions.sh 参数解释: 参数名 说明 --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --topics-to-move-json-file &lt;config_file&gt; 移动配置项 --broker-list &lt;broker_id&gt;,&lt;broker_id&gt; 目标设备 ID --generate 依据配置文件生成分配计划 --execute 执行重新分配计划 --verify 检查分区计划是否成功 --throttle 限速大小(Byte) 注：–execute 和 --verify 参数所采用的配置项是 --generate 参数生成的。 kafka-preferred-replica-election.sh 此工具使每个分区的领导权转移回“首选副本”，它可用于平衡服务器之间的领导权。 参数名 说明 --zookeeper &lt;zk_host_1&gt;:&lt;zk_port_1&gt;,&lt;zk_host_2&gt;:&lt;zk_port_2&gt;/&lt;chroot&gt; zookeeper 地址 --path-to-json-file 配置文件地址 kafka-consumer-groups.sh 此工具可以列出所有的消费组详情，删除位移(offset)，重置消费组位移。 消费组的重设策略如下： 纬度 策略 含义 位移维度 Earliest 最早 位移维度 Latest 当前最新 位移维度 Current 最新一次提交 位移维度 Specified-Offset 指定位移 位移维度 Shift-By-N 当前位移 +N 时间维度 DateTime 调整到大于给定时间的最小位移处 时间维度 Duration 调整到距离当前时间间隔的位移处 参数名 说明 --all-topics 管理所有话题 --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --by-duration Duration 策略，格式为 PnDTnHnMnS --command-config &lt;conf_path&gt; 配置文件地址 --delete 删除位移(将目标组以如下形式传递 --group g1 --group g2) --describe 显示位移信息 --dry-run 展示运行之后的情况(支持参数 reset-offsets) --execute 执行操作(支持参数 reset-offsets) --export 导出到csv(支持参数 reset-offsets) --from-file &lt;conf_path&gt; 将导入位移 --group 目标消费组 --list 列出消费组 --members 列出组成员 --offsets 查看位移 --reset-offsets 重置位移，可以配合试运行，执行操作，导出csv一起使用 --shift-by &lt;Long: N&gt; Shift-By-N策略，移动 N 个 --state 显示状态 --timeout &lt;Long: timeout (ms)&gt; 超时时间，默认 5000 --to-current Current 策略 --to-datetime &lt;String: datetime&gt; DateTime 策略 --to-earliest Earliest 策略 --to-latest Latest 策略 --to-offset &lt;Long: offset&gt; Specified-Offset 策略 --topic &lt;String: topic&gt; 目标主题，在“重置偏移量”时，可以使用以下格式指定分区：topic1:0,1,2，其中0,1,2是要包括在进程中的分区。 --verbose 提供详细信息 kafka-log-dirs.sh 查询各个 Broker 上的日志路径磁盘占用情况。 参数名 说明 --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --broker-list &lt;String: Broker list&gt; Broker 列表，样例如下 0,1,2 --command-config &lt;conf_path&gt; 配置文件地址 --describe 查看详情 --topic-list &lt;String: Topic list&gt; 话题列表，样例如下 topic1,topic2,topic3 kafka-consumer-perf-test.sh 消费者性能测试 kafka-producer-pref-test.sh 生产者性能测试 kafka-dump-log.sh 此工具有助于解析日志文件并将其内容转储到控制台，这对于调试看似损坏的日志段非常有用。 参数名 说明 --files &lt;String: file1,file2,...&gt; 储存文件列表 --max-message-size &lt;Integer: size&gt; 最大消息长度 --print-data-log 同时打印至控制台 --print-data-log 不输出元数据 kafka-delete-records.sh 此工具有助于将给定分区的记录向下删除到指定的偏移量。 参数名 说明 --bootstrap-server &lt;kafka_host&gt;:&lt;kafka_port&gt; Kafka 地址及端口(必填) --command-config &lt;conf_path&gt; 配置文件地址 --offset-json-file &lt;path&gt; 删除位移文件地址 删除位移配置文件： 12345678910&#123; &quot;partitions&quot;: [ &#123; &quot;topic&quot;: &quot;foo&quot;, &quot;partition&quot;: 1, &quot;offset&quot;: 1 &#125; ], &quot;version&quot;: 1&#125; 存储消息(日志)片段 1kafka-run-class.sh kafka.tools.DullplogSegllents --files 00000000000052368601.log --print-data-log 自动创建主题 默认情况下，Kafka 会在如下几种情形下自动创建主题： 当一个生产者开始往主题中写入消息时； 当一个消费者开始从出题读取消息时； 当任意一个客户端向主题发送元数据请求时。 如何选择分区数量 为主题选定分区数量并不是一件可有可无的事情，在进行数量选择时，需要考虑如下几个因素。 主题需要达到多大的吞吐量？例如，是希望每秒钟写入 1OO KB 还是 1GB? 从单个分区读取数据的最大吞吐量是多少？每个分区一般都会有一个消费者，如果你知道消费者将数据写入数据库的速度不会超过每秒 50 MB，那么你也该知道，从一个分区读取数据的吞吐量不需要超过每秒 50 MB。 可以通过类似的方法估算生产者向单个分区写入数据的吞吐量，不过生产者的速度一般比消费者快得多，所以最好为生产者多估算一些吞吐量。 每个 broker 包含的分区个数、可用的磁盘空间和网络带宽。 如果消息是按照不同的键采写入分区的，那么为已有的主题新增分区就会很困难。 单个 broker 对分区个数是有限制的，因为分区越多，占用的内存越多，完成首领选举需要的时间也越长。 根据经验，把分区的大小限制在 25 GB 以内可以得到比较理想的效果。 强制删除话题 在某些话题无法删除完成的时候可以直接操作 zookeeper 实现数据清除。 首先需要定位到数据存储路径，检查 server.properties 文件中的 log.dirs 配置项，然后记录下来。 然后使用如下命令进入 zookeeper 1zookeeper-shell.sh &lt;zookeeper_host&gt;:&lt;zookeeper_port&gt; 然后使用下面的命令删除 zookeeper 数据 1rmr /brokers/topics/&lt;name&gt; 如果之前已经设置过了 marked for deletion 则需要运行下面的语句 1rmr /admin/delete_topics/&lt;name&gt; 然后在文件夹中删除真实的数据文件即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://wangqian0306.github.io/tags/ZooKeeper/"}]},{"title":"Kafka Connect","slug":"bigdata/kafka-connect","date":"2020-06-09T14:43:13.000Z","updated":"2025-01-08T02:56:21.462Z","comments":true,"path":"2020/kafka-connect/","permalink":"https://wangqian0306.github.io/2020/kafka-connect/","excerpt":"","text":"Kafka Connect 简介 Kafka Connect 是一款接收数据库记录到 Kafka (source)和从 Kafka 将数据写入数据库(sink)的工具。 整体架构如下： 使用 Kafka Connect 随着 Kafka 本体一起发布无须单独安装，但是如果你打算在生产环境使用它来移动大量的数据，或者打算运行多个连接器，那么最好把 Connect 部署在独立于 broker 的服务器上。 修改配置文件 配置项 说明 bootstrap.servers Kafka Broker 地址 group.id 消费组 ID key.converter 键转换器 value.converter 值转换器 启动 worker 分布式启动 1bin/connect-distributed.sh config/connect-distributed.properties 单机启动 1bin/connect-standalone.sh config/connect-distributed.properties 检测服务状态 访问服务地址 1http://localhost:8083/ 使用文件插件读取数据至 Kafka 1echo &#x27;&#123;&quot;name&quot;:&quot;load-kafka-config&quot;, &quot;config&quot;:&#123;&quot;connector.class&quot;:&quot;FileStream-Source&quot;,&quot;file&quot;:&quot;config/server.properties&quot;,&quot;topic&quot;:&quot;kafka-config-topic&quot;&#125;&#125;&#x27; | curl -X POST -d @- http://localhost:8083/connectors --header &quot;content-Type:application/json&quot; 读取话题查看链接情况 1bin/kafka-console-consumer.sh --new --bootstrap-server=localhost:9092 --topic kafka-config-topic --from-beginning 从 Kafka 写入文件 1echo &#x27;&#123;&quot;name&quot;:&quot;dump-kafka-config&quot;, &quot;config&quot;:&#123;&quot;connector.class&quot;:&quot;FileStreamSink&quot;,&quot;file&quot;:&quot;copy-of-serverproperties&quot;,&quot;topics&quot;:&quot;kafka-config-topic&quot;&#125;&#125;&#x27; | curl -X POST -d @- http://localhost:8083/connectors --header &quot;content-Type:application/json&quot; 查看文件 1cat copy-of-server-properties 删除链接器 12curl -X DELETE http://localhost:8083/connectors/load-kafka-configcurl -X DELETE http://localhost:8083/connectors/dump-kafka-config 概念和原理 Kafka Connect 由如下内容组成： 连接器和任务(Connectors and tasks) 连接器负责以下三个内容： 决定需要多少任务 按照任务来拆分数据并进行复制 从 worker 进程获取任务配置并将其进行传递 任务负责如下内容： 将数据移入或移出 Kafka worker 进程 worker 进程是连接器和任务的 “容器”。它们负责： REST API 配置管理 可靠性、高可用性、伸缩性和负载均衡","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"}]},{"title":"zookeeper 基础使用","slug":"bigdata/zookeeper-cli","date":"2020-06-09T14:43:13.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/zookeeper-cli/","permalink":"https://wangqian0306.github.io/2020/zookeeper-cli/","excerpt":"","text":"数据模型 ZooKeeper 提供的命名空间和标准的Linux文件系统是很类似的。每个数据节点都存储在 ZooKeeper 命名空间的相应‘目录下’。 每个数据节点都具有以下的属性： 属性 说明 cZxid 数据节点创建时的事务ID ctime 数据节点创建时间 mZxid 数据节点被修改时的事务ID mtime 数据节点修改时间 pZxid 数据节点的子节点最后一次被修改时的事务ID cversion 子节点的更改次数 dataVersion 节点数据的更改次数 aclVersion 节点 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点会话的 SessionID;如果是持久节点则为0x0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 CLI 链接到 ZooKeeper 1./zkCli.sh -server &lt;ip&gt;:&lt;port&gt; 创建节点 1create [-s] [-e] [-c] [-t ttl] path [data] [acl] 获取节点信息 1get [-s] [-w] path 查看节点列表 1ls [-s] [-w] [-R] path 查看节点和属性列表 1ls2 path [watch] 获取当前节点属性 1stat [-w] path 修改节点 1set [-s] [-v version] path data 删除节点 1rmr &lt;path&gt; 参数说明: 参数 说明 -w 注册(观察数据变化) -e 临时数据(重启或者超时消失) -s 含有序列(补充编号) -R 递归 ACL 可以使用下面的命令来配置 ACL 1setAcl [-s] [-v version] [-R] path acl acl 的结构如下： 1&lt;授权模式&gt;:&lt;授权对象&gt;:&lt;具体权限&gt; 授权模式 描述 world 只有一个用户: anyone，代表登录的所有人(默认) ip IP地址认证 auth 使用已经添加认证的用户认证 digest 使用“用户名:密码”的方式认证 权限 具体权限 描述 create c 创建 delete d 删除子节点 read r 读取节点数据和显示子节点 write w 设置节点数据 a a 可以管理权限 除此之外还可以 查看权限 1getAcl [-s] path 添加认证用户(登录) 1addauth scheme auth 注：在使用 auth 模式的时候需要先登录然后在配置权限。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://wangqian0306.github.io/tags/ZooKeeper/"}]},{"title":"Redis 集群和高可用","slug":"database/redis-cluster","date":"2020-06-08T13:58:01.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/redis-cluster/","permalink":"https://wangqian0306.github.io/2020/redis-cluster/","excerpt":"","text":"主从模式 可以在 redis.conf 中编写下面的配置项来实现主从模式配置： 配置项 说明 replicof 主机地址 masterauth 主机密码 masteruser 主机用户名 replica-read-only 从机是否可以写数据 repl-backlog-size 数据同步缓冲区大小 在 Redis 6.0 中，将原先的slaveof 配置项修改为了 replicof 哨兵模式 哨兵模式是 Redis 官方提供的高可用解决方案。 注：哨兵模式并不负责数据相关内容仅仅负责在 Redis 中选举！ 可以在 sentinel.conf 中编写下面的配置来实现哨兵模式配置： 配置项 说明 bind 监听服务地址 port 哨兵模式端口 sentinel monitor mymaster 在 quorum 台设备同时检测到服务下线时进行重新选举 sentinel down-after-milliseconds mymaster 主机最大无响应时长 sentinel parallel-syncs mymaster 设置在故障转移后可以重新配置为使用新主服务器的副本数 sentinel failover-timeout mymaster 故障转移超时配置 在配置完成后可以使用如下命令启动哨兵服务。 1redis-server sentinel.conf --sentinel 集群模式 相较于哨兵模式，集群模式可以更好的利用系统资源缓解数据写入压力。 可以在 redis.conf 中编写下面的配置项来开启集群模式： 配置项 说明 cluster-enabled 集群模式开关 cluster-config-file 集群模式配置文件名 cluster-node-timeout 集群超时时间 在配置完成上述内容后可以正常启动服务，然后使用如下的命令来创建集群： 1redis-cli --cluster create &lt;ip1&gt;:&lt;port1&gt; &lt;ip2&gt;:&lt;port2&gt; ... --cluster-replicas 1 注：其中 cluster-replicas 参数表示的是1个主机所对应的从机数量。 在命令运行完成后 redis 集群会进行相应初始化操作，可以使用 cli 的方式进行功能验证： 1redis-cli -c 注：-c 为集群模式的标识。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"Redis 内存淘汰知识整理","slug":"database/redis-memory","date":"2020-06-07T09:48:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/redis-memory/","permalink":"https://wangqian0306.github.io/2020/redis-memory/","excerpt":"","text":"简介 Redis 可以通过 maxmemory 参数来限制最大可用内存，主要为了避免内存溢出，从而导致服务器响应变慢甚至死机的情况。 注：这是64位系统的默认将 maxmemory 设定为了0(代表不限制大小)，而32位系统则默认为3 GB。 内存淘汰策略 在 Redis 使用内存达到配置上限的时候可以设定 maxmemory-policy 配置项为下面的策略来进行回收： 配置项 说明 noeviction 返回写入错误(默认) allkeys-lru 在所有键中采用 LRU 算法删除键，直到腾出足够内存为止 volatile-lru 在设置了过期时间的键中采用 LRU 算法删除键，直到腾出足够内存为止 allkeys-random 在所有键中采用随机删除键，直到腾出足够内存为止 volatile-random 在设置了过期时间的键中随机删除键，直到腾出足够内存为止 volatile-ttl 在设置了过期时间的键空间中，具有更早过期时间的键优先移除 allkeys-lfu 在所有键中采用 LFU 算法删除键，直到腾出足够内存为止 volatile-lfu 在设置了过期时间的键中采用 LFU 算法删除键，直到腾出足够内存为止 那么 LRU 算法和 LFU 算法有什么差异呢 LFU(Least Frequently Used)：最近最少使用，跟使用的次数有关，淘汰使用次数最少的。 LRU(Least Recently Used)：最近最不经常使用，跟使用的最后一次时间有关，淘汰最近使用时间离现在最久的。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"IDEA 常用內容整理","slug":"tmp/idea","date":"2020-06-06T13:41:32.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2020/idea/","permalink":"https://wangqian0306.github.io/2020/idea/","excerpt":"","text":"快捷键 全局可用 快捷键 说明 ctrl+shift+f12 全屏 Editor alt+home 展开当前目录 ctrl+t 拉取代码(git) alt+~ 版本控制 alt+1 打开或者关闭 Project 选项卡 shift+esc 关闭小窗 双击shift 全局文件检索 ctrl+e 打开最近编辑的文件 ctrl+n 文件查找 ctrl+w 扩大选中区域 ctrl+shift+f 全局内容查找 ctrl+shift+r 全局内容替换 Project 部分 快捷键 说明 ← 关闭文件夹 → 打开单个文件夹 * 打开选中目录下的所有文件夹 alt+←/→ 切换视图种类 alt+insert 在选中目录下新建文件或者文件夹 Editor 部分 快捷键 说明 ctrl+tab 切换已打开的文件 alt+←/→ 切换编辑器内的文件 alt+↑/↓ 跳转至下一个方法/目录 ctrl+f12 展示文件结构 ctrl+h 显示类结构图 ctrl++ 展开代码 ctrl+- 折叠代码 ctrl+shift++ 展开全部代码 ctrl+shift+- 折叠全部代码 ctrl+[ 移动至括号开始位置 ctrl+] 移动至括号结束位置 ctrl+f1 错误提示说明 f2 跳转至下一处错误或者警告 shift+f2 跳转至上一处警告 ctrl+f3 跳转至选中元素的下一处引用 alt+insert 快速生成 ctrl+shift+space 信息提示 alt+enter 自动修复/提示 ctrl+p 提示方法参数 ctrl+alt+←/→ 最近编辑位置跳转 ctrl+f 文件内搜索 ctrl+r 文件内替换 ctrl+b 查看类/接口/方法的调用 ctrl+alt+b 查看实现方法 ctrl+q 查看文档 ctrl+shift+i 查看方法或者类的定义 shift+↑/↓ 选择行 ctrl+/ 单行注释 ctrl+d 复制行 ctrl+x 删除行 ctrl+shift+/ 多行注释 alt+shift+↑/↓ 调整代码位置 ctrl+shift+↑/↓ 调整方法位置 ctrl+alt+i 格式化缩进 ctrl+alt+o 格式化引入 ctrl+alt+l 格式化代码 ctrl+shift+t 生成/跳转至测试类 ctrl+f4 关闭当前编辑文件 终端部分 快捷键 说明 alt+f12 切换至终端 TODO 部分 快捷键 说明 alt+6 切换至 TODO 窗口 调试与运行 缩写 说明 双击ctrl 运行命令或者程序 ctrl+f9 编译项目 ctrl+shift+f9 以 Debug 模式运行 Editor 当前打开的代码 ctrl+shift+f10 运行 Editor 当前打开的代码 shift+f9 以 Debug 模式运行项目(目前的 debug/run configuration) shift+f10 运行项目(目前的 debug/run configuration) ctrl+f5 重新启动程序 ctrl+f2 终止程序运行 alt+f10 显示断点 f8 进入下一步 f9 进入代码 代码缩写 缩写 说明 psvm public static void main psf public static final prsf private static final sout System.out.println serr System.err.println Http 请求工具 随着 IDEA 的更新，目前 http 请求工具现在也可以通过请求设置参数了，比较方便进行测试。 在请求前可以声明当前请求的所属环境配置文件 http-client.env.json： 1234567891011&#123; &quot;local&quot;: &#123; &quot;host&quot;: &quot;localhost:8080&quot; &#125;, &quot;development&quot;: &#123; &quot;host&quot;: &quot;xxx.xxx.xxx&quot; &#125;, &quot;production&quot;: &#123; &quot;host&quot;: &quot;xxx.xxx.xxx&quot; &#125;&#125; 然后即可编写请求 check.http： 123456789101112131415### LOGINPOST http://&#123;&#123;host&#125;&#125;/api/v1/user/loginContent-Type: application/json&#123; &quot;username&quot;: &quot;xxxxx&quot;, &quot;password&quot;: &quot;xxxxx&quot;&#125;&gt; &#123;% client.global.set(&quot;token&quot;, response.body.token); %&#125;### GET_CURRENT_USERGET http://&#123;&#123;host&#125;&#125;/api/v1/userAuthorization: Bearer &#123;&#123;token&#125;&#125;Content-Type: application/json 常见问题 在 Linux 环境中无法输入中文 首先需要清除缓存 1rm ~/.cache/ibus/libpinyin/user.conf 点击菜单 Help | Edit Custom VM options... 添加如下内容 1-Drecreate.x11.input.method=true 重启IDEA 注：如果此处遇到问题，建议切换至 fcitx 输入法。","categories":[{"name":"JetBrains","slug":"JetBrains","permalink":"https://wangqian0306.github.io/categories/JetBrains/"}],"tags":[{"name":"JetBrains","slug":"JetBrains","permalink":"https://wangqian0306.github.io/tags/JetBrains/"}]},{"title":"Redis 持久化知识整理","slug":"database/redis-storage","date":"2020-06-04T13:41:32.000Z","updated":"2025-01-08T02:56:21.466Z","comments":true,"path":"2020/redis-storage/","permalink":"https://wangqian0306.github.io/2020/redis-storage/","excerpt":"","text":"持久化 Redis 的数据持久化选项有以下两种： RDB AOF 在官方文档中明确说明： 如果可以承受少量的数据损失，可以单独使用 RDB 作为存储方式。 单独使用 AOF 也是不被推荐的，因为配合 RDB 可以实现更快的重启，并且当 AOF 存在错误时仍可以通过 RDB 找回数据。 RDB(数据库快照) RDB(Redis Database)，RDB 可以通过配置项来实现或者使用 SAVE 或者 BGSAVE 命令实现手动备份，这两个命令的区别在于： SAVE 命令会造成阻塞，直到 RDB 文件创建完毕为止，服务器都不能处理任何命令请求。 BGSAVE 命令会派生出一个子进程，然后由子进程负责创建 RDB 文件，父进程继续处理命令请求。 详细配置请参照 RDB 相关配置项表： 配置项 说明 save 在 m 秒内有 n 次数据变化则为数据库创建快照(此配置项可以书写多个) stop-writes-on-bgsave-error 在制作快照失败时阻止写入(此配置项默认为 yes) rdbcompression 使用 LZF 算法压缩快照文件(此配置项默认为 yes) rdbchecksum 快照文件格式校验 dbfilename RDB 文件名 dir 快照存储地址 优点： 备份方便，便于还原不同的版本，存储效率高 利于灾难恢复 对性能上的影响小 可以使集群快速重启 缺点： 数据可能产生丢失 如果数据量较大则需要较长的时间处理 AOF(日志记录) AOF(Append Only File)，利用独立日志的方式记录写命令，在重启时重新执行 AOF 文件中的命令来实现数据恢复。 在实现的过程中，AOF 有以下三种策略： always(每次都执行) everysec(每秒) no(系统控制) 注：也可以使用手动输入命令 BGREWRITEAOF 触发。 详细配置请参照 AOF 相关配置表： 配置项 说明 appendonly AOF 功能开关(可选: yes,no) appendfilename AOF 日志文件(默认值为 appendonly.aof) appendfsync 执行策略(可选: always,everysec,no) no-appendfsync-on-rewrite 在有线程调用时不进行 AOF 写入操作，如果开启可优化性能但是可能丢 30s 的数据(可选：yes,no) auto-aof-rewrite-percentage 在AOF增长大小/AOF文件大小&gt;=时进行自动重写 auto-aof-rewrite-min-size 当 AOF 文件达到时进行自动重写 aof-load-truncated 在恢复数据时是否忽略最后一条可能存在问题的指令(默认值为 yes) dir 数据存储地址 注：重写 AOF 文件可以减少恢复时的资源损耗，优化日志文件的存储空间。 优点： 持久化策略更为灵活 备份文件不容易产生损坏的问题 可以通过重写的思路优化日志文件 利于手动编辑 缺点： 与 RDB 相较而言 AOF 要占用更多的存储空间 效率可能低于 RDB Redis 7.0 更新 从 Redis 7.0.0 开始，当计划进行 AOF 重写时，Redis 父进程会打开一个新的增量 AOF 文件(incremental AOF file)以继续写入。子进程将执行重写逻辑并生成新的基本 AOF(base AOF)。Redis 将使用临时清单文件(temporary manifest file)来跟踪新生成的基本文件和增量文件。在准备就绪后，Redis 将执行原子替换操作以使此临时清单文件生效。为了避免在 AOF 重写重复失败和重试的情况下创建许多增量文件的问题，Redis 引入了 AOF 重写限制机制，以确保以越来越慢的速度重试失败的 AOF 重写。 注：由于目前版本 AOF 是多个文件，所以在备份 AOF 文件的时候需要先关闭 AOF 自动重写，且确保有正在进行重写，如果正在进行重写需要等待重写完成之后再去复制文件。在复制完成后重新打开 AOF 重写功能。 RDB VS AOF 将以上两种方式进行对比可以得到下表： 持久化方式 RDB AOF 占用存储空间 小 大 存储速度 慢 快 恢复速度 快 慢 数据安全性 容易丢失一段时间的数据 依据执行策略决定 资源消耗 高 低 启动优先级 低 高 混合持久化 在 Redis 4.0 版本之后新增了混合持久化的方案，可以修改如下的配置项： 配置项 说明 aof-use-rdb-preamble 混合存储功能开关(可选: yes,no) 注：需要打开 AOF 才能使用此功能。 实现思路 Redis 是通过如下方式来实现混合持久化的。 1[RDB file][AOF tail] 在 Redis 恢复数据时，如果读取到命令以 REDIS 字符串开始则使用 RDB 文件恢复，然后再去执行剩余的 AOF 命令。 AOF 文件修复 由于磁盘空间不足，系统故障等原因 AOF 文件可能会存在错误，此时会有这样的日志： 12345* Reading RDB preamble from AOF file...* Reading the remaining AOF tail...# !!! Warning: short read while loading the AOF file !!!# !!! Truncating the AOF at offset 439 !!!# AOF loaded anyway because aof-load-truncated is enabled 此时就需要制作 AOF 文件的备份，然后尝试使用如下命令修复 AOF 文件 1redis-check-aof --fix &lt;filename&gt; 然后使用此 AOF 文件启动服务器即可。 参考资料 Redis persistence","categories":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"}]},{"title":"OpenShift 相关内容整理","slug":"kubernetes/openshift","date":"2020-04-07T12:52:13.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/openshift/","permalink":"https://wangqian0306.github.io/2020/openshift/","excerpt":"","text":"简介 OpenShift 是红帽“加强”过的 Kubernetes。 官方文档(v3.11.0) Yaml 模板 DeploymentConfig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: apps.openshift.io/v1kind: DeploymentConfigmetadata: name: demo labels: app: demospec: template: metadata: labels: app: demo spec: hostname: demo containers: - env: - name: SPRING_PROFILES_ACTIVE valueFrom: configMapKeyRef: key: DEFAULT name: profile - name: PORT value: &quot;8080&quot; livenessProbe: failureThreshold: 3 httpGet: path: /actuator/health port: 5273 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 readinessProbe: failureThreshold: 3 httpGet: path: /actuator/health port: 5273 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 image: docker.io/wqnice/demo:0.1.0 imagePullPolicy: Always name: demo ports: - containerPort: 8080 protocol: TCP replicas: 1 strategy: type: Rolling paused: false revisionHistoryLimit: 2 minReadySeconds: 0 Service 1234567891011121314kind: ServiceapiVersion: v1metadata: labels: app: demo name: demospec: ports: - name: http port: 8080 protocol: TCP targetPort: 8080 selector: app: demo Route 12345678910111213141516kind: RouteapiVersion: v1metadata: labels: app: demo name: demospec: host: demo.apps.&lt;hostname&gt; path: &quot;/&quot; port: targetPort: http to: kind: Service name: demo weight: 100 wildcardPolicy: None Template 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: Templatemetadata: name: redis-template annotations: description: &quot;Description&quot; iconClass: &quot;icon-redis&quot; tags: &quot;database,nosql&quot;parameters:- description: Password used for Redis authentication from: &#x27;[A-Z0-9]&#123;8&#125;&#x27; generate: expression name: REDIS_PASSWORDmessage: &#x27;demo description&#x27;labels: redis: masterobjects:- apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: redis spec: template: metadata: labels: app: redis spec: hostname: redis containers: - image: docker.io/wqnice/demo:0.1.0 imagePullPolicy: Always name: demo ports: - containerPort: 8080 protocol: TCP replicas: 1 strategy: type: Rolling paused: false revisionHistoryLimit: 2 minReadySeconds: 0 ConfigMap 123456789101112kind: ConfigMapapiVersion: v1metadata: name: example-config namespace: defaultdata: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3 PersistentVolume 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumemetadata: finalizers: - kubernetes.io/pv-protection name: demo-pvspec: accessModes: - ReadWriteMany capacity: storage: 10Gi claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: demo-pvc nfs: path: /demo server: 172.25.2.1 persistentVolumeReclaimPolicy: Retain storageClassName: demo PersistentVolumeCliam 12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: finalizers: - kubernetes.io/pvc-protection name: demo-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi volumeName: demo-pv CronJobs 123456789101112131415161718apiVersion: batch/v1beta1kind: CronJobmetadata: name: demospec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: metadata: labels: parent: &quot;cronjobpi&quot; spec: containers: - name: pi image: perl command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;] restartPolicy: OnFailure DaemonSet 12345678910111213141516171819202122apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: demospec: selector: matchLabels: name: demo template: metadata: labels: name: demo spec: containers: - image: docker.io/wqnice/demo:0.1.0 imagePullPolicy: Always name: demo ports: - containerPort: 8080 protocol: TCP serviceAccount: default terminationGracePeriodSeconds: 10 Robot 账户配置 创建账户 1oc create sa robot 获取令牌 1oc serviceaccounts get-token robot 授予机器人账户全部权限 1oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:&lt;namespace&gt;:&lt;robot name&gt;","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"OpenShift","slug":"OpenShift","permalink":"https://wangqian0306.github.io/tags/OpenShift/"}]},{"title":"Docker Compose 安装和基础命令","slug":"docker/docker-compose","date":"2020-04-04T12:41:32.000Z","updated":"2025-01-08T02:56:21.470Z","comments":true,"path":"2020/docker-compose/","permalink":"https://wangqian0306.github.io/2020/docker-compose/","excerpt":"","text":"Docker Compose 安装 推荐使用如下命令进行安装(但是需要python3环境) 1pip3 install docker-compose Docker Compose 样例文件 123456789101112131415161718services: demo: # 服务名 build: ../../.. # Dockerfile 的相对路 build: . # Dockerfile 的相对路径 image: demo:0.0.1 # 镜像名 command: [ ] # 覆写容器启动命令 ports: # 开启端口 - &quot;5000:5000&quot; # 本地端口:容器端口 environment: - JAVA_HOME=/opt/java/bin #环境变量(程序运行时生效，构建时不生效) deploy: resources: limits: # 资源限制 cpus: &#x27;0.50&#x27; memory: 50M volumes: - /opt/data:/var/lib/mysql# depends_on: # 启动依赖# - db # 依赖服务名 官方文档地址 Docker Compose 常用命令 构建镜像 1docker-compose build --no-cache 以后台模式启动容器 1docker-compose up -d 查看容器运行状态 1docker-compose ps 运行命令 1docker-compose exec &lt;服务名&gt; &lt;运行命令&gt; 注: 使用bash或者sh进入交互式执行模式。 关闭容器 1docker-compose down 使用 Makefile 优化使用流程 在实际项目中可以使用 Makefile 的方式来简化输入命令 Make 命令安装 windows 推荐使用 chocolate 安装 1choco install make CentOS 1yum install bash -y 文件样例 在项目中创建名为Makefile的文件然后填入如下内容即可： 12345678910111213141516171819IMAGE=docker.io/wangq/demo:latestPROJECT=demo.PHONY: build clean push save loadbuild: docker-compose build --no-cacheclean: docker rmi $(IMAGE)push: build docker push $(IMAGE)save: docker image save $(IMAGE) --output $(PROJECT).tarload: docker load --input $(PROJECT).tar 使用方式 使用make命令中写明的快捷方式即可, 例如 1make build","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://wangqian0306.github.io/tags/Docker-Compose/"}]},{"title":"Windows Terminal 集成 Git Bash","slug":"windows/terminal","date":"2020-04-04T12:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2020/terminal/","permalink":"https://wangqian0306.github.io/2020/terminal/","excerpt":"","text":"软件安装 Windows Terminal 可以在 Microsoft Store 安装 Git 可以在官网或者使用 Chocolatey 安装 软件配置 新增默认配置项方式 Windows Terminal 经过一段时间的更新目前可以完成使用 UI 配置 GitBash, 具体流程如下： 打开 Windows Terminal 设置 在配置文件一栏，选择添加新的配置文件 然后即可自行添加如下配置： 名称: Git Bash 命令行：C:\\\\Program Files\\Git\\bin\\bash.exe 启动目录：~ 图标：%SystemDrive%\\\\Program Files\\\\Git\\\\mingw64\\\\share\\\\git\\\\git-for-windows.ico 配置环境变量(解决Git Bash 中文显示的问题) 12LANG=&quot;zh_CN&quot;OUTPUT_CHARSET=&quot;utf-8&quot; 重新启动 Windows Terminal 即可 无法执行自定义脚本 以管理员权限运行如下指令即可 1set-executionpolicy remotesigned","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"Windows Terminal","slug":"Windows-Terminal","permalink":"https://wangqian0306.github.io/tags/Windows-Terminal/"},{"name":"Git Bash","slug":"Git-Bash","permalink":"https://wangqian0306.github.io/tags/Git-Bash/"}]},{"title":"Docker 安装和基础命令","slug":"docker/docker","date":"2020-04-03T13:41:32.000Z","updated":"2025-01-08T02:58:47.814Z","comments":true,"path":"2020/docker/","permalink":"https://wangqian0306.github.io/2020/docker/","excerpt":"","text":"Docker 安装 Yum 自带 使用如下命令进行安装 Rocky Linux: 1yum install docker -y 注：由于版权问题此处替换为了 Podman 但是基本使用逻辑是类似的，配置和管理等运维上则完全不同。 Docker CE 使用如下命令进行安装： 清除原版软件 1yum remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine 新增官方源 12yum install -y yum-utilsyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安装软件 1yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 关闭 SELinux 修改服务状态 1setenforce 0 全局配置 1vim /etc/selinux/config 修改如下内容 1SELINUX=disabled 非 Root 用户使用 Docker 使用如下命令进行服务配置 1234sudo groupadd dockersudo usermod -aG docker &lt;使用Docker的非Root用户&gt;sudo systemctl enable docker --nowsudo chmod 666 /var/run/docker.sock Dockerfile 可以参照官方文档 常用命令 拉取镜像 1docker pull &lt;镜像名&gt; 构建镜像 1docker build &lt;Dockerfile所在目录的路径&gt; -t &lt;镜像名&gt; 查看镜像列表 1docker images 运行镜像并在运行结束后清除镜像 1docker run -d &lt;镜像ID/镜像名&gt; --rm 查看容器列表 1docker ps -a 以交互式命令行进入容器 1docker exec -it &lt;容器ID/容器名&gt; &lt;容器交互式命令&gt; 注：常见的容器交互式命令为bash,但alpine的镜像需要用sh。 删除镜像 1docker rm -f &lt;容器ID/容器名&gt; 删除容器 1docker rmi -f &lt;镜像ID/镜像名&gt; 删除容器名为空的容器 1docker rmi -f `docker images | grep &#x27;&lt;none&gt;&#x27; | awk &#x27;&#123;print $3&#125;&#x27;` 清除所有的容器相关内容 1docker system prune -a 登录 DockerHub 1docker login -u &lt;用户名&gt; 推送 Docker 1docker push &lt;镜像名&gt; 将 Docker 保存为 tar 包 1docker save -o &lt;文件名&gt;.tar &lt;镜像名&gt; 将tar包导入为镜像 1docker load -i &lt;文件名&gt;.tar 查看镜像中的内容： 1docker run -it --entrypoint /bin/bash --name &lt;name&gt; &lt;image&gt; 配置远程链接 Docker 采用了 C/S 架构，所以能在客户机上仅仅安装一个 docker-cli 就可以方便的链接服务器使用 Docker 了。 注：Podman 无法这样使用 开启服务端远程链接 修改 daemon 配置，新增如下项目 12345&#123; &quot;hosts&quot;: [ &quot;tcp://0.0.0.0:2375&quot; ]&#125; 服务端允许docker链接 12firewall-cmd --permanent --add-service=dockerfirewall-cmd --reload 重启 docker 服务 1systemctl restart docker 客户端配置 win 10 安装 docker-cli(需要管理员权限和Chocolatey软件) 1choco install docker-cli -y 在系统变量中新增如下环境变量即可 1DOCKER_HOST=tcp://&lt;remote_ip&gt;:2375 检测远程链接是否可用 1docker info 配置代理 创建代理配置文件： 12mkdir -p /etc/systemd/system/docker.service.dvim /etc/systemd/system/docker.service.d/proxy.conf 1234[Service]Environment=&quot;HTTP_PROXY=http://127.0.0.1:8888/&quot;Environment=&quot;HTTPS_PROXY=http://127.0.0.1:8888/&quot;Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot; 重启服务 12systemctl daemon-reloadsystemctl restart docker 常见问题及解决方案 网络错误 在docker构建时发生registry链接失败的问题，可以通过如下方式进行解决 查看目前的容器网络列表 1docker network ls 删除构建失败容器相关的网络即可 1docker network rm &lt;NETWORK ID&gt; 注: 可以使用docker-compose。docker-compose down命令可以更好的管理容器相关内容。","categories":[{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"}]},{"title":"Chocolatey 的简单使用","slug":"windows/chocolate","date":"2020-04-03T13:41:32.000Z","updated":"2025-01-08T02:56:21.494Z","comments":true,"path":"2020/chocolate/","permalink":"https://wangqian0306.github.io/2020/chocolate/","excerpt":"","text":"简介 Chocolatey 像是个命令行版本的windows软件包管理工具。 安装方式 右键点击开始菜单 选择 Windows PowerShell (管理员) 输入如下命令即可安装 1Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&#x27;https://chocolatey.org/install.ps1&#x27;)) 如有问题请查阅官方安装手册 使用方式 可以在软件仓库中寻找你需要的软件包，然后使用下面的命令来对它们进行管理。 安装软件包 1choco install &lt;软件包名&gt; 升级软件包 1choco upgrade &lt;软件包名&gt; 删除已安装的某个软件包 1choco uninstall &lt;软件包名&gt;","categories":[{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"}],"tags":[{"name":"Chocolatey","slug":"Chocolatey","permalink":"https://wangqian0306.github.io/tags/Chocolatey/"},{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/tags/Windows/"}]},{"title":"Linux 网络配置","slug":"linux/network","date":"2020-04-02T13:37:32.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/linux_network/","permalink":"https://wangqian0306.github.io/2020/linux_network/","excerpt":"","text":"网络基础状态检查及配置 查看网卡列表及目前的配置信息 1ifconfig 开启/关闭某个网卡 1ifconfig &lt;网卡名&gt; up/down 主机名 设定 1hostname-ctl set-hostname &lt;主机名&gt; 网卡配置 字符模式 查看网卡列表 1nmcli connection show 查看网卡配置信息 1nmcli connection show &lt;name&gt; 修改网卡配置 1nmcli connection modify &lt;name&gt; &lt;property&gt; &lt;content&gt; 重启网卡 1nmcli connection down &lt;name&gt;; nmcli connection up &lt;name&gt; 图形模式 字符界面图形模式配置网络(IP地址，子网掩码，DNS等) 1nmtui 防火墙配置 查看防火墙状态 1systemctl status firewalld 关闭防火墙 1systemctl stop firewalld 开启防火墙 1systemctl start firewalld 开机启动防火墙 1systemctl enable firewalld 取消开机启动防火墙 1systemctl disable firewalld 为防火墙添加允许通过的服务(需要防火墙重新配置命令完成后才会生效) 1firewall-cmd --permanent --add-service=&lt;服务名&gt; 为防火墙开启端口(需要防火墙重新配置命令完成后才会生效) 1firewall-cmd --permanent --add-port=&lt;端口名&gt;/&lt;协议类型&gt; 注: 协议类型有’tcp’,‘udp’,‘sctp’,'dccp’四种 重新配置防火墙 1firewall-cmd --reload 查看目前开放的端口 1firewall-cmd --zone=public --list-ports 查看端口状态 查看目前开放的端口及对应的服务： 1netstat -lnpt","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"Network","slug":"Network","permalink":"https://wangqian0306.github.io/tags/Network/"}]},{"title":"Linux 基础知识整理","slug":"linux/linux","date":"2020-04-01T13:57:04.000Z","updated":"2025-01-08T02:56:21.474Z","comments":true,"path":"2020/linux_dir/","permalink":"https://wangqian0306.github.io/2020/linux_dir/","excerpt":"","text":"基础系统目录及说明 地址 说明 /tmp 缓存目录 /opt 程序额外安装位置 /home 用户文件存储根目录 /usr 系统应用程序目录 /etc 系统管理和配置文件 /bin 系统必备的二进制可执行文件 /proc 虚拟文件系统目录 /root 系统管理员目录 /sbin 系统管理必备文件 /dev/ 设备相关文件 /mnt 临时挂载路径 /boot 系统引导文件 /var 日志文件 查看配置 查看内容 查看命令 CPU cat /proc/cpuinfo MEM free -m DISK fdisk -l NET ethtool xxx(网卡名) Graphics lshw -C video 查看系统版本 可以使用如下命令查看当前系统的版本： 123cat /etc/os-releaselsb_release -ahostnamectl 可以使用如下命令查看内核版本： 1uname -r sudo免密配置 12sudo touch /etc/sudoers.d/adminecho &quot;&lt;用户名&gt; ALL=(root)NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/admin 配置 SELinux 查看目前的配置状态 1getenforce 设置SELinux为宽容模式 1setenforce 0 修改SELinux运行模式(配置完后需要重启系统) 1vim /etc/selinux/config 配置文件如下 1234567891011# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled # change to disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted 常用文件或目录整理 地址 说明 /etc/profile.d/ 每个用户都有的环境变量 ~/.bashrc 用户终端(shell)的相关配置 /etc/hosts host文件 /etc/exports NFS配置 /var/named* DNS配置 /etc/chrony.conf NTP配置 /etc/fstab 磁盘挂载配置 /etc/selinux/config SELinux配置 /etc/*release Linux 版本说明 配置 Terminal 快捷键 在 Fedora 系统上没有快捷打开 Terminal 的快捷键，需要进行如下设置： 打开 Settings 系统设置页 选择 Keyborad 键盘配置项 点击 Keyboard Shortcuts 配置中的 View and Customize Shortcuts 选项 选择 Custom Shortcut 并新增自定义快捷键即可： Name: Terminal Command: gnome-terminal Shortcut: Ctrl + Alt + T MD5 使用如下命令计算文件 MD5 1md5sum filename","categories":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"}]},{"title":"GitHub 小技巧","slug":"tmp/github","date":"2020-04-01T13:57:04.000Z","updated":"2025-01-08T02:56:21.486Z","comments":true,"path":"2020/github/","permalink":"https://wangqian0306.github.io/2020/github/","excerpt":"","text":"GitHub 小技巧 高级搜索 高级搜索入口 文件和代码检索 在项目中时可以使用 t 键进入文件检索页面 在项目中时可以使用 . 键进入 Web IDE 在浏览代码文件时可以使用 l 键进行行号跳转 在浏览代码文件时可以使用 b 键查看文件的改动记录 使用 ctrl + k 可以打开控制面板，具体操作请参照官方文档 Codespaces Github Codespaces 是官方提供的在线 IDE 工具，可以方便的通过网页进行项目测试。 注：目前使用体验还是比 GitPod 略差。 GitPod GitPod 是一款在线的 IDE 工具，提供了本地和网页端运行的两种功能，可以通过此工具快速的运行项目。 使用方式： 在浏览项目时，可以在地址栏添加 gitpod.io/#/&lt;repo&gt; 访问 Web IDE。样例如下： 12https://github.com/wangqian0306/wangqian0306.github.iohttps://gitpod.io/#/github.com/wangqian0306/wangqian0306.github.io 注：建议使用 VSCode 作为在线编辑工具，JetBrains 系列需要配合 Gateway 一起使用，不是很方便。 在容器内默认安装了 sdkman 和 pyenv 可以轻松的切换环境版本。 Vercel Vercel 是一个框架、它提供了工作流程和基础设施，可以将 github 项目部署到外网。 使用方式： 从 GitHub 项目创建(详情参照官方手册) 从 模板项目 创建 OpenCommit OpenCommit 是一款根据提交文件的变动自动生成 commit-log 的软件。 使用方式： 12npm install -g opencommitoco config set OCO_OPENAI_API_KEY=&lt;your_api_key&gt; 注：需要 OpenAI 的 Token，目前可以使用 Azure 免费搭建个人版。 参考资料 GitHub 官方手册 GitPod 官方手册 Codespaces 官方手册 Vercel 官方手册 OpenCommit","categories":[],"tags":[{"name":"Codespaces","slug":"Codespaces","permalink":"https://wangqian0306.github.io/tags/Codespaces/"},{"name":"Gitpod","slug":"Gitpod","permalink":"https://wangqian0306.github.io/tags/Gitpod/"}]}],"categories":[{"name":"Ocean","slug":"Ocean","permalink":"https://wangqian0306.github.io/categories/Ocean/"},{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/categories/Spring/"},{"name":"工具","slug":"工具","permalink":"https://wangqian0306.github.io/categories/%E5%B7%A5%E5%85%B7/"},{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/categories/MySQL/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/categories/Container/"},{"name":"前端","slug":"前端","permalink":"https://wangqian0306.github.io/categories/%E5%89%8D%E7%AB%AF/"},{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/categories/Windows/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/categories/Kubernetes/"},{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/categories/JAVA/"},{"name":"大数据","slug":"大数据","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/categories/Linux/"},{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/categories/Python/"},{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/categories/Redis/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://wangqian0306.github.io/categories/PostgreSQL/"},{"name":"参考资料","slug":"参考资料","permalink":"https://wangqian0306.github.io/categories/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/categories/Elastic-Stack/"},{"name":"大数据概念","slug":"大数据概念","permalink":"https://wangqian0306.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5/"},{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/categories/Flink/"},{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/categories/Spark/"},{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/categories/Scala/"},{"name":"LaTeX","slug":"LaTeX","permalink":"https://wangqian0306.github.io/categories/LaTeX/"},{"name":"数据结构","slug":"数据结构","permalink":"https://wangqian0306.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Web","slug":"Web","permalink":"https://wangqian0306.github.io/categories/Web/"},{"name":"MQ","slug":"MQ","permalink":"https://wangqian0306.github.io/categories/MQ/"},{"name":"设计模式","slug":"设计模式","permalink":"https://wangqian0306.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://wangqian0306.github.io/categories/MongoDB/"},{"name":"Low_Code","slug":"Low-Code","permalink":"https://wangqian0306.github.io/categories/Low-Code/"},{"name":"Go","slug":"Go","permalink":"https://wangqian0306.github.io/categories/Go/"},{"name":"MariaDB","slug":"MariaDB","permalink":"https://wangqian0306.github.io/categories/MariaDB/"},{"name":"Neo4j","slug":"Neo4j","permalink":"https://wangqian0306.github.io/categories/Neo4j/"},{"name":"JetBrains","slug":"JetBrains","permalink":"https://wangqian0306.github.io/categories/JetBrains/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wangqian0306.github.io/tags/Python/"},{"name":"OpenCV","slug":"OpenCV","permalink":"https://wangqian0306.github.io/tags/OpenCV/"},{"name":"QR Code","slug":"QR-Code","permalink":"https://wangqian0306.github.io/tags/QR-Code/"},{"name":"Java","slug":"Java","permalink":"https://wangqian0306.github.io/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wangqian0306.github.io/tags/Spring-Boot/"},{"name":"CMS","slug":"CMS","permalink":"https://wangqian0306.github.io/tags/CMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://wangqian0306.github.io/tags/MySQL/"},{"name":"Container","slug":"Container","permalink":"https://wangqian0306.github.io/tags/Container/"},{"name":"digital-signage","slug":"digital-signage","permalink":"https://wangqian0306.github.io/tags/digital-signage/"},{"name":"Apache Benchmark","slug":"Apache-Benchmark","permalink":"https://wangqian0306.github.io/tags/Apache-Benchmark/"},{"name":"React","slug":"React","permalink":"https://wangqian0306.github.io/tags/React/"},{"name":"Apache Tika","slug":"Apache-Tika","permalink":"https://wangqian0306.github.io/tags/Apache-Tika/"},{"name":"Kivy","slug":"Kivy","permalink":"https://wangqian0306.github.io/tags/Kivy/"},{"name":"Grafana","slug":"Grafana","permalink":"https://wangqian0306.github.io/tags/Grafana/"},{"name":"Loki","slug":"Loki","permalink":"https://wangqian0306.github.io/tags/Loki/"},{"name":"Spring","slug":"Spring","permalink":"https://wangqian0306.github.io/tags/Spring/"},{"name":"Tempo","slug":"Tempo","permalink":"https://wangqian0306.github.io/tags/Tempo/"},{"name":"oha","slug":"oha","permalink":"https://wangqian0306.github.io/tags/oha/"},{"name":"OS","slug":"OS","permalink":"https://wangqian0306.github.io/tags/OS/"},{"name":"AI","slug":"AI","permalink":"https://wangqian0306.github.io/tags/AI/"},{"name":"Windows","slug":"Windows","permalink":"https://wangqian0306.github.io/tags/Windows/"},{"name":"GENE","slug":"GENE","permalink":"https://wangqian0306.github.io/tags/GENE/"},{"name":"Video","slug":"Video","permalink":"https://wangqian0306.github.io/tags/Video/"},{"name":"AIS","slug":"AIS","permalink":"https://wangqian0306.github.io/tags/AIS/"},{"name":"antiSMASH ","slug":"antiSMASH","permalink":"https://wangqian0306.github.io/tags/antiSMASH/"},{"name":"Tropycal","slug":"Tropycal","permalink":"https://wangqian0306.github.io/tags/Tropycal/"},{"name":"MySQL Router","slug":"MySQL-Router","permalink":"https://wangqian0306.github.io/tags/MySQL-Router/"},{"name":"MQTT","slug":"MQTT","permalink":"https://wangqian0306.github.io/tags/MQTT/"},{"name":"Next.js","slug":"Next-js","permalink":"https://wangqian0306.github.io/tags/Next-js/"},{"name":"Leaflet","slug":"Leaflet","permalink":"https://wangqian0306.github.io/tags/Leaflet/"},{"name":"React Leaflet","slug":"React-Leaflet","permalink":"https://wangqian0306.github.io/tags/React-Leaflet/"},{"name":"OAuth","slug":"OAuth","permalink":"https://wangqian0306.github.io/tags/OAuth/"},{"name":"OIDC","slug":"OIDC","permalink":"https://wangqian0306.github.io/tags/OIDC/"},{"name":"Docker","slug":"Docker","permalink":"https://wangqian0306.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://wangqian0306.github.io/tags/Kubernetes/"},{"name":"AList","slug":"AList","permalink":"https://wangqian0306.github.io/tags/AList/"},{"name":"GeoServer","slug":"GeoServer","permalink":"https://wangqian0306.github.io/tags/GeoServer/"},{"name":"HTPC","slug":"HTPC","permalink":"https://wangqian0306.github.io/tags/HTPC/"},{"name":"Netty","slug":"Netty","permalink":"https://wangqian0306.github.io/tags/Netty/"},{"name":"JAVA","slug":"JAVA","permalink":"https://wangqian0306.github.io/tags/JAVA/"},{"name":"PDF","slug":"PDF","permalink":"https://wangqian0306.github.io/tags/PDF/"},{"name":"Android","slug":"Android","permalink":"https://wangqian0306.github.io/tags/Android/"},{"name":"SRS","slug":"SRS","permalink":"https://wangqian0306.github.io/tags/SRS/"},{"name":"Kafka","slug":"Kafka","permalink":"https://wangqian0306.github.io/tags/Kafka/"},{"name":"Linux","slug":"Linux","permalink":"https://wangqian0306.github.io/tags/Linux/"},{"name":"Alibaba Cloud","slug":"Alibaba-Cloud","permalink":"https://wangqian0306.github.io/tags/Alibaba-Cloud/"},{"name":"Umami","slug":"Umami","permalink":"https://wangqian0306.github.io/tags/Umami/"},{"name":"IDS","slug":"IDS","permalink":"https://wangqian0306.github.io/tags/IDS/"},{"name":"NCBI","slug":"NCBI","permalink":"https://wangqian0306.github.io/tags/NCBI/"},{"name":"LLAMA","slug":"LLAMA","permalink":"https://wangqian0306.github.io/tags/LLAMA/"},{"name":"nodejs","slug":"nodejs","permalink":"https://wangqian0306.github.io/tags/nodejs/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://wangqian0306.github.io/tags/TypeScript/"},{"name":"Drools","slug":"Drools","permalink":"https://wangqian0306.github.io/tags/Drools/"},{"name":"OpenTelemetry","slug":"OpenTelemetry","permalink":"https://wangqian0306.github.io/tags/OpenTelemetry/"},{"name":"Jaeger","slug":"Jaeger","permalink":"https://wangqian0306.github.io/tags/Jaeger/"},{"name":"Argo","slug":"Argo","permalink":"https://wangqian0306.github.io/tags/Argo/"},{"name":"RSocket","slug":"RSocket","permalink":"https://wangqian0306.github.io/tags/RSocket/"},{"name":"Jellyfin","slug":"Jellyfin","permalink":"https://wangqian0306.github.io/tags/Jellyfin/"},{"name":"Hugging Face","slug":"Hugging-Face","permalink":"https://wangqian0306.github.io/tags/Hugging-Face/"},{"name":"Redis","slug":"Redis","permalink":"https://wangqian0306.github.io/tags/Redis/"},{"name":"Arthas","slug":"Arthas","permalink":"https://wangqian0306.github.io/tags/Arthas/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://wangqian0306.github.io/tags/PostgreSQL/"},{"name":"HTTP","slug":"HTTP","permalink":"https://wangqian0306.github.io/tags/HTTP/"},{"name":"VMware","slug":"VMware","permalink":"https://wangqian0306.github.io/tags/VMware/"},{"name":"Actuator","slug":"Actuator","permalink":"https://wangqian0306.github.io/tags/Actuator/"},{"name":"container","slug":"container","permalink":"https://wangqian0306.github.io/tags/container/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wangqian0306.github.io/tags/Elasticsearch/"},{"name":"SSL","slug":"SSL","permalink":"https://wangqian0306.github.io/tags/SSL/"},{"name":"Spring Data JPA","slug":"Spring-Data-JPA","permalink":"https://wangqian0306.github.io/tags/Spring-Data-JPA/"},{"name":"ACME","slug":"ACME","permalink":"https://wangqian0306.github.io/tags/ACME/"},{"name":"OpenSSL","slug":"OpenSSL","permalink":"https://wangqian0306.github.io/tags/OpenSSL/"},{"name":"Github Copilot","slug":"Github-Copilot","permalink":"https://wangqian0306.github.io/tags/Github-Copilot/"},{"name":"Node.js","slug":"Node-js","permalink":"https://wangqian0306.github.io/tags/Node-js/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://wangqian0306.github.io/tags/NVIDIA/"},{"name":"FauxPilot","slug":"FauxPilot","permalink":"https://wangqian0306.github.io/tags/FauxPilot/"},{"name":"随笔","slug":"随笔","permalink":"https://wangqian0306.github.io/tags/%E9%9A%8F%E7%AC%94/"},{"name":"CalDAV","slug":"CalDAV","permalink":"https://wangqian0306.github.io/tags/CalDAV/"},{"name":"AdGuardHome","slug":"AdGuardHome","permalink":"https://wangqian0306.github.io/tags/AdGuardHome/"},{"name":"aria2","slug":"aria2","permalink":"https://wangqian0306.github.io/tags/aria2/"},{"name":"Samba","slug":"Samba","permalink":"https://wangqian0306.github.io/tags/Samba/"},{"name":"DNS","slug":"DNS","permalink":"https://wangqian0306.github.io/tags/DNS/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://wangqian0306.github.io/tags/Prometheus/"},{"name":"CNB","slug":"CNB","permalink":"https://wangqian0306.github.io/tags/CNB/"},{"name":"Helm","slug":"Helm","permalink":"https://wangqian0306.github.io/tags/Helm/"},{"name":"Nextcloud","slug":"Nextcloud","permalink":"https://wangqian0306.github.io/tags/Nextcloud/"},{"name":"PHP","slug":"PHP","permalink":"https://wangqian0306.github.io/tags/PHP/"},{"name":"CentOS","slug":"CentOS","permalink":"https://wangqian0306.github.io/tags/CentOS/"},{"name":"ksqlDB","slug":"ksqlDB","permalink":"https://wangqian0306.github.io/tags/ksqlDB/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://wangqian0306.github.io/tags/Elastic-Stack/"},{"name":"FTP","slug":"FTP","permalink":"https://wangqian0306.github.io/tags/FTP/"},{"name":"DataX","slug":"DataX","permalink":"https://wangqian0306.github.io/tags/DataX/"},{"name":"DolphinScheduler","slug":"DolphinScheduler","permalink":"https://wangqian0306.github.io/tags/DolphinScheduler/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://wangqian0306.github.io/tags/Home-Assistant/"},{"name":"Consul","slug":"Consul","permalink":"https://wangqian0306.github.io/tags/Consul/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud/"},{"name":"Ceph","slug":"Ceph","permalink":"https://wangqian0306.github.io/tags/Ceph/"},{"name":"Gateway","slug":"Gateway","permalink":"https://wangqian0306.github.io/tags/Gateway/"},{"name":"IK Analysis plugin","slug":"IK-Analysis-plugin","permalink":"https://wangqian0306.github.io/tags/IK-Analysis-plugin/"},{"name":"Nginx","slug":"Nginx","permalink":"https://wangqian0306.github.io/tags/Nginx/"},{"name":"元数据","slug":"元数据","permalink":"https://wangqian0306.github.io/tags/%E5%85%83%E6%95%B0%E6%8D%AE/"},{"name":"Metadata","slug":"Metadata","permalink":"https://wangqian0306.github.io/tags/Metadata/"},{"name":"DAMA","slug":"DAMA","permalink":"https://wangqian0306.github.io/tags/DAMA/"},{"name":"Flink","slug":"Flink","permalink":"https://wangqian0306.github.io/tags/Flink/"},{"name":"Pulsar","slug":"Pulsar","permalink":"https://wangqian0306.github.io/tags/Pulsar/"},{"name":"BookKeeper","slug":"BookKeeper","permalink":"https://wangqian0306.github.io/tags/BookKeeper/"},{"name":"算法","slug":"算法","permalink":"https://wangqian0306.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"Kibana","slug":"Kibana","permalink":"https://wangqian0306.github.io/tags/Kibana/"},{"name":"Go","slug":"Go","permalink":"https://wangqian0306.github.io/tags/Go/"},{"name":"SkyWalking","slug":"SkyWalking","permalink":"https://wangqian0306.github.io/tags/SkyWalking/"},{"name":"Spark","slug":"Spark","permalink":"https://wangqian0306.github.io/tags/Spark/"},{"name":"Swagger2Markup","slug":"Swagger2Markup","permalink":"https://wangqian0306.github.io/tags/Swagger2Markup/"},{"name":"Atmosphere","slug":"Atmosphere","permalink":"https://wangqian0306.github.io/tags/Atmosphere/"},{"name":"Windows Terminal","slug":"Windows-Terminal","permalink":"https://wangqian0306.github.io/tags/Windows-Terminal/"},{"name":"Git Bash","slug":"Git-Bash","permalink":"https://wangqian0306.github.io/tags/Git-Bash/"},{"name":"Bigtop","slug":"Bigtop","permalink":"https://wangqian0306.github.io/tags/Bigtop/"},{"name":"Scala","slug":"Scala","permalink":"https://wangqian0306.github.io/tags/Scala/"},{"name":"sbt","slug":"sbt","permalink":"https://wangqian0306.github.io/tags/sbt/"},{"name":"Heroku","slug":"Heroku","permalink":"https://wangqian0306.github.io/tags/Heroku/"},{"name":"Avro","slug":"Avro","permalink":"https://wangqian0306.github.io/tags/Avro/"},{"name":"Kryo","slug":"Kryo","permalink":"https://wangqian0306.github.io/tags/Kryo/"},{"name":"Maven","slug":"Maven","permalink":"https://wangqian0306.github.io/tags/Maven/"},{"name":"Gradle","slug":"Gradle","permalink":"https://wangqian0306.github.io/tags/Gradle/"},{"name":"OLAP","slug":"OLAP","permalink":"https://wangqian0306.github.io/tags/OLAP/"},{"name":"Doris","slug":"Doris","permalink":"https://wangqian0306.github.io/tags/Doris/"},{"name":"SeaTunnel","slug":"SeaTunnel","permalink":"https://wangqian0306.github.io/tags/SeaTunnel/"},{"name":"remote","slug":"remote","permalink":"https://wangqian0306.github.io/tags/remote/"},{"name":"wsl2","slug":"wsl2","permalink":"https://wangqian0306.github.io/tags/wsl2/"},{"name":"Ansible","slug":"Ansible","permalink":"https://wangqian0306.github.io/tags/Ansible/"},{"name":"论文","slug":"论文","permalink":"https://wangqian0306.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"Impala","slug":"Impala","permalink":"https://wangqian0306.github.io/tags/Impala/"},{"name":"Hive","slug":"Hive","permalink":"https://wangqian0306.github.io/tags/Hive/"},{"name":"regex","slug":"regex","permalink":"https://wangqian0306.github.io/tags/regex/"},{"name":"YARN","slug":"YARN","permalink":"https://wangqian0306.github.io/tags/YARN/"},{"name":"Borg","slug":"Borg","permalink":"https://wangqian0306.github.io/tags/Borg/"},{"name":"LaTeX","slug":"LaTeX","permalink":"https://wangqian0306.github.io/tags/LaTeX/"},{"name":"Spanner","slug":"Spanner","permalink":"https://wangqian0306.github.io/tags/Spanner/"},{"name":"Superset","slug":"Superset","permalink":"https://wangqian0306.github.io/tags/Superset/"},{"name":"CDH","slug":"CDH","permalink":"https://wangqian0306.github.io/tags/CDH/"},{"name":"Atlas","slug":"Atlas","permalink":"https://wangqian0306.github.io/tags/Atlas/"},{"name":"ncu","slug":"ncu","permalink":"https://wangqian0306.github.io/tags/ncu/"},{"name":"keytool","slug":"keytool","permalink":"https://wangqian0306.github.io/tags/keytool/"},{"name":"FreeIPA","slug":"FreeIPA","permalink":"https://wangqian0306.github.io/tags/FreeIPA/"},{"name":"NIS","slug":"NIS","permalink":"https://wangqian0306.github.io/tags/NIS/"},{"name":"Windows AD","slug":"Windows-AD","permalink":"https://wangqian0306.github.io/tags/Windows-AD/"},{"name":"HTTP/2","slug":"HTTP-2","permalink":"https://wangqian0306.github.io/tags/HTTP-2/"},{"name":"CDC","slug":"CDC","permalink":"https://wangqian0306.github.io/tags/CDC/"},{"name":"Spring Security","slug":"Spring-Security","permalink":"https://wangqian0306.github.io/tags/Spring-Security/"},{"name":"JWT","slug":"JWT","permalink":"https://wangqian0306.github.io/tags/JWT/"},{"name":"Excel","slug":"Excel","permalink":"https://wangqian0306.github.io/tags/Excel/"},{"name":"JConsul","slug":"JConsul","permalink":"https://wangqian0306.github.io/tags/JConsul/"},{"name":"JMX Exporter","slug":"JMX-Exporter","permalink":"https://wangqian0306.github.io/tags/JMX-Exporter/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://wangqian0306.github.io/tags/Docker-Compose/"},{"name":"Docker Swarm","slug":"Docker-Swarm","permalink":"https://wangqian0306.github.io/tags/Docker-Swarm/"},{"name":"Supabase","slug":"Supabase","permalink":"https://wangqian0306.github.io/tags/Supabase/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://wangqian0306.github.io/tags/RabbitMQ/"},{"name":"Nacos","slug":"Nacos","permalink":"https://wangqian0306.github.io/tags/Nacos/"},{"name":"Spring Cloud Alibaba","slug":"Spring-Cloud-Alibaba","permalink":"https://wangqian0306.github.io/tags/Spring-Cloud-Alibaba/"},{"name":"Airflow","slug":"Airflow","permalink":"https://wangqian0306.github.io/tags/Airflow/"},{"name":"Parquet","slug":"Parquet","permalink":"https://wangqian0306.github.io/tags/Parquet/"},{"name":"Paxos","slug":"Paxos","permalink":"https://wangqian0306.github.io/tags/Paxos/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://wangqian0306.github.io/tags/ZooKeeper/"},{"name":"Raft","slug":"Raft","permalink":"https://wangqian0306.github.io/tags/Raft/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://wangqian0306.github.io/tags/MapReduce/"},{"name":"HBase","slug":"HBase","permalink":"https://wangqian0306.github.io/tags/HBase/"},{"name":"Bigtable","slug":"Bigtable","permalink":"https://wangqian0306.github.io/tags/Bigtable/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://wangqian0306.github.io/tags/Hadoop/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://wangqian0306.github.io/tags/MongoDB/"},{"name":"Low_Code","slug":"Low-Code","permalink":"https://wangqian0306.github.io/tags/Low-Code/"},{"name":"ZeroMQ","slug":"ZeroMQ","permalink":"https://wangqian0306.github.io/tags/ZeroMQ/"},{"name":"Mosquitto","slug":"Mosquitto","permalink":"https://wangqian0306.github.io/tags/Mosquitto/"},{"name":"go","slug":"go","permalink":"https://wangqian0306.github.io/tags/go/"},{"name":"EdgeX","slug":"EdgeX","permalink":"https://wangqian0306.github.io/tags/EdgeX/"},{"name":"Asciidoc","slug":"Asciidoc","permalink":"https://wangqian0306.github.io/tags/Asciidoc/"},{"name":"UML","slug":"UML","permalink":"https://wangqian0306.github.io/tags/UML/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wangqian0306.github.io/tags/MyBatis/"},{"name":"IoTDB","slug":"IoTDB","permalink":"https://wangqian0306.github.io/tags/IoTDB/"},{"name":"GFS","slug":"GFS","permalink":"https://wangqian0306.github.io/tags/GFS/"},{"name":"HDFS","slug":"HDFS","permalink":"https://wangqian0306.github.io/tags/HDFS/"},{"name":"MariaDB","slug":"MariaDB","permalink":"https://wangqian0306.github.io/tags/MariaDB/"},{"name":"Phoenix","slug":"Phoenix","permalink":"https://wangqian0306.github.io/tags/Phoenix/"},{"name":"Hue","slug":"Hue","permalink":"https://wangqian0306.github.io/tags/Hue/"},{"name":"Kerberos","slug":"Kerberos","permalink":"https://wangqian0306.github.io/tags/Kerberos/"},{"name":"Druid","slug":"Druid","permalink":"https://wangqian0306.github.io/tags/Druid/"},{"name":"Kylin","slug":"Kylin","permalink":"https://wangqian0306.github.io/tags/Kylin/"},{"name":"数据仓库","slug":"数据仓库","permalink":"https://wangqian0306.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"Data warehouse","slug":"Data-warehouse","permalink":"https://wangqian0306.github.io/tags/Data-warehouse/"},{"name":"Data Mart","slug":"Data-Mart","permalink":"https://wangqian0306.github.io/tags/Data-Mart/"},{"name":"数据集市","slug":"数据集市","permalink":"https://wangqian0306.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82/"},{"name":"Spring REST Docs","slug":"Spring-REST-Docs","permalink":"https://wangqian0306.github.io/tags/Spring-REST-Docs/"},{"name":"Swagger","slug":"Swagger","permalink":"https://wangqian0306.github.io/tags/Swagger/"},{"name":"Oozie","slug":"Oozie","permalink":"https://wangqian0306.github.io/tags/Oozie/"},{"name":"Logstash","slug":"Logstash","permalink":"https://wangqian0306.github.io/tags/Logstash/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://wangqian0306.github.io/tags/HTTPS/"},{"name":"Sqoop","slug":"Sqoop","permalink":"https://wangqian0306.github.io/tags/Sqoop/"},{"name":"Livy","slug":"Livy","permalink":"https://wangqian0306.github.io/tags/Livy/"},{"name":"CSRF","slug":"CSRF","permalink":"https://wangqian0306.github.io/tags/CSRF/"},{"name":"Neo4j","slug":"Neo4j","permalink":"https://wangqian0306.github.io/tags/Neo4j/"},{"name":"OpenShift","slug":"OpenShift","permalink":"https://wangqian0306.github.io/tags/OpenShift/"},{"name":"NFS","slug":"NFS","permalink":"https://wangqian0306.github.io/tags/NFS/"},{"name":"JetBrains","slug":"JetBrains","permalink":"https://wangqian0306.github.io/tags/JetBrains/"},{"name":"Chocolatey","slug":"Chocolatey","permalink":"https://wangqian0306.github.io/tags/Chocolatey/"},{"name":"Network","slug":"Network","permalink":"https://wangqian0306.github.io/tags/Network/"},{"name":"Codespaces","slug":"Codespaces","permalink":"https://wangqian0306.github.io/tags/Codespaces/"},{"name":"Gitpod","slug":"Gitpod","permalink":"https://wangqian0306.github.io/tags/Gitpod/"}]}