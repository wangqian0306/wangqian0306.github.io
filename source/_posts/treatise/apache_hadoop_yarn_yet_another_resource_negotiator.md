---
title: "Apache Hadoop YARN: Yet Another Resource Negotiator 中文翻译"
date: 2022-01-04 22:26:13
tags:
- "论文"
- "YARN"
id: apache_hadoop_yarn_yet_another_resource_negotiator
no_word_count: true
no_toc: false
categories: 大数据
---

## Apache Hadoop YARN: Yet Another Resource Negotiator 中文翻译

作者：Vinod Kumar Vavilapalli, Arun C Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans, Thomas Graves, Jason Lowe, Hitesh Shah Siddharth Seth, Bikas Saha, Carlo Curino, Owen O’Malley, Sanjay Radia, Benjamin Reed, Eric Baldeschwieler

[英文原文](https://www.cse.ust.hk/~weiwa/teaching/Fall15-COMP6611B/reading_list/YARN.pdf)

### 版权说明

```text
Copyright c 2013 by the Association for Computing Machinery, Inc.
(ACM). Permission to make digital or hard copies of all or part of this
work for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
```

### 摘要

Apache Hadoop `[1]` 的最初设计主要集中在运行大量的 MapReduce 作业来处理网络爬虫。
对于日益多样化的公司而言，Hadoop 已成为数据和计算集市——实际上是共享和访问数据和计算资源的地方。
这种广泛采用和无处不在的使用使初始设计远远超出其预期目标，暴露出两个主要缺点：1) 特定编程模型与资源管理基础设施的紧密耦合，迫使开发人员滥用 MapReduce 编程模型，以及 2) 作业控制流的集中处理，这导致调度器对可扩展性方面的有很高的可能性出现问题。

在本文中，我们总结了下一代 Hadoop 计算平台 YARN 的设计、开发和当前部署状态。
我们引入的新架构将编程模型与资源管理基础设施分离，并将许多调度功能(例如，任务容错)委托给每个应用程序组件。
我们提供实验证据来证明我们所做的改进，通过报告在生产环境（包括 100% 的 Yahoo! 网格）上运行 YARN 的经验来确认提高的效率，并通过讨论将几个编程框架移植到 YARN 上来确认灵活性声明。
Dryad、Giraph、Hoya、Hadoop MapReduce、REEF、Spark、Storm、Tez。

### 1 引言

Apache Hadoop 最初是 MapReduce `[12]` 的众多开源实现之一，专注于解决索引网络爬行所需的前所未有的规模。
其执行架构针对此用例进行了调整，专注于为大规模数据密集型计算提供强大的容错能力。
在许多大型网络公司和初创公司中，Hadoop 集群是存储和处理运营数据的常见场所。

更重要的是，它成为组织内工程师和研究人员可以即时且几乎不受限制地访问大量计算资源和公司的数据宝库的。
这既是 Hadoop 成功的原因，也是其最大的诅咒，因为大多数的开发人员将 MapReduce 编程模型扩展到了集群管理基板的能力之外。
一种常见的模式提交“only-map”作业以在集群中生成任意进程。
常见的例子为复制 web 服务器和分组迭代完成的计划任务负载。
开发人员为了利用物理资源，经常采取巧妙的解决方法来避开 MapReduce API 的限制。

这些限制和误用使得很多的无关的论文使用了 Hadoop 环境作为基础。
虽然许多论文都暴露了 Hadoop 架构或实现的实质性问题，但有些论文只是简单地(或多或少巧妙地)谴责了这些滥用的一些副作用。
现在，学术界和开源社区都很好地理解了原始 Hadoop 架构的局限性。

在本文中，我们展示了一项社区驱动的努力，旨在让 Hadoop 超越其最初的设定。
我们展示了称为 YARN 的下一代 Hadoop 计算平台，它不同于其熟悉的单体架构。
通过将资源管理功能与编程模型分离，YARN 将许多与调度相关的功能委托给每个作业的组件。
在这个新环境中，MapReduce 只是运行在 YARN 之上的应用程序之一。这种分离为编程框架的选择提供了很大的灵活性。
YARN 上可用的替代编程模型的示例包括：Dryad `[18]`、Giraph、Hoya、REEF `[10]`、Spark `[32]`、Storm `[4]` 和 Tez `[2]`。
在 YARN 上运行的编程框架会根据他们认为合适的方式协调应用程序内通信、执行流程和动态优化，从而实现显着的性能改进。
我们从早期架构和实施者的角度来描述 YARN 的起始、设计、开源开发和部署阶段。

### 2 历史和理由

在本节中，我们提供了 YARN 从实际需求中产生的历史记录。
对于起因不感兴趣的读者可以跳过本节(本节高亮了需求以便阅读)，在后续的第 3 节我们提供了 YARN 架构的简述。

雅虎在 2006 年采用 Apache Hadoop 来替代其原有的平台并作为其 WebMap 应用程序的基础设施 `[11]`，并使用该技术构建已知网络的图谱以支持其搜索引擎。
当时此网络图谱包含超过 1000 亿个节点和 1 万亿条边。
之前名为“Dreadnaught”的基础设施 `[25]` 已达到其在 800 台机器上的可扩展性极限，并且需要对其架构进行重大转变以适应外部网络的发展速度。
Dreadnought 已经执行了类似于 MapReduce `[12]` 程序的分布式应用程序，因此通过采用更具可扩展性的 MapReduce 框架，可以轻松迁移搜索管道中的重要部分。
这突出了在 Hadoop 的早期版本中一直存在的第一个要求，一直到 YARN—— **`[R1:]` 可扩展性。**

除了用于雅虎搜索的超大规模管道外，优化广告分析、垃圾邮件过滤和科学的内容优化推动了许多早期需求。
随着 Apache Hadoop 社区为越来越大的 MapReduce 作业扩展平台，围绕 **`[R2:]` 多租户** 的需求开始形成。
在这种情况下可以很好的理解工程优先级和计算平台的中间阶段。
YARN 的架构基于 MapReduce 平台发展的经验，满足了许多长期存在的需求。
在本文的其余部分，我们将假设对经典 Hadoop 架构有一般的了解，附录 A 中提供了其简要总结。

#### 2.1 临时集群时代

一些 Hadoop 最早的用户会在少数节点上建立一个集群，将他们的数据加载到 Hadoop 分布式文件系统(HDFS `[27]`)，通过编写 MapReduce 作业获得他们感兴趣的结果，然后将其拆除 `[15]`。
随着 Hadoop 容错能力的提高，持久性 HDFS 集群成为常态。
在雅虎，运营商会将“有趣”的数据集加载到共享集群中，吸引有兴趣从中获取见解的科学家。
虽然大规模计算仍然是开发的主要驱动力，但 HDFS 还获得了权限模型、配额和其他功能以改进其多租户操作。

为了解决它的一些多租户问题，雅虎开发和部署 Hadoop on Demand (HoD)，它使用 Torque `[7]` 和 Maui `[20]` 在共享硬件池上分配 Hadoop 集群。
用户将他们的作业连同适当大小的计算集群的描述提交给 Torque，Torque 会将作业排入队列，直到有足够的节点可用。
一旦节点可用，Torque 将在头节点上启动 HoD 的“领导者”进程，然后该进程将与 Torque/Maui 交互以启动 HoD 的从属进程，这些进程随后为该用户生成 JobTracker 和 TaskTracker，然后接受一系列作业。
当用户释放集群时，系统会自动收集用户的日志并将节点返回到共享池。
由于 HoD 为每个作业设置了一个新集群，用户可以运行(稍微)旧版本的 Hadoop，而开发人员可以轻松测试新功能。Hadoop 每三个月发布一次重大修订。
HoD 的灵活性对于保持这种节奏至关重要——我们将这种升级依赖关系的解耦称为 **`[R3:]` 可服务性。**
随着 HDFS 的扩展，可以在其上分配更多的计算集群，从而在更多数据集上创建用户密度增加的良性循环，从而产生新的见解。

虽然 HoD 也可以部署 HDFS 集群，但大多数用户跨共享 HDFS 实例部署计算节点。
随着 HDFS 的扩展，可以在其上分配更多的计算集群，从而在更多数据集上创建用户密度增加的良性循环，从而产生新的领悟。
由于大多数 Hadoop 集群都小于雅虎最大的 HoD 作业，因此 JobTracker 很少成为瓶颈。

HoD 证明了自己是一个多功能平台，预见了 Mesos `[17]` 的一些品质，它将扩展框架主模型以支持并发、不同编程模型之间的动态资源分配。
HoD 也可以被视为 EC2 Elastic MapReduce 和 Azure HDInsight 产品的私有云前身——没有任何隔离和安全方面的问题。

#### 2.2 Hadoop on Demand 的缺点

雅虎由于其中等资源利用率，最终淘汰了 HoD，转而使用共享 MapReduce 集群。
在映射阶段，JobTracker 尽一切努力将任务放置在 HDFS 中靠近其输入数据的位置，理想情况下位于存储该数据副本的节点上。
由于 Torque 在不考虑位置的情况下分配节点，授予用户 JobTracker 的节点子集可能只包含少数相关副本。鉴于大量小作业，大多数读取来自远程主机。
打击这些作业的努力取得了好坏参半的结果；虽然将 TaskTracker 分布在机架上使得共享数据集的机架内读取更有可能，但 map 和 reduce 任务之间的记录洗牌必然会跨机架，并且 DAG 中的后续作业将有更少的机会来解释其祖先中的偏差。
**`[R4:]` 位置意识** 的这一方面是 YARN 的关键要求。

Pig `[24]` 和 Hive `[30]` 等高级框架通常在 DAG 中组成 MapReduce 作业的工作流，每个工作流在计算的每个阶段过滤、聚合和投影数据。
由于在使用 HoD 时没有在作业之间调整集群的大小，集群中的大部分容量都处于闲置状态，而随后的更精简的阶段完成。
在极端但非常常见的情况下，在一个节点上运行的单个 reduce 任务可能会阻止集群被回收。在此状态下，某些作业使数百个节点处于空闲状态。

最后，作业延迟主要由分配集群所花费的时间决定。
用户在估计他们的工作需要多少个节点时可以依靠很少的启发式方法，并且通常会要求 10 的任何倍数与他们的直觉相匹配。
集群分配延迟如此之高，用户通常会与同事共享期待已久的集群，持有节点的时间比预期的要长，从而进一步增加了延迟。
虽然用户喜欢 HoD 中的许多功能，但集群利用的经济性迫使雅虎将其用户打包到共享集群中。**`[R5:]` 高集群利用率**是 YARN 的首要任务。

#### 2.3 共享集群

最终结果是 HoD 获取的信息太少，无法对其分配做出明智的决策，其资源粒度太粗，其 API 迫使用户向资源层提供误导性的约束。

然而，迁移到共享集群并非易事。虽然 HDFS 多年来逐渐扩展，但 JobTracker 已被 HoD 与这些力量隔离开来。
当那个守卫被移除时，MapReduce 集群突然变得更大，作业吞吐量急剧增加，并且许多无辜添加到 JobTracker 的功能成为关键错误的来源。
更糟糕的是，JobTracker 故障并没有丢失单个工作流，而是导致中断，这将丢失集群中所有正在运行的作业，并要求用户手动恢复他们的工作流。

停机会导致处理管道积压，当重新启动时，会给 JobTracker 带来巨大压力。重启通常涉及手动杀死用户的作业，直到集群恢复。
由于为每个作业存储了复杂的状态，因此在重新启动期间保留作业的实现从未完全调试过。

运行一个大型的、多租户的 Hadoop 集群是很难的。虽然容错是一个核心设计原则，但暴露给用户应用程序的表面是巨大的。
鉴于单点故障暴露的各种可用性问题，持续监控集群中的工作负载是否存在功能失调的作业至关重要。
更微妙的是，由于 JobTracker 需要为它初始化的每个作业分配跟踪结构，它的准入控制逻辑包括保护自身可用性的保障措施；它可能会延迟将闲置集群资源分配给作业，因为跟踪它们的开销可能会使 JobTracker 进程不堪重负。
所有这些问题都可以归为对 **`[R6]` 可靠性/可用性的需求。**

随着 Hadoop 管理更多租户、多样化用例和原始数据，其对隔离的要求变得更加严格，但授权模型缺乏强大的、可扩展的身份验证——这是多租户集群的关键特性。
这被添加并反向移植到多个版本。 **`[R7:]` 安全和可审计的操作必须保留在 YARN 中。**
开发者逐渐强化系统以适应对资源的多样化需求，这与面向槽的资源观点不符。

虽然 MapReduce 支持广泛的用例，但它并不是所有大规模计算的理想模型。
例如，许多机器学习程序需要对数据集进行多次迭代才能收敛到结果。
如果将此流程组合为一系列 MapReduce 作业，则调度开销将显着延迟结果 `[32]`。
类似地，使用批量同步并行模型(BSP)可以更好地表达许多图算法，使用消息传递在顶点之间进行通信，而不是在容错、大规模 MapReduce 作业中使用繁重的全对全通信障碍 `[22]`。
这种不匹配成为了用户生产力的障碍，但 Hadoop 中以 MapReduce 为中心的资源模型承认没有竞争的应用程序模型。
Hadoop 在雅虎内部的广泛部署及其数据管道的严重性使这些紧张局势无法调和。用户会编写“MapReduce”程序，生成替代框架，但不会因此而气馁。
对于调度程序，它们表现为具有完全不同的资源曲线的 map-only 作业，阻碍了平台内置的假设并导致利用率低下、潜在的死锁和不稳定。
纱线必须与其用户宣布休战，并为 **`[R8:]`编程模型多样性** 提供明确的支持。

除了与新兴框架要求不匹配之外，类型化槽还会损害利用率。
虽然 map 和reduce 容量之间的分离可以防止死锁，但它也可能成为资源瓶颈。
在 Hadoop 中，两个阶段之间的重叠由用户为每个提交的作业配置；稍后启动 reduce 任务会增加集群吞吐量，而在作业执行的早期启动它们会减少其延迟。
map 和 reduce 槽的数量由集群操作员固定，因此休闲的 map 容量不能用于产生 reduce 任务，反之亦然。
因为两种任务类型以不同的速度完成，所以没有配置会完美平衡；当任一插槽类型变得饱和时，JobTracker 可能需要对作业初始化应用背压，从而产生典型的管道气泡。
可替代的资源使调度复杂化，但它们也使分配器能够更紧密地打包集群。这突出了对 **`[R9:]` 灵活资源模型的需求。**

虽然与 HoD 相比，迁移到共享集群提高了利用率和局部性，但它也显着缓解了对可服务性和可用性的担忧。
在共享集群中部署新版本的 Apache Hadoop 是一个精心设计的、令人遗憾的常见事件。
为了修复 MapReduce 实现中的错误，操作员必须安排停机时间、关闭集群、部署新位、验证升级，然后接受新工作。
通过将负责仲裁资源使用的平台与表达该程序的框架相结合，人们被迫同时发展它们；当运营商提高平台用户的分配效率时，必然会纳入框架的变化。
因此，升级集群需要用户停止、验证和恢复他们的管道以进行正交更改。
虽然更新通常只需要重新编译，但用户对内部框架细节的假设——或开发人员对用户程序的假设——偶尔会在网格上运行的管道上造成阻塞不兼容。

基于 Apache Hadoop MapReduce 的发展经验教训，YARN 旨在满足需求(R1-R9)。
然而，MapReduce 应用程序的庞大安装基础、相关项目的生态系统、陈旧的部署实践和紧迫的时间表无法容忍彻底的重新设计。
为了避免“第二系统综合症”`[6]` 的陷阱，新架构尽可能多地重用现有框架中的代码，以熟悉的模式运行，并在绘图板上留下了许多推测性的功能。
这导致了对 YARN 重新设计的最终要求：**`[R10:]` 向后兼容性。**

在本文的其余部分，我们提供了 YARN 架构的描述(第 3 节)，我们报告了 YARN 在现实世界中的采用(第 4 节)，提供了验证一些关键架构选择的实验证据(第 5 节)并通过将 YARN 与一些相关工作(第 6 节)进行比较来得出结论。

### 3 架构

为了满足我们在第 2 节中讨论的要求，YARN 将一些功能提升到负责资源管理的平台层，将逻辑执行计划的协调留给了许多框架实现。
具体来说，每个集群的 ResourceManager (RM) 跟踪资源使用情况和节点活跃度，强制分配不变量，并仲裁租户之间的争用。
通过在 JobTracker 的章程中分离这些职责，中央分配器可以使用租户需求的抽象描述，但仍然不知道每个分配的语义。
该职责被委托给 ApplicationMaster (AM)，它通过从 RM 请求资源、根据收到的资源生成物理计划以及围绕故障协调该计划的执行来协调单个作业的逻辑计划。

#### 3.1 概览

RM 在专用机器上作为守护进程运行，并充当集群中各种竞争应用程序之间的中央权威仲裁资源。
鉴于集群资源的这种中央和全局视图，它可以在租户之间强制执行丰富的、熟悉的属性，例如公平性 `[R10]`、容量 `[R10]` 和位置 `[R4]`。
根据应用程序需求、调度优先级和资源可用性，RM 动态地将租用(称为容器)分配给在特定节点上运行的应用程序。
容器是绑定到特定节点 `[R4,R9]` 的逻辑资源包(例如，2GB RAM、1 CPU)。
为了强制执行和跟踪此类分配，RM 与在每个节点上运行的称为 NodeManager(NM) 的特殊系统守护程序交互。
RM 和 NM 之间的通信是基于心跳的可扩展性。NM 负责监控资源可用性、报告故障和容器生命周期管理(例如，启动、终止)。RM 从这些 NM 状态的快照中组合其全局视图。

作业通过公共提交协议提交给 RM，并通过准入控制阶段，在此期间验证安全凭证并执行各种操作和管理检查 `[R7]`。接受的作业将传递给调度程序以运行。
一旦调度程序有足够的资源，应用程序就会从接受状态转移到运行状态。除了内部记录之外，这还涉及为 AM 分配一个容器并将其生成在集群中的一个节点上。
接受的应用程序的记录被写入持久存储RM 中并在重新启动或失败的情况下恢复。

ApplicationMaster 是作业的“头”，管理所有生命周期方面，包括动态增加和减少资源消耗，管理执行流程(例如，针对映射的输出运行减速器)，处理故障和计算偏差，以及执行其他本地优化。
事实上，AM 可以运行任意用户代码，并且可以用任何编程语言编写，因为与 RM 和 NM 的所有通信都是使用可扩展的通信协议进行编码的——例如考虑我们在第 4.2 节中讨论的 Dryad 端口。
YARN 对 AM 的假设很少，尽管在实践中我们预计大多数作业将使用更高级别的编程框架(例如 MapReduce、Dryad、Tez、REEF 等)。
通过将所有这些功能委托给 AM，YARN 的架构获得了极大的可扩展性 `[R1]`、编程模型灵活性 `[R8]` 和改进的升级/测试 `[R3]`(因为同一框架的多个版本可以共存)。

通常，AM 需要利用多个节点上可用的资源(CPU、RAM、磁盘等)来完成一项工作。
为了获取容器，AM 向 RM 发出资源请求。这些请求的形式包括容器的位置偏好和属性的规范。RM 将根据可用性和调度策略尝试满足来自每个应用程序的资源请求。
当代表 AM 分配资源时，RM 会为该资源生成一个租约，该租约由随后的 AM 心跳拉取。
当 AM 将容器租约提交给 NM `[R4]` 时，基于令牌的安全机制可保证其真实性。
一旦 ApplicationMaster 发现一个容器可供其使用，它就会使用租约对特定于应用程序的启动请求进行编码。
在 MapReduce 中，容器中运行的代码要么是 map 任务，要么是 reduce 任务。
如果需要，正在运行的容器可以通过特定于应用程序的协议直接与 AM 通信，以报告状态和活跃度并接收特定于框架的命令——YARN 既不促进也不强制这种通信。
总的来说，YARN 部署为容器的生命周期管理和监控提供了一个基本但健壮的基础设施，而特定于应用程序的语义由每个框架管理 `[R3,R8]`。