---
title: An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译 
date: 2021-06-24 22:26:13 
tags:
  - "论文"
  - "Spark"
id: architecture_of_next_generation_apache_hadoop_mapreduce_framework
no_word_count: true
no_toc: false
categories: 大数据
---

## An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译

作者：Matei Zaharia

[英文原文](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.html)

> 注：由于原文数量过于庞大，此处仅仅针对于目前博主不太理解的部分进行翻译和整理。

### 1 引言

我们可以设计一个统一的编程抽象，不仅可以处理这些不同的计算任务，而且能使新的应用更好的编程。
特别的是，我们将展示 MapReduce 的一个简单扩展，称为弹性分布式数据集（RDDs）,它增加了高效的数据共享元语，以及大大增加了它的通用性。
由此产生的架构比当前系统有几个关键优势：

1. 在相同的运行环境下，它支持批处理、交互式、迭代和流计算，结合这些模式提供丰富的应用编程，并且相对于单一模式的系统能更好的发挥其性能。
2. 它以很小的代价在这些计算模式上提供结点故障和 straggler 的容忍功能。
   事实上，在一些地方(如流和 SQL)，基于 RDD 产生的新系统比现有的系统有更强的容错性。
3. 它实现的性能往往比 MapReduce 高 100 倍，并可媲美各个应用领域的专业系统。
4. 这很适合多组织用户管理，允许应用程序弹性地扩缩容和响应式地共享资源。

#### 1.1 专业系统相关问题

1. 重复工作：许多专业系统仍然需要解决同样的潜在问题，如分布式执行和容错性。
   举个例子，分布式 SQL 引擎或机器学习引擎都需要执行并行聚合。
   对于独立的系统，针对每个领域也是需要解决这些问题。
2. 组成：不同系统的组合进行计算的方式即昂贵又笨重。
   尤其是对于“大数据”应用，中间处理过程的数据集是庞大的且难以移动的。
   为了使得在各个计算引擎之间共享数据，当前的环境需要将数据导出到稳定且多备份的存储系统中，通常这比实际计算要消耗更多的资源。
   因此，相比于一栈式的系统，由多个系统组成的管道常常效率很低。
3. 范围限制：如果应用程序不符合专业系统的编程模型，用户只能修改程序以适应当前的系统，否则就针对该程序写一个新的运行系统。
4. 资源共享：在计算引擎之间动态共享资源是很困难的，因为大多数引擎在应用程序运行期间都假定独自拥有一组机器。
5. 管理和管理员：相对单一的系统，独立的系统需要在管理和部署上处理更多的事务。
   对于用户来说，它们需要学习多种 API 和执行模型。

#### 1.2 弹性分布式数据集(RDD)

为了解决这个问题，我们引入一个新的概念，弹性分布式数据集(RDDs)，它是 MapReduce 模型一种简单的扩展和延伸。
进一步说，虽然乍一看那些不适合 MapReduce 的计算任务(例如，迭代，交互性和流查询)之间存在着明显的不同，但他们却都有一个功能特性，
也是 MapReduce 模型的缺陷：在并行计算阶段之间能够高效地数据共享，这正是 RDD 具有真知灼见的地方。
运用高效的数据共享概念和类似于 MapReduce 的操作方式，使得所有这些计算工作都可以有效地执行，并可以在当前特定的系统中获得关键性的优化。
RDDs 以一种既高效有能容错的方式为广泛的并行计算提出这样一个抽象。

特别提出的是，以前的这些集群容错处理模型，像 MapReduce、Dryad，将计算转换为一个有向无环图(DAG)的任务集合。
这使得它们能够高效地重复执行 DAG 里的其中一部分任务来完成容错恢复。
但对于一个独立的计算，（例如在一个迭代过程中），这些模型除了可复制的文件系统外没有提供其他存储的概念，
这就导致因为在网络上进行数据复制而增加了大量的消耗。
RDDs 是一个可以避免复制的容错分布式存储概念。
取而代之，每一个 RDD 都会记住由构建它的那些操作所构成的一个图，类似于批处理计算模型，可以有效地重新计算因故障丢失的数据。
由于创建 RDD 的操作是相对粗粒度的，即单一的操作应用于许多数据元素，该技巧比通过网络复制数据更高效。
RDDs 很好地运用于当前广泛的数据并行算法和处理模型中，所有的这些对多个任务使用同一种操作。

#### 1.3 基于 RDD 机制实现的模型

我们使用 RDD 机制实现了多类模型，包括多个现有的集群编程模型和之前模型所没有支持的新
应用。在这些模型中，RDD 机制不仅在性能方面能够和之前系统相匹配，在其他方面，他们也能加
入现有的系统所缺少的新特性，比如容错性，straggler 容忍和弹性。我们讨论以下四类模型。

**迭代式算法** 一种目前已经开发的针对特定系统最常见的的工作模式是迭代算法，比如应用于图处理，数值优化，以及机器学习中的算法。
RDD 可以支持广泛类型的各种模型，包括 Pregel，像 HaLoop 和 Twister 这类的迭代式 MapReduce 模型，
以及确定版本的 GraphLab 和 PowerGraph 模型。

**关系查询** 在 MapReduce 集群中的首要需求中的一类是执行 SQL 查询，长期运行或多个小时的批量计算任务和即时查询。
这促进了很多在商业集群中应用的并行数据库系统的发展。
MapReduce 相比并行数据库在交互式查询上有非常大的缺陷，例如 MapReduce 的容错机制模型，
而我们发现通过在 RDD 操作中实现很多常用的数据库引擎的特性(比如，列处理)，这样能够达到相当可观的性能。
由上述方式所构建的系统，Shark，提供完整的容错机制，能够在短查询和长查询中很好的扩展，同时也能在 RDD 之上提供复杂分析函数的调用(例如, 机器学习)。

> 注：Shark 是基于 Hive 进行实现的，通过更换了 Hive 的物理执行引擎进行了性能改进。但是它仍然限制于 MapReduce 的遗留问题。
> 之后被 Spark SQL 模块替换，本文中不会再去关注这些内容了。

**MapReduce RDD** 通过提供 MapReduce 的一个超集，能够高效地执行 MapReduce 程序，
同样也可以指向比如 DryadLINQ 这样常见的机遇 DAG 数据流的应用。

**流式数据处理** 我们的系统与定制化系统最大的区别是我们也使用 RDD 实现了流式处理。
流式数据处理已经在数据库和系统领域进行了很长时间研究，但是实现大规模流式数据处理仍然是一项挑战。
当前的模型并没有处理在大规模集群中频繁出现的 straggler 的问题，同时对故障恢复的方式也非常有限，需要大量的复制或浪费很长的恢复时间。
特别是，当前的系统是基于一种持续操作的模型，这就需要长时间的有状态的操作处理每一个到达的记录。
为了恢复一个丢失的节点，当前的系统需要保存每一个操作符的两个副本，或通过一系列耗费大量开销的串行处理来对上游的数据进行重放。

我们提出了一个新的模型，离散数据流(D-Streams),来解决这样的问题。
对使用长期状态处理的过程进行替换，D-Streams 把流式计算的执行当做一系列短而确定性的批量计算的序列，将状态保存在 RDD 里。
D-Stream 模型通过根据相关 RDD 的依赖关系图进行并行化恢复，就能达到快速的故障恢复，这样不需要通过复制。
另外，它通过推测(Speculative)来支持对 straggler 执行迁移，例如，对那些慢任务运行经过推测的备份副本。
尽管 D-Stream 将计算转换为许多不相关联的 jobs 来运行从而增加了部分延迟，然而我们证明了 D-Stream 能够被达到次秒级延时的实现，
这样能够达到以前系统单个节点的性能，并能线性扩展到 100 个节点。
D-Stream 的强恢复特性让他们成为了第一个处理大规模集群特性的流式处理模型，并且他们基于 RDD 的实现使得应用能够有效的整合批处理和交互式查询。

#### 1.4 总结

略

#### 1.5 论文计划

本文组织结构如下。
第 2 章介绍了 RDD 抽象并涵盖了一些简单的编程模型的应用。
第 3 章介绍了 Shark SQL 系统基于 RDDs 实现的更高级的存储和处理模型的技术。
第 4 章介绍了如何使用 RDDs 开发离散的流，这是一种新的流式处理模型。
第 5 章则介绍了为什么 RDD 模型在这些应用中如此通用，同时介绍它的限制和扩展性。
最后，在第 6 章，我们总结和讨论一些未来工作的可能方向。

### 2  弹性分布式数据集

#### 2.1 简介

我们所提出的弹性分布式数据集（RDDs），这种全新的抽象模式令用户可以直接控制数据的共享。
RDD 具有可容错和并行数据结构特征，这使得用户可以指定数据存储到硬盘还是内存、控制数据的分区方法并在数据集上进行种类丰富的操作。
他们提供了一个简单高效的编程接口，可以同时满足现有的特定模型和全新的应用场景。
RDD 设计时的最大挑战在于定义一个能提供高效容错能力的编程接口。
现有的基于集群的内存存储抽象，比如分布式共享内存，键-值存储，数据库，以及 Piccolo,提供了一个对内部状态基于细粒度更新的接口(例如,表格里面的 cell)。
在这样的设计之下，提供容错性的方法就要么是在主机之间复制数据，要么对各主机的更新情况做日志记录。
这两种方法对于数据密集型的任务来说代价很高，因为它们需要在带宽远低于内存的集群网络间拷贝大量的数据，同时还将产生大量的存储开销。

与上述系统不同的是，RDD 提供一种基于粗粒度变换(如， map, filter, join)的接口，该接口会将相同的操作应用到多个数据集上。
这使得他们可以通过记录用来创建数据集的变换(lineage)，而不需存储真正的数据，进而达到高效的容错性。
当一个 RDD 的某个分区丢失的时候，RDD 记录有足够的信息记录其如何通过其他的 RDD 进行计算，且只需重新计算该分区。
因此，丢失的数据可以被很快的恢复，而不需要昂贵的复制代价。

#### 2.2 RDD 概述

##### 2.2.1 概念

从形式上看，RDD是一个分区的只读记录的集合。
RDD只能通过在(1)稳定的存储器或(2)其他RDD的数据上的确定性操作来创建。
我们把这些操作称作变换以区别其他类型的操作。例如 map, filter, 和 join。

> 注：尽管单个的 RDDS 是不可变的，但可以通过多个 RDDs 来表示一个数据集的多个版本来实现可变。
> 这种性质(可变)使得描述其 lineage(获取 RDD 所需要经过的变换)变得容易。
> 可以这样理解，RDD 是版本化的数据集，并且可以通过变换记录追踪版本。

RDD 在任何时候都不需要被"物化"(进行实际的变换并最终写入稳定的存储器上)。
实际上，一个 RDD 有足够的信息描述着其如何从其他稳定的存储器上的数据生成。
它有一个强大的特性：从本质上说，若 RDD 失效且不能重建，程序将不能引用该 RDD。

最后，用户可以控制 RDD 的其他两个方面：持久化和分区。
用户可以选择重用哪个 RDD，并为其制定存储策略(比如， 内存存储)。
也可以让 RDD 中的数据根据记录的 key 分布到集群的多个机器。
这对位置优化来说是有用的，比如可用来保证两个要 join 的数据集都使用了相同的哈希分区方式。

##### 2.2.2 Spark 编程接口

具体来说，每一个数据集都会表示为一个对象，而各种变换则通过该对象相应方法的调用而实现。

在最开始，编程人员通过对稳定存储上的数据进行变换操作(例如, map 和 filter) 来得到一个或多个 RDD。
之后，他们可以调用这些 RDD 的 actions(动作)类的操作。
这类操作的目的或是返回一个值，或是将数据导入到存储系统中。
动作类的操作如 count(返回数据集的元素数), collect(返回元素本身的集合)和 save(输出数据集到存储系统)。
与 DryadLINQ 一样，Spark 直到 RDD 第一次调用一个动作时才真正计算 RDD。
这也就使得 Spark 可以按序缓存多个变换

此外，编程人员还可以调用 RDD 的 persist(持久化)方法来表明该 RDD 在后续操作中还会用到。
默认情况下，Spark 会将调用过 persist 的 RDD 存在内存中。
但若内存不足，也可 以将其写入到硬盘上。
通过指定 persist 函数中的参数，用户也可以请求其他持久化策略并通过标记来进行 persist，比如仅存储到硬盘上，又或是在各机器之间复制一份。
最后，用户可以在每个 RDD 上设定一个持久化的优先级来指定内存中的哪些数据应该被优先写入到磁盘。

##### 2.2.3 优点

略

##### 2.2.4 不适合 RDD 的应用

正如在引言中讨论的，RDDs 最适合对数据集中所有的元素进行相同的操作的批处理类应用。
在这些情况下，作为整个 lineage 图中的其中一步，RDD 高效地记住每一次变换，从而不需要对大量数据做日志记录便可恢复失效分区。
RDDs 不太适用于通过异步细粒度更新来共享状态的应用，比如针对 Web 应用或增量网络爬虫的存储系统。
对于这些应用，那些传统的更新日志和数据检查点的系统会更有效。

#### 2.3 Spark 编程接口

为了使用 Spark, 开发者需要写一个 driver program 来连接到 workers 集群。
driver program 定义一个或多个 RDDs 以及相关的一些 action 操作。
driver 上的 spark 代码也跟踪记录 RDDs 的继承关系，即 lineage。
Worker 是一直运行着的进程，它将经过一系列操作后的 RDD 分区数据保存在内存中。

用户通过传递闭包的方式将参数传递给 Map 等操作。
在 Scala 中每个闭包都代表一个 Java 对象，这些对象可以被序列化，也可以通过网络将闭包传递给其他节点并加载。
Scala 会将闭包中的所有变量转义成 Java 对象的属性域。

#### 2.4 抽象 RDDs

抽象 RDDs 的一个挑战是如何在经过一系列 transform 操作后追踪其继承关系。
理想情况下，一个实现了 RDD 的系统必须尽可能多地提供各种变换操作，并允许用户随意进行组合。
我们提出了一种基于图的方式来抽象 RDD，它可以实现上述目标。
我们已经在 Spark 中使用了这种表现形式来提供各种 transform 操作，而无需为每个 transform 操作的调度增加额外的逻辑。
这极大度简化了系统的设计。

简而言之，我们提供了一个通用接口来抽象每个 RDD，并提供 5 种信息：
一种是分区信息，这些信息是数据集最小的分片；
一种是依赖关系信息，指向其父 RDD；
一种是函数，基于父 RDD 进行计算；
一种是分区方案；
最后一种是数据存放位置。
例如：一个表现 HDFS 文件的 RDD 将文件的每个文件块表示为一个分区，并且知道每个文件块的位置信息。
同时，对 RDD 进行 map 操作后具有相同的划分。
当计算其元素时，将 map 函数应用于父 RDD 的数据。

在设计接口的过程中，最有趣的问题在于如何表示 RDD 之间的依赖关系。
我们发现，比较合理的方式是将依赖关系分成两类：
窄依赖，每个父 RDD 的分区都至多被一个子 RDD 的分区使用；
宽依赖：多个子 RDD 的分区依赖一个父 RDD 的分区。

这两种依赖的的区别从两个方面来说比较有用。
首先，窄依赖允许在单个集群节点上流水线式执行，这个节点可以计算所有父级分区。
例如，可以逐个元素地依次执行 filter 操作和 map 操作。
相反，宽依赖需要所有的父 RDD 数据可用并且数据已经通过类 MapReduce 的操作 shuffle 完成。
其次，在窄依赖中，节点失败后的恢复更加高效。
因为只有丢失的父级分区需要重新计算，并且这些丢失的父级分区可以并行地在不同节点上重新计算。
与此相反，在宽依赖的继承关系中，单个失败的节点可能导致一个 RDD 的所有先祖 RDD 中的一些分区丢失，导致计算的重新执行。

RDD 的这种通用接口使得在 Spark 中使用不到 20 行的代码来实现大多数 transform 操作。
事实上，即使是 Spark 的新用户也能实现新的 transform 操作(如：抽样和各种类型的 join)而不必了解调度细节。
下面是一些 RDD 实现的概略说明：

**HDFS 文件**：在我们的例子中，HDFS 文件作为输入 RDD 的元素。
对于这些 RDD，partitions 代表文件中每个文件块的分区(包含文件块在每个分区对象中的偏移量)，
preferredLocations 表示文件块所在的节点，而 iterator 读取这些文件块。

**map**：在任何一个 RDD 上调用 map 操作将返回一个 MappedRDD 对象。
这个对象与其父对象具有相同的分区以及首选地点(preferredLocations)，但在其迭代方法(iterator）中，
传递给 map 的函数会应用到父对象记录。

**union**：在两个 RDD 上调用 union 操作将返回一个 RDD，这个 RDD 的分区为原始两个 RDD 的父 RDD 的分区进行 union 后的结果。
每个子分区都是通过窄依赖于同一个父级分区计算出来的。

**sample**：抽样类似于映射。不同之处在于，RDD 会为每一个分区保存一个生成随机数的种子，来对确定如何对父级记录进行抽样。

**join**：连接两个 RDD 可能会产生两个窄依赖，或两个宽依赖，或一个窄依赖和一个宽依赖。
如果两个 RDD 都是基于相同的 Hash/范围划分策略，那么就会产生窄依赖；
如果一个父 RDD 具有某种划分策略而另一个不具有，则会同时产生窄依赖和宽依赖。
无论哪种情况，结果 RDD 都具有一个划分策略(要么继承自父 RDD，要么是一个默认的 Hash 划分策略)。

![图 2.4：宽依赖和窄依赖的样例。每一个方框表示一个 RDD，其内的阴影矩形表示 RDD 的分区。](https://i.loli.net/2021/06/24/ph3keMn6vCNHyaS.png)

#### 2.5 实现

每一个 Spark 程序都以一个独立的应用在集群上运行，它有它自己的驱动节点(主节点,Master)和工作节点(Workers)。
各个应用之间的资源共享则通过集群管理器来控制。
Spark 可以从任何 Hadoop 的输入源(例如使用 Hadoop 的 HDFS 和 HBase)中使用 Hadoop 的现有输入插件 APIs 读取数据，
并且在未更改的 Scala 版本上运行。
现在我们描述了几种在系统中有趣的技术：我们的作业调度程序，多用户支持，Spark 解析器的交互式使用，内存管理，并且检查点支持。

##### 2.5.1 作业调度

![图 2.5：Spark 如何计算 job 的 stage 的例子。](https://i.loli.net/2021/06/24/RfB4iedokxc9nsE.png)

> 注：实线圆角方框标识的是 RDD。阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。
> 阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。
> RDD G 上一个 Action 的执行将会以宽依赖为分区来构建各个 stage，对各 stage 内部的窄依赖则前后连接构成流水线。
> 在本例中，stage 1 的输出已经存在 RAM 中，所以直接执行 stage 2 ，然后 stage 3。

总的来说，我们的调度器与 Dryad 的[61]类似，但它额外会考虑被持久化(persist)的 RDD 的哪个分区保存在内存中并可供使用。
当用户对一个 RDD 执行 Action(如 count 或 save)操作时，调度器会根据该 RDD 的 lineage，
来构建一个由若干阶段(stage)组成的一个 DAG(有向无环图)以执行程序，正如图 2.5 所示。
每个 stage 都包含尽可能多的连续的窄依赖型转换。
各个阶段之间的分界则是宽依赖所需的 shuffle 操作，或者是 DAG 中一个经由该分区能更快到达父 RDD 的已计算分区。
之后，调度器运行多个任务来计算各个阶段所缺失的分区，直到最终得出目标 RDD。

调度器向各机器的任务分配采用延时调度机制并根据数据存储位置(本地性)来确定。
若一个任务需要处理的某个分区刚好存储在某个节点的内存中，则该任务会分配给那个节点。
否则，如果一个任务处理的某个分区，该分区含有的 RDD 提供较佳的位置(例如，一个 HDFS 文件)，我们把该任务分配到这些位置。

对应宽依赖类的操作(比如 shuffle 依赖)，我们会将中间记录物理化到保存父分区的节点上。
这和 MapReduce 临时存储 Map 的输出类似，能简化数据的故障恢复过程。

对于执行失败的任务，只要它对应 stage 的父类信息仍然可用，它便会在其他节点上重新执行。
如果某些 stage 变为不可用(例如，因为 shuffle 在 map 阶段的某个输出丢失了)，则重新提交相应的任务以并行计算丢失的分区。
我们还不能接受调度程序的失败，尽管复制相应 RDD 的 lineage 是比较直接的解决之道。

若某个任务执行缓慢(即"落后者"straggler)，系统则会在其他节点上执行该任务的拷贝这与 MapReduce 做法类似，并取最先得到的结果作为最终的结果。

最后，虽然目前在 Spark 中所有的计算都是为了对驱动程序中调用动作的响应而执行，我们也试验让集群上的任务（如映射）调用查找操作，
它根据键值能够随机访问散列分区的 RDDs 的元素。
在这种设计下，如果任务所需要的分区丢失了，则该任务需要告知调用器去重新计算该分区。

##### 2.5.2 多用户管理

RDD 模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群上能支持多种资源共享算法。
特别地是，每个 RDD 应用可以在执行过程中动态增长，并且可以轮询访问每台设备，或者可以被高优先级的应用占用。
Spark 应用中大多数的任务的执行周期在 50 毫秒到数秒之间，这使得共享请求能得到快速响应。

虽然多用户共享算法并非本论文的主题，但如下我们列出了所支持的那些具体算法，以给读者一个感性的认识：

- 在每个应用程序中，Spark 允许多线程同时提交作业，并通过一种等级公平调用器来实现多个作业对集群资源的共享。
  这种调用器和 Hadoop Fair Scheduler 类似。
  此特性主要用于创建基于针对相同内存数据的多用户应用，例如：Shark SQL 引擎有一个服务模式支持多用户并行运行查询。
  公平共享确保作业彼此分离，同时短的作业能在即使长作业占满集群资源的情况下也可尽早完成。
- Spark 的公平调度也使用延迟调度[117]，通过轮询每台机器的数据，在保持公平的情况下给予作业高的数据本地性。
  在本章几乎所有的试验中，内存本地化访问(Memory Locality)为 100%。
  Spark 支持多级本地化访问策略(本地性)，包括内存、磁盘和机架，以降低在一个集群里不同方式下的数据访问的代价。
- 由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。
- 纵观 Spark 的应用，Spark 仍然使用 Mesos 中资源提供的概念来支持细粒度共享，它让不同的应用使用相同的 API 发起细粒度的任务请求。
  这使得 Spark 应用能相互之间或在不同的计算框架(例如 Hadoop)之间实现资源的动态共享。
  延迟调度仍然能够在资源提供模型中提供数据本地性。
- 最后，Spark 使用 Sparrow 系统扩展支持分布式调度。
  该系统允许多个 Spark 应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。
  通过以去主节点方式来进行多任务提交时，分布式调度可极大地提升系统的可扩展性。

##### 2.5.3 解析器集成

与 Ruby 和 Python 类似，Scala 也提供了一个交互式 Shell(解析器)。
借助内存数据所带来的低延迟特性，我们希望让用户也能通过解析器来运行 Spark 并对大数据集进行交互式查询。

Scala 解析器通常会为用户输入的每一行生成一个类，把它导入 JVM ，调用上面的一个函
数。Scala 解析器的解析通常有如下组成：

1. 将用户输入的每一行编译出其所对应的一个类；
2. 将该类载入到 JVM 中；
3. 调用该类的某个函数。
   
这个类包含一个单例对象，对象中包含当前行的变量或函数，在初始化方法中包含运行该行的代码。
例如，如果用户输入 `var x = 5`，换一行再输入 `println(x)`，那解析器会定义一个叫 Line1 的类，该类包含 x。
第二行编译成 `println(Line1.getInstance().x)`。

Spark 中我们做了两个改变：

1. 类传输：为了让工作节点能够从各行生成的类中获取到字节码，我们让解析器通过 HTTP 来为类提供服务。
   为能让 Worker 节点能获取到各行对应类的字节码，我们让解析器通过 HTTP 来提供这些类。
2. 代码生成器的改动：通常，各种代码生成的单例对象是经由其相应类的一个静态方法来访问的。
   也就是说，当我们序列化一个引用了上一行中定义的变量的闭包（例如上面例子中的 Line1.x） 时，Java 不会通过检索对象树的方式去传输包含 x 的 Line1 实例。
   因此，工作节点不能得到 x。
   我们修改了代码生成器的逻辑，让各行对象的实例可以被直接引用。

##### 2.5.4 内存管理

Spark 提供了三种对持久化 RDD 的存储策略：
未序列化 Java 对象存于内存中、序列化后的数据存于内存及磁盘存储。
第一个选项的性能表现是最优秀的，因为可以直接访问在 JAVA 虚拟机内存里的 RDD 对象。
在空间有限的情况下，第二种方式可以让用户采用比 JAVA 对象图更有效的内存组织方式，代价是降低了性能。
第三种策略适用于 RDD 太大难以存储在内存的情形，但每次重新计算该 RDD 会带来额外的资源开销。

对于有限可用内存，我们使用以 RDD 为对象的 LRU 回收算法来进行管理。
当计算得到一个新的 RDD 分区，但却没有足够空间来存储它时，系统会从最近最少使用的 RDD 中回收其一个分区的空间。
除非该 RDD 便是新分区对应的 RDD，这种情况下，Spark 会将旧的分区继续保留在内存，防止同一个 RDD 的分区被循环调入调出。
这点很关键--因为大部分的操作会在一个 RDD 的所有分区上进行，那么很有可能已经存在内存中的分区将会被再次使用。
到目前为止，这种默认的策略在我们所有的应用中都运行很好，当然我们也为用户提供了“持久化优先级”选项来控制 RDD 的存储。
最后，Spark 集群中的每一个实例都有其自己独立的内存空间。
在后续的工作中，我们计划通过一个统一的内存管理器来实现多个 Spark 实例之间的 RDD 共享。
Berkeley 正在进行的 Tachyon 项目便是朝着这个目标。

##### 2.5.5 检查点相关支持

虽然 lineage 可用于错误后 RDD 的恢复，但对于很长的 lineage 的 RDD 来说，这样的恢复耗时较长。
由此，将某些 RDD 进行检查点操作(Checkpoint)保存到稳定存储上，是有帮助的。

通常情况下，对于包含宽依赖的长血统的 RDD 设置检查点操作是非常有用的，比如 PageRank 例子 (§2.3.2)中的排名数据集；
在这种情况下，集群中某个节点的故障会使得从各个父 RDD 得出某些数据丢失，这时就需要完全重算。
相反，对于那些窄依赖于稳定存储上数据的 RDD 来说，对其进行检查点操作就不是有必要的。
这样的 RDD 如 logistic 回归的例子(第 2.3.2 节)和 PageRank 中的链接列表。
如果一个节点发生故障，RDD 在该节点中丢失的分区数据可以通过并行的方式从其他节点中重新计算出来，计算成本只是复制整个 RDD 的很小一部分。

Spark 当前提供了为 RDD 设置检查点(用一个 REPLICATE 标志来持久化)操作的 API,让用户自行决定需要为哪些数据设置检查点操作。
但是，我们也正在对检查点操作自动化进行研究。
因为调度器知道每个数据集的大小以及计算它的消耗的时间，那它应该可以选出所需 Checkpoint 的那些 RDD 以最小化系统恢复所需时间。

最后，由于 RDD 的只读特性使得比常用的共享内存更容易做 checkpoint。
由于不需要关心一致性的问题，RDD 的写出可在后台进行，而不需要程序暂停或进行分布式快照。

#### 2.6 性能评估

略

#### 2.7 讨论

虽然 RDD 的不可变性质和粗粒度变换特质，使得其编程接口看上去能力有限，但实际中我们发现它们能适应的应用种类广泛。
具体来说，RDD 可以表达大量的集群编程模型，而这些模型之前都是针对独立框架而提出。
从而，RDD 使得用户可以在一个程序中组合这些模型 (比如, 先运行一个 MapReduce 操作来构建一个图，而后对该图调用 Pregel)，并在它们之间共享数据。
在本节中，我们将讨论 RDD 可以表达哪些编程模型，以及为什么它们被广泛应用(第 2.7.1 节)。
另外，我们还将讨论 RDD 中 lineage 信息的另一个好处--它使得在这些模型之间的调试变得容易。

##### 2.7.1 对现有编程模型的表达

RDD 可以高效的表现一些此前相对独立的集群编程模型。
所谓“高效”，是指 RDD 不仅能得到与它们相同的输出结果，
而且还囊括了对这些框架所进行的优化，比如将特定的数据保持在内存中、数据分区优化以降低网络通讯和高效率的故障恢复。
可以用 RDD 表达的模型包括：

**MapReduce**：这个模型可以由 Spark 中的 flatMap 与 groupByKey 操作进行表达，而如果用到 combiner 时则可引入 reduceByKey 操作。

**DryadLINQ**：DryadLINQ 系统基于 Dryad 更通用的运行机制上，提供了比 MapReduce 更为丰富的操作。
但这些操作都是批处理操作，且 Spark 中都有相应的 RDD 变换操作与之对应(如 map，groupByKey，join 等等)。

**Pregel**：谷歌的 Pregel 是一个专门针对迭代图型应用的模型。
这种模型初看之下与其他系统面向集合的编程模型有很大的不同。
在 Pregel 中，一个程序以一系列协调好的 superstep 运行。
在每一个 superstep 里，图内的各节点都通过执行一个用户定义的函数来实现对自身状态的更新和对图的拓扑结构的改变，
并向其他节点发送包含它们在下一超步所需要的信息的消息。 
该模型可以表达许多图形算法，包括最短路径，二分匹配，和 PageRank。
在 Pregel 的关键是每次迭代中，它是将相同的用户自定义函数运用到所有节点上。
这是 RDD 可以表达 Pregel 模型的关键。
具体来说，我们可以将各次迭代时的节点状态保存为一个 RDD，然后调用一个变换(flatMap)来执行用户自定义的函数，并生成上述消息所对应的 RDD。
之后，通过将该 RDD 和节点状态的 RDD 进行 join 操作，便可实现消息的交换。
同样值得注意的是，RDD 同时也能提供如 Pregel 那样将节点状态保存在内存中、控制节点分区策略来减少网络通讯以及出现故障时的部分恢复功能。
我们在 Spark 上实现了一个 200 行的 Pregel 库。

**迭代式 MapReduce**：近期所提出的系统，包括 HaLoop 和 Twister ， 提供了一种迭代式的 MapReduce 模型。
在该模型下用户可以指定系统运行一系列的 MapReduce 任务。
这些系统可以保持每次迭代的数据分区一致性，其中 Twister 还可将讲数据保持在内存中。
这些优化都可轻松地用 RDD 来表达，HaLoop 模型的实现对应一个 200 行左右的库。

