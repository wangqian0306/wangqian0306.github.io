---
title: An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译 
date: 2021-06-24 22:26:13 
tags:
- "论文"
- "Spark"
id: architecture_of_next_generation_apache_hadoop_mapreduce_framework
no_word_count: true
no_toc: false
categories: 大数据
---

## An Architecture for Fast and General Data Processing on Large Clusters 部分中文翻译

作者：Matei Zaharia

[英文原文](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.html)

> 注：由于原文数量过于庞大，此处仅仅针对于目前自己不太理解的部分进行翻译和整理。

### 1 引言

我们可以设计一个统一的编程抽象，不仅可以处理这些不同的计算任务，而且能使新的应用更好的编程。
特别的是，我们将展示 MapReduce 的一个简单扩展，称为弹性分布式数据集（RDDs）,它增加了高效的数据共享元语，以及大大增加了它的通用性。
由此产生的架构比当前系统有几个关键优势：

1. 在相同的运行环境下，它支持批处理、交互式、迭代和流计算，结合这些模式提供丰富的应用编程，并且相对于单一模式的系统能更好的发挥其性能。
2. 它以很小的代价在这些计算模式上提供结点故障和 straggler 的容忍功能。
   事实上，在一些地方(如流和 SQL)，基于 RDD 产生的新系统比现有的系统有更强的容错性。
3. 它实现的性能往往比 MapReduce 高 100 倍，并可媲美各个应用领域的专业系统。
4. 这很适合多组织用户管理，允许应用程序弹性地扩缩容和响应式地共享资源。

#### 1.1 专业系统相关问题

1. 重复工作：许多专业系统仍然需要解决同样的潜在问题，如分布式执行和容错性。
   举个例子，分布式 SQL 引擎或机器学习引擎都需要执行并行聚合。
   对于独立的系统，针对每个领域也是需要解决这些问题。
2. 组成：不同系统的组合进行计算的方式即昂贵又笨重。
   尤其是对于“大数据”应用，中间处理过程的数据集是庞大的且难以移动的。
   为了使得在各个计算引擎之间共享数据，当前的环境需要将数据导出到稳定且多备份的存储系统中，通常这比实际计算要消耗更多的资源。
   因此，相比于一栈式的系统，由多个系统组成的管道常常效率很低。
3. 范围限制：如果应用程序不符合专业系统的编程模型，用户只能修改程序以适应当前的系统，否则就针对该程序写一个新的运行系统。
4. 资源共享：在计算引擎之间动态共享资源是很困难的，因为大多数引擎在应用程序运行期间都假定独自拥有一组机器。
5. 管理和管理员：相对单一的系统，独立的系统需要在管理和部署上处理更多的事务。
   对于用户来说，它们需要学习多种 API 和执行模型。

#### 1.2 弹性分布式数据集(RDD)

为了解决这个问题，我们引入一个新的概念，弹性分布式数据集(RDDs)，它是 MapReduce 模型一种简单的扩展和延伸。
进一步说，虽然乍一看那些不适合 MapReduce 的计算任务(例如，迭代，交互性和流查询)之间存在着明显的不同，但他们却都有一个功能特性，
也是 MapReduce 模型的缺陷：在并行计算阶段之间能够高效地数据共享，这正是 RDD 具有真知灼见的地方。
运用高效的数据共享概念和类似于 MapReduce 的操作方式，使得所有这些计算工作都可以有效地执行，并可以在当前特定的系统中获得关键性的优化。
RDDs 以一种既高效有能容错的方式为广泛的并行计算提出这样一个抽象。

特别提出的是，以前的这些集群容错处理模型，像 MapReduce、Dryad，将计算转换为一个有向无环图(DAG)的任务集合。
这使得它们能够高效地重复执行 DAG 里的其中一部分任务来完成容错恢复。
但对于一个独立的计算，(例如在一个迭代过程中)，这些模型除了可复制的文件系统外没有提供其他存储的概念，
这就导致因为在网络上进行数据复制而增加了大量的消耗。
RDDs 是一个可以避免复制的容错分布式存储概念。
取而代之，每一个 RDD 都会记住由构建它的那些操作所构成的一个图，类似于批处理计算模型，可以有效地重新计算因故障丢失的数据。
由于创建 RDD 的操作是相对粗粒度的，即单一的操作应用于许多数据元素，该技巧比通过网络复制数据更高效。
RDDs 很好地运用于当前广泛的数据并行算法和处理模型中，所有的这些对多个任务使用同一种操作。

#### 1.3 基于 RDD 机制实现的模型

我们使用 RDD 机制实现了多类模型，包括多个现有的集群编程模型和之前模型所没有支持的新应用。
在这些模型中，RDD 机制不仅在性能方面能够和之前系统相匹配，在其他方面，他们也能加
入现有的系统所缺少的新特性，比如容错性，straggler 容忍和弹性。我们讨论以下四类模型。

**迭代式算法** 一种目前已经开发的针对特定系统最常见的的工作模式是迭代算法，比如应用于图处理，数值优化，以及机器学习中的算法。
RDD 可以支持广泛类型的各种模型，包括 Pregel，像 HaLoop 和 Twister 这类的迭代式 MapReduce 模型，
以及确定版本的 GraphLab 和 PowerGraph 模型。

**关系查询** 在 MapReduce 集群中的首要需求中的一类是执行 SQL 查询，长期运行或多个小时的批量计算任务和即时查询。
这促进了很多在商业集群中应用的并行数据库系统的发展。
MapReduce 相比并行数据库在交互式查询上有非常大的缺陷，例如 MapReduce 的容错机制模型，
而我们发现通过在 RDD 操作中实现很多常用的数据库引擎的特性(比如，列处理)，这样能够达到相当可观的性能。
由上述方式所构建的系统 Shark，提供完整的容错机制，能够在短查询和长查询中很好的扩展，同时也能在 RDD 之上提供复杂分析函数的调用(例如, 机器学习)。

> 注：Shark 是基于 Hive 进行实现的，通过更换了 Hive 的物理执行引擎进行了性能改进。但是它仍然限制于 MapReduce 的遗留问题。
> 之后被 Spark SQL 模块替换，本文中不会再去关注这些内容了。

**MapReduce RDD** 通过提供 MapReduce 的一个超集，能够高效地执行 MapReduce 程序，
同样也可以指向比如 DryadLINQ 这样常见的机遇 DAG 数据流的应用。

**流式数据处理** 我们的系统与定制化系统最大的区别是我们也使用 RDD 实现了流式处理。
流式数据处理已经在数据库和系统领域进行了很长时间研究，但是实现大规模流式数据处理仍然是一项挑战。
当前的模型并没有处理在大规模集群中频繁出现的 straggler 的问题，同时对故障恢复的方式也非常有限，需要大量的复制或浪费很长的恢复时间。
特别是，当前的系统是基于一种持续操作的模型，这就需要长时间的有状态的操作处理每一个到达的记录。
为了恢复一个丢失的节点，当前的系统需要保存每一个操作符的两个副本，或通过一系列耗费大量开销的串行处理来对上游的数据进行重放。

我们提出了一个新的模型，离散数据流(D-Streams),来解决这样的问题。
对使用长期状态处理的过程进行替换，D-Streams 把流式计算的执行当做一系列短而确定性的批量计算的序列，将状态保存在 RDD 里。
D-Stream 模型通过根据相关 RDD 的依赖关系图进行并行化恢复，就能达到快速的故障恢复，这样不需要通过复制。
另外，它通过推测(Speculative)来支持对 straggler 执行迁移，例如，对那些慢任务运行经过推测的备份副本。
尽管 D-Stream 将计算转换为许多不相关联的 jobs 来运行从而增加了部分延迟，然而我们证明了 D-Stream 能够被达到次秒级延时的实现，
这样能够达到以前系统单个节点的性能，并能线性扩展到 100 个节点。
D-Stream 的强恢复特性让他们成为了第一个处理大规模集群特性的流式处理模型，并且他们基于 RDD 的实现使得应用能够有效的整合批处理和交互式查询。

#### 1.4 总结

略

#### 1.5 论文计划

本文组织结构如下。
第 2 章介绍了 RDD 抽象并涵盖了一些简单的编程模型的应用。
第 3 章介绍了 Shark SQL 系统基于 RDDs 实现的更高级的存储和处理模型的技术。
第 4 章介绍了如何使用 RDDs 开发离散的流，这是一种新的流式处理模型。
第 5 章则介绍了为什么 RDD 模型在这些应用中如此通用，同时介绍它的限制和扩展性。
最后，在第 6 章，我们总结和讨论一些未来工作的可能方向。

### 2 弹性分布式数据集

#### 2.1 简介

我们所提出的弹性分布式数据集(RDDs)，这种全新的抽象模式令用户可以直接控制数据的共享。
RDD 具有可容错和并行数据结构特征，这使得用户可以指定数据存储到硬盘还是内存、控制数据的分区方法并在数据集上进行种类丰富的操作。
他们提供了一个简单高效的编程接口，可以同时满足现有的特定模型和全新的应用场景。
RDD 设计时的最大挑战在于定义一个能提供高效容错能力的编程接口。
现有的基于集群的内存存储抽象，比如分布式共享内存，键-值存储，数据库，以及 Piccolo,提供了一个对内部状态基于细粒度更新的接口(例如,表格里面的 cell)。
在这样的设计之下，提供容错性的方法就要么是在主机之间复制数据，要么对各主机的更新情况做日志记录。
这两种方法对于数据密集型的任务来说代价很高，因为它们需要在带宽远低于内存的集群网络间拷贝大量的数据，同时还将产生大量的存储开销。

与上述系统不同的是，RDD 提供一种基于粗粒度变换(如， map, filter, join)的接口，该接口会将相同的操作应用到多个数据集上。
这使得他们可以通过记录用来创建数据集的变换(lineage)，而不需存储真正的数据，进而达到高效的容错性。
当一个 RDD 的某个分区丢失的时候，RDD 记录有足够的信息记录其如何通过其他的 RDD 进行计算，且只需重新计算该分区。
因此，丢失的数据可以被很快的恢复，而不需要昂贵的复制代价。

#### 2.2 RDD 概述

##### 2.2.1 概念

从形式上看，RDD是一个分区的只读记录的集合。
RDD只能通过在(1)稳定的存储器或(2)其他RDD的数据上的确定性操作来创建。
我们把这些操作称作变换以区别其他类型的操作。例如 map, filter, 和 join。

> 注：尽管单个的 RDDs 是不可变的，但可以通过多个 RDDs 来表示一个数据集的多个版本来实现可变。
> 这种性质(可变)使得描述其 lineage(获取 RDD 所需要经过的变换)变得容易。
> 可以这样理解，RDD 是版本化的数据集，并且可以通过变换记录追踪版本。

RDD 在任何时候都不需要被"物化"(进行实际的变换并最终写入稳定的存储器上)。
实际上，一个 RDD 有足够的信息描述着其如何从其他稳定的存储器上的数据生成。
它有一个强大的特性：从本质上说，若 RDD 失效且不能重建，程序将不能引用该 RDD。

最后，用户可以控制 RDD 的其他两个方面：持久化和分区。
用户可以选择重用哪个 RDD，并为其制定存储策略(比如， 内存存储)。
也可以让 RDD 中的数据根据记录的 key 分布到集群的多个机器。
这对位置优化来说是有用的，比如可用来保证两个要 join 的数据集都使用了相同的哈希分区方式。

##### 2.2.2 Spark 编程接口

具体来说，每一个数据集都会表示为一个对象，而各种变换则通过该对象相应方法的调用而实现。

在最开始，编程人员通过对稳定存储上的数据进行变换操作(例如, map 和 filter) 来得到一个或多个 RDD。
之后，他们可以调用这些 RDD 的 actions(动作)类的操作。
这类操作的目的或是返回一个值，或是将数据导入到存储系统中。
动作类的操作如 count(返回数据集的元素数), collect(返回元素本身的集合)和 save(输出数据集到存储系统)。
与 DryadLINQ 一样，Spark 直到 RDD 第一次调用一个动作时才真正计算 RDD。
这也就使得 Spark 可以按序缓存多个变换。

此外，编程人员还可以调用 RDD 的 persist(持久化)方法来表明该 RDD 在后续操作中还会用到。
默认情况下，Spark 会将调用过 persist 的 RDD 存在内存中。
但若内存不足，也可 以将其写入到硬盘上。
通过指定 persist 函数中的参数，用户也可以请求其他持久化策略并通过标记来进行 persist，比如仅存储到硬盘上，又或是在各机器之间复制一份。
最后，用户可以在每个 RDD 上设定一个持久化的优先级来指定内存中的哪些数据应该被优先写入到磁盘。

##### 2.2.3 优点

略

##### 2.2.4 不适合 RDD 的应用

正如在引言中讨论的，RDDs 最适合对数据集中所有的元素进行相同的操作的批处理类应用。
在这些情况下，作为整个 lineage 图中的其中一步，RDD 高效地记住每一次变换，从而不需要对大量数据做日志记录便可恢复失效分区。
RDDs 不太适用于通过异步细粒度更新来共享状态的应用，比如针对 Web 应用或增量网络爬虫的存储系统。
对于这些应用，那些传统的更新日志和数据检查点的系统会更有效。

#### 2.3 Spark 编程接口

为了使用 Spark, 开发者需要写一个 driver program 来连接到 workers 集群。
driver program 定义一个或多个 RDDs 以及相关的一些 action 操作。
driver 上的 spark 代码也跟踪记录 RDDs 的继承关系，即 lineage。
Worker 是一直运行着的进程，它将经过一系列操作后的 RDD 分区数据保存在内存中。

用户通过传递闭包的方式将参数传递给 Map 等操作。
在 Scala 中每个闭包都代表一个 Java 对象，这些对象可以被序列化，也可以通过网络将闭包传递给其他节点并加载。
Scala 会将闭包中的所有变量转义成 Java 对象的属性域。

#### 2.4 抽象 RDDs

抽象 RDDs 的一个挑战是如何在经过一系列 transform 操作后追踪其继承关系。
理想情况下，一个实现了 RDD 的系统必须尽可能多地提供各种变换操作，并允许用户随意进行组合。
我们提出了一种基于图的方式来抽象 RDD，它可以实现上述目标。
我们已经在 Spark 中使用了这种表现形式来提供各种 transform 操作，而无需为每个 transform 操作的调度增加额外的逻辑。
这极大度简化了系统的设计。

简而言之，我们提供了一个通用接口来抽象每个 RDD，并提供 5 种信息：
一种是分区信息，这些信息是数据集最小的分片；
一种是依赖关系信息，指向其父 RDD；
一种是函数，基于父 RDD 进行计算；
一种是分区方案；
最后一种是数据存放位置。
例如：一个表现 HDFS 文件的 RDD 将文件的每个文件块表示为一个分区，并且知道每个文件块的位置信息。
同时，对 RDD 进行 map 操作后具有相同的划分。
当计算其元素时，将 map 函数应用于父 RDD 的数据。

在设计接口的过程中，最有趣的问题在于如何表示 RDD 之间的依赖关系。
我们发现，比较合理的方式是将依赖关系分成两类：
窄依赖，每个父 RDD 的分区都至多被一个子 RDD 的分区使用；
宽依赖：多个子 RDD 的分区依赖一个父 RDD 的分区。

这两种依赖的的区别从两个方面来说比较有用。
首先，窄依赖允许在单个集群节点上流水线式执行，这个节点可以计算所有父级分区。
例如，可以逐个元素地依次执行 filter 操作和 map 操作。
相反，宽依赖需要所有的父 RDD 数据可用并且数据已经通过类 MapReduce 的操作 shuffle 完成。
其次，在窄依赖中，节点失败后的恢复更加高效。
因为只有丢失的父级分区需要重新计算，并且这些丢失的父级分区可以并行地在不同节点上重新计算。
与此相反，在宽依赖的继承关系中，单个失败的节点可能导致一个 RDD 的所有先祖 RDD 中的一些分区丢失，导致计算的重新执行。

RDD 的这种通用接口使得在 Spark 中使用不到 20 行的代码来实现大多数 transform 操作。
事实上，即使是 Spark 的新用户也能实现新的 transform 操作(如：抽样和各种类型的 join)而不必了解调度细节。
下面是一些 RDD 实现的概略说明：

**HDFS 文件**：在我们的例子中，HDFS 文件作为输入 RDD 的元素。
对于这些 RDD，partitions 代表文件中每个文件块的分区(包含文件块在每个分区对象中的偏移量)，
preferredLocations 表示文件块所在的节点，而 iterator 读取这些文件块。

**map**：在任何一个 RDD 上调用 map 操作将返回一个 MappedRDD 对象。
这个对象与其父对象具有相同的分区以及首选地点(preferredLocations)，但在其迭代方法(iterator）中，
传递给 map 的函数会应用到父对象记录。

**union**：在两个 RDD 上调用 union 操作将返回一个 RDD，这个 RDD 的分区为原始两个 RDD 的父 RDD 的分区进行 union 后的结果。
每个子分区都是通过窄依赖于同一个父级分区计算出来的。

**sample**：抽样类似于映射。不同之处在于，RDD 会为每一个分区保存一个生成随机数的种子，来对确定如何对父级记录进行抽样。

**join**：连接两个 RDD 可能会产生两个窄依赖，或两个宽依赖，或一个窄依赖和一个宽依赖。
如果两个 RDD 都是基于相同的 Hash/范围划分策略，那么就会产生窄依赖；
如果一个父 RDD 具有某种划分策略而另一个不具有，则会同时产生窄依赖和宽依赖。
无论哪种情况，结果 RDD 都具有一个划分策略(要么继承自父 RDD，要么是一个默认的 Hash 划分策略)。

![图 2.4：宽依赖和窄依赖的样例。每一个方框表示一个 RDD，其内的阴影矩形表示 RDD 的分区。](https://i.loli.net/2021/06/24/ph3keMn6vCNHyaS.png)

#### 2.5 实现

每一个 Spark 程序都以一个独立的应用在集群上运行，它有它自己的驱动节点(主节点,Master)和工作节点(Workers)。
各个应用之间的资源共享则通过集群管理器来控制。
Spark 可以从任何 Hadoop 的输入源(例如使用 Hadoop 的 HDFS 和 HBase)中使用 Hadoop 的现有输入插件 APIs 读取数据，
并且在未更改的 Scala 版本上运行。
现在我们描述了几种在系统中有趣的技术：我们的作业调度程序，多用户支持，Spark 解析器的交互式使用，内存管理，并且检查点支持。

##### 2.5.1 作业调度

![图 2.5：Spark 如何计算 job 的 stage 的例子。](https://i.loli.net/2021/06/24/RfB4iedokxc9nsE.png)

> 注：实线圆角方框标识的是 RDD。阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。
> 阴影背景的矩形是分区，若已存于内存中则用黑色背景标识。
> RDD G 上一个 Action 的执行将会以宽依赖为分区来构建各个 stage，对各 stage 内部的窄依赖则前后连接构成流水线。
> 在本例中，stage 1 的输出已经存在 RAM 中，所以直接执行 stage 2 ，然后 stage 3。

总的来说，我们的调度器与 Dryad `[61]` 类似，但它额外会考虑被持久化(persist)的 RDD 的哪个分区保存在内存中并可供使用。
当用户对一个 RDD 执行 Action(如 count 或 save)操作时，调度器会根据该 RDD 的 lineage，
来构建一个由若干阶段(stage)组成的一个 DAG(有向无环图)以执行程序，正如图 2.5 所示。
每个 stage 都包含尽可能多的连续的窄依赖型转换。
各个阶段之间的分界则是宽依赖所需的 shuffle 操作，或者是 DAG 中一个经由该分区能更快到达父 RDD 的已计算分区。
之后，调度器运行多个任务来计算各个阶段所缺失的分区，直到最终得出目标 RDD。

调度器向各机器的任务分配采用延时调度机制并根据数据存储位置(本地性)来确定。
若一个任务需要处理的某个分区刚好存储在某个节点的内存中，则该任务会分配给那个节点。
否则，如果一个任务处理的某个分区，该分区含有的 RDD 提供较佳的位置(例如，一个 HDFS 文件)，我们把该任务分配到这些位置。

对应宽依赖类的操作(比如 shuffle 依赖)，我们会将中间记录物理化到保存父分区的节点上。
这和 MapReduce 临时存储 Map 的输出类似，能简化数据的故障恢复过程。

对于执行失败的任务，只要它对应 stage 的父类信息仍然可用，它便会在其他节点上重新执行。
如果某些 stage 变为不可用(例如，因为 shuffle 在 map 阶段的某个输出丢失了)，则重新提交相应的任务以并行计算丢失的分区。
我们还不能接受调度程序的失败，尽管复制相应 RDD 的 lineage 是比较直接的解决之道。

若某个任务执行缓慢(即"落后者"straggler)，系统则会在其他节点上执行该任务的拷贝这与 MapReduce 做法类似，并取最先得到的结果作为最终的结果。

最后，虽然目前在 Spark 中所有的计算都是为了对驱动程序中调用动作的响应而执行，我们也试验让集群上的任务（如映射）调用查找操作，
它根据键值能够随机访问散列分区的 RDDs 的元素。
在这种设计下，如果任务所需要的分区丢失了，则该任务需要告知调用器去重新计算该分区。

##### 2.5.2 多用户管理

RDD 模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群上能支持多种资源共享算法。
特别地是，每个 RDD 应用可以在执行过程中动态增长，并且可以轮询访问每台设备，或者可以被高优先级的应用占用。
Spark 应用中大多数的任务的执行周期在 50 毫秒到数秒之间，这使得共享请求能得到快速响应。

虽然多用户共享算法并非本论文的主题，但如下我们列出了所支持的那些具体算法，以给读者一个感性的认识：

- 在每个应用程序中，Spark 允许多线程同时提交作业，并通过一种等级公平调用器来实现多个作业对集群资源的共享。
  这种调用器和 Hadoop Fair Scheduler 类似。
  此特性主要用于创建基于针对相同内存数据的多用户应用，例如：Shark SQL 引擎有一个服务模式支持多用户并行运行查询。
  公平共享确保作业彼此分离，同时短的作业能在即使长作业占满集群资源的情况下也可尽早完成。
- Spark 的公平调度也使用延迟调度 `[117]`，通过轮询每台机器的数据，在保持公平的情况下给予作业高的数据本地性。
  在本章几乎所有的试验中，内存本地化访问(Memory Locality)为 100%。
  Spark 支持多级本地化访问策略(本地性)，包括内存、磁盘和机架，以降低在一个集群里不同方式下的数据访问的代价。
- 由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。
- 纵观 Spark 的应用，Spark 仍然使用 Mesos 中资源提供的概念来支持细粒度共享，它让不同的应用使用相同的 API 发起细粒度的任务请求。
  这使得 Spark 应用能相互之间或在不同的计算框架(例如 Hadoop)之间实现资源的动态共享。
  延迟调度仍然能够在资源提供模型中提供数据本地性。
- 最后，Spark 使用 Sparrow 系统扩展支持分布式调度。
  该系统允许多个 Spark 应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。
  通过以去主节点方式来进行多任务提交时，分布式调度可极大地提升系统的可扩展性。

##### 2.5.3 解析器集成

与 Ruby 和 Python 类似，Scala 也提供了一个交互式 Shell(解析器)。
借助内存数据所带来的低延迟特性，我们希望让用户也能通过解析器来运行 Spark 并对大数据集进行交互式查询。

Scala 解析器通常会为用户输入的每一行生成一个类，把它导入 JVM ，调用上面的一个函数。
Scala 解析器的解析通常有如下组成：

1. 将用户输入的每一行编译出其所对应的一个类；
2. 将该类载入到 JVM 中；
3. 调用该类的某个函数。

这个类包含一个单例对象，对象中包含当前行的变量或函数，在初始化方法中包含运行该行的代码。
例如，如果用户输入 `var x = 5`，换一行再输入 `println(x)`，那解析器会定义一个叫 Line1 的类，该类包含 x。
第二行编译成 `println(Line1.getInstance().x)`。

Spark 中我们做了两个改变：

1. 类传输：为了让工作节点能够从各行生成的类中获取到字节码，我们让解析器通过 HTTP 来为类提供服务。
   为能让 Worker 节点能获取到各行对应类的字节码，我们让解析器通过 HTTP 来提供这些类。
2. 代码生成器的改动：通常，各种代码生成的单例对象是经由其相应类的一个静态方法来访问的。
   也就是说，当我们序列化一个引用了上一行中定义的变量的闭包(例如上面例子中的 Line1.x)时，Java 不会通过检索对象树的方式去传输包含 x 的 Line1 实例。
   因此，工作节点不能得到 x。
   我们修改了代码生成器的逻辑，让各行对象的实例可以被直接引用。

##### 2.5.4 内存管理

Spark 提供了三种对持久化 RDD 的存储策略：
未序列化 Java 对象存于内存中、序列化后的数据存于内存及磁盘存储。
第一个选项的性能表现是最优秀的，因为可以直接访问在 JAVA 虚拟机内存里的 RDD 对象。
在空间有限的情况下，第二种方式可以让用户采用比 JAVA 对象图更有效的内存组织方式，代价是降低了性能。
第三种策略适用于 RDD 太大难以存储在内存的情形，但每次重新计算该 RDD 会带来额外的资源开销。

对于有限可用内存，我们使用以 RDD 为对象的 LRU 回收算法来进行管理。
当计算得到一个新的 RDD 分区，但却没有足够空间来存储它时，系统会从最近最少使用的 RDD 中回收其一个分区的空间。
除非该 RDD 便是新分区对应的 RDD，这种情况下，Spark 会将旧的分区继续保留在内存，防止同一个 RDD 的分区被循环调入调出。
这点很关键--因为大部分的操作会在一个 RDD 的所有分区上进行，那么很有可能已经存在内存中的分区将会被再次使用。
到目前为止，这种默认的策略在我们所有的应用中都运行很好，当然我们也为用户提供了“持久化优先级”选项来控制 RDD 的存储。
最后，Spark 集群中的每一个实例都有其自己独立的内存空间。
在后续的工作中，我们计划通过一个统一的内存管理器来实现多个 Spark 实例之间的 RDD 共享。
Berkeley 正在进行的 Tachyon 项目便是朝着这个目标。

##### 2.5.5 检查点相关支持

虽然 lineage 可用于错误后 RDD 的恢复，但对于很长的 lineage 的 RDD 来说，这样的恢复耗时较长。
由此，将某些 RDD 进行检查点操作(Checkpoint)保存到稳定存储上，是有帮助的。

通常情况下，对于包含宽依赖的长血统的 RDD 设置检查点操作是非常有用的，比如 PageRank 例子 (§2.3.2)中的排名数据集；
在这种情况下，集群中某个节点的故障会使得从各个父 RDD 得出某些数据丢失，这时就需要完全重算。
相反，对于那些窄依赖于稳定存储上数据的 RDD 来说，对其进行检查点操作就不是有必要的。
这样的 RDD 如 logistic 回归的例子(第 2.3.2 节)和 PageRank 中的链接列表。
如果一个节点发生故障，RDD 在该节点中丢失的分区数据可以通过并行的方式从其他节点中重新计算出来，计算成本只是复制整个 RDD 的很小一部分。

Spark 当前提供了为 RDD 设置检查点(用一个 REPLICATE 标志来持久化)操作的 API,让用户自行决定需要为哪些数据设置检查点操作。
但是，我们也正在对检查点操作自动化进行研究。
因为调度器知道每个数据集的大小以及计算它的消耗的时间，那它应该可以选出所需 Checkpoint 的那些 RDD 以最小化系统恢复所需时间。

最后，由于 RDD 的只读特性使得比常用的共享内存更容易做 checkpoint。
由于不需要关心一致性的问题，RDD 的写出可在后台进行，而不需要程序暂停或进行分布式快照。

#### 2.6 性能评估

略

#### 2.7 讨论

虽然 RDD 的不可变性质和粗粒度变换特质，使得其编程接口看上去能力有限，但实际中我们发现它们能适应的应用种类广泛。
具体来说，RDD 可以表达大量的集群编程模型，而这些模型之前都是针对独立框架而提出。
从而，RDD 使得用户可以在一个程序中组合这些模型 (比如, 先运行一个 MapReduce 操作来构建一个图，而后对该图调用 Pregel)，并在它们之间共享数据。
在本节中，我们将讨论 RDD 可以表达哪些编程模型，以及为什么它们被广泛应用(第 2.7.1 节)。
另外，我们还将讨论 RDD 中 lineage 信息的另一个好处--它使得在这些模型之间的调试变得容易。

##### 2.7.1 对现有编程模型的表达

RDD 可以高效的表现一些此前相对独立的集群编程模型。
所谓“高效”，是指 RDD 不仅能得到与它们相同的输出结果，
而且还囊括了对这些框架所进行的优化，比如将特定的数据保持在内存中、数据分区优化以降低网络通讯和高效率的故障恢复。
可以用 RDD 表达的模型包括：

**MapReduce**：这个模型可以由 Spark 中的 flatMap 与 groupByKey 操作进行表达，而如果用到 combiner 时则可引入 reduceByKey 操作。

**DryadLINQ**：DryadLINQ 系统基于 Dryad 更通用的运行机制上，提供了比 MapReduce 更为丰富的操作。
但这些操作都是批处理操作，且 Spark 中都有相应的 RDD 变换操作与之对应(如 map，groupByKey，join 等等)。

**Pregel**：谷歌的 Pregel 是一个专门针对迭代图型应用的模型。
这种模型初看之下与其他系统面向集合的编程模型有很大的不同。
在 Pregel 中，一个程序以一系列协调好的 superstep 运行。
在每一个 superstep 里，图内的各节点都通过执行一个用户定义的函数来实现对自身状态的更新和对图的拓扑结构的改变，
并向其他节点发送包含它们在下一超步所需要的信息的消息。 
该模型可以表达许多图形算法，包括最短路径，二分匹配，和 PageRank。
在 Pregel 的关键是每次迭代中，它是将相同的用户自定义函数运用到所有节点上。
这是 RDD 可以表达 Pregel 模型的关键。
具体来说，我们可以将各次迭代时的节点状态保存为一个 RDD，然后调用一个变换(flatMap)来执行用户自定义的函数，并生成上述消息所对应的 RDD。
之后，通过将该 RDD 和节点状态的 RDD 进行 join 操作，便可实现消息的交换。
同样值得注意的是，RDD 同时也能提供如 Pregel 那样将节点状态保存在内存中、控制节点分区策略来减少网络通讯以及出现故障时的部分恢复功能。
我们在 Spark 上实现了一个 200 行的 Pregel 库。

**迭代式 MapReduce**：近期所提出的系统，包括 HaLoop 和 Twister ， 提供了一种迭代式的 MapReduce 模型。
在该模型下用户可以指定系统运行一系列的 MapReduce 任务。
这些系统可以保持每次迭代的数据分区一致性，其中 Twister 还可将讲数据保持在内存中。
这些优化都可轻松地用 RDD 来表达，HaLoop 模型的实现对应一个 200 行左右的库。

##### 2.7.2 RDD 表达能力的相关说明

为什么 RDD 能够表达这些不同的编程模型？
原因就是 RDD 上的限制在许多并行应用程序中影响非常小。
其原因在于，虽然 RDD 仅能通过批量变换来创建，但众多的并行程序本质上都是对多条记录执行相同的操作，而这点便使得它们易于表达。
另外，RDD 的不变性也不会影响其表达，因为相同数据集的各个不同版本可以通过多个对应的 RDD 来表示。
事实上，大多数当前的 MapReduce 应用所基于的文件系统，比如 HDFS，并不允许更新文件。
在后续章节(3 和 5)中，我们会对 RDD 表达进行更为详细的阐述。

##### 2.7.3 利用 RDD 进行调试

RDD 的最初始设计时能为容错进行确切的重算特性，该特性也方便了对其的调试。
具体来说，通过记录在作业中创建的 RDD 的 lineage，借助重算所依赖的的RDD分区，人们可以：

1. 重建这些 RDD，同时对其进行交互式查询
2. 在一个单进程调试器中从该作业里运行任意一个任务。

不同于传统的针对一般分布式系统的重放(replay)调试器, 需要对多个节点记录并推断出其事件的先后顺序，而 RDD 只需要记录 lineage 图即可。
因此基本上不会引入任何记录开销。
我们现在就是基于这些概念进行 Spark 调试器的开发工作。

#### 2.8 相关工作

略

#### 2.9 总结

略

### 3 基于 RDD 的模型

#### 3.1 简介

尽管在之前的章节中已经介绍了一些简单的基于RDD的编程模型，例如 Pregel 和 MapReduce 的迭代计算。
不过 RDD 的抽象模型可以用来实现更复杂的工作，包括专用引擎中关键的优化(例如，列式存储的处理和索引)自 Spark 发布以来，已经实现了如下的一些模型：

- SQL 引擎
- 图计算系统
- 机器学习库

> 注：此处没有说明 Spark Streaming。

简单回顾一下之前的章节，RDD 可以提供如下特性：

- 在一个集群中对于任意记录具有不变性的存储（在 Spark 中以 Java 对象的方式来表示）
- 通过每一条记录的 key 字段来控制数据分区
- 将粗粒度的操作用于分区的操作
- 利用内存存储的低延迟特性

接下来，我们将展示如何利用这些特性来实现更复杂的数据处理和存储。

#### 3.2 一些在 RDDs 上实现其他模型的技术

在特定的引擎上，不仅仅优化了数据上的运算符，也优化了数据的存储格式和数据的访问方式。
例如，像 Shark 这样的 SQL 引擎可能会按列来处理数据，但是像 GraphX 这样的图引擎按照索引来处理数据，使得效率表现很出色。
下面我们讨论几个已经在 RDDs 上实现了的这些优化的常见方法，这些方法使得可以在享受 RDD 模型带来的容错等好处的同时，还能保持特定系统的性能。

##### 3.2.1 RDD 里的数据格式

虽然 RDD 存储的是简单、扁平的数据记录，但我们发现一个实现更丰富的存储格式的有效策略是通过在同一条 RDD 记录中存储多个数据项，
并对每一条记录实施更加复杂的存储。
用这样的方式批处理即使几千条记录所带来的效率就足以非常接近使用专门的数据结构，同时仍然保持了每个 RDD 的大小为几兆字节。

RDD模型有两方面的因素使得这个方法非常有效。
首先，RDD通常在内存中，因此对每个操作可以用指针来只读取整个“组合”记录中相关的部分。
例如，一组用列表示的(整型，浮点型)记录可以用一个整型数组和浮点数组来实现。
如果我们只想读取整数字段，我们可以根据指针找到第一个数组而不用在内存中扫描第二个数组。

其次，一个很自然的问题便是，如果每个计算模型都有自己的批处理记录的表示方式，那么如何有效地把要处理的类型结合起来？
幸运的是，RDD 底层接口是基于迭代器的(见 2.4 节中的计算方法)，这可以实现数据在不同格式中快速和流式地转换。
含有复合记录的 RDD 可以通过 flatMap 批量操作有效地在解压的记录上返回一个迭代器，
并且这个迭代器可以被进一步地在解压后的记录上进行管道化的窄变换，或者被重新打包成另一种格式进行转换，从而使未提交的且未解压的数据量最小化。
迭代器由于通常进行扫描操作，一般情况下是用于内存数据中的一个高效的接口。
只要每个批记录能放在 CPU 缓存中，这使得数据能够快速地转换和转化。

##### 3.2.2 数据分区

在特定模型中的第二个常见优化是在一个集群中用特定领域的方式对数据进行划分来提高应用程序的性能。
例如，Pregel 和 HaLoop 使用了一个可能的用户自定义函数来划分数据，从而加快针对一个数据集的连接操作。并行数据库通常也提供了多种数据划分形式。

在 RDD 里，对于每条记录都可以通过记录里的 key 值很容易地进行数据划分。
(事实上，这是在 RDD 拥有的一条记录元素的唯一标识。)
值得注意的是，即使 key 是针对整个记录的，但含有多个潜在数据项的“复合”记录仍然可以有一个有意义的 key 值。
例如，在 GraphX 中，每一个分区中的记录都有相同的散列码来取分区数的模，但是为了能够高效的查找仍然需要在内部进行散列。
当系统使用复合记录来进行 shuffle 操作时，如 groupBy，它们可以将每个突出的记录散列为目标组合记录的 key 值。

最后一个有趣的划分用例是复合数据结构，数据结构的一些字段会随着时间被更新而另外一些字段则不被更新。

一般情况下，用户可以认为 RDDs 是作为一个在集群环境里更为具体的内存抽象。
当在单台机器上使用内存时，程序员主要为了优化查找和最大化提高常访问信息的集中式放置，而需要考虑数据的分布。
RDDs 通过让用户选择一个划分函数和划分的数据集，来提供对分布式内存的控制，但 RDDs 避免了要求用户精确的指定每一个分区的位置。
因此，运行时系统可以基于可用资源对分区数据进行有效地放置，或在出现故障时对分区数据移动，而程序员仍然可以控制访问的性能。

##### 3.2.3 关于不可变性

RDD 模型与大多数特定系统的第三个区别是 RDDs 是不可变的。
不可变性对于 lineage 和错误恢复来说是很重要的，尽管它与为这些目的而具有可变的数据集和记录版本号没有本质上的不同。
但是，可能有人会问这是否会导致性能低下。
虽然不可变性和容错性肯定会导致一些开销，但是我们发现这两项技术在很多情况下都能够表现出良好的性能。

1. 我们用多个 co-partitioned RDDs 来表示复合数据结构，只允许程序修改需要修改部分的状态。
   在很多算法中，尽管记录的其他字段在每次迭代中都会改变，但是有些字段是永远不变的，所以这种方式就可以取得了很好的性能。
2. 当内部的数据结构是不可变的时候，即使在一条记录中，我们也可以用指针来重复使用记录之前"版本"的状态。
   例如，在 Java 中字符串是不可变的，所以一个(Int, String)记录上的 Map 如果保持 String 不变，只改变 Int 值的话，
   我们只需要使用一个指向之前 String 对象指针，而不是去复制它。
   更笼统地说，在函数式编程中的持久化数据结构可以用其他形式的数据上的增量更新(例如，散列表) 来表示之前版本的增量。
   令人很愉快的是，许多函数式编程中的想法可以直接帮助 RDD。

在今后的工作中，我们将继续寻求其他方式来跟踪多个版本的状态，从而接近可变状态系统的性能。

##### 3.2.4 实现自定义转换

最后，我们发现在一些应用中，使用低级的 RDD 接口实现自定义的依赖模式和转换是有用的(参见 2.4 节)。
该接口非常简单，实现他们仅仅需要依赖于父 RDDs 的列表和为 RDD 的分区从父 RDDs 给定的迭代器里进行迭代计算的一个函数。
在 Shark 和 GraphX 的第一版中，我们实现了一些这样的自定义运算符，这导致了很多新的运算符也被加入到 Spark 中去。
比如说，mapPartitions，是我们实现的一个有用的运算符，
在给定一个 `RDD[T]` 和一个计算迭代器函数(给定 `Iterator[T]`，计算出 `Iterator[U]`)的情况下，通过将这个函数作用到每一个分区上，
最后能返回一个 `RDD[U]`。
这是非常接近于 RDDs 最低级的接口，允许在每个分区里执行非功能性的操作(例如，使用可变状态)。
在 Shark 的实现中同样包含 join 和 groupBy 的自定义版本，这是为了取代内置的相应运算符的工作。
但是，请注意即使是实现了自定义转换的应用，这些应用依然能够自动地享受到 RDD 模型的容错、多租户和组合所带来的好处，
并且使得开发将会比独立系统更加简单。

#### 3.3 Shark：RDDs 上的 SQL

略

#### 3.4 实现

略

> 注：此处暂时略过，若 Spark SQL 部分的论文进行了详细阐述则不会重新更新此部分。


#### 3.5 性能

略

#### 3.6 与 SQL 相结合的复杂分析

略

#### 3.7 总结

略

### 4 离散化的流数据处理

本章讲述第二章里的 RDD 模型在一个有可能是最令人心动领域里的应用：大规模流处理。
虽然其设计与传统流系统不同，但它提供丰富的故障恢复，以及强大的与其它处理类型融合的能力。

大规模流处理的动机很简单：大部分“大数据”都是实时获取的，并且到达之时最有价值。
例如，社交网络或想检测出近几分钟内的热点话题，搜索网站会想对哪些用户会访问新网页进行建模，又或是服务运营商对程序日志进行秒级监控以实现实时故障侦测。
要实现这些应用，就需要能扩展到大型集群的流处理模型。

然而，设计这样的模型并不容易，因为应用(比如实时日志处理或机器学习)所需的规模可达数百个节点。
在这种规模下，系统故障和慢节点（straggler）问题会变得很严重，并且流式应用尤其需要快速恢复。
事实上，相比在批处理类应用中，快速恢复在流应用中更显重要：在批处理下，用 30 秒钟从系统故障或慢节点里恢复或许可以接受，而在流处理中，
这 30 秒便可错过一个重要的决策。

不幸的是，现有的流系统对系统故障或慢任务（straggler）的应对能力有限。
大多数分布式流式系统，包括 Storm，TimeStream，MapReduce Online，和流数据库，都基于连续操作模型。
在这种模型中，长期运行的带有状态的操作会接收每条记录，更新内部状态，并且发送新的记录。
这样的模型设计很自然，但也让它难以应对系统故障和慢任务问题。

具体来说，给定连续运算符模型，系统通过两种方法执行恢复：复制，每个节点存储两个副本，
或者 upstream backup，使节点缓存发送信息并在故障节点的新副本里重新执行。
两种途径都不太适合大规模集群：复制要占用 2 倍的硬件，而 upstream backup 需要长时间的恢复，
因为整个系统必须等待一个新的节点通过重新运行操作数据串行重建故障节点的状态。
此外，这两种途经都不能处理慢任务问题：
在 upstream backup 方案下，慢任务需要当作系统故障来处理，而这样的恢复代价高昂；
在复制方案的系统里，采用如 Flux 的同步协议来进行副本之间的协调，恢复速度将受限于慢任务。

这里提出一种名为 D-Stream 的新式流数据处理模型来克服上述问题。
与管理长时间存在的操作不同，D-Streams 结构将各运算流化成为一系列短时间间隔的无状态、确定性的批计算。
例如，我们可以将每秒钟(或每 100 毫秒)所接受的数据按照较短的时间间隔来分段，然后对每一段数据进行 MapReduce 操作来实现计数。
同样的，可将各间隔求出的新的计数加到旧的结果上而实现滚动计数。
通过以这样的方式构建计算，D-Streams 可以确保：

1. 对于给定输入数据，每个时间间隔内的状态完全确定，而不需要同步协议；
2. 当前状态和旧数据的依赖关系细粒度可见。

这样的设计能提供如批处理系统那样的强大的恢复机制，比复制和 upstream backup 方案更好。

实现 D-Stream 模型存在两方面的挑战。首先是要降低延时（间隔粒度）。
传统批处理，如 Hadoop，在这方面有明显缺陷，因为它们任务间使用复制、磁盘储存的方式保存状态。
相反，我们建立在第二章提到的 RDD 数据结构上，可以在内存中保存数据，并且使用操作的 lineage 进行恢复，从而避免了复制。
通过 RDD，我们证明可以达到低于秒级的端对端延迟。
相信这足以满足许多实际大数据应用的需要，一般来说这些应用的时间尺度(例如：社会媒体的倾向)要高得多。

第二个挑战是从故障和慢任务中快速恢复。
这里我们通过 D-Streams 的确定性提供一种新的恢复机制，这种机制在以往的流系统中均未使用过。
丢失节点状态的并行恢复，当某个节点失效时，集群中的各个节点都分担并计算出丢失节点 RDD 的一部分，
从而使得恢复速度远快于 upstream backup，且没有复制开销。
由于需要复杂的状态同步协议，即使是简单的复制操作(例如，Flux)，在连续处理系统中并行恢复也难以实现，但这对完全确定性的 D-stream 模型却变得简单。

与前一条类似，D-Stream 可通过推测性执行(speculative execution)来从慢任务中恢复，而之前的系统均不处理该类任务。

基于 Spark，我们已经在 Spark Streaming 中实现了 D-Streams。
这个系统在 100 个节点上能够处理超过 6000 万记录/秒，延迟在亚秒级，并且可以亚秒时间内从故障和慢任务中恢复。
Spark Streaming 的单节点吞吐量可与商用流数据库相当，但同时提供百级节点上线性扩展的能力。

最后，因为 D-Streams 使用与批任务相同的处理模型和数据结构（RDD），该流处理模型能实现流查询和交互式计算以及批计算无缝结合。
这是一个明显的优势。在 Spark Streaming 中，我们利用这个特性让用户使用 Spark 在流上进行即时(Ad-hoc)查询，
或把流和已计算出的历史数据连接成一个 RDD。
在实际使用中这种特性很有价值，它使得用户通过单一 API 来整合以前彼此不同的计算。
下文将阐述 D-Stream 是如何被用来桥接在线处理和离线处理处理的。

#### 4.2 目标与背景

许多重要的应用需要对实时到达的大规模数据流进行处理。
我们的工作目标是应用需要在几十到数百台机器上执行，并且可以容忍几秒钟的延迟。
一些示例如下：

- 网站活动的统计数据：Facebook 建立了一个分布式聚合系统 Puma，来让广告者统计用户在 10-30 秒内点击他们网页的次数和处理时间。
- 集群监控：数据中心运营商往往使用由数百个节点组成的如 Flume 这样的系统，来对程序日志进行收集和挖掘以发现问题。
- 垃圾邮件检测：社交网络如 Twitter 可能希望利用统计学习算法来实时识别新的垃圾邮件。

对这些应用，我们认为，D-Streams 的 0.5-2 秒的延迟是足够的，因为该延迟远高于所监控的趋势的时间响应需求。
我们特意不针对那些延迟要求低于几百毫秒的应用程序，如高频交易。

##### 4.2.1 目标

为使得这类应用可大规模运行，理想的系统设计需满足如下四个目标：

1. 成百上千的可伸缩节点数目。
2. 基本计算之外的开销最小--例如，不希望付出 2 倍的备份开销。
3. 二级延迟。
4. 从系统故障和慢节点恢复所带来的二级恢复。

据我们所知，之前的系统无法满足这些目标：
基于复制的系统开销很大，而基于 upstream backup 的系统则需数十秒来恢复丢失的状态信息，另外两者均不处理慢节点问题。

##### 4.2.2 以往的处理模型

![图 4.1：对比传统连续性流处理(a)与离散流(b)](https://i.loli.net/2021/06/25/g3YcICvXUdqMyzb.png)

> a (左图)：连续操作处理模型。每个节点连续地接收数据、更新内部状态并且发出新的记录。
> 容错一般来说是通过复制数据来实现的，用类似于 Flux 或 DPC 的同步协议来确保副本数据在每个节点看来都是相同的顺序
> (例如, 当它们有多个父节点时)。
> 
> b (右图)：D-Stream 处理模型。
> 在每个时间间隔，到达的记录被可靠的存储在集群中，形成一个不可变的分区的数据集。
> 之后，这个数据集通过确定性的并行操作，计算其他表示程序输出或状态的分布式数据集。
> 这些会作为下一个间隔里的输入。
> 一个系列里的各个数据集构成一个 D-Stream。

虽然人们已经对分布式流处理进行了广泛的研究，但大部分现有系统均使用连续操作处理模型。
在该模型下，流计算被分隔为由多个有状态的算子(运算)的集合，而各算子以新到的记录为输入来更新自身状态(比如一个统计某个时间段内页面浏览次数的表)，
从而完成对其的计算，并发出新的记录来作为回应，如图 4.1(a)。

尽管连续处理最小化了延迟，但是操作的状态化的特征和由于网络传输记录导致的不确定性，很难有效提供容错机制。
具体来说，恢复的最大挑战在于操作状态在丢失节点或慢节点上的重建。
之前的系统使用两种方案中的一种：复制或 upstream backup。
这并不能在恢复开销和恢复时间之间进行良好平衡。

在复制模式下，这种模型是数据库系统中常用的模型，处理流程会有一个备份，而输入数据会都发给它们。
然而，只是对节点做备份并不够，系统还需如 Flux 或者 Borealis's DPC 那样运行一个同步协议，来保证每个操作(含备份的)会以相同的顺序来对待上游发来的消息。
比如说：一个输出联合（union）两个父运算流的操作需要确保父运算流顺序相同，才能得出相同的输出流，所以操作与其拷贝之间需要协调。
因此，复制方案虽然可以很快恢复，但是耗费大量资源。

upstream backup 模式下：每个节点在检查点时保存其所发出数据的副本。
当一个节点失败之后，备用节点马上接管，父运算流会重新发送信息给备用节点来重建。
这种方式需要花费大量恢复时间，因为通过运行一系列带状态的操作的代码来重新计算出丢失的状态只能在同一个节点上进行。
TimeStream 和 MapReduce Online 使用的是这个模型。
主流的消息队列系统，比如 Storm，也是使用的这种模式，而且通常只保证“至少一次”发送消息，这依赖用户代码来实现处理状态恢复。

更重要的是，复制和 upstream backup 模式都不能应对慢任务问题。
如果在复制模式下，如果一个节点运算很慢，为了确保复制能够保持同序，整个系统都会很慢。
在 upstream backup 模式下，处理慢任务的唯一方法就是标记为失败，这就需要经历前面所提到的缓慢的状态恢复进程，对于偶尔发生的问题，这种规模是太过笨重。
因此，传统的流方法在小规模环境中工作良好，但是在大规模集群中会面对大量问题。

#### 4.3 D-Streams

D-Streams 通过将计算构造为一组短的，无状态的，确定性的任务代替连续的，有状态的操作来避免传统流处理的问题。
然后，它们将状态存放在内存中，再通过容错的数据结构(RDDs)可以重新计算出该状态。
将计算分解成短任务并暴露其细粒度的依赖性，并允许像并行恢复和推测(speculate)这样强大的恢复技术。
除了容错，D-Stream 模型提供了其他好处，比如与批处理相结合。

##### 4.3.1 计算模型

我们把流计算看作在一小段时间周期上进行的一系列确定性的批处理计算。
对于每个时间周期收到的数据，通过集群可靠地存储成一个输入数据集。
一旦时间周期完成时，该数据集便通过确定的并行操作来处理，
例如通过 map,reduce 和 groupBy 等操作来产生新的数据集，该数据集可以表示程序输出或中间状态。
对于前面的情况，结果可以以分布式的方式推送到一个外部系统。
在后面的例子中，中间状态可以通过弹性分布式数据集（RDDs）的高效抽象的存储方式保存，这样可以避免为了恢复使用 lineage 而产生冗余。
该状态数据集可以随同下一批输入数据一起处理，以产生一个新的数据集来更新中间状态。
图 4.1(b) 显示了我们的模型。

![图 4.2：Spark Streaming 系统的高级概述。](https://i.loli.net/2021/06/25/8kaiRCzmbLMc1Il.png)

基于这个模型，我们用 Spark Streaming 实现了这个系统。
我们对每一批数据使用 Spark 作为批处理引擎。
图 4.2 大致描绘了 Spark Streaming 上下文中的计算模型，后面我们会作更详细的解释。

在我们的 API 中，用户通过操纵对象来定义流程，我们称之为 D-Stream。
D-Stream 是一系列具有不可变性的分区数据集（RDDs），我们可以通过确定的转换对它们进行操作。
这些转换生成新的 D-Streams，并且可以通过 RDDs 的形式创建中间状态。

我们通过 Spark Steaming 流式计算运行的程序实现了这个想法。

通过URL计算访问事件。类似于 LINQ，Spark Streaming 通过 Scala 语言的可编程的API 使用 D-Streams。

我们的程序代码如下：

```text
pageViews = readStream("http://...", "1s")
ones = pageViews.map(event => (event.url, 1))
counts = ones.runningReduce((a, b) => a + b)
```

这段代码创建了一个名叫 pageViews 的 D-Stream，通过从 HTTP 读取事件流将他们用 1 秒的时间周期来分组页面访问。
然后将这个事件流通过建立（URL，1）这样的键值的变化对来形成新的 D-Stream，再通过一个状态相关的 runningReduce 转换来对他们进行计数操作。
传入 map 和 runningReduce 的参数是 Scala 的函数字面量。

为了执行这个程序，Spark Streaming 接收数据流，然后将其划分成秒级的批处理任务，并将其存储在 Spark 的 RDDs 内存中(见图 4.2)。
同时，他也会调用 RDD 的转换操作如同 map 和 reduce 来对 RDD 进行处理。
为了执行这些转换，首先 Spark 会启动 map 任务来对这些事件进行处理，同时生成 (url,1) 这样的键值对。
然后，会对 map 的结果和之前 reduce 得到的结果启动 reduce 任务，最后将结果存储在 RDD 里。
这些任务会产生一个更新计数的新 RDD。
程序中的每个 D-Stream 因此变成了一系列的 RDD。

最后，为了错误恢复和慢任务，D-Streams 和 RDDs 要跟踪他们的 lineage,，即用于生成他们的确定性的操作图。
在每一个分布的数据集中，Spark 会在分区的层面跟踪这些信息，如图 4.3 所示。
如果一个节点任务失败，通过重新运行从集群中可靠存储的输入数据构建的任务来重新计算相应的RDD分区。
这个系统还周期性的检查 RDD 的状态(例如通过异步的方式对每十个RDD进行复制)以避免过度重算。
但是不需要对所有数据都进行那样的操作，因为恢复总是很快的：丢失的分区可以在不同的节点上并行计算。
类似的，当一个节点运行缓慢时，因为总会产生同样的结果，我们可以在其他节点上对任务的副本进行推测执行。

我们发现在 D-Streams 中并行恢复比在上游备份具有更高的可用性，即使每个节点上执行了多个操作。
D-Stream 从操作分区和时间两个方面展现了并行化：

1. 如同每个节点执行多个任务的批处理系统，每个节点在每个转换操作的时间片会产生多个 RDD 分区(例如 100 核的集群产生 1000 个 RDD 分区)。
   当节点出现故障时，我们可以在其它节点以并行方式重新计算其分区。
2. lineage 图通常可以使数据从不同的时间片并行地进行重建。
   如图 4.3 所示，如果一个节点出错，我们可能丢失一些时间片的 map 的输出，不同时间片的 map 任务可以并行的重新执行。
   假设需要执行一系列的操作，这在一个连续处理的系统中是无法实现那样的功能的。

依赖这些特性，当每 30 秒建立一次检查点时(参见 4.6.2 节)，D-Streams 仅用 1-2 秒就可以在数百个核心上并行恢复。
我们将在本节的剩余部分更详细地介绍 D-Streams 的可靠性和编程接口。
并在第 4.4 节中讨论如何实现。

##### 4.3.2 时序方面的考虑

D-Streams 将每个记录按其到达系统的时间存入输入数据集。 
这样做可以确保系统总是可以及时开始一个新的批次，尤其是在那些记录从相同的地方里产生的应用中，例如，同一数据中心的服务产生的数据。
这样分割处理的方式，在语义上不会产生错误。 
而在其他应用中，开发者可能希望基于事件发生的外部时间戳将记录分组，例如，基于用户点击某一个链接的时间。
这样一来记录的到达可能是无序的。
D-Streams 提供了两种方法来处理这种情况：

1. 系统可以在开始每个批次之前等待一个有限的“空闲时间”
2. 用户程序可以在应用级别上对晚到的记录进行纠正。
   例如，假设一个应用想要在 t 时刻 与 t+1 时刻间对某广告的点击数进行统计。
   一旦 t+1 时刻过去，应用就可以使用以一秒为周期的 D-Streams，对 t 时刻与 t+1 时刻间接收的点击数进行统计。
   然后，在后面的时间周期里系统可以进一步收集 t 与 t+1 时刻间的其他带有外部时间戳的事件并计算更新统计结果。
   例如，它可能将基于从 t 到 t+5 时间段内收到的记录，在 t+5 时刻产生一个关于 [t, t + 1) 时间区间的新的计数。
   这种计算可以应用一种高效的增量 reduce 操作，即在 t+1 时刻的老计数基础上加上对之后新记录的计数，以避免重复计算。
   此方法类似于处理和顺序无关的程序。

这些时序性的考虑是流式处理系统所必须面对的，因为任何系统都会有外部延时。
数据库领域对此已经进行了详细的研究。
一般来说，这些技术都可以通过D-Streams 来实现，即将计算”离散化“到小批次数据的计算(相同批次的处理逻辑相同)。
因此我们将不在本文中对这些方法做进一步的探讨。

##### 4.3.3 D-Stream API

在 Spark Streaming 中，用户使用函数 API 来注册一个或多个数据流。
程序可以将输入数据流定义为从外部系统中读取数据，该系统通过从对节点端口监听或周期性地从一个存储系统(例如,HDFS)加载来获取数据。
它可以适用于两种类型对这些数据流的操作：

- **转换操作**：从一个或多个父数据流创建一个新的 D-Stream。
  这些操作可能是无状态的，在每个时间周期内对 RDD 分别进行处理，或跨越不同周期来生成状态。
- **输出操作**：使得程序将数据写入外部系统。
  例如，save 操作将 D-Stream 中的每一个 RDD 输出到数据库。

D-Streams 支持在典型批处理框架中所拥有的无状态的转换操作，包括 Map，Reduce，GroupBy 和 Join。

此外，为了支持跨越多个周期的计算，D-Streams 提供了多个有状态的转换操作，这些操作是基于类似于滑动窗口的数据流处理技术基础进行设计的。
这些操作包括：

- 窗口：窗口操作将每一个过去的时间周期的滑动窗口里的所有记录组合到一个 RDD。
- 增量式聚合：对于常用的聚合计算的用例，就像在一个滑动窗口上进行 count 或 max 操作，D-Streams 有增量 reduceByWindow 操作的几个变种操作。
  最简单的一个是仅仅用一个关联的合并函数来对值进行合并。
- 状态跟踪：通常，应用程序为了对表示状态变化的事件流进行响应，需要对各类对象进行状态跟踪。
  例如，一个监控在线视频传输的程序可能会希望对活跃连接的数量进行追踪，一个连接表示从系统收到一个新客户端的“join”事件和当它收到“exit”的时间。
  然后，它能够解决这样的问题：“有多少个传输比特率大于 X 的会话”。
  D-Streams 提供了一个转换数据流的操作 updateStateByKey。
  基于三个参数记录(Key, Event)到(Key, State)记录的数据流中。
  - 一个从第一个事件中为新的键值创建一个 State 值的 initialize 函数。
  - 一个从给定的一个旧状态和一个事件里为它的键值返回一个新的 State 值的 update 函数。
  - 一个用于删除旧的状态的 timeout 函数。

---

例如，用户可以从一个(ClientID, Event) 对的数据流中计算活跃的会话数，如下所示：

```text
sessions = events.track(
(key, ev) =>1， // initialize function
(key, st, ev) => (ev == Exit ? null:1)， // update 函数
"30s") // 超时
counts = sessions.count() // 一个整型的数据流
```

这段代码给每一个活跃客户的状态设为 1，并且在它退出时通过从 update 中返回 null 来将它删掉。
因此，会话对于每一个活跃客户含有一个(ClientID, 1) 元素，同时 counts 用来计算会话的总数。

---

最后，用户调用输出操作符 将 Spark Streaming 的结果发送到外部系统(例如，展示在 dashboard 上)。
我们提供了两个这样的操作：save 操作， 将 D-Stream 中的每一个 RDD 写入到一个存储系统(例如, HDFS 或 HBase)，
和 foreachRDD 在每一个 RDD 上执行一段用户代码段(任意的 Spark 代码)。

##### 4.3.4 一致性语义

D-Streams 的一个好处是，它们具有真正的一致性语义。
跨节点的状态一致性在以记录为基础的流式系统中是一个迫切的问题。
例如，有这样一个系统，按国家来计算其页面访问量，每个页面的浏览事件被发送给负责汇总其国家统计数据的不同节点上。
如果负责英国的节点落后于负责法国的节点，例如，由于加载的原因，那么它们的状态快照将会出现不一致：
英国的计数与法国的相比将会反映流的一个较老的状态，而且计数值通常会较低，从而混淆有关事件的推论。
有些系统，像 Borealis , 其同步节点会避免这个问题。
而其他的系统，像 Storm，却是忽略它。

在 D-Streams 中，一致性语义是非常明确的，因为时间会自然离散为时间周期，每个时间周期的输出 RDDs 反映当前时间周期内以及以前的时间周期收到的所有输入。
这是真实的，无论输出 RDDs 和状态 RDDs 是否分布在集群中，用户无需担心是否有节点在执行上落后。
具体来讲，由于计算的确定性和不同时间周期的数据集的单独命名，
每个输出 RDD 的计算效果相当于以前的时间周期上所有批量作业已经步调一致地运行，并且没有落后的和失败的。
因此，D-Streams 提供一致并“恰好一次”的处理。

##### 4.3.5 批处理与交互式处理的统一

因为 D-Streams 遵循与比处理系统相同的处理模型，数据结构(RDDs)，和类似批处理系统的容错机制，因此两者可以无缝结合。
Spark Streaming 提供了多种强大的功能来统一流式计算和批处理计算。

首先，D-Streams 能够使用标准的 Spark 作业与静态的 RDDs 结合进行计算。
例如，我们可以将消息事件流和预先计算的垃圾过滤器进行连接操作，或者与历史数据进行比较。

其次，用户可以使用“批处理”的模式对历史数据运行一个 D-Stream 程序。
这可以非常方便为历史数据计算一个新的数据流报告。

第三，附加一个 Scala 控制台到 Spark Streaming 程序里，用户可以在 D-Streams 上进行交互式 询，并且在 RDDs 上运行任意的 Spark 操作。

与流式系统和批处理系统拥有各自单独 API 的系统相比，共享同一份代码库可以节省许多开发时间。
同时在流系统中交互式查询状态的能力则更加吸引人；它使得调试一个运行程序，或者在聚类操作的流式作业中查询未定义状态变得更加容易。
如果没有这种特性，用户通常需要等待数十分钟来将数据导入到集群里，即使流式系统处理节点的内存具有所有相关的状态信息。

##### 4.3.6 总结

|方面|D-Streams|持续处理系统|
|:---:|:---:|:---:|
|延时|0.5-2s|1-100ms 除非对记录进行批处理以保持一致性|
|一致性|记录以到达的时间间隔进行原子处理|有些系统可以等待短暂的时间再继续执行同步操作| 
|记录延迟|宽松的时间延迟或者应用程序级别的校正|宽松的时间延迟但是无序的进行处理|
|故障恢复|快速并行恢复|在单节点上复制或串行恢复|
|慢任务恢复|推测执行的可能|通常情况下没有处理|
|批处理混合操作|通过简单统一的 RDD API|在某些数据库中存在，但在消息队列系统中不存在|

D-Streams 将任务划分成小的且确定性的批量操作。
这会导致最小的延迟时间变长，但是可以让系统采取更高效的可恢复技术。
这导致了延迟时间由过去的毫秒级变为 D-Streams 里面的秒级。

#### 4.4 系统架构

我们已经在 Spark Streaming 上实现了 D-Stream，它是基于 Spark 处理引擎的一个修改版本。
Spark Streaming 由三部分组成，如图 4.6 所示

![图 4.6：Spark Streaming 的组件，并与 Spark 原始内容进行对比](https://i.loli.net/2021/06/28/PxSs4hvABEi1Wty.png)

- master 跟踪 D-Stream lineage，并调度任务来计算新的 RDD 分区。
- 工作节点接收数据，保存输入分区和已计算的 RDD，并执行任务。
- 客户端库将数据发送到系统中

如图中所示，Spark Streaming 重用了 Spark 的许多组件，但仍然需要修改和添加多个组件来支持流处理。
这些变化将在第 4.4.2 节讨论。

从架构角度来看，Spark Streaming 和传统的流系统之间区别在于，Spark Streaming 将计算过程分解为小的、无状态的、确定的任务。
每个任务都可以在集群中的任何节点或同时在多个节点运行。
在传统系统的固定拓扑结构中，将部分计算过程转移到另一台机器是一个很大的动作。
Spark Streaming 的做法，可以非常直接地在集群上进行负载均衡，应对故障或启动慢节点恢复。
同理也能用于批处理系统——如 MapReduce。
然而，由于 RDD 运行于内存中，Spark Streaming 的任务执行时间会短得多，一般只有 50-200 毫秒。

不同于以前系统将状态存储在长时间运行的处理过程中，Spark Streaming 中的所有状态都以容错数据结构(RDD)来保存。
由于 RDD 分区被确定性地计算出来，它可以驻留在任何节点上，甚至可以在多个节点上进行计算。
这套系统试图最大限度地提高数据局部性，同时这种底层的灵活性使得推测执行和并行恢复成为可能。

这些优势在批处理平台(Spark)上运行时可以很自然地获得。
但依然需要进行显著的修改来支持流处理。
在介绍这些修改之前，先讨论任务执行的更多细节。

##### 4.4.1 应用程序执行

Spark Streaming 的应用从一个或多个输入流开始执行。
系统加载数据流的方式，要么是通过直接从客户端接收记录数据，要么是通过周期性的从外部存储系统中加载数据，如 HDFS，外部的存储系统也可以被日志收集系统所代替。
在前一种方式下，由于 D-Streams 需要输入的数据被可靠地进行存储来重新计算结果，
因此我们需要确保新的数据在向客户端程序发送确认之前，在两个工作节点间复制数据。
如果一个工作节点发生故障，客户端程序向另一个工作节点发送未经确认的数据。

所有的数据在每一个工作节点上被一个块存储进行管理，同时利用主服务器上的跟踪器来让各个节点找到数据块的位置。
由于我们的输入数据块和我们从数据块计算得到的 RDD 的分区是不可变的，因此对块存储的跟踪是相对简单的：
每一个数据块只是简单的给定一个唯一 ID，并所有拥有这个 ID 的节点都能够对其进行操作(例如,多个节点同时计算它)。
块存储将新的数据块存储在内存中，但会以 LRU 策略将这些数据块丢弃，这在后面会进行描述。

为了确定何时开始一个新的时间周期，我们假设各个节点通过 NTP 进行了时钟同步，并且在每一个周期结束时每一个节点都会向主服务器报告它所接收到的数据块 ID。
主服务器之后会启动任务来计算这个周期内的输出 RDDs，不需要其他任何同步。
和其他的批处理调度器一样，一旦完成上个周期任务，它就简单地开始每个后续任务。

Spark Streaming 依赖于每一个时间间隔内 Spark 现有的批处理调度器，并加入了像 DryadLINQ 系统中的大量优化：

- 它对一个单独任务中的多个操作进行了管道式执行，如一个 map 操作后紧跟着另一个 map 操作。
- 它根据数据的本地性对各个任务进行调度。
- 它对 RDD 的各个划分进行了控制，以避免在网络中数据的 shuffle。
  例如，在一个 reduceByWindow 的操作中，每一个周期内的任务需要从当前的周期内“增加”新的部分结果(例如，每一个页面的点击数)，
  和“删除”多个周期以前的结果。
  调度器使用相同的方式对不同周期内的状态 RDD 进行切分，以使在同一个节点的每一个 key 的数据(例如,一个页面) 在各时间分片间保持一致。
  更多的细节见 2.5.1 节。

##### 4.4.2 流处理优化

尽管 Spark Streaming 建立在 Spark 之上，我们仍然必须优化这个批处理引擎以使其支持流处理。
这些优化包括以下几个方面：

- 网络通信：我们重写了 Spark 的数据层，通过使用异步 I/O 使得带有远程输入的任务，比如说 reduce 任务，能够更快地获取它们。
- 时间间隔流水线化：因为每一个时间间隔内的任务都可能没有充分地使用集群的资源，
  所以，我们修改了 Spark 的调度器，使它允许在当前的时间间隔还没有结束的时候调用下一个时间间隔的任务。
- 任务调度：我们对 Spark 的任务调度器做了大量的优化，比如说手工调整控制消息的大小，使得每隔几百毫秒就可以启动上百个任务的并行作业。
- 存储层：为了支持 RDDs 的异步检查点和性能提升，我们重写了 Spark 的存储层。
  因为 RDDs 是不可变的，所以可以在不阻塞计算和减慢作业的情况下通过网络对 RDDs 设置检查点。
  在可能的情况下，新的数据层还会使用零拷贝特性。
- lineage 截断：因为在 D-Streams 中 RDDs 之间的 lineage 可以无限增长，
  我们修改了调度器使之在一个 RDD 被设置检查点之后删除自己的 lineage，修改之后 RDDs 之间的 lineage 不能任意生长。
  类似地，对于 Spark 中的其他无限增长的数据结构来说，将会定期调用一个清理进程来清理它们。
- Master 的恢复：因为流应用需要不间断运行，我们给 Spark 加入对 master 状态恢复的支持(见 4.5.3 节)。

##### 4.4.3 内存管理

在我们当前的 Spark Streaming 实现中，每个结点的块存储管理 RDD 的分片是以 LRU(最近最少使用)的方式，如果内存不够会依 LRU 算法将数据调换到磁盘。
另外，用户可以设置最大的超时时间，当达到这个时间之后系统会直接将旧的数据块丢弃而不进行磁盘 I/O 操作(这个超时时间必须大于检查点间隔的时间)。
我们发现在很多应用中，Spark Streaming 需要的内存并不是很多，这是因为一个计算中的状态通常比输入数据少很多(很多应用是计算聚合统计)，
并且任何可靠的流式处理系统都需要像我们这样通过网络来复制数据到多个结点。
但是，我们还是会计划探索优化内存使用的方式。

#### 4.5 故障和慢节点恢复

D-Streams 的确定性使得可以使用两种有效却不适合常规流式系统的恢复技术来恢复工作
节点状态：并行恢复和推测执行。此外，它也简化了主节点的恢复。

##### 4.5.1 并行恢复

当一个节点失败，D-Streams 允许节点上 RDD 分片的状态以及运行中的所有任务能够在其它节点并行地重新计算。
通过异步地复制 RDD 状态到其它的工作节点，系统可以周期性地设置 RDDs 状态的检查点。
例如，在运行时统计页面浏览数的程序中，系统可能对于该计算每分钟选择一个检查点。
然后，如果一个节点失败了，系统会检查所有丢失的 RDD 分片，然后启动一个任务从上次的检查点开始重新计算。
多个任务可以同时启动去计算不同的 RDD 分片，使得整个集群参与恢复。
如 4.3 节所述，D-Stream 在每个时间片中并行地计算 RDDs 的分区以及并行处理每个时间片中相互独立的操作(例如开始的map操作)，
因为可以从 lineage 中细粒度地获得依赖关系。

##### 4.5.2 减缓慢节点

除了节点故障，在大型集群中另一个值得关注的问题是运行较慢的节点。
幸运的是, D-Streams 同样也可以让我们像批处理系统那样减少较慢节点的影响，这是通过推测性(speculative)地运行较慢任务的备份副本实现的。
这种推测执行在连续的处理系统中可能很难实现，因为它需要启动一个结点的新副本，填充新副本的状态，并追赶上较慢的副本。
事实上, 流式处理中的复制算法, 比如 Flux 和 DPC，主要在研究两个副本之间的同步。

在我们的实现中，我们使用了一个简单的阈值来检测较慢的节点：如果一个任务的运行时长比它所处的工作阶段中的平均值高 1.4 倍以上，那么我们标记它为慢节点。
我们将来也可能会采用更精细的算法，但是我们看到目前的方法仍然工作的很好，它能够在 1 秒内从较慢节点中恢复过来。

##### 4.5.3 Master 恢复

持续不断运行 Spark Streaming 的一个最终要求是能够容忍 Spark master 的故障。
我们通过两个步骤来做到这些，第一步是当开始每个时序时可靠地记录计算的状态，第二步是当旧的 master 失败时，
让计算节点连接到一个新的 master 并且报告他们的 RDD 分区。
D-Streams 简化恢复的一个关键方面是如果一个给定的 RDD 被计算两次是没有问题的。
因为操作是确定的，这一结果与从故障中进行恢复类似。
因为任务可以重新计算，这意味着当 master 重新连接时丢掉一些运行中的任务也是可以的。

我们目前的实现方式是将 D-Stream 元数据存储在 HDFS 中，

1. 用户 D-Streams 的图以及表明用户代码的 Scala 的函数对象
2. 最后的检查点的时间
3. 自检查点开始的 RDD 的 ID 号，其中检查点通过在每个时序进行重命名(原子操作)来更新 HDFS 文件。

恢复后，新 master 会读取这个文件找到它断开的地方，并重新连接到计算节点，以便确定哪些 RDD 分区是在内存中。
然后再继续处理每一个漏掉的时序。
虽然我们还没有优化恢复处理，但它是相当快了，100 个节点的集群可以在 12 秒内恢复。

#### 4.6 评估 

略

#### 4.7 讨论 

虽然我们给出了 D-Streams 的一个基本实现，但是未来还有几个方面需要完善。

- 表现力：一般来说，既然 D-Streams 主要是一个执行策略，通过简单地把算法的执行划分成批处理步骤然后在这些步骤中间发送状态，就应该能够运行大多数的流数据算法。
  在 D-Streams 上调用流 SQL 和复杂事件处理模型将会是一件很有意思的事情。
- 设置批处理间隔：给定任何一个应用，设置一个合适的批次间隔是非常重要的，因为这个批处理间隔直接决定了端对端的延迟与整个流负载吞吐量之间的权衡。
  目前来说，开发人员必须自己探索这个权衡并且手动确定批处理间隔。
  未来有可能实现系统自动调整。
- 内存使用：每处理完一批数据，我们的状态流处理模型就会生成一个新的 RDD 来存放每个运算的状态。
  在我们目前的实现中，相比于使用可变状态的连续运算，它会使用更多的内存。
  为了让系统能够执行基于 lineage 的错误恢复，必须要存储状态 RDDs 的不同版本。
  但是，可以通过存储不同状态 RDDs 之间的变化量来达到减少内存使用的目的。
- 检查点和容错策略：因为检查点的成本很高，所以选择一个合适的频率对每一个 D-Stream 自动设置检查点是很有价值的。
  另外，除了检查点之外，D-Streams 还允许使用大量的其他容错策略，比如说计算的部分复制，在部分复制中，任务的一个子集被复制
  (例如， 我们在 4.6.2 节中复制的 reduce 任务)。自
  动应用这些策略也是一件有趣的事。
- 近似的结果：除了重新计算丢失的工作，另外一种处理失败的方式就是返回近似的部分结果。
  通过在父节点全部结束之前启动一个任务，并且提供用来推断哪些父节点丢失了的 lineage 数据，D-Streams 提供了一个计算部分结果的机会。

#### 4.8 相关工作 

略

#### 4.9 总结

略

### 5 RDD 的通用性

#### 5.1 简介

首先，从表述的观点看，RDDs 可以模拟任何分布式系统，并且在大多数情况下这样做都是高效的，除非系统对网络延迟非常敏感。
特别是，增加了数据共享的 MapReduce 使得这个模拟更高效。
第二，从系统的角度来看，在集群环境中 RDDs 能给应用程序对常见的资源瓶颈加以控制(特别是网络和存储 I/O)，
这使得应用程序能够表达那些特定系统所具有的资源优化，并因此达到相似的性能。
最后，RDDs 通用性的探索也决定了模型的一些局限性，即它可能不能有效地模仿其他分布式系统并导致一些扩展性方面的问题。

#### 5.2 观点描述

为了从理论角度描述 RDDs，我们首先拿 RDDs 和从其派生和借鉴所得的 MapReduce 模型进行比较。

##### 5.2.1 MapReduce 所能涵盖的计算范围

MapReduce 提供了 Map 操作用来执行本地计算和 Reduce 操作用来所有节点间相互通信。
这样，通过将计算拆分成多个时间步长的方式，任何分布式系统都能够被模拟(或许有点低效率)，
通过运行 Map 任务来执行每个时间步长上的本地计算任务，并在每个时间步长的最后进行消息的打包和交换。
一系列的 MapReduce 步骤足以获得整个结果。

![图 5.1：使用 MapReduce 模拟任意分布式系统。](https://i.loli.net/2021/06/28/DpLyvSWt6qZN9Vw.png)

> 注：MapReduce 为本地计算和 all-to-all 通信提供原语。
> (a) 通过将这些步骤链接在一起，我们可以模拟任何分布式系统。
> (b) 这种模拟的主要成本将是轮次的延迟和跨步骤传递状态的开销。

两方面因素导致了这种模拟的效率低下。
第一，就如我们在论文的其他部分讨论的那样，MapReduce 在时间步长间的 共享数据的方式是低效的，因为这种共享是基于可复制的外部存储系统。
因此，由于每个时间步长都要输出自己的状态，我们模拟的分布式系统可能变得比较缓慢。
第二，MapReduce 步骤的延迟决定了我们的模拟如何匹配一个真实的网络，并且大多数 MapReduce 的实现是为耗时几分钟到几小时的批量环境设计的。

RDD 架构和 Spark 系统解决了这两方面的限制。在数据共享方面，RDDs 的架构是通过避免复制的方式，使得数据快速共享。
并且能够比较贴切地模拟跨越时间的 “内存数据共享”，这是由多个长驻进程组成的分布式系统所实现的。
在延迟方面上，Spark 展示了在 100 多个节点组成的商业集群中执行 MapReduce 计算任务，有 100ms 的延迟——没有固有的 MapReduce 模型能够避免这种情况。
然而，一些应用程序可能需要更细粒度的时间步长和通信，这样 100ms 的延迟已经足够实现许多数据密集型的计算，而在通信密集的情况下，大量的计算可以被批量执行。

##### 5.2.2 lineage 和故障恢复

上面基于 RDD 模拟的一个有趣特性是它也提供了故障容错。
特别的，每个步骤的 RDD 计算仅仅比前面的步骤多一个常数大小的继承结构，这意味着存储 lineage 和执行故障恢复的代价很小。

每个步骤的本地消息处理在“map”函数中循环进行(它的旧状态作为输入，新状态作为输出)，然后在时间步长间使用“reduce”函数来进行消息的交换。
只要将每个过程中的程序计数存储在它的状态里，这些 map 和 reduce 函数在每个时间步骤中是一样的：
它们仅仅读取程序的计数，接受消息和状态，然后模拟执行过程。
因此它们能够在常量空间内编码。由于在状态中增加了程序计数，这可能引起一些开销，这通常只是状态的一小部分，并且这些状态只是在节点本地共享。

默认情况下，上面模拟过程的 lineage 可能使得状态恢复代价很高，这是因为每一步增加了一个新的所有节点相互间的 shuffle 依赖。
但是，如果应用程序将计算语义表达的更精确(比如 ：说明某些步骤仅仅产生窄依赖)，或者将每个节点的工作切分到多个任务，我们就可以在集群间来并行恢复。

基于 RDD 模拟分布式系统的最后一个问题是在本地维护多个版本状态的代价，这些状态会被转换操作使用，以及为了便于故障恢复而维护对外发送消息的拷贝。
这个代价不小，但是在很多应用中，我们可以通过一段时间执行异步的状态检查点
(比如, 如果检查点的可用带宽比内存带宽低 10 倍, 我们可以在每 10 个步骤执行检查点)或者通过保存多个版本的差异(如 3.2.3 节所述)来限制它。
只要每台机器上的“快速”存储足够储存对外发送的消息以及一些版本的状态，我们就可以达到原来系统的性能。

##### 5.2.3 与 BSP 的比较

作为关于 MapReduce 与 RDDs 的通用性的第二个例子，我们注意到上述“本地计算和所有结点相互间通讯”模式与 Valian 的批量同步并行模型(BSP)非常吻合。
BSP 是一个桥接模型，旨在捕捉真实的硬件上简单却最显着的特性(即通信具有延迟且同步是昂贵的)并对其进行简单的数据分析。
因此，它不仅被直接用于设计一些并行算法，而且其成本(即通信的步骤数，每一步中的本地计算量以及每一步骤中各处理器之间通信的数据量)也是大多数并行
应用中用来优化的自然因素。
因此，我们可以预期与 BSP 吻合的算法都可以用 RDDs 进行有效的评估。

请注意，这个 RDDs 的仿真参数因此也可应用于基于 BSP 的分布式运行时，如 Pregel。
RDD 相比与 Pregel 增加了两个好处。
首先，Google 论文中描述的 Pregel 只支持‘检查点回滚’的系统错误恢复机制。
随着系统规模的扩大，这使得系统扩展的效率降低并且节点失效会变得越来越频繁。
该论文中确实介绍了一种还在开发中的‘限制性恢复’模式，它记录下传出的信息并且并行地恢复丢失的系统状态。
这与 RDDs 的并行恢复机制类似。第二，因为 RDDs 有一个基于遍历器的接口，它们可以更有效地对不同类库编写的计算流水线化，这对于编写程序是非常有用的。
更一般地，从编程接口的角度来看，我们发现 RDD 允许用户使用更高层次的抽象(例如，将状态分割为多个分区数据集或允许用户建立可窄可宽的的依赖模式而不需要在
每一步都进行所有结点相互间的通信)，同时还提供一个简单通用的接口使数据可以按上述讨论进行共享。

#### 5.3 系统角度

完全不同于仿真的方法来表征 RDD 的特性，我们可以采取一种系统方法：在大多数集群计算中资源的瓶颈是什么，能否用 RDD 来有效的解决这些问题？
从这个角度来看，大多数集群应用最明显的瓶颈是通信和存储。
RDD 的分区和本地特性使得应用有足够的控制力来对这些资源进行优化，从而使得在许多应用中达到类似的性能。

##### 5.3.1 瓶颈资源 

虽然集群应用是多种多样的，但是它们都受到相同的底层硬件的限制。
目前的数据中心有一个非常不合理的存储层次结构，这将会因相同的原因限制大多数应用。
例如，现在一个典型的数据中心可能有以下硬件特性：

- 每个节点的本地内存大约有 50 GB/s 的内存带宽以及多个磁盘(通常在 Hadoop 集群中为 12-24 个)。
  也就是说，假设有 20 个磁盘，每个磁盘带宽 100 MB/s，那么将意味着本地存储带宽约为 2 GB/s。
- 每个节点都有一个 10 Gbps (1.3 GB/s) 的网络输出带宽，大约比内存带宽小 40 倍，比它的磁盘总带宽小 2 倍。
- 20-40 台机器节点组成机架，机架间的带宽为 20-40 Gbps，这比机架内部的网络性能要低 10 倍。

鉴于这些特性，许多应用所关心的最重要的性能指标就是控制网络布局和通信。
幸运的是，RDDs 提供了这样的条件：其接口在运行时将计算调度在离数据最近的节点，就像 MapReduce 的 Map 任务
(其实在 2.4 章节的定义中，RRDs 有一个“优先位置”的 API)，并且 RDDs 还提供了数据分区和共存。
不像 MapReduce 中的数据共享，总是隐式地需要经过网络传输。
而 RRDs 是不会造成网络流量的，除非用户明确调用了一个跨节点操作，或是对数据集设置检查点。

从这个角度看，如果大部分的应用都有网络带宽限制，那么一个节点(例如,数据结构或 CPU 开销)的本地效率的影响将小于网络通信的效率。
以我们的经验，很多 Spark 应用都有带宽限制的，特别是如果数据可以放进内存中。
当数据无法放进内存中时，应用受 I/O 限制，并且数据本地性将是最重要的一个因素。
CPU 密集型应用通常更容易执行，(例如,许多应用在MapReduce 上也都做得很好)。
就像在上一章节的讨论中，RDDs 明显增加成本的地方就是网络延迟，但是 Spark 的工作表明，这种延迟对很多应用来说可能会足够小，甚至小到足够支持数据流。

##### 5.3.2 容错的开销

最后从系统的角度要说明的一点是，由于其容错性，基于 RDD 的系统产生了一些额外的开销。
例如，在 Spark 中，每个 shuffle 操作中的“map”任务将他们的输出保存到本地文件系统中，所以之后 "reduce" 任务可以重复获取。
另外，Spark(就像原生的MapReduce)在 shuffle 阶段执行了一个 "barrier"，所以 "reduce" 任务不会启动，直到所有的map任务完成。
相比于直接从 map 任务以管道的方式直接推送到 reduce 任务，这简化了容错的复杂性。

尽管移除一些低效率之处会加快系统运行速度，并且我们也打算在以后的工作中这样做，但是即便如此 Spark 依旧性能突出。
最主要的原因是前一节中的一个说法：许多应用程序都受 I/O 所限制，
例如，通过网络传输大量数据，或从磁盘中读取数据，除此之外，如流水线技术仅增加了一个少量的改进。
例如，可以考虑从 map 任务直接推送数据到 reduce 任务，而不是等待所有的 map 任务执行完再调度 reduce 任务：
最理想情况下，如果 map 任务的 CPU 计算时间刚好与网络传输时间重叠，那么这将使速度加快 2 倍。
当然，这是一个非常有用的优化，但不像将 map 任务调度到数据所在节点或者避免中间状态的复制那么重要。
此外，如果运行过程中其他部分占主要开销 (例如,map 任务花费很长时间从磁盘中读取，或 shuffle 过程比计算时间慢很多)，那么效益就会降低。

最后我们注意到，即使故障并 不经常发生，Spark 和类 MapReduce 的设计将任务划分成细粒度的独立的任务会有其他的好处。
首先，它可以缓解慢节点(straggler)问题，这在传统的基于推送的数据流设计中显得异常复杂。
(类似于我们第 4 章中比较 D-streams 和连续操作)。
甚至在较小的集群上，慢节点问题也比故障更常见，尤其是在虚拟化环境。
其次，独立的任务模式有利于多用户管理：来自不同用户的多个应用程序可以动态共享资源，从而实现多用户的交互执行。
我们大部分并行编程模式的工作，已经使动态资源在集群用户之间进行共享，大型群集必然有很多用户，并且在这些集群中所有的应用都需要实现快速定位和数据本地化
基于这些原因，一个细粒度的任务设计可能会让大多数用户在多用户环境中有更好的性能体验，即使单一应用的性能还比较差。

鉴于这些容错成本和其他独立任务模型的优点，我们认为，集群系统设计者应该考虑容错性和弹性因素，即使仅仅是针对短期工作。
提供这些特性的系统将会更容易得扩展到大规模查询以及多用户环境。

#### 5.4 限制与扩展

虽然先前的章节讲述了 RDD 能够有效模拟分布式系统，但它也有无法做到的情况。
现在，我们研究一些关键限制，并讨论几种可能绕过它们的模型扩展。

##### 5.4.1 延迟

正如前面几章说明的那样，基于 RDD 分布式系统的仿真与实际系统之间差距的主要系能指标就是延迟。
因为 RDD 操作在整个集群中是确定且同步的，又由于启动每个"时步(timestep)"计算存在固有的延迟，所以大量的时步计算导致系统更慢了。

这类应用有两种设计方式，低延迟流式系统(如毫秒级请求)和细粒度时间片模拟(如，科学模型)。
而我们发现，在实践中 RDD 操作可以低至 100ms 的延迟，这对一些应用程序还不够。
从“人”的时间尺度来看，延迟已经足够低来跟踪事件(如 Web 点击率和社交媒体的趋势)，并且符合互联网大范围的延迟。
在仿真应用中，对抖动容忍的最新研究工作非常适用于 RDD。
这项研究工作在每台机器的本地网络区域上模拟多个时步。
在大多数模拟中，信息需要花费几个时间片在整个网络中转移，并需要更多的时间来与其他节点进行同步。
它被专门用来处理慢任务倾向(straggler-prone)的云环境。

##### 5.4.2 通信模式

5.2.1 章节表明 MapReduce 和 RDD 可以仿真一个分布式系统。
当使用 reduce 操作时，系统的节点之间通过点对点通信，虽然这是一个常见的场景，但实际网络也有其他有效的原语，如广播或者网络内聚合。
一些数据密集型应用能够从这些中显著受益(如：在机器学习算法将当前模型广播出去，或者收集反馈结果)这些原语仅仅通过点对点的消息传输是非常难以效仿的，
因此在 RDDs 之上直接支持这些原语是有帮助的。
比如，spark 已经包含了一个有效的操作，“广播”，它通过 BitTorrent 来实现。

##### 5.4.3 异步

RDD 操作，例如多对多的 reduce 操作，都是通过同步来提供确定性。
当节点间的工作不均衡或者某些节点是慢节点时，这可能会减缓计算速度。
最近，一些集群计算系统提出了让节点异步发送消息，这使得即使存在慢节点，计算块也可以继续。
尽管仍然保留故障恢复，一个类似 RDD 的模型是否能够支持这样的操作，这个话题值得探讨。
例如，节点可能可靠地记录每次迭代计算的信息 ID，这可能仍然比记录信息更节省。
请注意，对一般的计算，从丢失故障中重建不同的状态通常没有用，因为在丢失节点上后续计算可能已经使用了丢失状态。
一些算法，例如统计优化，能够从没有完全丢失所有过程的损坏状态继续执行。
这种情况下，一个基于 RDD 的系统可以在一个特殊模式下运行这些算法，它不执行恢复，但对结果执行检查点，
允许在其上进行后续计算(例如, 交互查询)以得到一个一致的结果。

##### 5.4.4 细粒度更新

由于记录每个操作的 lineage 代价很高，用户对 RDDs 进行许多细粒度更新时是不高效的。
例如，Spark 并不适合实现一个分布式的 key-value 存储系统。
在某些情况下，我们或许可以将多个操作聚合起来一次处理以及将多个更新操作合为一个粗粒度操作。
就如同在 D-streams 中的“离散化”流处理。
当然在目前的 Spark 系统中，对于一个 key-value 存储系统来说还算不上低延迟，如果能在一个低延迟的系统中实现这个模型还是很有趣的。
这个方法类似于 Calvin 分布式数据库，通过批处理事务以及确定性执行来获得更好的扩展性和可靠性。

##### 5.4.5 不变形和版本追踪

正如本章之前讨论的，不变性可能会增加开销，因为更多的数据需要在基于 RDD 的系统中进行复制。
RDDS 被设计为不可变的主要原因是为了跟踪不同版本的数据集的依赖关系，并恢复依赖于旧版本数据集的状态。
但是，在任务执行时仍可通过别的途径来跟踪这些依赖关系，以及使用基于 RDD 抽象的可变状态。
当处理流数据或者细粒度更新时，这将会是个很有趣的补充。
正如 3.2.3 节所讨论的，用户可以使用其他的方法来手动把状态分割为多个 RDD，或是借助存储的(持久化)数据来对旧数据进行重用。

#### 5.5 相关工作

略

#### 5.6 小结

略

### 6 总结

略

#### 6.1 经验总结

略

#### 6.2 更深远的影响

略

#### 6.3 未来的工作

如 5.4 节所述，我们对 RDDs 的分析也表现出了模型的局限性，这是我们未来研究以进一步泛化该模型的兴趣点。
这里重述下这部分内容，未来扩展的主要领域包括：

- 通信延迟：基于 RDD 模拟任意分布式系统的一个主要缺点是，它需要额外的延迟去同步每一个步骤以使得计算是确定性的。
  未来在这方面有两个有趣的方向。
  第一个是系统方面的挑战，我们要研究一个基于内存集群计算系统其延迟时间能够达到的水平-新的数据中心网络可能达到微秒级的延迟，
  另外，对于一个优化好的代码库，每一步的延迟可能只需要几毫秒。
  (在当前的基于 JVM 的 Spark 系统中，java 程序运行时间会使得延迟更高)第二个工作是在 RDDs 中使用延迟隐藏技术用以执行需要紧密同步的应用，
  它是通过将工作划分为不同的区块或是推测其它机器的响应时间来实现的。
- 新的通信模式：RDDs 目前只在通信节点之间提供了点对点的 shuffle 模式，但也有些并行计算采用其他的通信模式会带来更好的结果，
  比如广播或者多对一的聚和。
  研究发掘这些模式也许能提高应用程序的性能，以及创造新的运行时优化和故障恢复方法的机会。
- 异步：虽然基于 RDD 的计算是同步和确定性的，但它可能也适用于在模型内执行异步计算步骤，同时在这些步骤之间提供故障恢复保障。
- 细粒度的更新：RDDs 擅长于粗粒度和数据并行的操作，但是，如第 5.4.4 节所述，使用细粒度的操作来模拟这样一个系统也是可行的，
  如读写一个键值对数据时，通过将这些操作分组来批量执行。
  特别是在更低的延迟运行时间下，可能非常有趣的是这种方法对比传统数据库设计究竟能有多快，以及通过运行事务和分析工作共存的情况下能带来什么好处。
- 版本跟踪：RDDs 定义为不可变的数据集,以允许依赖关系执行具体的版本，但是在这个抽象框架中通过使用可变的存储和更高效的版本追踪方法还有很大的提升空间。

除了以上这些方面来扩展 RDD 的编程模型外，我们在 Spark 方面的经验还指出了用户面临的实际系统相关的问题，这些可能也是未来研究工作的兴趣点。
一些重要的领域有：

- 正确性调试：分布式应用的调试和正确性测试是复杂的，尤其是操作大量的，没有对比的数据集。
  在 Spark 中我们已经探索过的一个想法是使用 RDDs 的依赖关系信息高效地重现调试应用程序的一部分
  (例如，异常引起任务的崩溃，或产生某个特定输出的执行图的一部分)。
  该工具还可以在第二次运行时修改 lineage 图，比如，在用户的函数中添加日志记录或增加错误跟踪记录。
- 性能调试：在 Spark 的邮件列表中，最常咨询的调试问题是关于性能的而不是程序正确性的。
  分布式应用程序的调优是非常困难的，一部分原因是用户对于什么是好的性能缺少直觉。
  如果一个 PageRank 的实现在有 5 个节点的集群上，处理 100 GB 的数据需要 30 分钟，这个性能好吗？
  这个应用能够只使用 2 分钟，或 10 秒吗？
  诸多因素如通信成本，各种数据表示的存储开销和数据倾斜等，都可能明显的影响并行应用程序的性能。
  开发 一些可以自动检测这些低效率因素的工具，或者甚至是能够给用户提供足够的关于应用程序性能信息以辨别问题的监控工具，都是有趣且具有挑战性的工作。
- 调度策略：虽然 RDD 模型非常灵活的支持运行时细粒度任务的调度，并且 RDD 已被用于 实现了一些调度机制，如公平共享调度，
  但是在用这种模型编写的应用中找到一个正确的调度策略仍然是一个有挑战的问题。
  例如，在 Spark 的流式应用中，在有多个数据流的情况下，我们该如何调度计算以满足任务的执行期限或优先级？
  如果同样的应用同时在数据流上执行交互式查询呢？
  同样的，给定一个 RDDs 或 D-Streams 的图结构，我们能够自动确定检查点以减少预期的执行时间吗？
  随着应用变的越来越复杂以及用户要求更多的响应接口，这些策略对于维持良好的性能是很重要的。
- 内存管理：在大多数集群中分配有限的内存是一个有趣的挑战，同时也取决于应用程序定义的优先级和使用模式。
  问题之所以特别有趣，是因为有不同的“层次”的存储，需要权衡内存大小与访问速度。
  例如，在内存中的数据能够被压缩，这可能使它需要更大的计算开销，但所需的内存更少；或者数据也可以被换出到 SSD 中，或者是磁盘上。
  有一些 RDDs 可能没有任何持久化会更高效，它是通过在运行过程中对前一个 RDD 运行 map 函数来重新计算
  (对于大多数用户而言计算可能足够快了，这对于节省空间是值得的)。
  一些 RDDs 的计算可能要很大开销，因此 RDDs 一直需要被复制。
  特别的，因为 RDDs 总是能够可以从头开始计算丢失的数据，所以为存储管理策略留下了很大的优化空间。

我们希望在开源系统 Spark 上的持续经验将有助于我们应对这些挑战，并设计出适用于 Spark 和其他集群计算系统的解决方案。

### 参考文献

略